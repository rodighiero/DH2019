[{
    "id": "oai:dspace.mit.edu:1721.1/32741",
    "title": "Ca2+ dependant synaptic modification",
    "abstract": "It has been assumed that Ca2+ influx of different duration and amplitude would generate different level of potentiation. The conventional protocols of generating LTP have been 1. tetanic stimulation of presynaptic cell, 2. theta burst stimulation of presynaptic cell, and 3. correlated stimulation of pre- and post-synaptic cells. However, the effects of different Ca2+ influx can not be precisely dissected with the conventional protocols for the following defects: 1. the protocols do not discriminate between pre- and post-synaptic side plasticity, 2. the protocols observe synaptic plasticity between two cells which involve multiple synapses with heterogeneous properties, 3. precise control and measurement of the amount of Ca2+ influx are not possible in the protocols. In the present experiment, we perfused glutamate directly on to a single postsynaptic site, depolarized the postsynaptic intracellular potential to a controlled voltage for a controlled duration of time, thus controlling the opening of postsynaptic NMDA receptors and Ca2+ influx. By using this method, we found 1. that modification of synaptic strength has a bell-shaped dependency to the amount of Ca2+ influx, 2. that weak Ca2+ current through desensitized NMDA receptors sustained for a long period of time (160 ms) generates LTD, 3. evidence that phosphorylation of AMPAR leads to insertion of AMPAR.",
    "advisors": ["Guosong Liu"],
    "text": "Ca2+ dependant synaptic modification It has been assumed that Ca2+ influx of different duration and amplitude would generate different level of potentiation. The conventional protocols of generating LTP have been 1. tetanic stimulation of presynaptic cell, 2. theta burst stimulation of presynaptic cell, and 3. correlated stimulation of pre- and post-synaptic cells. However, the effects of different Ca2+ influx can not be precisely dissected with the conventional protocols for the following defects: 1. the protocols do not discriminate between pre- and post-synaptic side plasticity, 2. the protocols observe synaptic plasticity between two cells which involve multiple synapses with heterogeneous properties, 3. precise control and measurement of the amount of Ca2+ influx are not possible in the protocols. In the present experiment, we perfused glutamate directly on to a single postsynaptic site, depolarized the postsynaptic intracellular potential to a controlled voltage for a controlled duration of time, thus controlling the opening of postsynaptic NMDA receptors and Ca2+ influx. By using this method, we found 1. that modification of synaptic strength has a bell-shaped dependency to the amount of Ca2+ influx, 2. that weak Ca2+ current through desensitized NMDA receptors sustained for a long period of time (160 ms) generates LTD, 3. evidence that phosphorylation of AMPAR leads to insertion of AMPAR."
}, {
    "id": "oai:dspace.mit.edu:1721.1/98599",
    "title": "Defining \"good science\" in today's World : a video compilation of perspectives and advice for incoming graduate students",
    "abstract": "Although graduate science education does an excellent job training students in the technical and career aspects of science, there is too little attention paid to teaching the wisdom of \"good\" science that encourages riskier path-breaking work over fluff, with the highest goal of research being discovery rather than scholarly publication. In an attempt to help fill this gap, I interviewed fifteen senior life scientists over the past year. These interviews were filmed and edited into four topic videos: The Allure of Science, I-ow to Do Good Science, On Mentorship, and Where Science Is Headed. Geared towards graduate students in the life sciences, these videos are designed to start a conversation between students and their advisors on important but currently ignored aspects of doing good science.",
    "advisors": ["Rosalind Williams"],
    "text": "Defining \"good science\" in today's World : a video compilation of perspectives and advice for incoming graduate students Although graduate science education does an excellent job training students in the technical and career aspects of science, there is too little attention paid to teaching the wisdom of \"good\" science that encourages riskier path-breaking work over fluff, with the highest goal of research being discovery rather than scholarly publication. In an attempt to help fill this gap, I interviewed fifteen senior life scientists over the past year. These interviews were filmed and edited into four topic videos: The Allure of Science, I-ow to Do Good Science, On Mentorship, and Where Science Is Headed. Geared towards graduate students in the life sciences, these videos are designed to start a conversation between students and their advisors on important but currently ignored aspects of doing good science."
}, {
    "id": "oai:dspace.mit.edu:1721.1/118130",
    "title": "Detecting the inclusion and exclusion of a neuronal XDP-associated microexon in situ",
    "abstract": "X-linked dystonia parkinsonism (XDP), also known as torsion dystonia type 3 (DYT3), afflicts hundreds of individuals. Under an X-linked mode of inheritance, the DYT3 haplotype occurs in Filipino populations and is of the highest frequency in the Panay Islands of the Philippines. Recently, convincing evidence has shown the causative mutation to be an insertion of the repetitive sequence SINE-VNTR-Alus (SVA). This insertion is associated with misregulation of 3' end exons in the gene TBP-associated factor 1 (TAF1). TAF1, the largest of fourteen TAF proteins, incorporates into a TATA binding complex that promotes transcription by RNA polymerase II. In a collaborative effort, singleplex BaseScope\" probes as well as antibodies have been produced to target two TAF1 isoforms, canonical TAF1, C-TAF1, and neuronal TAF1, N-TAF1, separately. N-TAF1 differs from C-TAF1 by the inclusion of a two amino acid microexon, 3' to the SVA insertion, known as 34'. Here, I show that N-TAF1 expression is confined to neurons and interneurons whereas C-TAF1 is widely expressed, particularly by astrocytes, interneurons, neurons, and cells present in other organs including the heart and liver in mouse. Additionally, the antibodies produced show promise for use in human tissue. These results support the hypothesis that C-TAF1 and N-TAF1 have canonical and neuron-specific functions, respectively, and misregulation of N-TAF1 is capable of causing neuronal degeneration. Ultimately these results set the foundation for the study of C-TAF1 and N-TAF1 functions and isoform misregulation in XDP diseased tissue. Furthermore, these probes and antibodies may serve as tools for the validation of XDP models, under development, in which forthcoming XDP therapies may be tested.",
    "advisors": ["Ann Graybiel"],
    "text": "Detecting the inclusion and exclusion of a neuronal XDP-associated microexon in situ X-linked dystonia parkinsonism (XDP), also known as torsion dystonia type 3 (DYT3), afflicts hundreds of individuals. Under an X-linked mode of inheritance, the DYT3 haplotype occurs in Filipino populations and is of the highest frequency in the Panay Islands of the Philippines. Recently, convincing evidence has shown the causative mutation to be an insertion of the repetitive sequence SINE-VNTR-Alus (SVA). This insertion is associated with misregulation of 3' end exons in the gene TBP-associated factor 1 (TAF1). TAF1, the largest of fourteen TAF proteins, incorporates into a TATA binding complex that promotes transcription by RNA polymerase II. In a collaborative effort, singleplex BaseScope\" probes as well as antibodies have been produced to target two TAF1 isoforms, canonical TAF1, C-TAF1, and neuronal TAF1, N-TAF1, separately. N-TAF1 differs from C-TAF1 by the inclusion of a two amino acid microexon, 3' to the SVA insertion, known as 34'. Here, I show that N-TAF1 expression is confined to neurons and interneurons whereas C-TAF1 is widely expressed, particularly by astrocytes, interneurons, neurons, and cells present in other organs including the heart and liver in mouse. Additionally, the antibodies produced show promise for use in human tissue. These results support the hypothesis that C-TAF1 and N-TAF1 have canonical and neuron-specific functions, respectively, and misregulation of N-TAF1 is capable of causing neuronal degeneration. Ultimately these results set the foundation for the study of C-TAF1 and N-TAF1 functions and isoform misregulation in XDP diseased tissue. Furthermore, these probes and antibodies may serve as tools for the validation of XDP models, under development, in which forthcoming XDP therapies may be tested."
}, {
    "id": "oai:dspace.mit.edu:1721.1/68968",
    "title": "The Toppler Effect : irregular leader transitions and the rate of state failure recovery",
    "abstract": "State failure is becoming increasingly prevalent across the globe, creating human suffering, black markets, lost economic opportunities, and safe havens for militant actors. It is imperative that the international community find a way to combat state failure. This study investigates the effects of irregular leadership transitions on state failure recovery. Irregular leadership transitions occur when the executive of a state comes to power through unconstitutional means. Regular leaders are more likely than irregular leaders to have personal experience as a ruler, beneficial domestic and international ties, and familiarity among the population. Irregular transitions may damage bureaucracies, damaging government functionality and halting development projects that had already been underway. Regular leaders benefit from a legacy that was likely able to pass spoils onto an elite group. This elite group is likely to resist relative losses to power more than lower status groups would fight to gain power because of the cognitive principles of risk aversion, and the sensitivity to status inherent to social identity theory. Regular leaders also have traditional legitimacy, while irregular leaders are more likely to have to gain legitimacy. State failure and failure recovery are overdetermined, so it is impossible to be able to confidently determine the direction of causal flow. Every determinant of failure is related to every other, and it is difficult to separate their effects. The role of leadership regularity is therefore investigated as a proxy that can predict variation on the rate of failure recovery. The quantitative analysis consisted of multi and bivariate regressions investigating the effects of leadership regularity on failure duration, as well as the relative explanatory power held by several factors associated with leadership regularity. Robustness checks were performed using Bayesian statistics, and survival analyses. Irregular leadership transitions were found to predict a roughly five year increase in state failure duration. The Afghan Civil War was used as an illustrative case, describing the ways in which Daoud, Taraki, Amin, Karmal, Massoud, Hekmatyar, and Mullah Omar all overcame, or failed to overcome, different obstacles associated with their irregularity and how these obstacles affected their relative levels of success attempting to extend governance.",
    "advisors": ["Fotini Christia"],
    "text": "The Toppler Effect : irregular leader transitions and the rate of state failure recovery State failure is becoming increasingly prevalent across the globe, creating human suffering, black markets, lost economic opportunities, and safe havens for militant actors. It is imperative that the international community find a way to combat state failure. This study investigates the effects of irregular leadership transitions on state failure recovery. Irregular leadership transitions occur when the executive of a state comes to power through unconstitutional means. Regular leaders are more likely than irregular leaders to have personal experience as a ruler, beneficial domestic and international ties, and familiarity among the population. Irregular transitions may damage bureaucracies, damaging government functionality and halting development projects that had already been underway. Regular leaders benefit from a legacy that was likely able to pass spoils onto an elite group. This elite group is likely to resist relative losses to power more than lower status groups would fight to gain power because of the cognitive principles of risk aversion, and the sensitivity to status inherent to social identity theory. Regular leaders also have traditional legitimacy, while irregular leaders are more likely to have to gain legitimacy. State failure and failure recovery are overdetermined, so it is impossible to be able to confidently determine the direction of causal flow. Every determinant of failure is related to every other, and it is difficult to separate their effects. The role of leadership regularity is therefore investigated as a proxy that can predict variation on the rate of failure recovery. The quantitative analysis consisted of multi and bivariate regressions investigating the effects of leadership regularity on failure duration, as well as the relative explanatory power held by several factors associated with leadership regularity. Robustness checks were performed using Bayesian statistics, and survival analyses. Irregular leadership transitions were found to predict a roughly five year increase in state failure duration. The Afghan Civil War was used as an illustrative case, describing the ways in which Daoud, Taraki, Amin, Karmal, Massoud, Hekmatyar, and Mullah Omar all overcame, or failed to overcome, different obstacles associated with their irregularity and how these obstacles affected their relative levels of success attempting to extend governance."
}, {
    "id": "oai:dspace.mit.edu:1721.1/42226",
    "title": "Judicious imitation : children differentially imitate deterministically and probabilistically effective actions",
    "abstract": "Three studies look at whether the assumption of causal determinism (the assumption that all else being equal, causes generate effects deterministically) affects children's imitation of modeled actions. We show that, even when the frequency of an effect is matched, both preschoolers and toddlers imitate actions more faithfully when modeled actions are deterministically rather than probabilistically effective. A third study suggests that preschoolers' imitation is affected, not just by whether the agent's goal is satisfied but also by whether the action is a reliable means to the goal. Children's tendency to generate variable responses to probabilistically effective modeled actions could support causal learning.",
    "advisors": ["Lara Schulz"],
    "text": "Judicious imitation : children differentially imitate deterministically and probabilistically effective actions Three studies look at whether the assumption of causal determinism (the assumption that all else being equal, causes generate effects deterministically) affects children's imitation of modeled actions. We show that, even when the frequency of an effect is matched, both preschoolers and toddlers imitate actions more faithfully when modeled actions are deterministically rather than probabilistically effective. A third study suggests that preschoolers' imitation is affected, not just by whether the agent's goal is satisfied but also by whether the action is a reliable means to the goal. Children's tendency to generate variable responses to probabilistically effective modeled actions could support causal learning."
}, {
    "id": "oai:dspace.mit.edu:1721.1/28851",
    "title": "Seeking the neural basis of grammar : English noun and verb morphological processing investigated with rapid event-related fMRI and intracortical electrophysiology",
    "abstract": "(cont.) the functionality of the fMRI data analysis and visualization tools used at Massachusetts General Hospital. I analyze and interpret an 18-subject fMRI experiment I ran using the new task design and software tools. Finally, I present preliminary findings on linguistic questions as well as the nature of fMRI signal, using direct Electrophysiological data recorded from electrodes implanted in the brains of two Epilepsy patients. These patients had electrodes implanted through or near classical language areas of their brains, as a necessary clinical step in locating and surgically removing the seizure-causing tissue. The main findings of this thesis are: 1.) Morphology alone can activate Broca's area, 2.) Other areas are involved, including BA47, anterior insula, and SMA, 3.) Broca's area and BA47 respond to application of abstract grammatical features, even without phonological manipulations, 4.) Morphophonological manipulation additionally recruits insula and SMA, 5.) While simply accessing nouns versus verbs may involve separable brain regions, inflectional processing of the two categories may be done by the same process, 6.) Regularly and Irregularly inflected verbs show a double dissociation of activation in frontal and medial regions, 7.) Processing of English noun more than verb morphology may rely on some contribution from number processing brain systems ...",
    "advisors": ["Steven Pinker"],
    "text": "Seeking the neural basis of grammar : English noun and verb morphological processing investigated with rapid event-related fMRI and intracortical electrophysiology (cont.) the functionality of the fMRI data analysis and visualization tools used at Massachusetts General Hospital. I analyze and interpret an 18-subject fMRI experiment I ran using the new task design and software tools. Finally, I present preliminary findings on linguistic questions as well as the nature of fMRI signal, using direct Electrophysiological data recorded from electrodes implanted in the brains of two Epilepsy patients. These patients had electrodes implanted through or near classical language areas of their brains, as a necessary clinical step in locating and surgically removing the seizure-causing tissue. The main findings of this thesis are: 1.) Morphology alone can activate Broca's area, 2.) Other areas are involved, including BA47, anterior insula, and SMA, 3.) Broca's area and BA47 respond to application of abstract grammatical features, even without phonological manipulations, 4.) Morphophonological manipulation additionally recruits insula and SMA, 5.) While simply accessing nouns versus verbs may involve separable brain regions, inflectional processing of the two categories may be done by the same process, 6.) Regularly and Irregularly inflected verbs show a double dissociation of activation in frontal and medial regions, 7.) Processing of English noun more than verb morphology may rely on some contribution from number processing brain systems ..."
}, {
    "id": "oai:dspace.mit.edu:1721.1/46662",
    "title": "Synaptic plasticity in the MyosinVa mutant mouse",
    "abstract": "The trafficking of essential proteins into spines is an important aspect of synaptic plasticity. MyosinVa, an actin-based motor protein, has been implicated in the synaptic delivery of AMPARs during LTP [1]. However an earlier study showed that LTP and LTD were unaffected in the MyosinVa-null dilute-lethal mice [2]. To evaluate the role of MyosinVa in synaptic plasticity, we studied different forms of LTP and LTD in the CA1 region of the hippocanmpus from MyosinVa dominant negative mutant flailer mouse using field potential recordings. Flailer mice showed no impairment of LTP or NMDAR-dependent LTD, consistent with the findings of the study on dilute-lethal. In addition, MyosinVa has been implicated in the transport of an RNA-binding protein into the spines upon mGluR activation [3]. We explored protein synthesis and mGluR-dcpendent LTD in flailer. The preliminary data we obtained show a transient impairment in mGluR.-LTD, suggesting a role for MyosinVa in protein synthesis dependent plasticity.",
    "advisors": ["Martha Constantine-Paton"],
    "text": "Synaptic plasticity in the MyosinVa mutant mouse The trafficking of essential proteins into spines is an important aspect of synaptic plasticity. MyosinVa, an actin-based motor protein, has been implicated in the synaptic delivery of AMPARs during LTP [1]. However an earlier study showed that LTP and LTD were unaffected in the MyosinVa-null dilute-lethal mice [2]. To evaluate the role of MyosinVa in synaptic plasticity, we studied different forms of LTP and LTD in the CA1 region of the hippocanmpus from MyosinVa dominant negative mutant flailer mouse using field potential recordings. Flailer mice showed no impairment of LTP or NMDAR-dependent LTD, consistent with the findings of the study on dilute-lethal. In addition, MyosinVa has been implicated in the transport of an RNA-binding protein into the spines upon mGluR activation [3]. We explored protein synthesis and mGluR-dcpendent LTD in flailer. The preliminary data we obtained show a transient impairment in mGluR.-LTD, suggesting a role for MyosinVa in protein synthesis dependent plasticity."
}, {
    "id": "oai:dspace.mit.edu:1721.1/28355",
    "title": "Development of and proposed applications for tetrodes in functional mapping of rodent sensorimotor striatum",
    "abstract": "The Wilson-McNaughton tetrode preparation for awake, behaving rodents was adapted by a group of investigators for use in dorsolateral striatum. Measures were taken to improve the reliability of reaching the target area in the brain and the stability of the implanted tetrode drive over several weeks of recording. Novel methods were developed to confirm the dorsoventral level of tetrodes at intermediate stages of advancement during recording, and to reconstruct estimated directions and distances of recorded sources from tetrodes by post-hoc analysis. Alternative methods of source separation and data visualization were implemented. Additional refinements to improve unit separation within and across recording sessions are proposed. The resulting recording technique is expected to have considerable potential in clarifying behavioral and other functional correlates of systems of striatal anatomical compartmentalization. A set of experiments is proposed to investigate how dorsolateral striatal neuronal activity changes in correlation with learning of three stimulus-response tasks relative to three control tasks with similar sensory, motor and motivational aspects but different learning and memory requirements, and to localize task-responsive units with respect to striosomes and body part areas identified by neuronal responses to cutaneous stimulation/passive manipulation and anterograde anatomical tracers from primary motor cortex. Neuronal activity in the globus pallidus and substantia nigra pars reticulata, output areas of the basal ganglia, is also to be examined over the course of acquisition of the three stimulus-response tasks.",
    "advisors": ["Earl Miller"],
    "text": "Development of and proposed applications for tetrodes in functional mapping of rodent sensorimotor striatum The Wilson-McNaughton tetrode preparation for awake, behaving rodents was adapted by a group of investigators for use in dorsolateral striatum. Measures were taken to improve the reliability of reaching the target area in the brain and the stability of the implanted tetrode drive over several weeks of recording. Novel methods were developed to confirm the dorsoventral level of tetrodes at intermediate stages of advancement during recording, and to reconstruct estimated directions and distances of recorded sources from tetrodes by post-hoc analysis. Alternative methods of source separation and data visualization were implemented. Additional refinements to improve unit separation within and across recording sessions are proposed. The resulting recording technique is expected to have considerable potential in clarifying behavioral and other functional correlates of systems of striatal anatomical compartmentalization. A set of experiments is proposed to investigate how dorsolateral striatal neuronal activity changes in correlation with learning of three stimulus-response tasks relative to three control tasks with similar sensory, motor and motivational aspects but different learning and memory requirements, and to localize task-responsive units with respect to striosomes and body part areas identified by neuronal responses to cutaneous stimulation/passive manipulation and anterograde anatomical tracers from primary motor cortex. Neuronal activity in the globus pallidus and substantia nigra pars reticulata, output areas of the basal ganglia, is also to be examined over the course of acquisition of the three stimulus-response tasks."
}, {
    "id": "oai:dspace.mit.edu:1721.1/100874",
    "title": "Benchmarking models of the ventral stream",
    "abstract": "This work establishes a benchmark by which to measure models of the ventral stream using crowd-sourced human behavioral measurements. We collected human error patterns on an object recognition task across a variety of images. By comparing the error pattern of these models to the error pattern of humans, we can measure how similar to the human behavior the model's behavior is. Each model we tested was composed of two parts: an encoding phase which translates images to features, and a decoding phase which translates features to a classifier decision. We measured the behavioral consistency of three encoder models: a convolutional neural network, and a particular view of neural activity of either are V4 or IT. We measured three decoder models: logistic regression and 2 different types of support vector machines. We found the most consistent error pattern to come from a combination of IT neurons and a logistic regression but found that this model performed far worse than humans. After accounting for performance, the only model that was not invalidated was a combination of IT neurons and an SVM.",
    "advisors": ["James J. DiCarlo"],
    "text": "Benchmarking models of the ventral stream This work establishes a benchmark by which to measure models of the ventral stream using crowd-sourced human behavioral measurements. We collected human error patterns on an object recognition task across a variety of images. By comparing the error pattern of these models to the error pattern of humans, we can measure how similar to the human behavior the model's behavior is. Each model we tested was composed of two parts: an encoding phase which translates images to features, and a decoding phase which translates features to a classifier decision. We measured the behavioral consistency of three encoder models: a convolutional neural network, and a particular view of neural activity of either are V4 or IT. We measured three decoder models: logistic regression and 2 different types of support vector machines. We found the most consistent error pattern to come from a combination of IT neurons and a logistic regression but found that this model performed far worse than humans. After accounting for performance, the only model that was not invalidated was a combination of IT neurons and an SVM."
}, {
    "id": "oai:dspace.mit.edu:1721.1/120630",
    "title": "Towards optical connectomics : feasibility of 3D reconstruction of neural morphology using expansion microscopy and in situ molecular barcoding",
    "abstract": "Reconstruction of the 3D morphology of neurons is an essential step towards the analysis and understanding of the structures and functions of neural circuits. Optical microscopy has been a key technology for mapping brain circuits throughout the history of neuroscience due to its low cost, wide usage in biological sciences and ability to read out information-rich molecular signals. However, conventional optical microscopes have limited spatial resolution due to the diffraction limit of light, requiring a tradeoff between the density of neurons to be imaged and the ability to resolve finer structures. A new technology called expansion microscopy (ExM), which physically expands the specimen multiple times before optical imaging, enables us to image fine structures of biological tissues with super resolution using conventional optical microscopes. With the help of ExM, we can also read out molecular information in the neural tissues optically. In this thesis, I will introduce our study of the properties, via computer simulation, of a candidate automated approach to algorithmic reconstruction of dense neural morphology, based on simulated data of the kind that would be obtained via two emerging molecular technologies-expansion microscopy (ExM) and in-situ molecular barcoding. We utilize a convolutional neural network to detect neuronal boundaries from protein-tagged plasma membrane images obtained via ExM, as well as a subsequent supervoxel-merging pipeline guided by optical readout of information-rich, cell-specific nucleic acid barcodes. We attempt to use conservative imaging and labeling parameters, with the goal of establishing a baseline case that points to the potential feasibility of optical circuit reconstruction, leaving open the possibility of higher-performance labeling technologies and algorithms. We find that, even with these conservative assumptions, an all-optical approach to dense neural morphology reconstruction may be possible via the proposed algorithmic framework.",
    "advisors": ["Edward S. Boyden, III"],
    "text": "Towards optical connectomics : feasibility of 3D reconstruction of neural morphology using expansion microscopy and in situ molecular barcoding Reconstruction of the 3D morphology of neurons is an essential step towards the analysis and understanding of the structures and functions of neural circuits. Optical microscopy has been a key technology for mapping brain circuits throughout the history of neuroscience due to its low cost, wide usage in biological sciences and ability to read out information-rich molecular signals. However, conventional optical microscopes have limited spatial resolution due to the diffraction limit of light, requiring a tradeoff between the density of neurons to be imaged and the ability to resolve finer structures. A new technology called expansion microscopy (ExM), which physically expands the specimen multiple times before optical imaging, enables us to image fine structures of biological tissues with super resolution using conventional optical microscopes. With the help of ExM, we can also read out molecular information in the neural tissues optically. In this thesis, I will introduce our study of the properties, via computer simulation, of a candidate automated approach to algorithmic reconstruction of dense neural morphology, based on simulated data of the kind that would be obtained via two emerging molecular technologies-expansion microscopy (ExM) and in-situ molecular barcoding. We utilize a convolutional neural network to detect neuronal boundaries from protein-tagged plasma membrane images obtained via ExM, as well as a subsequent supervoxel-merging pipeline guided by optical readout of information-rich, cell-specific nucleic acid barcodes. We attempt to use conservative imaging and labeling parameters, with the goal of establishing a baseline case that points to the potential feasibility of optical circuit reconstruction, leaving open the possibility of higher-performance labeling technologies and algorithms. We find that, even with these conservative assumptions, an all-optical approach to dense neural morphology reconstruction may be possible via the proposed algorithmic framework."
}, {
    "id": "oai:dspace.mit.edu:1721.1/107879",
    "title": "Improved methods for rapid and scalable tissue clearing and labeling",
    "abstract": "Combined measurement of diverse molecular and anatomical traits that span multiple levels remains a major challenge in biology. Here, we introduce a simple method that enables proteomic imaging for scalable, integrated, high-dimensional phenotyping of both animal tissues and human clinical samples. This method, termed SWITCH, uniformly secures tissue architecture, native biomolecules, and antigenicity across an entire system by synchronizing the tissue preservation reaction. The heat- and chemical-resistant nature of the resulting framework permits multiple rounds (>20) of relabeling. We have performed 22 rounds of labeling of a single tissue with precise co-registration of multiple datasets. Furthermore, SWITCH synchronizes labeling reactions to improve probe penetration depth and uniformity of staining. With SWITCH, we performed combinatorial protein expression profiling of the human cortex and also interrogated the geometric structure of the fiber pathways in mouse brains. Such integrated high-dimensional information may accelerate our understanding of biological systems at multiple levels.",
    "advisors": ["Kwanghun Chung"],
    "text": "Improved methods for rapid and scalable tissue clearing and labeling Combined measurement of diverse molecular and anatomical traits that span multiple levels remains a major challenge in biology. Here, we introduce a simple method that enables proteomic imaging for scalable, integrated, high-dimensional phenotyping of both animal tissues and human clinical samples. This method, termed SWITCH, uniformly secures tissue architecture, native biomolecules, and antigenicity across an entire system by synchronizing the tissue preservation reaction. The heat- and chemical-resistant nature of the resulting framework permits multiple rounds (>20) of relabeling. We have performed 22 rounds of labeling of a single tissue with precise co-registration of multiple datasets. Furthermore, SWITCH synchronizes labeling reactions to improve probe penetration depth and uniformity of staining. With SWITCH, we performed combinatorial protein expression profiling of the human cortex and also interrogated the geometric structure of the fiber pathways in mouse brains. Such integrated high-dimensional information may accelerate our understanding of biological systems at multiple levels."
}, {
    "id": "oai:dspace.mit.edu:1721.1/9676",
    "title": "Postnatal development of brainstem cholinergic inputs to the dorsal lateral geniculate nucleus of the domesticated ferret, Mustela putorius furo",
    "abstract": "The ferret dorsal lateral geniculate nucleus (dLGN) undergoes two periods of retinal afferent segregation during postnatal development. The first establishes the eye-specific laminae, A and A 1, and the second establishes ON/OFF sublaminae within laminae A and At. In contrast to eyespecific segregation, which seems to rely only on presynaptic activity, ON /OFF sublamination requires both pre- and postsynaptic activity. Because of its dependence on postsynaptic (relay cell) activity, sub lamination may be influenced by extra retinal inputs which alter relay cell excitability. We have examined the postnatal development of cholinergic brainstem inputs to the dLGN to determine whether these inputs arrive in time to influence sublamination and whether the cholinergic innervation is present in laminar zones (i.e., whether it might target relay cells). Choline acetyltransferase (ChA1) immunoreactivity is not detected in the dLGN until a few days after the second postnatal week Gust after sublamination begins), at which time it can be seen in both A and C laminae. ChA T labeling increases in intensity until two days before the end of the fourth postnatal week (when ON/OFF sublamination is complete), when it drops dramatically throughout the dLGN . ChAT labeling returns a few days later, but appears in the interlaminar and intersublaminar zones instead of within the A and C laminae. However, the pattern of ChA T labeling reverses once more, so that in the adult, ChA T labeling appears in the A and C laminae and is relatively absent from interlaminar zones. Acetylcholinesterase (AChE) labeling in the dLGN shows a similar ontogenetic pattern and time course. Retrngrade labeling of brainstem cholinergic nuclei demonstrates that these inputs are in place in the dLGN after the second postnatal week. Thus, cholinergic inputs to A and A 1 laminae of the ferret dLG N do arrive in time to influence ON/OFF sublamination.",
    "advisors": ["Mriganka Sur"],
    "text": "Postnatal development of brainstem cholinergic inputs to the dorsal lateral geniculate nucleus of the domesticated ferret, Mustela putorius furo The ferret dorsal lateral geniculate nucleus (dLGN) undergoes two periods of retinal afferent segregation during postnatal development. The first establishes the eye-specific laminae, A and A 1, and the second establishes ON/OFF sublaminae within laminae A and At. In contrast to eyespecific segregation, which seems to rely only on presynaptic activity, ON /OFF sublamination requires both pre- and postsynaptic activity. Because of its dependence on postsynaptic (relay cell) activity, sub lamination may be influenced by extra retinal inputs which alter relay cell excitability. We have examined the postnatal development of cholinergic brainstem inputs to the dLGN to determine whether these inputs arrive in time to influence sublamination and whether the cholinergic innervation is present in laminar zones (i.e., whether it might target relay cells). Choline acetyltransferase (ChA1) immunoreactivity is not detected in the dLGN until a few days after the second postnatal week Gust after sublamination begins), at which time it can be seen in both A and C laminae. ChA T labeling increases in intensity until two days before the end of the fourth postnatal week (when ON/OFF sublamination is complete), when it drops dramatically throughout the dLGN . ChAT labeling returns a few days later, but appears in the interlaminar and intersublaminar zones instead of within the A and C laminae. However, the pattern of ChA T labeling reverses once more, so that in the adult, ChA T labeling appears in the A and C laminae and is relatively absent from interlaminar zones. Acetylcholinesterase (AChE) labeling in the dLGN shows a similar ontogenetic pattern and time course. Retrngrade labeling of brainstem cholinergic nuclei demonstrates that these inputs are in place in the dLGN after the second postnatal week. Thus, cholinergic inputs to A and A 1 laminae of the ferret dLG N do arrive in time to influence ON/OFF sublamination."
}, {
    "id": "oai:dspace.mit.edu:1721.1/103219",
    "title": "Knockout of the glutamate transporter GLT-1 specifically from neurons drastically alters transcriptome profiles in CA3, CA1, and Striatum",
    "abstract": "Precise regulation of glutamate homeostasis is critical for normal brain function, as its disruption can impair excitatory transmission and result in neurodegenerative and neuropsychiatric disorders. Critical to maintaining glutamate homeostasis is a family of sodium-dependent glutamate transporters. GLT-1, the major glutamate transporter, is responsible for >90% of brain glutamate uptake. While previously thought to exist solely on astrocytes, the Rosenberg lab has identified GLT-1 as the major, if not only, glutamate transporter associated with excitatory terminals, particularly in CA3 pyramidal neuron axon terminals within CA3 and CA1 as well as in cortical layer V pyramidal neuron axon terminals within striatum. The specific functions of GLT-1 in axon terminals in regulating glutamate homeostasis and synaptic transmission are unknown; in order to investigate these functions, the Rosenberg lab has generated a conditional GLT- 1 KO mouse line where GLT-1 can be specifically deleted from neurons. The aim of this project was to investigate the transcriptome profiles resultant from knockout of neuronal GLT-1 (nGLT-1), within regions known to express GLT-1 on neurons, and to identify and characterize alterations in known biological pathways. I report that deletion of nGLT-1 results in a high degree of differential gene expression within CA3 (1509), CAl (322), and Striatum (1268). Furthermore, these alterations in gene expression were enriched in annotated biological pathways related to energy metabolism and neurotransmission. These findings challenge the long-held assumption that, because GLT-1 expression on neurons is significantly lower than on astrocytes, nGLT-1 contributes little to the regulation of synaptic glutamate homeostasis.",
    "advisors": ["Matthew A. Wilson"],
    "text": "Knockout of the glutamate transporter GLT-1 specifically from neurons drastically alters transcriptome profiles in CA3, CA1, and Striatum Precise regulation of glutamate homeostasis is critical for normal brain function, as its disruption can impair excitatory transmission and result in neurodegenerative and neuropsychiatric disorders. Critical to maintaining glutamate homeostasis is a family of sodium-dependent glutamate transporters. GLT-1, the major glutamate transporter, is responsible for >90% of brain glutamate uptake. While previously thought to exist solely on astrocytes, the Rosenberg lab has identified GLT-1 as the major, if not only, glutamate transporter associated with excitatory terminals, particularly in CA3 pyramidal neuron axon terminals within CA3 and CA1 as well as in cortical layer V pyramidal neuron axon terminals within striatum. The specific functions of GLT-1 in axon terminals in regulating glutamate homeostasis and synaptic transmission are unknown; in order to investigate these functions, the Rosenberg lab has generated a conditional GLT- 1 KO mouse line where GLT-1 can be specifically deleted from neurons. The aim of this project was to investigate the transcriptome profiles resultant from knockout of neuronal GLT-1 (nGLT-1), within regions known to express GLT-1 on neurons, and to identify and characterize alterations in known biological pathways. I report that deletion of nGLT-1 results in a high degree of differential gene expression within CA3 (1509), CAl (322), and Striatum (1268). Furthermore, these alterations in gene expression were enriched in annotated biological pathways related to energy metabolism and neurotransmission. These findings challenge the long-held assumption that, because GLT-1 expression on neurons is significantly lower than on astrocytes, nGLT-1 contributes little to the regulation of synaptic glutamate homeostasis."
}, {
    "id": "oai:dspace.mit.edu:1721.1/70385",
    "title": "Viral delivery of recombinant growth hormone to rescue effects of chronic stress on hippocampal learning",
    "abstract": "Chronic stress has been linked to variation in gene regulation in the hippocampus (HIP) among other areas. These lead to cytoskeletal and volumetric rearrangements in various nuclei of the central nervous system and are thought to contribute to several stress-sensitive disorders. One such gene that has been shown to be downregulated in HIP in response to stress is somatotropin, colloquially known as growth hormone (GH). These experiments were conducted to develop a novel assay for examination of working memory in rats and explore the nature of stress-induced impairment of hippocampal function and determine whether infusion of a modified herpes simplex virus (HSV) carrying the recombinant rodent growth hormone (GH) would be sufficient to restore normal hippocampal function. After 21 days of chronic immobilization stress (CIS), animals received bilateral infusions into the dorsal HIP of 2[mu]l HSV carrying either GH with green florescent protein (GFP) or GFP only. On the second day following the infusion, the animals received trace conditioning, a HIP-dependent task, with five tone-shock pairings of a 16 second tone followed by a 30 second trace interval terminating with a 1 second 0.85 milliamp footshock. An inter-trial interval of 3 minutes was used to separate the tone-shock pairings. The following day the animals were tested for fear to the context and for fear to the tone in a novel context, measured by amount of time the animal spent freezing. Using this criterion, animals that had undergone stress that received the control vector were less likely to freeze when presented with the tone, indicating an impairment of hippocampal function. Viral-mediated overexpression of GH in the dorsal HIP was able to reverse the CIS-related impairment in hippocampal function. ELISA was used to verify the expression of GH from the infused vector. These experiments may yield future directions of investigation for stress-based disorders.",
    "advisors": ["Ki A. Goosens"],
    "text": "Viral delivery of recombinant growth hormone to rescue effects of chronic stress on hippocampal learning Chronic stress has been linked to variation in gene regulation in the hippocampus (HIP) among other areas. These lead to cytoskeletal and volumetric rearrangements in various nuclei of the central nervous system and are thought to contribute to several stress-sensitive disorders. One such gene that has been shown to be downregulated in HIP in response to stress is somatotropin, colloquially known as growth hormone (GH). These experiments were conducted to develop a novel assay for examination of working memory in rats and explore the nature of stress-induced impairment of hippocampal function and determine whether infusion of a modified herpes simplex virus (HSV) carrying the recombinant rodent growth hormone (GH) would be sufficient to restore normal hippocampal function. After 21 days of chronic immobilization stress (CIS), animals received bilateral infusions into the dorsal HIP of 2[mu]l HSV carrying either GH with green florescent protein (GFP) or GFP only. On the second day following the infusion, the animals received trace conditioning, a HIP-dependent task, with five tone-shock pairings of a 16 second tone followed by a 30 second trace interval terminating with a 1 second 0.85 milliamp footshock. An inter-trial interval of 3 minutes was used to separate the tone-shock pairings. The following day the animals were tested for fear to the context and for fear to the tone in a novel context, measured by amount of time the animal spent freezing. Using this criterion, animals that had undergone stress that received the control vector were less likely to freeze when presented with the tone, indicating an impairment of hippocampal function. Viral-mediated overexpression of GH in the dorsal HIP was able to reverse the CIS-related impairment in hippocampal function. ELISA was used to verify the expression of GH from the infused vector. These experiments may yield future directions of investigation for stress-based disorders."
}, {
    "id": "oai:dspace.mit.edu:1721.1/100876",
    "title": "Generation and tuning of learned sensorimotor behavior by multiple neural circuit architectures",
    "abstract": "Organisms have a remarkable ability to respond to complex sensory inputs with intricate, tuned motor patterns. How does the brain organize and tune these motor responses, and are certain circuit architectures, or connectivity patterns, optimally suited for certain sensorimotor applications? This thesis presents progress towards this particular problem in three subprojects. The first section re-analyzes a large data set of single-unit recordings in zebra finch area HVC during singing. While HVC is known to be essential for proper expression of adult vocalization, its circuit architecture is contentious. Evidence is presented against the recently postulated gesture-trajectory extrema hypothesis for the organization of area HVC. Instead, the data suggest that the synaptic chain model of HVC organization is a better fit for the data, where chains of RA-projecting HVC neurons are synaptically connected to walk the bird through each time-step of the song. The second section examines how optimal sensorimotor estimation using a Bayesian inference framework could be implemented in a cerebellar circuit. Two novel behavioral paradigms are developed to assess how rats might tune their motor output to the statistics of the sensory inputs, and whether their behavior might be consistent with the use of a Bayesian inference paradigm. While neither behavior generated stable behavior, evidence indicates that rats may use a spinal circuit to rapidly and dynamically adjust motor output. The third section addresses the formation of habitual behaviors in a cortico-striatal network using rats. Stress and depression are known to significantly alter decision-making abilities, but the neural substrate of this is poorly understood. Towards this goal, rats are trained on a panel of decision-making tasks in a forced-choice T-maze, and it is shown that a chronic stress procedure produces a dramatic shift in behavior in a subset of these tasks but not the rest. This behavioral shift is reversed by optogenetic stimulation of prelimbic input to striatum, pinpointing a circuit element which may control stress-induced behavioral changes. Furthermore, a circuit hypothesis is presented to explain why sensitivity to changing reward values diminishes with overtraining.",
    "advisors": ["Matthew A. Wilson"],
    "text": "Generation and tuning of learned sensorimotor behavior by multiple neural circuit architectures Organisms have a remarkable ability to respond to complex sensory inputs with intricate, tuned motor patterns. How does the brain organize and tune these motor responses, and are certain circuit architectures, or connectivity patterns, optimally suited for certain sensorimotor applications? This thesis presents progress towards this particular problem in three subprojects. The first section re-analyzes a large data set of single-unit recordings in zebra finch area HVC during singing. While HVC is known to be essential for proper expression of adult vocalization, its circuit architecture is contentious. Evidence is presented against the recently postulated gesture-trajectory extrema hypothesis for the organization of area HVC. Instead, the data suggest that the synaptic chain model of HVC organization is a better fit for the data, where chains of RA-projecting HVC neurons are synaptically connected to walk the bird through each time-step of the song. The second section examines how optimal sensorimotor estimation using a Bayesian inference framework could be implemented in a cerebellar circuit. Two novel behavioral paradigms are developed to assess how rats might tune their motor output to the statistics of the sensory inputs, and whether their behavior might be consistent with the use of a Bayesian inference paradigm. While neither behavior generated stable behavior, evidence indicates that rats may use a spinal circuit to rapidly and dynamically adjust motor output. The third section addresses the formation of habitual behaviors in a cortico-striatal network using rats. Stress and depression are known to significantly alter decision-making abilities, but the neural substrate of this is poorly understood. Towards this goal, rats are trained on a panel of decision-making tasks in a forced-choice T-maze, and it is shown that a chronic stress procedure produces a dramatic shift in behavior in a subset of these tasks but not the rest. This behavioral shift is reversed by optogenetic stimulation of prelimbic input to striatum, pinpointing a circuit element which may control stress-induced behavioral changes. Furthermore, a circuit hypothesis is presented to explain why sensitivity to changing reward values diminishes with overtraining."
}, {
    "id": "oai:dspace.mit.edu:1721.1/50517",
    "title": "Prism adaptation in a case of cerebellar agenesis",
    "abstract": "Normal subjects adapt quickly to the displacing effects of prism goggles. A measure of this adaptation comes from the negative aftereffects in reaching that subjects show after the prism goggles are removed. Neural circuitry within the cerebellar cortex has been implicated as the site of plasticity for visuomotor adaptation. An opportunity to test a 15-year-old boy, A.C., with near complete cerebellar agenesis allowed us to determine whether cerebellar structures are critical for prism adaptation to occur. A.C. was tested on two separate occasions, twice using his left hand, and once using his right hand. He wore prism goggles while pointing to a vertical line at each of nine target locations in baseline, exposure, and postexposure conditions. The position of his finger was recorded after each response. In the exposure condition, the goggles were adjusted to 11\" displacement to the right when A.C. pointed with his left hand, and to the left when he pointed with his right hand. He received visual feedback only in the exposure condition. His results were compared to those of 20 normal control subjects (NCS). Independent measures of performance and adaptation were calculated for left- and right-handed pointing by each subject. A.C. showed greater variablity in pointing with his right (nonpreferred) hand compared to his left hand and compared to NCS. An ordinal ranking indicated that his adaptation scores did not differ significantly from those of the NCS for either the left (p = 0.30 ) or right hand (p = 0.22). While these results do not disprove the theory that the cerebellum plays a role in normal adaptation, it does indicate that neural structures outside the cerebellum are sufficient to allow adaptation to occur.",
    "advisors": ["Suzanne Corkin"],
    "text": "Prism adaptation in a case of cerebellar agenesis Normal subjects adapt quickly to the displacing effects of prism goggles. A measure of this adaptation comes from the negative aftereffects in reaching that subjects show after the prism goggles are removed. Neural circuitry within the cerebellar cortex has been implicated as the site of plasticity for visuomotor adaptation. An opportunity to test a 15-year-old boy, A.C., with near complete cerebellar agenesis allowed us to determine whether cerebellar structures are critical for prism adaptation to occur. A.C. was tested on two separate occasions, twice using his left hand, and once using his right hand. He wore prism goggles while pointing to a vertical line at each of nine target locations in baseline, exposure, and postexposure conditions. The position of his finger was recorded after each response. In the exposure condition, the goggles were adjusted to 11\" displacement to the right when A.C. pointed with his left hand, and to the left when he pointed with his right hand. He received visual feedback only in the exposure condition. His results were compared to those of 20 normal control subjects (NCS). Independent measures of performance and adaptation were calculated for left- and right-handed pointing by each subject. A.C. showed greater variablity in pointing with his right (nonpreferred) hand compared to his left hand and compared to NCS. An ordinal ranking indicated that his adaptation scores did not differ significantly from those of the NCS for either the left (p = 0.30 ) or right hand (p = 0.22). While these results do not disprove the theory that the cerebellum plays a role in normal adaptation, it does indicate that neural structures outside the cerebellum are sufficient to allow adaptation to occur."
}, {
    "id": "oai:dspace.mit.edu:1721.1/16863",
    "title": "Qualitative representation for recognition",
    "abstract": "This thesis describes a representation for objects and scenes that is stable against variations in image intensity caused by illumination changes and tolerant to image degradations such as sensor noise. The representation, called a ratio-template, uses low-resolution ordinal contrast relationships as its matching primitives. The choice of these primitives was inspired not only by considerations of computational simplicity and robustness, but also by current knowledge of the early stages of visual processing in the primate brain. The resulting representation is biologically plausible, although there is currently no evidence to suggest that the representation is actually used by the primate visual system. Constructed manually at first, the ratio-template can be learned automatically from a set of examples. Two applications--face detection and scene indexing--are described. The ratio-template achieves detection rates higher than 90% and can process a 320280 pixel image in 2.6 seconds at multiple scales.",
    "advisors": ["Pawan Sinha"],
    "text": "Qualitative representation for recognition This thesis describes a representation for objects and scenes that is stable against variations in image intensity caused by illumination changes and tolerant to image degradations such as sensor noise. The representation, called a ratio-template, uses low-resolution ordinal contrast relationships as its matching primitives. The choice of these primitives was inspired not only by considerations of computational simplicity and robustness, but also by current knowledge of the early stages of visual processing in the primate brain. The resulting representation is biologically plausible, although there is currently no evidence to suggest that the representation is actually used by the primate visual system. Constructed manually at first, the ratio-template can be learned automatically from a set of examples. Two applications--face detection and scene indexing--are described. The ratio-template achieves detection rates higher than 90% and can process a 320280 pixel image in 2.6 seconds at multiple scales."
}, {
    "id": "oai:dspace.mit.edu:1721.1/106442",
    "title": "Chronic stress-dependent activation of somatostatin neurons in the nucleus accumbens facilitates maladaptive eating behaviors",
    "abstract": "Stressors are known to impact eating behaviors. However, recapitulating the intricate interplay between chronic stress and aberrant human eating patterns in an animal model remains a challenge. Notably, binge eating, a diagnostic feature associated with many types of eating abnormalities, particularly pertains to the binge eating disorder. To more closely investigate the etiology underlying eating behavior-associated maladaptation, the present study provides a novel and ethologically relevant animal model based on predatory odor stress. My data show that chronic stress in female mice selectively increases consumption of highly palatable, but not the regular, diet, when it is presented during a limited time following stress exposure. In addition, the nucleus accumbens (NAc), a key component in the neural circuitry of reward, is also an established neural substrate susceptible to the effects of stress. Given the cellular complexity in NAc, identifying the neuronal subtypes that are selectively involved in chronic stress-elicited physiological and behavioral alterations will provide grounds for further understanding in the underlying cellular changes. Because deficits in the somatostatin (SOM) neurons have been implicated in mice exhibiting traits of anxiety and depression, this neuron subtype may play an important role in modulating negative behavioral emotionality. Here I report an abundance of somatostatin neurons, majority of which are located in the rostral-ventral region of the NAc and are activated by chronic stress exposure. Together, these results provide the first line of evidence in linking chronic stress and the somatostatin neurons within the NAc to binge eating. Further fluorescent labeling quantification and cell-type-specific optogenetic manipulation will be needed to further delineate the role of SOM neurons in orchestrating the inhibitory components of stress-modulated reward circuitry.",
    "advisors": ["Ki Goosens"],
    "text": "Chronic stress-dependent activation of somatostatin neurons in the nucleus accumbens facilitates maladaptive eating behaviors Stressors are known to impact eating behaviors. However, recapitulating the intricate interplay between chronic stress and aberrant human eating patterns in an animal model remains a challenge. Notably, binge eating, a diagnostic feature associated with many types of eating abnormalities, particularly pertains to the binge eating disorder. To more closely investigate the etiology underlying eating behavior-associated maladaptation, the present study provides a novel and ethologically relevant animal model based on predatory odor stress. My data show that chronic stress in female mice selectively increases consumption of highly palatable, but not the regular, diet, when it is presented during a limited time following stress exposure. In addition, the nucleus accumbens (NAc), a key component in the neural circuitry of reward, is also an established neural substrate susceptible to the effects of stress. Given the cellular complexity in NAc, identifying the neuronal subtypes that are selectively involved in chronic stress-elicited physiological and behavioral alterations will provide grounds for further understanding in the underlying cellular changes. Because deficits in the somatostatin (SOM) neurons have been implicated in mice exhibiting traits of anxiety and depression, this neuron subtype may play an important role in modulating negative behavioral emotionality. Here I report an abundance of somatostatin neurons, majority of which are located in the rostral-ventral region of the NAc and are activated by chronic stress exposure. Together, these results provide the first line of evidence in linking chronic stress and the somatostatin neurons within the NAc to binge eating. Further fluorescent labeling quantification and cell-type-specific optogenetic manipulation will be needed to further delineate the role of SOM neurons in orchestrating the inhibitory components of stress-modulated reward circuitry."
}, {
    "id": "oai:dspace.mit.edu:1721.1/100875",
    "title": "Investigating the influence of LH-projecting BLA neurons upon motivated behavioral responding and appetitive learning",
    "abstract": "To optimize survival, organisms must be able to learn contingencies between external stimuli and rewards and appropriately respond to these associations. Deficits in reward-related learning or reward-seeking are thought to occur in a host of psychopathologies, including depression (Drevets, 2001), eating disorders (Wagner et al., 2007), and substance abuse (Wrase et al., 2007), such that improved understanding of reward processing could potentially aid in the development of therapies. Two neural regions, the basolateral amygdala (BLA) and lateral hypothalamus (LH), are both implicated in reward processing (Adamantidis et al., 2007; Anand and Brobeck, 1951; Brobeck, 1946; Gutierrez et al., 2011; Hoebel and Teitelbaum, 1962; Kempadoo et al., 2013; Margules and Olds, 1962; Muramoto et al., 1993; Sakurai, 2007; Schoenbaum et al., 1998; Tye and Janak, 2007; Tye et al., 2008, 2010), but the role of the BLA's projection to LH in appetitive conditioning and reward-seeking remains unclear. Through the use of optogenetic techniques in mice, I have investigated the influence of LH-projecting BLA neurons upon motivated behavioral responding, which has indicated that the projection may support intracranial self-stimulation (ICSS). Further experiments with in vivo extracellular electrophysiological recordings from LH-projecting BLA neurons may also shed light on the encoding properties of these neurons during appetitive learning.",
    "advisors": ["Kay Tye"],
    "text": "Investigating the influence of LH-projecting BLA neurons upon motivated behavioral responding and appetitive learning To optimize survival, organisms must be able to learn contingencies between external stimuli and rewards and appropriately respond to these associations. Deficits in reward-related learning or reward-seeking are thought to occur in a host of psychopathologies, including depression (Drevets, 2001), eating disorders (Wagner et al., 2007), and substance abuse (Wrase et al., 2007), such that improved understanding of reward processing could potentially aid in the development of therapies. Two neural regions, the basolateral amygdala (BLA) and lateral hypothalamus (LH), are both implicated in reward processing (Adamantidis et al., 2007; Anand and Brobeck, 1951; Brobeck, 1946; Gutierrez et al., 2011; Hoebel and Teitelbaum, 1962; Kempadoo et al., 2013; Margules and Olds, 1962; Muramoto et al., 1993; Sakurai, 2007; Schoenbaum et al., 1998; Tye and Janak, 2007; Tye et al., 2008, 2010), but the role of the BLA's projection to LH in appetitive conditioning and reward-seeking remains unclear. Through the use of optogenetic techniques in mice, I have investigated the influence of LH-projecting BLA neurons upon motivated behavioral responding, which has indicated that the projection may support intracranial self-stimulation (ICSS). Further experiments with in vivo extracellular electrophysiological recordings from LH-projecting BLA neurons may also shed light on the encoding properties of these neurons during appetitive learning."
}, {
    "id": "oai:dspace.mit.edu:1721.1/70388",
    "title": "Individual differences in sentence processing",
    "abstract": "This thesis aims to elucidate shared mechanisms between retrieval in sentence processing and memory retrieval processes in nonlinguistic domains using an individual differences approach. Prior research in individual differences in sentence processing has provided conflicting evidence as to whether the same memory mechanisms operate in linguistic processing, potentially a quite specialized cognitive domain, and in other, more general areas of cognition (Just & Carpenter, 1992; Caplan & Waters, 1999). This question has been primarily addressed from the point of view of capacity-based theories of working memory (Baddeley, 1986). Under these theories, verbal working memory is either comprised of multiple components including separate components for syntactic and non-syntactic verbal processing, or is dependent on a unitary pool of resources shared across all verbal domains. However, recent memory research has suggested that the capacity-theory architecture may be incorrect. Instead of a three-part memory system composed of focal attention, working memory, and long-term memory, a better model of the memory system may be bipartite, comprising focal attention and long-term memory. In the bipartite theory, working memory is viewed as a set of mechanisms mediating between these two stores, and accurately describes empirical data (McElree, 2006). If the latter hypothesis is correct, then it follows that the bipartite system underlying sentence processing should rely on the same set of working memory mechanisms as in general memory processes. In particular, a number of empirical studies have shown that both general memory and sentence processing are subject to interference from contextually-relevant intervening elements. Such interference is thought to occur at retrieval (as opposed to encoding) both for general memory tasks (e.g., retrieving items from a list) and in sentence processing (e.g., retrieving elements in long-distance syntactic dependencies). However, no systematic attempts have been made to investigate whether this interference results from the same processing limitations. In Study 1, performance on a battery of memory and cognitive tasks is compared to performance on sentence processing tasks. One of the sentence processing tasks correlated with multiple measures likely to rely on general memory mechanisms involved in resolution of retrieval interference. However, low internal reliability of the language tasks in the first study was observed. In Study 2, a series of sentence processing tasks is examined in order to determine which tasks exhibit the highest internal reliability. The results indicate that syntactic complexity manipulations presented in null (isolated) contexts exhibit highest internal reliability and are good candidates for future studies investigating individual differences in sentence processing. Suggestions for future studies investigating shared resources between sentence processing tasks and general memory mechanism are then discussed, informed by the results from these studies.",
    "advisors": ["John Gabrieli"],
    "text": "Individual differences in sentence processing This thesis aims to elucidate shared mechanisms between retrieval in sentence processing and memory retrieval processes in nonlinguistic domains using an individual differences approach. Prior research in individual differences in sentence processing has provided conflicting evidence as to whether the same memory mechanisms operate in linguistic processing, potentially a quite specialized cognitive domain, and in other, more general areas of cognition (Just & Carpenter, 1992; Caplan & Waters, 1999). This question has been primarily addressed from the point of view of capacity-based theories of working memory (Baddeley, 1986). Under these theories, verbal working memory is either comprised of multiple components including separate components for syntactic and non-syntactic verbal processing, or is dependent on a unitary pool of resources shared across all verbal domains. However, recent memory research has suggested that the capacity-theory architecture may be incorrect. Instead of a three-part memory system composed of focal attention, working memory, and long-term memory, a better model of the memory system may be bipartite, comprising focal attention and long-term memory. In the bipartite theory, working memory is viewed as a set of mechanisms mediating between these two stores, and accurately describes empirical data (McElree, 2006). If the latter hypothesis is correct, then it follows that the bipartite system underlying sentence processing should rely on the same set of working memory mechanisms as in general memory processes. In particular, a number of empirical studies have shown that both general memory and sentence processing are subject to interference from contextually-relevant intervening elements. Such interference is thought to occur at retrieval (as opposed to encoding) both for general memory tasks (e.g., retrieving items from a list) and in sentence processing (e.g., retrieving elements in long-distance syntactic dependencies). However, no systematic attempts have been made to investigate whether this interference results from the same processing limitations. In Study 1, performance on a battery of memory and cognitive tasks is compared to performance on sentence processing tasks. One of the sentence processing tasks correlated with multiple measures likely to rely on general memory mechanisms involved in resolution of retrieval interference. However, low internal reliability of the language tasks in the first study was observed. In Study 2, a series of sentence processing tasks is examined in order to determine which tasks exhibit the highest internal reliability. The results indicate that syntactic complexity manipulations presented in null (isolated) contexts exhibit highest internal reliability and are good candidates for future studies investigating individual differences in sentence processing. Suggestions for future studies investigating shared resources between sentence processing tasks and general memory mechanism are then discussed, informed by the results from these studies."
}, {
    "id": "oai:dspace.mit.edu:1721.1/114077",
    "title": "A prefrontal source of visual target enhancement in the macaque area V4",
    "abstract": "The ventral pre-arcuate area (VPA) in the primate prefrontal cortex has recently been found to play an important role in feature-based selection of visual targets in the context of a naturalistic free-gaze visual search task. While VPA neuronal activation was found to be necessary for behavioral performance as well as target selection in the FEF, its role in the broader context of the visual system remains to be addressed. To this end, we have interrogated the role of the VPA in mediating the effects of feature attention in the macaque visual area V4 by recording in V4 with and without muscimol inactivation in the VPA. We report here that neuronal activation in the VPA is necessary for firing rate increases related to target selection in V4. KEYWORDS: feature attention, visual search, muscimol, neurophysiology",
    "advisors": ["Robert Desimone"],
    "text": "A prefrontal source of visual target enhancement in the macaque area V4 The ventral pre-arcuate area (VPA) in the primate prefrontal cortex has recently been found to play an important role in feature-based selection of visual targets in the context of a naturalistic free-gaze visual search task. While VPA neuronal activation was found to be necessary for behavioral performance as well as target selection in the FEF, its role in the broader context of the visual system remains to be addressed. To this end, we have interrogated the role of the VPA in mediating the effects of feature attention in the macaque visual area V4 by recording in V4 with and without muscimol inactivation in the VPA. We report here that neuronal activation in the VPA is necessary for firing rate increases related to target selection in V4. KEYWORDS: feature attention, visual search, muscimol, neurophysiology"
}, {
    "id": "oai:dspace.mit.edu:1721.1/9674",
    "title": "Individuation, identity and proper names in cognitive development",
    "abstract": "The ability to individuate entities (i.e. conceptualize one entity as distinct from two) and trace their identity (i.e. judge that an entity is the same one as an entity encountered before) is a fundamental component of the human mind and is critical to proper name reference (i.e. a proper name, like lvfax, refers to a unique individual, namely Max). Philosophers have proposed that sortals-concepts which refer to kinds of individuals-support these abilities (Gupta, 1980; Hirsch, 1982; Macnamara, 1986; Wiggins, 1967, 1980). However, while adults may well have sortal concepts and learn proper names for individuals, it is an open question whether children do so also. Proponents of the Continuity hypothesis (e.g. Macnamara, 1982; Pinker, 1984) argue that children and adults have fundamentally the same conceptual resources, whereas proponents of the Discontinuity hypothesis (e.g. Piaget, 1954; Quine, 1960, 1969) argue that child.en and adults have qualitatively different conceptual systems. In this thesis, evidence is reviewed that very young infants have at least one sortal, physical object, which suggests that infants have the conceptual structure needed to support representations of kinds and individuals. Experiments probing infant understanding of the concept, person, suggest that infants have the ability to reason about the action and appearance of others, but data presented in the thesis falls short of providing conclusive evidence that infants under a year are able to individuate people. Evidence is presented that by age three, children represent unique individuals and interpret proper names in an adult like manner as referring to unique individuals. This rules out a discontinuity alternative, namely that preschoolers represent proper names as referring to highly similar objects or to restricted subkinds. Evidence is also presented that children as young as two years are like adults in being willing to accept a range of individuals as nameable if given information which highlights the named objects' importance, such as the attribution of mental states to the object. Together these findings provide support for the continuity hypothesis and suggest a number of avenues of research into children's understanding of kinds, individuals, and their names.",
    "advisors": ["Susan Carey", "Elizabeth S. Spelke"],
    "text": "Individuation, identity and proper names in cognitive development The ability to individuate entities (i.e. conceptualize one entity as distinct from two) and trace their identity (i.e. judge that an entity is the same one as an entity encountered before) is a fundamental component of the human mind and is critical to proper name reference (i.e. a proper name, like lvfax, refers to a unique individual, namely Max). Philosophers have proposed that sortals-concepts which refer to kinds of individuals-support these abilities (Gupta, 1980; Hirsch, 1982; Macnamara, 1986; Wiggins, 1967, 1980). However, while adults may well have sortal concepts and learn proper names for individuals, it is an open question whether children do so also. Proponents of the Continuity hypothesis (e.g. Macnamara, 1982; Pinker, 1984) argue that children and adults have fundamentally the same conceptual resources, whereas proponents of the Discontinuity hypothesis (e.g. Piaget, 1954; Quine, 1960, 1969) argue that child.en and adults have qualitatively different conceptual systems. In this thesis, evidence is reviewed that very young infants have at least one sortal, physical object, which suggests that infants have the conceptual structure needed to support representations of kinds and individuals. Experiments probing infant understanding of the concept, person, suggest that infants have the ability to reason about the action and appearance of others, but data presented in the thesis falls short of providing conclusive evidence that infants under a year are able to individuate people. Evidence is presented that by age three, children represent unique individuals and interpret proper names in an adult like manner as referring to unique individuals. This rules out a discontinuity alternative, namely that preschoolers represent proper names as referring to highly similar objects or to restricted subkinds. Evidence is also presented that children as young as two years are like adults in being willing to accept a range of individuals as nameable if given information which highlights the named objects' importance, such as the attribution of mental states to the object. Together these findings provide support for the continuity hypothesis and suggest a number of avenues of research into children's understanding of kinds, individuals, and their names."
}, {
    "id": "oai:dspace.mit.edu:1721.1/33215",
    "title": "Neuromuscular modularity and behavioral correlates of motor control",
    "abstract": "I studied organizational principles that may subserve the control and learning of forelimb movements. Among these principles, I focused on muscular coordination patterns, motor cortical excitability, and sensorimotor interactions. I found that muscle activity in grasping and reaching behaviors could be reconstructed by linear combinations of a small number of time-varying muscle synergies, each fit with coefficients unique to the behavior. However, the generalization of these synergies between behavioral conditions was limited, in part by the sensitivity of the extraction algorithm to stereotyped muscular relations within contrasted conditions. In reaching studies designed to assist or resist different movement directions, I found a gradual change in the structure, as well as recruitment, of synergies. When a perturbation was targeted to the activity within a single muscle, I found a transient, relative suppression of this muscle in response to descending motor commands. In other motor cortical microstimulation experiments, I confirmed that long-train microstimulation is able to evoke complex, convergent movements. Even during highly-trained reaching movements, I found that there was relatively little invariance of the muscular patterns in relation to kinematic variables coding for the hand's displacement and velocity.",
    "advisors": ["Emilio Bizzi"],
    "text": "Neuromuscular modularity and behavioral correlates of motor control I studied organizational principles that may subserve the control and learning of forelimb movements. Among these principles, I focused on muscular coordination patterns, motor cortical excitability, and sensorimotor interactions. I found that muscle activity in grasping and reaching behaviors could be reconstructed by linear combinations of a small number of time-varying muscle synergies, each fit with coefficients unique to the behavior. However, the generalization of these synergies between behavioral conditions was limited, in part by the sensitivity of the extraction algorithm to stereotyped muscular relations within contrasted conditions. In reaching studies designed to assist or resist different movement directions, I found a gradual change in the structure, as well as recruitment, of synergies. When a perturbation was targeted to the activity within a single muscle, I found a transient, relative suppression of this muscle in response to descending motor commands. In other motor cortical microstimulation experiments, I confirmed that long-train microstimulation is able to evoke complex, convergent movements. Even during highly-trained reaching movements, I found that there was relatively little invariance of the muscular patterns in relation to kinematic variables coding for the hand's displacement and velocity."
}, {
    "id": "oai:dspace.mit.edu:1721.1/62045",
    "title": "Early word learning through communicative inference",
    "abstract": "How do children learn their first words? Do they do it by gradually accumulating information about the co-occurrence of words and their referents over time, or are words learned via quick social inferences linking what speakers are looking at, pointing to, and talking about? Both of these conceptions of early word learning are supported by empirical data. This thesis presents a computational and theoretical framework for unifying these two different ideas by suggesting that early word learning can best be described as a process of joint inferences about speakers' referential intentions and the meanings of words. Chapter 1 describes previous empirical and computational research on \"statistical learning\"--the ability of learners to use distributional patterns in their language input to learn about the elements and structure of language-and argues that capturing this abifity requires models of learning that describe inferences over structured representations, not just simple statistics. Chapter 2 argues that social signals of speakers' intentions, even eye-gaze and pointing, are at best noisy markers of reference and that in order to take advantage of these signals fully, learners must integrate information across time. Chapter 3 describes the kinds of inferences that learners can make by assuming that speakers are informative with respect to their intended meaning, introducing and testing a formalization of how Grice's pragmatic maxims can be used for word learning. Chapter 4 presents a model of cross-situational intentional word learning that both learns words and infers speakers' referential intentions from labeled corpus data.",
    "advisors": ["Edward Gibson"],
    "text": "Early word learning through communicative inference How do children learn their first words? Do they do it by gradually accumulating information about the co-occurrence of words and their referents over time, or are words learned via quick social inferences linking what speakers are looking at, pointing to, and talking about? Both of these conceptions of early word learning are supported by empirical data. This thesis presents a computational and theoretical framework for unifying these two different ideas by suggesting that early word learning can best be described as a process of joint inferences about speakers' referential intentions and the meanings of words. Chapter 1 describes previous empirical and computational research on \"statistical learning\"--the ability of learners to use distributional patterns in their language input to learn about the elements and structure of language-and argues that capturing this abifity requires models of learning that describe inferences over structured representations, not just simple statistics. Chapter 2 argues that social signals of speakers' intentions, even eye-gaze and pointing, are at best noisy markers of reference and that in order to take advantage of these signals fully, learners must integrate information across time. Chapter 3 describes the kinds of inferences that learners can make by assuming that speakers are informative with respect to their intended meaning, introducing and testing a formalization of how Grice's pragmatic maxims can be used for word learning. Chapter 4 presents a model of cross-situational intentional word learning that both learns words and infers speakers' referential intentions from labeled corpus data."
}, {
    "id": "oai:dspace.mit.edu:1721.1/49738",
    "title": "Deficient experience-dependent plasticity in the visual cortex of Arc null mice",
    "abstract": "Within the visual cortex a vast assortment of molecules work in concert to sharpen and refine neuronal circuits throughout development. With the advent of genetic mouse models it is now possible to probe the individual contributions of single molecules implicated in this process. The Arc (activity-regulated cytoskeletal associated) gene is an effector immediate early gene that has been suggested to play a critical role in synaptic plasticity. The goal of this thesis is to understand the workings of Arc within the visual cortex. Specifically, we ask how genetic deletion of Arc influences plasticity, and how visual response properties differ between cells types containing, and not containing Arc. To elucidate a role for Arc in visual cortical plasticity we took advantage of knockin mice expressing GFP in place of Arc protein (referred to as KO mice for simplicity). We combined intrinsic signal imaging, visually evoked potentials, and two-photon in vivo calcium imaging to assess plasticity in juvenile and adult wild-type (WT), heterozygote, and KO mice. We find that plasticity is disrupted in the visual cortex of Arc KO mice in the absence of obvious deficits at the level of basal response properties. In addition, this work has revealed that: 1) Arc is necessary for the establishment of normal ocular dominance during development and critical for deprived eye depression in the visual cortex of juvenile animals 2) Loss of Arc impairs AMPA receptor internalization in visual cortex- a necessary requirement for synaptic weakening after lid suture.",
    "advisors": ["Mriganka Sur"],
    "text": "Deficient experience-dependent plasticity in the visual cortex of Arc null mice Within the visual cortex a vast assortment of molecules work in concert to sharpen and refine neuronal circuits throughout development. With the advent of genetic mouse models it is now possible to probe the individual contributions of single molecules implicated in this process. The Arc (activity-regulated cytoskeletal associated) gene is an effector immediate early gene that has been suggested to play a critical role in synaptic plasticity. The goal of this thesis is to understand the workings of Arc within the visual cortex. Specifically, we ask how genetic deletion of Arc influences plasticity, and how visual response properties differ between cells types containing, and not containing Arc. To elucidate a role for Arc in visual cortical plasticity we took advantage of knockin mice expressing GFP in place of Arc protein (referred to as KO mice for simplicity). We combined intrinsic signal imaging, visually evoked potentials, and two-photon in vivo calcium imaging to assess plasticity in juvenile and adult wild-type (WT), heterozygote, and KO mice. We find that plasticity is disrupted in the visual cortex of Arc KO mice in the absence of obvious deficits at the level of basal response properties. In addition, this work has revealed that: 1) Arc is necessary for the establishment of normal ocular dominance during development and critical for deprived eye depression in the visual cortex of juvenile animals 2) Loss of Arc impairs AMPA receptor internalization in visual cortex- a necessary requirement for synaptic weakening after lid suture."
}, {
    "id": "oai:dspace.mit.edu:1721.1/103213",
    "title": "Neural mechanisms underlying the emergence of rhythmic and stereotyped vocalizations in juvenile songbirds",
    "abstract": "Complex motor behaviors in humans, such as speech, are not innate, but instead are learned. How does the brain construct neural circuits that generate these motor behaviors during learning? To understand the neural mechanisms underlying learned motor skills, I use vocal learning in songbirds as a model. While previous studies have shown that a premotor area in the songbird brain, HVC, is important for stereotyped adult song, the role of HVC in juvenile song is less known. This thesis characterizes how activity in HVC develops during song learning in juvenile birds. Early in song learning, temporal structure emerged in HVC. During the earliest vocalization of juvenile birds (subsong), HVC neurons exhibit bursts of action potentials. However, only half of the neurons show bursts that are temporally aligned to syllables, and most of these bursts are clustered around onsets of subsong syllables. Over several days, as the bird starts producing the earliest stereotyped vocalization called protosyllables, HVC neurons start exhibiting rhythmic bursts at 5-10 Hz. These rhythmic bursts are aligned to protosyllables, and bursts from different neurons are active at different latencies relative to protosyllables. Thus, as a population, HVC neurons start forming a rhythmic neural sequence. As the bird matures, multiple distinct syllable types emerge from a protosyllable. During this process, some neurons are active only during a specific syllable type ('specific neurons') while others are active during both syllable types ('shared neurons'). These shared neurons are active at similar latencies for both syllable types, and therefore form a shared neural sequence. Over development, fraction of shared neurons decrease and more neurons become specific. These results demonstrate that splitting of a neural sequence into multiple sequences underlies the emergence of a multiple syllable types. Moreover, this sequence splitting is observed during different song learning strategies, suggesting that this is a fundamental neural mechanism for song learning. This work demonstrates how the growth of a rhythmic neural sequence and its subsequence splitting gives rise to complex vocalization in songbirds. This may be a general neural mechanism in which the brain constructs neural circuits during learning of a complex motor behavior.",
    "advisors": ["Michale S. Fee"],
    "text": "Neural mechanisms underlying the emergence of rhythmic and stereotyped vocalizations in juvenile songbirds Complex motor behaviors in humans, such as speech, are not innate, but instead are learned. How does the brain construct neural circuits that generate these motor behaviors during learning? To understand the neural mechanisms underlying learned motor skills, I use vocal learning in songbirds as a model. While previous studies have shown that a premotor area in the songbird brain, HVC, is important for stereotyped adult song, the role of HVC in juvenile song is less known. This thesis characterizes how activity in HVC develops during song learning in juvenile birds. Early in song learning, temporal structure emerged in HVC. During the earliest vocalization of juvenile birds (subsong), HVC neurons exhibit bursts of action potentials. However, only half of the neurons show bursts that are temporally aligned to syllables, and most of these bursts are clustered around onsets of subsong syllables. Over several days, as the bird starts producing the earliest stereotyped vocalization called protosyllables, HVC neurons start exhibiting rhythmic bursts at 5-10 Hz. These rhythmic bursts are aligned to protosyllables, and bursts from different neurons are active at different latencies relative to protosyllables. Thus, as a population, HVC neurons start forming a rhythmic neural sequence. As the bird matures, multiple distinct syllable types emerge from a protosyllable. During this process, some neurons are active only during a specific syllable type ('specific neurons') while others are active during both syllable types ('shared neurons'). These shared neurons are active at similar latencies for both syllable types, and therefore form a shared neural sequence. Over development, fraction of shared neurons decrease and more neurons become specific. These results demonstrate that splitting of a neural sequence into multiple sequences underlies the emergence of a multiple syllable types. Moreover, this sequence splitting is observed during different song learning strategies, suggesting that this is a fundamental neural mechanism for song learning. This work demonstrates how the growth of a rhythmic neural sequence and its subsequence splitting gives rise to complex vocalization in songbirds. This may be a general neural mechanism in which the brain constructs neural circuits during learning of a complex motor behavior."
}, {
    "id": "oai:dspace.mit.edu:1721.1/106429",
    "title": "The neural and psychophysical bases of memorability",
    "abstract": "Every person has a unique set of individual experiences that make up their memories. Yet surprisingly, recent work has shown that people tend to remember and forget the same images. This is because these images differ in their memorability - a predictive value of whether an image is likely to be later remembered or not. However, the properties of memorability and its effects in the brain are unexplored. Here, I describe the first characterization of the neural and psychophysical bases of memorability. First, I show that memorability is highly consistent in the domain of face images, despite their similar perceptual features and same basic-level category, and examine what facial attributes are predictive of memorability (Chapter 2). I extend these findings to demonstrate that memorability is not only consistent across images, but across different images of the same face identity (Chapter 3); thus, memorability can be conceptualized as an intrinsic property to whole entities. I then compare memorability to several phenomena shown to influence memory - bottom-up attention, top-down attention, and priming - and find that memorability effects remain independent of these phenomena (Chapter 4). Lastly, I investigate the neural correlates of memorability in a human functional magnetic resonance imaging experiment (Chapter 5). I find sensitivity to memorability in the medial temporal lobe and ventral visual stream, with a memorability-centric representational geometry in the neural patterns in these regions. Importantly, this sensitivity is dissociable from classical individual subsequent memory effects that I show to be localized in the prefrontal cortex. These results also indicate that until now, memory work has largely confounded the effects of the participant (individual memory) and the stimulus (memorability), and I propose a re-examination of past memory findings through the lens of memorability. In whole, this work presents memorability as a novel phenomenon, easily quantified for images and entities, with its own dedicated neural signatures at the intersection of perception and memory.",
    "advisors": ["Aude Oliva"],
    "text": "The neural and psychophysical bases of memorability Every person has a unique set of individual experiences that make up their memories. Yet surprisingly, recent work has shown that people tend to remember and forget the same images. This is because these images differ in their memorability - a predictive value of whether an image is likely to be later remembered or not. However, the properties of memorability and its effects in the brain are unexplored. Here, I describe the first characterization of the neural and psychophysical bases of memorability. First, I show that memorability is highly consistent in the domain of face images, despite their similar perceptual features and same basic-level category, and examine what facial attributes are predictive of memorability (Chapter 2). I extend these findings to demonstrate that memorability is not only consistent across images, but across different images of the same face identity (Chapter 3); thus, memorability can be conceptualized as an intrinsic property to whole entities. I then compare memorability to several phenomena shown to influence memory - bottom-up attention, top-down attention, and priming - and find that memorability effects remain independent of these phenomena (Chapter 4). Lastly, I investigate the neural correlates of memorability in a human functional magnetic resonance imaging experiment (Chapter 5). I find sensitivity to memorability in the medial temporal lobe and ventral visual stream, with a memorability-centric representational geometry in the neural patterns in these regions. Importantly, this sensitivity is dissociable from classical individual subsequent memory effects that I show to be localized in the prefrontal cortex. These results also indicate that until now, memory work has largely confounded the effects of the participant (individual memory) and the stimulus (memorability), and I propose a re-examination of past memory findings through the lens of memorability. In whole, this work presents memorability as a novel phenomenon, easily quantified for images and entities, with its own dedicated neural signatures at the intersection of perception and memory."
}, {
    "id": "oai:dspace.mit.edu:1721.1/106437",
    "title": "Characterizing corticostriatal circuit function during performance of habitual action sequences",
    "abstract": "The striatum is the largest nucleus in the basal ganglia and the recipient of dense dopamine input. Multiple cortico-basal ganglia-thalamic loops are thought to function together during the learning and performance of reinforced behaviors, with the dorsolateral circuit being particularly critical for the learning of habitual chains of action sequences. However, how this circuit works to generate such behavior is poorly understood. To explore the nature of striatal neural representations during learned action sequences, I designed a task targeted at disambiguating movement-related responses from habit representations in striatum. In combination with this task, I employed electrophysiology and optogenetics techniques to characterize task-related neuronal activity in the corticostriatal circuit. I found that, unlike in motor cortex, neurons in striatum did not respond simply to particular individual actions, but responded preferentially at the initiation and termination of learned action sequences. These experiments provide a test for the existence of a generalized striatal signal marking the start and end of units of habitual behaviors which may be produced with the contribution of striatal interneurons, providing a mechanism by which striatum can control the encoding and performance of chunked action sequences. In a separate set of experiments, I explored the effect of dopamine depletion on local field potential oscillations in the same region of striatum. My goal was to investigate the interaction between abnormal oscillations caused by dopamine depletion in Parkinson's disease and the functional task-related oscillations that normally occur in healthy striatum. Against our expectations, I found that local unilateral dopamine depletion in dorsolateral striatum did not result in changes in pre-task baseline strength of oscillations, but rather in the overexpression of the normal task-related oscillations. These studies add support to theories of striatal function and dysfunction that emphasize selective network modulation by learned behaviors.",
    "advisors": ["Ann M. Graybiel"],
    "text": "Characterizing corticostriatal circuit function during performance of habitual action sequences The striatum is the largest nucleus in the basal ganglia and the recipient of dense dopamine input. Multiple cortico-basal ganglia-thalamic loops are thought to function together during the learning and performance of reinforced behaviors, with the dorsolateral circuit being particularly critical for the learning of habitual chains of action sequences. However, how this circuit works to generate such behavior is poorly understood. To explore the nature of striatal neural representations during learned action sequences, I designed a task targeted at disambiguating movement-related responses from habit representations in striatum. In combination with this task, I employed electrophysiology and optogenetics techniques to characterize task-related neuronal activity in the corticostriatal circuit. I found that, unlike in motor cortex, neurons in striatum did not respond simply to particular individual actions, but responded preferentially at the initiation and termination of learned action sequences. These experiments provide a test for the existence of a generalized striatal signal marking the start and end of units of habitual behaviors which may be produced with the contribution of striatal interneurons, providing a mechanism by which striatum can control the encoding and performance of chunked action sequences. In a separate set of experiments, I explored the effect of dopamine depletion on local field potential oscillations in the same region of striatum. My goal was to investigate the interaction between abnormal oscillations caused by dopamine depletion in Parkinson's disease and the functional task-related oscillations that normally occur in healthy striatum. Against our expectations, I found that local unilateral dopamine depletion in dorsolateral striatum did not result in changes in pre-task baseline strength of oscillations, but rather in the overexpression of the normal task-related oscillations. These studies add support to theories of striatal function and dysfunction that emphasize selective network modulation by learned behaviors."
}, {
    "id": "oai:dspace.mit.edu:1721.1/86285",
    "title": "Independent two-color optogenetic excitation of neural populations",
    "abstract": "The optical modulation of neurons with channelrhodopsins, a class of genetically encoded light-gated ion channels, has enabled the spatiotemporally precise interrogation of the roles individual cell types play in neural circuit dynamics. A topic of great interest to the neuroscience community is the independent optical excitation of two distinct neuron populations with different wavelengths, which would enable the interrogation of emergent phenomena such as circuit dynamics, plasticity, and neuromodulation. Previous implementations have focused on maximizing spectral separation by driving one channelrhodopsin in the violet (405 nm) and the other in the yellow (590 nm), yet it has not been possible to achieve independent violet excitation without eliciting spikes from both populations, due to the intrinsic UV-blue light sensitivity of the retinal chromophore. This thesis designs and implements an improved two-color excitation scheme where effective light sensitivity is utilized to achieve independent optical excitation in blue (470 nm) and red (625 nm) channels. Zero post-synaptic crosstalk is demonstrated in acute murine slice, using two novel channeirhodopsins identified from a systematic screen of 80 naturally occurring, previously uncharacterized opsins in primary neuron culture. Gene88 is the first known yellow-peaked channelrhodopsin, with a peak 45 nm more red-shifted than any previous channelrhodopsin, while Gene90 has the fastest channel turn on, turn off, and recovery kinetics of any known channelrhodopsin. These opsins' novel properties enable the first known demonstration of post-synaptic crosstalk-free two-color excitation with temporally precise modulation of spatially inseparable neuron populations.",
    "advisors": ["Edward S. Boyden, III"],
    "text": "Independent two-color optogenetic excitation of neural populations The optical modulation of neurons with channelrhodopsins, a class of genetically encoded light-gated ion channels, has enabled the spatiotemporally precise interrogation of the roles individual cell types play in neural circuit dynamics. A topic of great interest to the neuroscience community is the independent optical excitation of two distinct neuron populations with different wavelengths, which would enable the interrogation of emergent phenomena such as circuit dynamics, plasticity, and neuromodulation. Previous implementations have focused on maximizing spectral separation by driving one channelrhodopsin in the violet (405 nm) and the other in the yellow (590 nm), yet it has not been possible to achieve independent violet excitation without eliciting spikes from both populations, due to the intrinsic UV-blue light sensitivity of the retinal chromophore. This thesis designs and implements an improved two-color excitation scheme where effective light sensitivity is utilized to achieve independent optical excitation in blue (470 nm) and red (625 nm) channels. Zero post-synaptic crosstalk is demonstrated in acute murine slice, using two novel channeirhodopsins identified from a systematic screen of 80 naturally occurring, previously uncharacterized opsins in primary neuron culture. Gene88 is the first known yellow-peaked channelrhodopsin, with a peak 45 nm more red-shifted than any previous channelrhodopsin, while Gene90 has the fastest channel turn on, turn off, and recovery kinetics of any known channelrhodopsin. These opsins' novel properties enable the first known demonstration of post-synaptic crosstalk-free two-color excitation with temporally precise modulation of spatially inseparable neuron populations."
}, {
    "id": "oai:dspace.mit.edu:1721.1/109020",
    "title": "Gamma frequency entrainment attenuates amyloid load and modifies microglia",
    "abstract": "Gamma oscillations (20-50 Hz), a common local field potential signature in many brain regions, are generated by a resonant circuit between fast-spiking (FS)-parvalbumin (PV)-interneurons and pyramidal cells. Changes in gamma oscillations have been observed in several neurological disorders. However, the relationship between gamma oscillations and cellular pathologies of these disorders is unclear. Here, we investigated this relationship using the 5XFAD mouse model of Alzheimer's disease (AD) and found reduced behaviorally driven gamma activity before the onset of plaque formation or evidence of cognitive decline. Because of the early onset of gamma deficits, we aimed to determine if exogenous gamma manipulations could influence disease pathology progression. We discovered that optogenetically driving FS-PV-interneurons at gamma frequency (40 Hz) reduced levels of amyloid-[beta] (A[beta])- and A[beta] - isoforms in the hippocampus of 5XFAD mice. Neither driving FS-PV-interneurons at other frequencies, nor driving excitatory neurons, reduced A[beta] levels. Furthermore, driving FS-PV-interneurons at 40 Hz reduced enlarged endosomes and amyloid precursor protein (APP) cleavage intermediates in hippocampus. Gene expression profiling revealed an induction of microglia specific genes associated with morphological transformation of microglia and increased A[beta] phagocytosis by microglia. Inspired by these observations, we designed a non-invasive light-flickering paradigm that induced 40 Hz activity in visual cortex. The light-flickering paradigm profoundly reduced A[beta]- and A[beta]- levels in the visual cortex of pre-depositing mice and mitigated plaque load in aged, depositing mice. A GABAA antagonist completely blocked this effect; further evidence that GABAergic signaling is essential for this neuroprotective gamma activity. Finally, we showed that 40 Hz activity reduced tau phosphorylation in the TauP301S mouse model. Overall, our findings uncover a previously unappreciated function of the brain's gamma rhythms in neuroprotection by recruiting both neuronal and glial responses to mitigate AD-associated pathology.",
    "advisors": ["Li-Huei Tsai"],
    "text": "Gamma frequency entrainment attenuates amyloid load and modifies microglia Gamma oscillations (20-50 Hz), a common local field potential signature in many brain regions, are generated by a resonant circuit between fast-spiking (FS)-parvalbumin (PV)-interneurons and pyramidal cells. Changes in gamma oscillations have been observed in several neurological disorders. However, the relationship between gamma oscillations and cellular pathologies of these disorders is unclear. Here, we investigated this relationship using the 5XFAD mouse model of Alzheimer's disease (AD) and found reduced behaviorally driven gamma activity before the onset of plaque formation or evidence of cognitive decline. Because of the early onset of gamma deficits, we aimed to determine if exogenous gamma manipulations could influence disease pathology progression. We discovered that optogenetically driving FS-PV-interneurons at gamma frequency (40 Hz) reduced levels of amyloid-[beta] (A[beta])- and A[beta] - isoforms in the hippocampus of 5XFAD mice. Neither driving FS-PV-interneurons at other frequencies, nor driving excitatory neurons, reduced A[beta] levels. Furthermore, driving FS-PV-interneurons at 40 Hz reduced enlarged endosomes and amyloid precursor protein (APP) cleavage intermediates in hippocampus. Gene expression profiling revealed an induction of microglia specific genes associated with morphological transformation of microglia and increased A[beta] phagocytosis by microglia. Inspired by these observations, we designed a non-invasive light-flickering paradigm that induced 40 Hz activity in visual cortex. The light-flickering paradigm profoundly reduced A[beta]- and A[beta]- levels in the visual cortex of pre-depositing mice and mitigated plaque load in aged, depositing mice. A GABAA antagonist completely blocked this effect; further evidence that GABAergic signaling is essential for this neuroprotective gamma activity. Finally, we showed that 40 Hz activity reduced tau phosphorylation in the TauP301S mouse model. Overall, our findings uncover a previously unappreciated function of the brain's gamma rhythms in neuroprotection by recruiting both neuronal and glial responses to mitigate AD-associated pathology."
}, {
    "id": "oai:dspace.mit.edu:1721.1/65287",
    "title": "Mechanisms of ocular dominance plasticity in the juvenile and adult mouse visual cortex",
    "abstract": "Ocular dominance (OD) plasticity is a classic example of bidirectional experience-dependent plasticity in the primary visual cortex. This form of plasticity is most robust during early postnatal development (termed the \"critical period\"), when monocular deprivation (MD) leads to a rapid weakening of responses evoked through the deprived eye followed by a delayed strengthening of non-deprived eye inputs. It has been proposed that these bidirectional changes occur as a three-stage process: first, degradation of patterned visual input weakens deprived-eye responses via homosynaptic long-term depression (LTD); this is accompanied by a shift in the plasticity modification threshold (0m) that determines the direction of synaptic plasticity, such that synaptic strengthening is favored over synaptic weakening; finally, weak open-eye responses are strengthened via the mechanisms of homosynaptic long-term potentiation (LTP). Despite the growing evidence supporting this model of experience-dependent synaptic modification, the exact molecular and synaptic mechanisms that are responsible for these processes remain controversial. In my thesis work, I address three questions. First, I attempt to parse the relative contribution of excitatory and inhibitory processes to expression of the OD shift in order to understand how deprived-eye depression is expressed in the cortex. To address this, I first induce a shift in OD with 3 days of MD and then use several pharmacological methods to shut off cortical inhibitory synaptic transmission. I demonstrate that rapid deprived-eye depression is strongly expressed at excitatory thalamocortical synapses without any influences of polysynaptic intracortical inhibition. In the second part of my work, I try to resolve the nature/identity of the molecular mechanism that underlies the regulation of [theta]m. Using a transgenic mouse model, I find that a reduction in the NR2A/B subunit ratio of the N-methyl-d-aspartate (NMDA) receptor during MD alters the qualities of OD plasticity by impairing weakening of deprived-eye inputs and enhancing strengthening of open-eye inputs. These findings suggest that NMDAR subunit composition may specify the value and the rate of adjustment of synaptic 0m, which in turn determines the bidirectional cortical response to MD. The final portion of my thesis addresses the factors that limit OD plasticity beyond the critical period. I test the hypothesis that the developmental increase in intracortical GABAergic inhibitory synaptic transmission is a fundamental restricting factor for adult cortical plasticity and demonstrate that parvalbumin-expressing fast-spiking basket cells are specifically implicated in the absence of juvenile-like deprived-eye depression in adult mice.",
    "advisors": ["Mark F. Bear"],
    "text": "Mechanisms of ocular dominance plasticity in the juvenile and adult mouse visual cortex Ocular dominance (OD) plasticity is a classic example of bidirectional experience-dependent plasticity in the primary visual cortex. This form of plasticity is most robust during early postnatal development (termed the \"critical period\"), when monocular deprivation (MD) leads to a rapid weakening of responses evoked through the deprived eye followed by a delayed strengthening of non-deprived eye inputs. It has been proposed that these bidirectional changes occur as a three-stage process: first, degradation of patterned visual input weakens deprived-eye responses via homosynaptic long-term depression (LTD); this is accompanied by a shift in the plasticity modification threshold (0m) that determines the direction of synaptic plasticity, such that synaptic strengthening is favored over synaptic weakening; finally, weak open-eye responses are strengthened via the mechanisms of homosynaptic long-term potentiation (LTP). Despite the growing evidence supporting this model of experience-dependent synaptic modification, the exact molecular and synaptic mechanisms that are responsible for these processes remain controversial. In my thesis work, I address three questions. First, I attempt to parse the relative contribution of excitatory and inhibitory processes to expression of the OD shift in order to understand how deprived-eye depression is expressed in the cortex. To address this, I first induce a shift in OD with 3 days of MD and then use several pharmacological methods to shut off cortical inhibitory synaptic transmission. I demonstrate that rapid deprived-eye depression is strongly expressed at excitatory thalamocortical synapses without any influences of polysynaptic intracortical inhibition. In the second part of my work, I try to resolve the nature/identity of the molecular mechanism that underlies the regulation of [theta]m. Using a transgenic mouse model, I find that a reduction in the NR2A/B subunit ratio of the N-methyl-d-aspartate (NMDA) receptor during MD alters the qualities of OD plasticity by impairing weakening of deprived-eye inputs and enhancing strengthening of open-eye inputs. These findings suggest that NMDAR subunit composition may specify the value and the rate of adjustment of synaptic 0m, which in turn determines the bidirectional cortical response to MD. The final portion of my thesis addresses the factors that limit OD plasticity beyond the critical period. I test the hypothesis that the developmental increase in intracortical GABAergic inhibitory synaptic transmission is a fundamental restricting factor for adult cortical plasticity and demonstrate that parvalbumin-expressing fast-spiking basket cells are specifically implicated in the absence of juvenile-like deprived-eye depression in adult mice."
}, {
    "id": "oai:dspace.mit.edu:1721.1/103203",
    "title": "The Discovery of perceptual structure from visual co-occurrences in space and time",
    "abstract": "Although impressionists assure us that the world is just dabs of light, we cannot help but see surfaces and contours, objects and events. How can a visual system learn to organize pixels into these higher-level structures? In this thesis I argue that perceptual organization reflects statistical regularities in the environment. When visual primitives occur together much more often than one would expect by chance, we may learn to associate those primitives and to form a perceptual group. The first half of the thesis deals with the identification of such groups at the pixel level. I show that low-level image statistics are surprisingly effective at higher-level segmentation. I present an algorithm that groups pixels by identifying meaningful co-occurrences in an image's color statistics. Consider a zebra. Black-next-to-white occurs suspiciously often, hinting that these colors have a common cause. I model these co-occurrences using pointwise mutual information (PMI). If the PMI between two colors is high, then the colors probably belong to the same object. Grouping pixels with high PMI reveals object segments. Separating pixels with low PMI marks perceived boundaries. If simple color co-occurrences can tell us about object segments, what might more complex statistics tell us? The second half of the thesis investigates high dimensional visual data, such as image patches and video frames. In high dimensions, it is intractable to directly model co-occurrences. Instead, I show that modeling PMI can be posed as a simpler binary classification problem in which the goal is to predict if two primitives occur in the same spatial or temporal context. This allows us to model PMI associations between complex inputs. I demonstrate the effectiveness of this approach on three domains: discovering objects by associating image patches, discovering movie scenes by associating frames, and discovering place categories by associating geotagged photos. Together, these results shed light on how a visual system can learn to organize raw sensory input into meaningful percepts.",
    "advisors": ["Edward H. Adelson"],
    "text": "The Discovery of perceptual structure from visual co-occurrences in space and time Although impressionists assure us that the world is just dabs of light, we cannot help but see surfaces and contours, objects and events. How can a visual system learn to organize pixels into these higher-level structures? In this thesis I argue that perceptual organization reflects statistical regularities in the environment. When visual primitives occur together much more often than one would expect by chance, we may learn to associate those primitives and to form a perceptual group. The first half of the thesis deals with the identification of such groups at the pixel level. I show that low-level image statistics are surprisingly effective at higher-level segmentation. I present an algorithm that groups pixels by identifying meaningful co-occurrences in an image's color statistics. Consider a zebra. Black-next-to-white occurs suspiciously often, hinting that these colors have a common cause. I model these co-occurrences using pointwise mutual information (PMI). If the PMI between two colors is high, then the colors probably belong to the same object. Grouping pixels with high PMI reveals object segments. Separating pixels with low PMI marks perceived boundaries. If simple color co-occurrences can tell us about object segments, what might more complex statistics tell us? The second half of the thesis investigates high dimensional visual data, such as image patches and video frames. In high dimensions, it is intractable to directly model co-occurrences. Instead, I show that modeling PMI can be posed as a simpler binary classification problem in which the goal is to predict if two primitives occur in the same spatial or temporal context. This allows us to model PMI associations between complex inputs. I demonstrate the effectiveness of this approach on three domains: discovering objects by associating image patches, discovering movie scenes by associating frames, and discovering place categories by associating geotagged photos. Together, these results shed light on how a visual system can learn to organize raw sensory input into meaningful percepts."
}, {
    "id": "oai:dspace.mit.edu:1721.1/120628",
    "title": "Dopaminergic modulation of prefrontal cortex subpopulations",
    "abstract": "Despite abundant evidence that dopamine modulates medial prefrontal cortex (mPFC) activity to mediate diverse behavioral functions, the precise circuit computations remain elusive. One potentially unifying theoretical model by which dopamine can modulate functions from working memory to schizophrenia is that dopamine serves to increase the signal-to-noise ratio in mPFC neurons, where neuronal activity conveying sensory information (signal) are amplified relative to spontaneous firing (noise). To connect theory to biology, we lack direct evidence for dopaminergic modulation of signal-to-noise in neuronal firing patterns in vivo and a mechanistic explanation of how such computations would be transmitted downstream to instruct specific behavioral functions. Here, we demonstrate that dopamine increases signal-to-noise ratio in mPFC neurons projecting to the dorsal periaqueductal gray (dPAG) during the processing of an aversive stimulus. First, using electrochemical approaches, we reveal the precise time course of tail pinch-evoked dopamine release in the mPFC. Second, we show that dopamine signaling in the mPFC biases behavioral responses to punishment-predictive stimuli, rather than reward-predictive cues. Third, in contrast to the well-characterized mPFC-NAc projection, we show that activation of mPFC-dPAG neurons is sufficient to drive place avoidance and defensive behaviors. Fourth, to determine the natural dynamics of individual mPFC neurons, we performed single-cell projection-defined microendoscopic calcium imaging to reveal a robust preferential excitation of mPFC-dPAG, but not mPFC-NAc, neurons to aversive stimuli. Finally, photostimulation of VTA dopamine terminals in the mPFC revealed an increase in signal-to-noise ratio in mPFC-dPAG neuronal activity during the processing of aversive, but not rewarding stimuli. Together, these data unveil the utility of dopamine in the mPFC to effectively filter sensory information in a valence-specific manner.",
    "advisors": ["Kay M. Tye"],
    "text": "Dopaminergic modulation of prefrontal cortex subpopulations Despite abundant evidence that dopamine modulates medial prefrontal cortex (mPFC) activity to mediate diverse behavioral functions, the precise circuit computations remain elusive. One potentially unifying theoretical model by which dopamine can modulate functions from working memory to schizophrenia is that dopamine serves to increase the signal-to-noise ratio in mPFC neurons, where neuronal activity conveying sensory information (signal) are amplified relative to spontaneous firing (noise). To connect theory to biology, we lack direct evidence for dopaminergic modulation of signal-to-noise in neuronal firing patterns in vivo and a mechanistic explanation of how such computations would be transmitted downstream to instruct specific behavioral functions. Here, we demonstrate that dopamine increases signal-to-noise ratio in mPFC neurons projecting to the dorsal periaqueductal gray (dPAG) during the processing of an aversive stimulus. First, using electrochemical approaches, we reveal the precise time course of tail pinch-evoked dopamine release in the mPFC. Second, we show that dopamine signaling in the mPFC biases behavioral responses to punishment-predictive stimuli, rather than reward-predictive cues. Third, in contrast to the well-characterized mPFC-NAc projection, we show that activation of mPFC-dPAG neurons is sufficient to drive place avoidance and defensive behaviors. Fourth, to determine the natural dynamics of individual mPFC neurons, we performed single-cell projection-defined microendoscopic calcium imaging to reveal a robust preferential excitation of mPFC-dPAG, but not mPFC-NAc, neurons to aversive stimuli. Finally, photostimulation of VTA dopamine terminals in the mPFC revealed an increase in signal-to-noise ratio in mPFC-dPAG neuronal activity during the processing of aversive, but not rewarding stimuli. Together, these data unveil the utility of dopamine in the mPFC to effectively filter sensory information in a valence-specific manner."
}, {
    "id": "oai:dspace.mit.edu:1721.1/30112",
    "title": "Human visual perception under real-world illumination",
    "abstract": "How does the visual system achieve stable estimates of surface properties - such as reflectance and 3D shape - across changes in the illumination? Under arbitrary patterns of illumination this problem is ill-posed. However, in the real world, illumination is not arbitrary. Here I argue that the visual system exploits the statistical regularities of real-world illuminations to achieve stable estimates of shape and surface reflectance properties. Specifically, I suggest that the visual system derives measurements from specular reflections that are (i) diagnostic of surface properties and (ii) relatively well-conserved across real-world scenes. One consequence of the theory is that the visual system does not have to estimate and explicitly 'discount' illumination to recover shape and surface reflectance. In support of this idea, subjects are shown to be good at estimating surface reflectance and 3D shape without any context to specify the surrounding scene, as long as the illumination is realistic. However, when the pattern of illumination is unrealistic, shape and surface reflectance estimation degrade in predictable ways. Systematic manipulation of illumination statistics reveals some properties of illumination that are important for surface reflectance estimation. To understand 3D shape constancy, I discuss the way that 3D surface curvature distorts the reflected world. For the special case of mirrored surfaces, I show how populations of oriented linear filters can 'read' the pattern of distortions to recover 3D surface curvatures. Finally I show that this principle applies to cases other than perfect mirrors, and can predict both successes and failures of human shape constancy as the illumination changes.",
    "advisors": ["Edward H. Adelson"],
    "text": "Human visual perception under real-world illumination How does the visual system achieve stable estimates of surface properties - such as reflectance and 3D shape - across changes in the illumination? Under arbitrary patterns of illumination this problem is ill-posed. However, in the real world, illumination is not arbitrary. Here I argue that the visual system exploits the statistical regularities of real-world illuminations to achieve stable estimates of shape and surface reflectance properties. Specifically, I suggest that the visual system derives measurements from specular reflections that are (i) diagnostic of surface properties and (ii) relatively well-conserved across real-world scenes. One consequence of the theory is that the visual system does not have to estimate and explicitly 'discount' illumination to recover shape and surface reflectance. In support of this idea, subjects are shown to be good at estimating surface reflectance and 3D shape without any context to specify the surrounding scene, as long as the illumination is realistic. However, when the pattern of illumination is unrealistic, shape and surface reflectance estimation degrade in predictable ways. Systematic manipulation of illumination statistics reveals some properties of illumination that are important for surface reflectance estimation. To understand 3D shape constancy, I discuss the way that 3D surface curvature distorts the reflected world. For the special case of mirrored surfaces, I show how populations of oriented linear filters can 'read' the pattern of distortions to recover 3D surface curvatures. Finally I show that this principle applies to cases other than perfect mirrors, and can predict both successes and failures of human shape constancy as the illumination changes."
}, {
    "id": "oai:dspace.mit.edu:1721.1/46816",
    "title": "The involvement of the primate frontal cortex-basal ganglia system in arbitrary visuomotor association learning",
    "abstract": "It is the goal of this thesis to examine the frontal cortex-basal ganglia system during arbitrary visuomotor association learning, the forming of arbitrary links between visual stimuli and motor responses (e.g. red means stop), a fundamental learning process that underlies much of our complex behavior such as written language. The experiments contained in this thesis investigate the involvement of four components of this system in the acquisition of these associations: dorsolateral prefrontal cortex (dlPFC), caudate nucleus (Cd), frontal eye field (FEF), and the internal segment of the globus pallidus (GPi). Extracellular electrophysiological recordings were performed in awake-behaving primates performing three different learning tasks. In the different behavioral paradigms used in these studies, learning with and without reversals is investigated and compared both directly within the same experiment and indirectly across experiments. The results of these studies suggest that a complex interplay between brain areas in the frontal cortex-basal ganglia system exists. The study of FEF during reversal learning revealed that FEF contains task-related information from the start of learning, suggesting that it may be passing information onto PFC and Cd to aid the learning process. In addition, GPi is shown to contain more specific information about the learned association during the reversal task, providing evidence for an increase in the complexity of information processing through the basal ganglia.",
    "advisors": ["Earl K. Miller"],
    "text": "The involvement of the primate frontal cortex-basal ganglia system in arbitrary visuomotor association learning It is the goal of this thesis to examine the frontal cortex-basal ganglia system during arbitrary visuomotor association learning, the forming of arbitrary links between visual stimuli and motor responses (e.g. red means stop), a fundamental learning process that underlies much of our complex behavior such as written language. The experiments contained in this thesis investigate the involvement of four components of this system in the acquisition of these associations: dorsolateral prefrontal cortex (dlPFC), caudate nucleus (Cd), frontal eye field (FEF), and the internal segment of the globus pallidus (GPi). Extracellular electrophysiological recordings were performed in awake-behaving primates performing three different learning tasks. In the different behavioral paradigms used in these studies, learning with and without reversals is investigated and compared both directly within the same experiment and indirectly across experiments. The results of these studies suggest that a complex interplay between brain areas in the frontal cortex-basal ganglia system exists. The study of FEF during reversal learning revealed that FEF contains task-related information from the start of learning, suggesting that it may be passing information onto PFC and Cd to aid the learning process. In addition, GPi is shown to contain more specific information about the learned association during the reversal task, providing evidence for an increase in the complexity of information processing through the basal ganglia."
}, {
    "id": "oai:dspace.mit.edu:1721.1/62984",
    "title": "The design and assembly of neural circuits for vocal communication in songbirds",
    "abstract": "Unlike the human brain, which produces few neurons in adulthood, the brains of songbirds continue to produce new neurons throughout life. The function of these new neurons is not know, although it has been suggested that they endow the avian brain with a remarkable regenerative capacity that does not exist in mammals. It has also been proposed that the addition of new neurons in adulthood underlies behavioral plasticity, such as song learning. A better understanding of the cellular mechanisms that control the addition of new neurons to the postnatal brain may help clarify its biological function. This thesis is an investigation of the cell biology of postnatal neurogenesis in the songbird forebrain, with special emphasis on the High Vocal Center. Neuronal progenitors in the juvenile zebra finch brain were identified by fate mapping using engineered retroviruses. Multiple populations of neural progenitors appear to exist in the juvenile zebra finch brain, and each produces different types of neurons. At least three cell types appear to be added to the postnatal finch brain. Homology between neurogenesis in the postnatal finch and embryonic mammalian forebrain was also assessed. To characterize the mechanism of cell addition, videos were made, documenting the migration and integration of new neurons into the High Vocal Center. Neural progenitors were labeled using retroviruses, carrying the gene for the green fluorescent protein, allowing new neurons to be observed in the intact brain, with a powerful infrared laser. By replacing a small hole in the skull with a piece of optical glass, one could observe labeled neurons periodically over many days as they were born until they wired up to the existing circuitry. New neurons engaged in a previously undescribed form of migration. Further study of this form of neuron migration as well as other aspects of postnatal neurogenesis may lead to the development of strategies for replacing neurons in the human brain lost to death or disease.",
    "advisors": ["Carlos Lois"],
    "text": "The design and assembly of neural circuits for vocal communication in songbirds Unlike the human brain, which produces few neurons in adulthood, the brains of songbirds continue to produce new neurons throughout life. The function of these new neurons is not know, although it has been suggested that they endow the avian brain with a remarkable regenerative capacity that does not exist in mammals. It has also been proposed that the addition of new neurons in adulthood underlies behavioral plasticity, such as song learning. A better understanding of the cellular mechanisms that control the addition of new neurons to the postnatal brain may help clarify its biological function. This thesis is an investigation of the cell biology of postnatal neurogenesis in the songbird forebrain, with special emphasis on the High Vocal Center. Neuronal progenitors in the juvenile zebra finch brain were identified by fate mapping using engineered retroviruses. Multiple populations of neural progenitors appear to exist in the juvenile zebra finch brain, and each produces different types of neurons. At least three cell types appear to be added to the postnatal finch brain. Homology between neurogenesis in the postnatal finch and embryonic mammalian forebrain was also assessed. To characterize the mechanism of cell addition, videos were made, documenting the migration and integration of new neurons into the High Vocal Center. Neural progenitors were labeled using retroviruses, carrying the gene for the green fluorescent protein, allowing new neurons to be observed in the intact brain, with a powerful infrared laser. By replacing a small hole in the skull with a piece of optical glass, one could observe labeled neurons periodically over many days as they were born until they wired up to the existing circuitry. New neurons engaged in a previously undescribed form of migration. Further study of this form of neuron migration as well as other aspects of postnatal neurogenesis may lead to the development of strategies for replacing neurons in the human brain lost to death or disease."
}, {
    "id": "oai:dspace.mit.edu:1721.1/77843",
    "title": "Structure-function relationships in human brain development",
    "abstract": "The integration of anatomical, functional, and developmental approaches in cognitive neuroscience is essential for generating mechanistic explanations of brain function. In this thesis, I first establish a proof-of-principle that neuroanatomical connectivity, as measured with diffusion weighted imaging (DWI), can be used to calculate connectional fingerprints that are sufficient to delineate fine anatomical distinctions in the human brain (Chapter 2). Next, I describe the maturation of structural connectivity patterns by applying these connectional fingerprints to over a hundred participants ranging from five to thirty years of age, and show that these connectional patterns have different developmental trajectories (Chapter 3). I then illustrate how anatomical connections may shape (or in turn be shaped by) function and behavior, within the framework of reading ability and describe how white matter tract integrity may predict future acquisition of reading ability in children (Chapter 4). I conclude by summarizing how these experiments offer testable hypotheses of the maturation of structure and function. Studying the complex interplay between structure, function, and development will get us closer to understanding both the constraints present at birth, and the effect of experience, on the biological mechanisms underlying brain function.",
    "advisors": ["John D. E. Gabrieli"],
    "text": "Structure-function relationships in human brain development The integration of anatomical, functional, and developmental approaches in cognitive neuroscience is essential for generating mechanistic explanations of brain function. In this thesis, I first establish a proof-of-principle that neuroanatomical connectivity, as measured with diffusion weighted imaging (DWI), can be used to calculate connectional fingerprints that are sufficient to delineate fine anatomical distinctions in the human brain (Chapter 2). Next, I describe the maturation of structural connectivity patterns by applying these connectional fingerprints to over a hundred participants ranging from five to thirty years of age, and show that these connectional patterns have different developmental trajectories (Chapter 3). I then illustrate how anatomical connections may shape (or in turn be shaped by) function and behavior, within the framework of reading ability and describe how white matter tract integrity may predict future acquisition of reading ability in children (Chapter 4). I conclude by summarizing how these experiments offer testable hypotheses of the maturation of structure and function. Studying the complex interplay between structure, function, and development will get us closer to understanding both the constraints present at birth, and the effect of experience, on the biological mechanisms underlying brain function."
}, {
    "id": "oai:dspace.mit.edu:1721.1/107557",
    "title": "Learning structured representations for perception and control",
    "abstract": "I argue that the intersection of deep learning, hierarchical reinforcement learning, and generative models provides a promising avenue towards building agents that learn to produce goal-directed behavior given sensations. I present models and algorithms that learn from raw observations and will emphasize on minimizing their sample complexity and number of training steps required for convergence. To this end, I introduce hierarchical variants of deep reinforcement learning algorithms, which produce and utilize temporally extended abstractions over actions. I also present a hybrid model-free and model-based deep reinforcement learning model, which can also be potentially used to automatically extract subgoals for bootstrapping temporal abstractions. I will then present a model-based approach for perception, which unifies deep learning and probabilistic models, to learn powerful representations of images without labeled data or external rewards. Learning goal-directed behavior with sparse and delayed rewards is a fundamental challenge for reinforcement learning algorithms. The primary difficulty arises due to insufficient exploration, resulting in an agent being unable to learn robust value functions. I present the Deep Hierarchical Reinforcement Learning (h-DQN) approach, which integrates hierarchical value functions operating at different time scales, along with goal-driven intrinsically motivated behavior for efficient exploration. Intrinsically motivated agents can explore new behavior for its own sake rather than to directly solve problems. Such intrinsic behaviors could eventually help the agent solve tasks posed by the environment. h-DQN allows for flexible goal specifications, such as functions over entities and relations. This provides an efficient space for exploration in complicated environments. I will demonstrate h-DQN's ability to learn optimal behavior given raw pixels in environments with very sparse and delayed feedback. I will then introduce the Deep Successor Reinforcement (DSR) learning approach. DSR is a hybrid model-free and model-based RL algorithm. It learns the value function of a state by taking the inner product between the state's expected future feature occupancy and the corresponding immediate rewards. This factorization of the value function has several appealing properties - increased sensitivity to changes in the reward structure and potentially the ability to automatically extract subgoals for learning temporal abstractions. Finally, I argue for the need for better representations of images, both in reinforcement learning tasks and in general. Existing deep learning approaches learn useful representations given lots of labeled data or rewards. Moreover, they also lack the inductive biases needed to disentangle causal structure in images such as objects, shape, pose and other intrinsic scene properties. I present generative models of vision, often referred to as analysis-by-synthesis approaches, by combining deep generative methods with probabilistic modeling. This approach aims to learn structured representations of images given raw observations. I argue that such intermediate representations will be crucial to scale-up deep reinforcement learning algorithms, and to bridge the gap between machine and human learning.",
    "advisors": ["Joshua B. Tenenbaum"],
    "text": "Learning structured representations for perception and control I argue that the intersection of deep learning, hierarchical reinforcement learning, and generative models provides a promising avenue towards building agents that learn to produce goal-directed behavior given sensations. I present models and algorithms that learn from raw observations and will emphasize on minimizing their sample complexity and number of training steps required for convergence. To this end, I introduce hierarchical variants of deep reinforcement learning algorithms, which produce and utilize temporally extended abstractions over actions. I also present a hybrid model-free and model-based deep reinforcement learning model, which can also be potentially used to automatically extract subgoals for bootstrapping temporal abstractions. I will then present a model-based approach for perception, which unifies deep learning and probabilistic models, to learn powerful representations of images without labeled data or external rewards. Learning goal-directed behavior with sparse and delayed rewards is a fundamental challenge for reinforcement learning algorithms. The primary difficulty arises due to insufficient exploration, resulting in an agent being unable to learn robust value functions. I present the Deep Hierarchical Reinforcement Learning (h-DQN) approach, which integrates hierarchical value functions operating at different time scales, along with goal-driven intrinsically motivated behavior for efficient exploration. Intrinsically motivated agents can explore new behavior for its own sake rather than to directly solve problems. Such intrinsic behaviors could eventually help the agent solve tasks posed by the environment. h-DQN allows for flexible goal specifications, such as functions over entities and relations. This provides an efficient space for exploration in complicated environments. I will demonstrate h-DQN's ability to learn optimal behavior given raw pixels in environments with very sparse and delayed feedback. I will then introduce the Deep Successor Reinforcement (DSR) learning approach. DSR is a hybrid model-free and model-based RL algorithm. It learns the value function of a state by taking the inner product between the state's expected future feature occupancy and the corresponding immediate rewards. This factorization of the value function has several appealing properties - increased sensitivity to changes in the reward structure and potentially the ability to automatically extract subgoals for learning temporal abstractions. Finally, I argue for the need for better representations of images, both in reinforcement learning tasks and in general. Existing deep learning approaches learn useful representations given lots of labeled data or rewards. Moreover, they also lack the inductive biases needed to disentangle causal structure in images such as objects, shape, pose and other intrinsic scene properties. I present generative models of vision, often referred to as analysis-by-synthesis approaches, by combining deep generative methods with probabilistic modeling. This approach aims to learn structured representations of images given raw observations. I argue that such intermediate representations will be crucial to scale-up deep reinforcement learning algorithms, and to bridge the gap between machine and human learning."
}, {
    "id": "oai:dspace.mit.edu:1721.1/57795",
    "title": "Changes in the firing patterns in neurons of the sensorimotor striatum during learning : what changes and why",
    "abstract": "The basal ganglia, and specifically the sensorimotor (dorsolateral) striatum, have been implicated in stimulus-response learning. Here, I analyze the role the striatum plays in learning. We recorded from neurons of the sensorimotor striatum as rats learn, are over-trained, are extinguished, and are re-trained on a discriminative T-maze task. In this T-maze a gate was lowered immediately after an auditory click and the rats were allowed to proceed down the long arm of the maze. Mid-run, one of two tones was played. Rats had to choose to turn down either the left or right arm of the T-maze based on which tone was played. We discovered that population neural activity becomes restructured during learning and overtraining to emphasize the beginning and end of each trial. We also created a short-term memory version of the T-maze task by moving the location of the tone cue in order to determine if this affects the strength of the restructuring seen in the firing patterns of the striatum as learning progressed. Lastly, we examined the relationship the training induced pattern had to learning the tone-turn association and to other things that changed systematically throughout learning, such as speed.",
    "advisors": ["Ann M. Graybiel"],
    "text": "Changes in the firing patterns in neurons of the sensorimotor striatum during learning : what changes and why The basal ganglia, and specifically the sensorimotor (dorsolateral) striatum, have been implicated in stimulus-response learning. Here, I analyze the role the striatum plays in learning. We recorded from neurons of the sensorimotor striatum as rats learn, are over-trained, are extinguished, and are re-trained on a discriminative T-maze task. In this T-maze a gate was lowered immediately after an auditory click and the rats were allowed to proceed down the long arm of the maze. Mid-run, one of two tones was played. Rats had to choose to turn down either the left or right arm of the T-maze based on which tone was played. We discovered that population neural activity becomes restructured during learning and overtraining to emphasize the beginning and end of each trial. We also created a short-term memory version of the T-maze task by moving the location of the tone cue in order to determine if this affects the strength of the restructuring seen in the firing patterns of the striatum as learning progressed. Lastly, we examined the relationship the training induced pattern had to learning the tone-turn association and to other things that changed systematically throughout learning, such as speed."
}, {
    "id": "oai:dspace.mit.edu:1721.1/106436",
    "title": "Interrogation and control of mammalian transcription",
    "abstract": "Gene expression is dynamic in living systems, enabling environmental adaptation and homeostasis. Transcript levels may change temporarily during distinct phases of biological processes, while longer lasting modifications to their regulatory machinery can lead to specific cell states or disease phenotypes. However, versatile and robust methods to investigate causal relationships between gene expression states and biological phenotypes remain elusive. My thesis work - divided into two main parts - has focused on the development of technologies to enable efficient, generalizable, and precise perturbation of mammalian gene expression. The first part of my research focused on the development of light-inducible transcriptional effectors (LITEs) to mediate positive and negative regulation of endogenous mammalian gene expression (Konermann et al. Nature 2013). Optical stimulation enables precise spatiotemporal control to closely match endogenous transcriptional dynamics. I engineered the LITE system based on the programmable TALE DNA binding proteins from plant pathogens in combination with the light-inducible dimer cryptochrome 2 - cibi from Arabidopsis thaliana. Light enables fast and reversible recruitment of transcriptional effector domains to the TALE bound to the endogenous target promoter through dimerization of cryptochrome 2 - cibi. I applied LITEs to control gene expression in primary neurons as well as in the mouse brain in vivo, demonstrating their potential to dissect genetic contributions to dynamic behaviors such as learning. Epigenetic regulation of transcriptional state is an additional layer of endogenous control exerted by the cell to store more permanent states such as memories. To interrogate epigenetic in addition to transcriptional dynamics, I next developed TALE-mediated targeting of 32 repressive histone effectors to alter epigenetic states in a locus-specific manner. The LITE system establishes a novel mode of optogenetic control of endogenous cellular processes and enables direct testing of the causal roles of genetic and epigenetic regulation in normal biological processes and disease states. One major limiting aspect of TALE-based transcriptional activators is the costly and labor-intensive construction of their repetitive DNA binding domains. As a result, the utility of TALEs for higher-throughput gene targeting experiments remains limited. The CRISPR nuclease Cas9, however, can be easily programmed using a short guide RNA homologous to the target genomic DNA of interest. Additionally, Cas9 can be easily converted into an RNA-guided DNA binding protein (dCas9) via inactivation of its two catalytic domains. The ease and scalability of the CRISPR-Cas9 system potentially enables systematic, genome-scale perturbation, but the magnitude of transcriptional upregulation achieved by the current generation of Cas9 transcriptional activators typically ranges from low to ineffective. In order to achieve a system where the majority of Cas9 activators are highly functional, I undertook structure-guided engineering to generate a potent, sjynergistic Cas9 activation complex (SAM) capable of mediating robust upregulation with a single sgRNA (Konermann et al., Nature, 2015) which outperforms current systems by more than two orders of magnitude. I demonstrated that these transcriptional effectors are capable of activating up to 10 genes simultaneously, allowing for understanding of complex genetic and regulatory networks. Genome-scale GOF screening approaches have largely remained limited to the use of cDNA library systems, which are costly and challenging to use in a pooled format. To overcome this limitation, I designed a genome-scale sgRNA library targeting every coding isoform from the RefSeq database (23,430 isoforms) for a final library of 70,290 guides. I next aimed to identify gain-of-function changes that can lead to the development of BRAF inhibitor resistance in BRAFV 600 mutant melanoma cells. The screen results highlighted a number of gene candidates that both confirm known BRAF inhibitor-resistance pathways and suggest novel mechanisms of action. SAM activators present a highly reliable and generalizable tool for genome-wide interrogation of gene function and interaction in diverse biological processes. Recently, we have extended the utility of the SAM system to enable bimodal control through the use of modified, truncated deadRNAs (dRNAs) (Dahlman, et al., Nature Biotechnology 2015). These dRNAs prevent nucleolytic activity of an active Cas9 nuclease and transform the wildtype enzyme into an efficient transcriptional activator when combined with the SAM activator-components. This system enables simultaneous knock-out of gene A and activation of gene B in the same cell population, enabling bidirectional interrogation of gene interaction and regulatory networks.",
    "advisors": ["Feng Zhang"],
    "text": "Interrogation and control of mammalian transcription Gene expression is dynamic in living systems, enabling environmental adaptation and homeostasis. Transcript levels may change temporarily during distinct phases of biological processes, while longer lasting modifications to their regulatory machinery can lead to specific cell states or disease phenotypes. However, versatile and robust methods to investigate causal relationships between gene expression states and biological phenotypes remain elusive. My thesis work - divided into two main parts - has focused on the development of technologies to enable efficient, generalizable, and precise perturbation of mammalian gene expression. The first part of my research focused on the development of light-inducible transcriptional effectors (LITEs) to mediate positive and negative regulation of endogenous mammalian gene expression (Konermann et al. Nature 2013). Optical stimulation enables precise spatiotemporal control to closely match endogenous transcriptional dynamics. I engineered the LITE system based on the programmable TALE DNA binding proteins from plant pathogens in combination with the light-inducible dimer cryptochrome 2 - cibi from Arabidopsis thaliana. Light enables fast and reversible recruitment of transcriptional effector domains to the TALE bound to the endogenous target promoter through dimerization of cryptochrome 2 - cibi. I applied LITEs to control gene expression in primary neurons as well as in the mouse brain in vivo, demonstrating their potential to dissect genetic contributions to dynamic behaviors such as learning. Epigenetic regulation of transcriptional state is an additional layer of endogenous control exerted by the cell to store more permanent states such as memories. To interrogate epigenetic in addition to transcriptional dynamics, I next developed TALE-mediated targeting of 32 repressive histone effectors to alter epigenetic states in a locus-specific manner. The LITE system establishes a novel mode of optogenetic control of endogenous cellular processes and enables direct testing of the causal roles of genetic and epigenetic regulation in normal biological processes and disease states. One major limiting aspect of TALE-based transcriptional activators is the costly and labor-intensive construction of their repetitive DNA binding domains. As a result, the utility of TALEs for higher-throughput gene targeting experiments remains limited. The CRISPR nuclease Cas9, however, can be easily programmed using a short guide RNA homologous to the target genomic DNA of interest. Additionally, Cas9 can be easily converted into an RNA-guided DNA binding protein (dCas9) via inactivation of its two catalytic domains. The ease and scalability of the CRISPR-Cas9 system potentially enables systematic, genome-scale perturbation, but the magnitude of transcriptional upregulation achieved by the current generation of Cas9 transcriptional activators typically ranges from low to ineffective. In order to achieve a system where the majority of Cas9 activators are highly functional, I undertook structure-guided engineering to generate a potent, sjynergistic Cas9 activation complex (SAM) capable of mediating robust upregulation with a single sgRNA (Konermann et al., Nature, 2015) which outperforms current systems by more than two orders of magnitude. I demonstrated that these transcriptional effectors are capable of activating up to 10 genes simultaneously, allowing for understanding of complex genetic and regulatory networks. Genome-scale GOF screening approaches have largely remained limited to the use of cDNA library systems, which are costly and challenging to use in a pooled format. To overcome this limitation, I designed a genome-scale sgRNA library targeting every coding isoform from the RefSeq database (23,430 isoforms) for a final library of 70,290 guides. I next aimed to identify gain-of-function changes that can lead to the development of BRAF inhibitor resistance in BRAFV 600 mutant melanoma cells. The screen results highlighted a number of gene candidates that both confirm known BRAF inhibitor-resistance pathways and suggest novel mechanisms of action. SAM activators present a highly reliable and generalizable tool for genome-wide interrogation of gene function and interaction in diverse biological processes. Recently, we have extended the utility of the SAM system to enable bimodal control through the use of modified, truncated deadRNAs (dRNAs) (Dahlman, et al., Nature Biotechnology 2015). These dRNAs prevent nucleolytic activity of an active Cas9 nuclease and transform the wildtype enzyme into an efficient transcriptional activator when combined with the SAM activator-components. This system enables simultaneous knock-out of gene A and activation of gene B in the same cell population, enabling bidirectional interrogation of gene interaction and regulatory networks."
}, {
    "id": "oai:dspace.mit.edu:1721.1/46817",
    "title": "In vivo visualization of CaMKII activity in ocular dominance plasticity",
    "abstract": "Alterations in sensory experience can persistently modify the responses of cortical neurons. Ocular dominance (OD) plasticity, a process in which alternation of visual input induces a shift in cortical responsiveness, is an extensively studied model of such experience-dependent plasticity. However, the synaptic mechanisms underlying OD plasticity are not well understood. Recent studies revealed that both Hebbian and homeostatic mechanisms play a role in OD plasticity. Therefore, we were interested in monitoring the process of rapid plasticity at individual synapses in vivo to gain insight into the interplay of these two mechanisms. Ca2+/calmodulin dependent protein kinase II (CaMKII) is a major component of the postsynaptic density. Activation of CaMKII is necessary and sufficient for LTP induction, is required for OD plasticity, and its expression pattern coincides with the site of rapid plasticity in the supragranular layers II/III of the visual cortex. Moreover, CaMKII can convert transient Ca2+ influx into a prolonged biochemical process via autophosphorylation that renders CaMKII activity Ca2+ independent. Hence, CaMKII is well suited as a reporter of synaptic activity. We previously engineered a probe, Camui, which utilizes the optical phenomenon of fluorescence resonance energy transfer (FRET), to monitor CaMKII activation. This thesis embodies the work done to improve Camui to be a better tool for in vivo reporting of CaMKII activity, as well as the use of this improved probe for in vivo detection of CaMKII activity in single spines before and after 4 hrs of monocular deprivation (MD) in the ferret visual cortex.",
    "advisors": ["Yasunori Hayashi"],
    "text": "In vivo visualization of CaMKII activity in ocular dominance plasticity Alterations in sensory experience can persistently modify the responses of cortical neurons. Ocular dominance (OD) plasticity, a process in which alternation of visual input induces a shift in cortical responsiveness, is an extensively studied model of such experience-dependent plasticity. However, the synaptic mechanisms underlying OD plasticity are not well understood. Recent studies revealed that both Hebbian and homeostatic mechanisms play a role in OD plasticity. Therefore, we were interested in monitoring the process of rapid plasticity at individual synapses in vivo to gain insight into the interplay of these two mechanisms. Ca2+/calmodulin dependent protein kinase II (CaMKII) is a major component of the postsynaptic density. Activation of CaMKII is necessary and sufficient for LTP induction, is required for OD plasticity, and its expression pattern coincides with the site of rapid plasticity in the supragranular layers II/III of the visual cortex. Moreover, CaMKII can convert transient Ca2+ influx into a prolonged biochemical process via autophosphorylation that renders CaMKII activity Ca2+ independent. Hence, CaMKII is well suited as a reporter of synaptic activity. We previously engineered a probe, Camui, which utilizes the optical phenomenon of fluorescence resonance energy transfer (FRET), to monitor CaMKII activation. This thesis embodies the work done to improve Camui to be a better tool for in vivo reporting of CaMKII activity, as well as the use of this improved probe for in vivo detection of CaMKII activity in single spines before and after 4 hrs of monocular deprivation (MD) in the ferret visual cortex."
}, {
    "id": "oai:dspace.mit.edu:1721.1/120622",
    "title": "Social influences on children's learning",
    "abstract": "Adults greatly impact children's learning: they serve as models of how to behave, and as parents, provide the larger social context in which children grow up. This thesis explores how adults impact children's learning across two time scales. Chapters 2 and 3 ask how a brief exposure to an adult model impacts children's moment-to-moment approach towards learning, and Chapters 4 and 5 look at how children's long-term social context impacts their brain development and capacity to learn. In Chapter 2, I show that preschool-age children integrate information from adults' actions, outcomes, and testimony to decide how hard to try on novel tasks. Children persist the longest when adults practice what they preach: saying they value effort, or giving children a pep talk, in conjunction with demonstrating effortful success on their own task. Chapter 3 demonstrates that social learning about effort is present in the first year of life and generalizes across tasks. In Chapter 4, I find that adolescents' long-term social environments have a selective impact on neural structure and function: socioeconomic-status (SES) relates to hippocampal-prefrontal declarative memory, but not striatal-dependent procedural memory. Finally, in Chapter 5 I demonstrate that the neural correlates of fluid reasoning differ by SES, suggesting that positive brain development varies by early life environment. Collectively, this work elucidates both the malleable social factors that positively impact children's learning and the unique neural and cognitive adaptations that children develop in response to adverse environments.",
    "advisors": ["John D.E. Gabrieli", "Laura E. Schulz"],
    "text": "Social influences on children's learning Adults greatly impact children's learning: they serve as models of how to behave, and as parents, provide the larger social context in which children grow up. This thesis explores how adults impact children's learning across two time scales. Chapters 2 and 3 ask how a brief exposure to an adult model impacts children's moment-to-moment approach towards learning, and Chapters 4 and 5 look at how children's long-term social context impacts their brain development and capacity to learn. In Chapter 2, I show that preschool-age children integrate information from adults' actions, outcomes, and testimony to decide how hard to try on novel tasks. Children persist the longest when adults practice what they preach: saying they value effort, or giving children a pep talk, in conjunction with demonstrating effortful success on their own task. Chapter 3 demonstrates that social learning about effort is present in the first year of life and generalizes across tasks. In Chapter 4, I find that adolescents' long-term social environments have a selective impact on neural structure and function: socioeconomic-status (SES) relates to hippocampal-prefrontal declarative memory, but not striatal-dependent procedural memory. Finally, in Chapter 5 I demonstrate that the neural correlates of fluid reasoning differ by SES, suggesting that positive brain development varies by early life environment. Collectively, this work elucidates both the malleable social factors that positively impact children's learning and the unique neural and cognitive adaptations that children develop in response to adverse environments."
}, {
    "id": "oai:dspace.mit.edu:1721.1/33171",
    "title": "Remembering the past : multimodal imaging of cortical contributions to episodic retrieval",
    "abstract": "What is the nature of the neural processes that allow humans to remember past events? The theoretical framework adopted in this thesis builds upon cognitive models that suggest that episodic retrieval can be decomposed into two classes of computations: (1) recovery processes that serve to reactivate stored memories, making information from a past episode readily available, and (2) control processes that serve to guide the retrieval attempt and monitor/evaluate information arising from the recovery processes. A multimodal imaging approach that combined fMRI and MEG was adopted to gain insight into the spatial and temporal brain mechanisms supporting episodic retrieval. Chapter 1 reviews major findings and theories in the episodic retrieval literature grounding the open questions and controversies within the suggested framework. Chapter 2 describes an fMRI and MEG experiment that identified medial temporal cortical structures that signal item memory strength, thus supporting the perception of item familiarity. Chapter 3 describes an fMRI experiment that demonstrated that retrieval of contextual details involves reactivation of neural patterns engaged at encoding.",
    "advisors": ["Anthony D. Wagner"],
    "text": "Remembering the past : multimodal imaging of cortical contributions to episodic retrieval What is the nature of the neural processes that allow humans to remember past events? The theoretical framework adopted in this thesis builds upon cognitive models that suggest that episodic retrieval can be decomposed into two classes of computations: (1) recovery processes that serve to reactivate stored memories, making information from a past episode readily available, and (2) control processes that serve to guide the retrieval attempt and monitor/evaluate information arising from the recovery processes. A multimodal imaging approach that combined fMRI and MEG was adopted to gain insight into the spatial and temporal brain mechanisms supporting episodic retrieval. Chapter 1 reviews major findings and theories in the episodic retrieval literature grounding the open questions and controversies within the suggested framework. Chapter 2 describes an fMRI and MEG experiment that identified medial temporal cortical structures that signal item memory strength, thus supporting the perception of item familiarity. Chapter 3 describes an fMRI experiment that demonstrated that retrieval of contextual details involves reactivation of neural patterns engaged at encoding."
}, {
    "id": "oai:dspace.mit.edu:1721.1/61878",
    "title": "Analysis of neural circuits in vitro",
    "abstract": "This thesis is a collection of manuscripts addressing connectivity of neural circuits in cultured hippocampal neurons. These studies begin with an investigation of dopaminergic modulation of excitatory synapses in small circuits of neurons grown on glial micro islands. We found that dopamine transiently depressed excitatory synaptic transmission. Scaling up to larger circuits of neurons proved more challenging, since finding connected pairs became combinatorially more improbable. The discovery and use of light-activatable ion channel channel rhodopsin-2 (ChR2) promised to revolutionize the way in which we could map connectivity in vitro. We successfully delivered the gene for ChR2 in hippocampal cultures using recombinant adeno-associated virus and characterized the spatial resolution, as well as the reliability of stimulating action potentials. However, there were limitations to this technique that would render circuit maps ambiguous and incomplete. More recently, the engineering of rabies virus (RV) as a neural circuit tracer has produced an exciting method whereby viral infection can be targeted to a population of neurons and spread of the virus restricted to monosynaptically connected neurons. We further investigated potential mechanisms for previous observations which claim that RV spread is restricted to synaptically connected neurons by manipulating neural activity and synaptic vesicle release. We found that RV spread increased for blockade of synaptic vesicle exocytosis and for blockade of neural activity. The underlying premise for pursuing these methods to elucidate connectivity is that the computational power of the brain comes from changeable, malleable connectivity and that to test network models of computation in a biological brain, we must map the connectivity between individual neurons. This thesis builds a framework for experiments designed to bridge the gap between computational learning theories and networks of live neurons.",
    "advisors": ["H. Sebastian Seung"],
    "text": "Analysis of neural circuits in vitro This thesis is a collection of manuscripts addressing connectivity of neural circuits in cultured hippocampal neurons. These studies begin with an investigation of dopaminergic modulation of excitatory synapses in small circuits of neurons grown on glial micro islands. We found that dopamine transiently depressed excitatory synaptic transmission. Scaling up to larger circuits of neurons proved more challenging, since finding connected pairs became combinatorially more improbable. The discovery and use of light-activatable ion channel channel rhodopsin-2 (ChR2) promised to revolutionize the way in which we could map connectivity in vitro. We successfully delivered the gene for ChR2 in hippocampal cultures using recombinant adeno-associated virus and characterized the spatial resolution, as well as the reliability of stimulating action potentials. However, there were limitations to this technique that would render circuit maps ambiguous and incomplete. More recently, the engineering of rabies virus (RV) as a neural circuit tracer has produced an exciting method whereby viral infection can be targeted to a population of neurons and spread of the virus restricted to monosynaptically connected neurons. We further investigated potential mechanisms for previous observations which claim that RV spread is restricted to synaptically connected neurons by manipulating neural activity and synaptic vesicle release. We found that RV spread increased for blockade of synaptic vesicle exocytosis and for blockade of neural activity. The underlying premise for pursuing these methods to elucidate connectivity is that the computational power of the brain comes from changeable, malleable connectivity and that to test network models of computation in a biological brain, we must map the connectivity between individual neurons. This thesis builds a framework for experiments designed to bridge the gap between computational learning theories and networks of live neurons."
}, {
    "id": "oai:dspace.mit.edu:1721.1/8187",
    "title": "Understanding the role of referential processing in sentence complexity",
    "abstract": "Language comprehension requires syntactic, semantic and pragmatic processing. The work presented in this thesis clarifies the role that the resource demands of syntactic and referential processing play in sentence complexity. Results are interpreted within the framework of the Dependency Locality Theory (Gibson, 1998), which provides a hypothesis about how computational resources constrain the process of sentence comprehension. These new results support and further develop the DLT's discourse-based distance metric for computing locality. The experiments presented here were designed to investigate the referential processing load imposed by relating noun phrase (NP) anaphors to their antecedents and to discover the ramifications of increased referential processing load on behavioral measures of language comprehension. Four questionnaire experiments tested the intuitive complexity of doubly nested sentences containing NPs that were differently referentially accessible. These experiments demonstrated that sentences with structural dependencies crossing less accessible referents are judged more difficult than sentences with structural dependencies crossing more accessible referents. They also showed that referential accessibility manipulations had a negligible effect on intuitive complexity in positions that did not interrupt long distance structural dependencies.",
    "advisors": ["Edward A.F. Gibson"],
    "text": "Understanding the role of referential processing in sentence complexity Language comprehension requires syntactic, semantic and pragmatic processing. The work presented in this thesis clarifies the role that the resource demands of syntactic and referential processing play in sentence complexity. Results are interpreted within the framework of the Dependency Locality Theory (Gibson, 1998), which provides a hypothesis about how computational resources constrain the process of sentence comprehension. These new results support and further develop the DLT's discourse-based distance metric for computing locality. The experiments presented here were designed to investigate the referential processing load imposed by relating noun phrase (NP) anaphors to their antecedents and to discover the ramifications of increased referential processing load on behavioral measures of language comprehension. Four questionnaire experiments tested the intuitive complexity of doubly nested sentences containing NPs that were differently referentially accessible. These experiments demonstrated that sentences with structural dependencies crossing less accessible referents are judged more difficult than sentences with structural dependencies crossing more accessible referents. They also showed that referential accessibility manipulations had a negligible effect on intuitive complexity in positions that did not interrupt long distance structural dependencies."
}, {
    "id": "oai:dspace.mit.edu:1721.1/81090",
    "title": "Endogenous control of stochastic gene expression in the development of Caenorhabditis elegans",
    "abstract": "Studies in the past decade have established gene expression as an inherently variable process. Accompanying this exciting finding is a fundamental question: how do physiological events, such as cell fate specification, proceed so robustly in the face of gene expression variability? In this thesis, I took a fresh attack at this question by examining the control of variability in the context of the stereotyped development of the nematode C. elegans. Specifically, I focused on the regulation of a Hox gene by the Wnt signaling pathway in a single C. elegans neuroblast. Analogous to vertebrate neural crest cells, Hox gene expression determines the migratory direction and the subsequent fate choices of cells that descend from the original neuroblast. Intrigued by the earlier observation that perturbation to Wnt signaling disrupts the wild-type stereotypy in migratory decision, I speculated that variable gene expression may underlie the partial penetrance in the mutants and subsequently questioned what mechanism safeguards against variability in the wild type. Combining single-cell transcript counting with genetic manipulation, I quantified the variability in Hox gene expression in the Q neuroblasts in both the wild type and a series of Wnt signaling mutants. Interestingly, I observed increased expression variability in a number of mutants and an overall complex relationship between expression variability and mean expression level. Distinct features in the gene expression profile embarked me on a search for network interactions, leading to the discovery of multiple novel feedback loops within the Wnt pathway. Applying computational network inference, I revealed a network of interlocking positive and negative feedback loops, which I subsequently show to have a topological advantage in dampening stochastic noise in gene expression.",
    "advisors": ["Alexander van Oudenaarden"],
    "text": "Endogenous control of stochastic gene expression in the development of Caenorhabditis elegans Studies in the past decade have established gene expression as an inherently variable process. Accompanying this exciting finding is a fundamental question: how do physiological events, such as cell fate specification, proceed so robustly in the face of gene expression variability? In this thesis, I took a fresh attack at this question by examining the control of variability in the context of the stereotyped development of the nematode C. elegans. Specifically, I focused on the regulation of a Hox gene by the Wnt signaling pathway in a single C. elegans neuroblast. Analogous to vertebrate neural crest cells, Hox gene expression determines the migratory direction and the subsequent fate choices of cells that descend from the original neuroblast. Intrigued by the earlier observation that perturbation to Wnt signaling disrupts the wild-type stereotypy in migratory decision, I speculated that variable gene expression may underlie the partial penetrance in the mutants and subsequently questioned what mechanism safeguards against variability in the wild type. Combining single-cell transcript counting with genetic manipulation, I quantified the variability in Hox gene expression in the Q neuroblasts in both the wild type and a series of Wnt signaling mutants. Interestingly, I observed increased expression variability in a number of mutants and an overall complex relationship between expression variability and mean expression level. Distinct features in the gene expression profile embarked me on a search for network interactions, leading to the discovery of multiple novel feedback loops within the Wnt pathway. Applying computational network inference, I revealed a network of interlocking positive and negative feedback loops, which I subsequently show to have a topological advantage in dampening stochastic noise in gene expression."
}, {
    "id": "oai:dspace.mit.edu:1721.1/79184",
    "title": "Selectivity and development of the visual word form area",
    "abstract": "An area of left occipitotemporal cortex commonly referred to as the visual word form area (VWFA), has consistently been shown to activate during the processing of written language. However, the exact nature of the region's selectivity is still under debate. In this thesis, I explore the selectivity of the visual word form area at three different levels. First, I examine whether the VWFA differentiates between letter strings of different lexicality and pronounceability and argue that the VWFA's selectivity is greatly influenced by attention. Second, I explore the developmental course of mirror discrimination in the VWFA, and show that children do not display adult-like mirror discrimination of letters even into early adolescence. Finally, I look at the developmental course of VWFA selectivity for words compared to nonlinguistic visual stimuli. While children have adult-like activation patterns when words are compared to a low-level visual control, they show less specialization compared to adults when objects are used as a control.",
    "advisors": ["John D.E. Gabrieli"],
    "text": "Selectivity and development of the visual word form area An area of left occipitotemporal cortex commonly referred to as the visual word form area (VWFA), has consistently been shown to activate during the processing of written language. However, the exact nature of the region's selectivity is still under debate. In this thesis, I explore the selectivity of the visual word form area at three different levels. First, I examine whether the VWFA differentiates between letter strings of different lexicality and pronounceability and argue that the VWFA's selectivity is greatly influenced by attention. Second, I explore the developmental course of mirror discrimination in the VWFA, and show that children do not display adult-like mirror discrimination of letters even into early adolescence. Finally, I look at the developmental course of VWFA selectivity for words compared to nonlinguistic visual stimuli. While children have adult-like activation patterns when words are compared to a low-level visual control, they show less specialization compared to adults when objects are used as a control."
}, {
    "id": "oai:dspace.mit.edu:1721.1/42082",
    "title": "Comparison of frontal and parietal cortices in the control of visual attention",
    "abstract": "The ability to switch between tasks reflects a fundamental part of our intelligence. A foundation of this ability lies in perceiving and processing information pertinent to the situation at hand. It is our capacity to attend to specific objects and, more importantly, our ability to switch our attention from object to object, that supports complex cognitive behavior. Therefore, by understanding the neural mechanisms involved in directing attention we hope to better understand cognition. Previous work investigating the ability to control attention has suggested that attention is influenced from two sources -- attention can either be driven from external sources in an bottom-up, exogenous manner or directed internally in an top-down, endogenous manner.This project will utilize two different forms of visual search in order to emphasize these two different types of attentional control. Both the prefrontal and parietal regions are implicated as the source of this control. In order to investigate their relative roles we recorded simultaneously from both parietal cortex (specifically, the lateral intraparietal cortex) and prefrontal cortex (specifically, the frontal eye fields and dorsolateral prefrontal cortex). We address four main questions. First, we contrast the respective roles of frontal and parietal cortex in the direction of attention when it is under either top-down or bottom-up control. We use the timing of attention signals between frontal and parietal cortex to establish that frontal cortex directs top-down attention back into parietal cortex, while bottom-up attention is reflected first in parietal cortex, flowing forward to frontal cortex. Secondly, we investigated the role of synchrony and the inter-areal relationships underlying top-down and bottom-up control of attention. Our results suggest synchrony between areas shifts as the task shifts, likely aiding in the selection of the network best suited to the current task. Third, we compare the neural mechanisms between internal and external control of attention.",
    "advisors": ["Earl K. Miller"],
    "text": "Comparison of frontal and parietal cortices in the control of visual attention The ability to switch between tasks reflects a fundamental part of our intelligence. A foundation of this ability lies in perceiving and processing information pertinent to the situation at hand. It is our capacity to attend to specific objects and, more importantly, our ability to switch our attention from object to object, that supports complex cognitive behavior. Therefore, by understanding the neural mechanisms involved in directing attention we hope to better understand cognition. Previous work investigating the ability to control attention has suggested that attention is influenced from two sources -- attention can either be driven from external sources in an bottom-up, exogenous manner or directed internally in an top-down, endogenous manner.This project will utilize two different forms of visual search in order to emphasize these two different types of attentional control. Both the prefrontal and parietal regions are implicated as the source of this control. In order to investigate their relative roles we recorded simultaneously from both parietal cortex (specifically, the lateral intraparietal cortex) and prefrontal cortex (specifically, the frontal eye fields and dorsolateral prefrontal cortex). We address four main questions. First, we contrast the respective roles of frontal and parietal cortex in the direction of attention when it is under either top-down or bottom-up control. We use the timing of attention signals between frontal and parietal cortex to establish that frontal cortex directs top-down attention back into parietal cortex, while bottom-up attention is reflected first in parietal cortex, flowing forward to frontal cortex. Secondly, we investigated the role of synchrony and the inter-areal relationships underlying top-down and bottom-up control of attention. Our results suggest synchrony between areas shifts as the task shifts, likely aiding in the selection of the network best suited to the current task. Third, we compare the neural mechanisms between internal and external control of attention."
}, {
    "id": "oai:dspace.mit.edu:1721.1/79187",
    "title": "Intracranial electroencephalography signatures of the induction of general anesthesia with Propofol",
    "abstract": "General anesthesia is a drug-induced, reversible behavioral state characterized by hypnosis (loss of consciousness), amnesia (loss of memory), analgesia (loss of pain perception), akinesia (loss of movement), and hemodynamic stability (stability and control of the cardiovascular, respiratory, and autonomic nervous systems). Each year, more than 25 million patients receive general anesthesia in the United States. Anesthesia-related morbidity is a significant medical problem, including nausea, vomiting, respiratory distress, post-operative cognitive dysfunction, and post-operative recall. To eliminate anesthesia-related morbidity, the brain systems involved in producing general anesthesia must be identified and characterized, and methods must be devised to monitor those brain systems and guide drug administration. A priority for anesthesia research is to identify the brain networks responsible for the characteristic electroencephalography (EEG) signals of anesthesia in relation to sensory, cognitive, memory, and pain systems. In this thesis, we recorded simultaneous intracranial and surface EEG, and single unit data in patients with intractable epilepsy who had been previously implanted with clinical and/or research electrodes. The aims of this research were to characterize the neural signals of anesthesia in a regionally and temporally precise way that is relevant to clinical anesthesia, and to identify dynamic neuronal networks that underlie these signals. We demonstrated region-specific, frequency-band-specific changes in neural recordings at loss of consciousness. We related these findings to theories of how anesthetic drugs may impart their behavioral effects.",
    "advisors": ["Emery N. Brown", "Patrick L. Purdon"],
    "text": "Intracranial electroencephalography signatures of the induction of general anesthesia with Propofol General anesthesia is a drug-induced, reversible behavioral state characterized by hypnosis (loss of consciousness), amnesia (loss of memory), analgesia (loss of pain perception), akinesia (loss of movement), and hemodynamic stability (stability and control of the cardiovascular, respiratory, and autonomic nervous systems). Each year, more than 25 million patients receive general anesthesia in the United States. Anesthesia-related morbidity is a significant medical problem, including nausea, vomiting, respiratory distress, post-operative cognitive dysfunction, and post-operative recall. To eliminate anesthesia-related morbidity, the brain systems involved in producing general anesthesia must be identified and characterized, and methods must be devised to monitor those brain systems and guide drug administration. A priority for anesthesia research is to identify the brain networks responsible for the characteristic electroencephalography (EEG) signals of anesthesia in relation to sensory, cognitive, memory, and pain systems. In this thesis, we recorded simultaneous intracranial and surface EEG, and single unit data in patients with intractable epilepsy who had been previously implanted with clinical and/or research electrodes. The aims of this research were to characterize the neural signals of anesthesia in a regionally and temporally precise way that is relevant to clinical anesthesia, and to identify dynamic neuronal networks that underlie these signals. We demonstrated region-specific, frequency-band-specific changes in neural recordings at loss of consciousness. We related these findings to theories of how anesthetic drugs may impart their behavioral effects."
}, {
    "id": "oai:dspace.mit.edu:1721.1/106438",
    "title": "Critical window in autism : a study on Shank3",
    "abstract": "Autism and autism spectrum disorders (ASDs) are clinically defined by the symptoms of social impairment and repetitive behavior, affecting 1 in 68 children in the United States. Because patients with ASDs typically display symptoms before the age of three, the ASDs are classically categorized as developmental disorders. One of the key questions in autism research is whether the pathology is reversible in adults. Many studies of simple sensory systems have reported that there is a distinct critical period for synaptic plasticity. This is most famously supported by the monocular deprivation studies in young kittens, which resulted in irreversible visual impairment in adulthood (Hubel and Wiesel, 1970). However, it is not clear whether this principle extends to more complicated multi-modal behavioral systems. Here we demonstrate that adult rescue can lead to improvements in selective phenotypes of ASD by generating and using a novel Shank3 conditional knock-in mouse model. Estimated to contribute to about 1% of all ASD cases, Shank3 is one of the most prominent genes associated with autism. It is a master postsynaptic scaffolding protein that mediates synaptic plasticity and remodeling by regulating many neurotransmitter receptors including NMDAR, AMPAR, and numerous actin-binding regulators. Disruptions of Shank3 in mouse models have robustly recapitulated the cardinal phenotypes of autism including anxiety, social interaction deficits, and compulsive/stereotyped behavior. By specifically expressing Shank3 in adult mice that were initially born as Shank3 knockouts, we show that deficits in the synaptic protein composition and striatal neurotransmission can be fully recovered. We developed a novel neuronal tracing technique to study the dendritic spine density, and found that the dendritic spine number is also significantly increased in the rescue condition after development. In addition, we show that while anxiety and motor coordination are not improved, social interaction and repetitive behavior can be significantly rescued. This suggests that plasticity for certain neural circuits persist into adulthood in the diseased brain, and that the underlying mechanisms for different autistic-like phenotypes have distinct properties.",
    "advisors": ["Guoping Feng"],
    "text": "Critical window in autism : a study on Shank3 Autism and autism spectrum disorders (ASDs) are clinically defined by the symptoms of social impairment and repetitive behavior, affecting 1 in 68 children in the United States. Because patients with ASDs typically display symptoms before the age of three, the ASDs are classically categorized as developmental disorders. One of the key questions in autism research is whether the pathology is reversible in adults. Many studies of simple sensory systems have reported that there is a distinct critical period for synaptic plasticity. This is most famously supported by the monocular deprivation studies in young kittens, which resulted in irreversible visual impairment in adulthood (Hubel and Wiesel, 1970). However, it is not clear whether this principle extends to more complicated multi-modal behavioral systems. Here we demonstrate that adult rescue can lead to improvements in selective phenotypes of ASD by generating and using a novel Shank3 conditional knock-in mouse model. Estimated to contribute to about 1% of all ASD cases, Shank3 is one of the most prominent genes associated with autism. It is a master postsynaptic scaffolding protein that mediates synaptic plasticity and remodeling by regulating many neurotransmitter receptors including NMDAR, AMPAR, and numerous actin-binding regulators. Disruptions of Shank3 in mouse models have robustly recapitulated the cardinal phenotypes of autism including anxiety, social interaction deficits, and compulsive/stereotyped behavior. By specifically expressing Shank3 in adult mice that were initially born as Shank3 knockouts, we show that deficits in the synaptic protein composition and striatal neurotransmission can be fully recovered. We developed a novel neuronal tracing technique to study the dendritic spine density, and found that the dendritic spine number is also significantly increased in the rescue condition after development. In addition, we show that while anxiety and motor coordination are not improved, social interaction and repetitive behavior can be significantly rescued. This suggests that plasticity for certain neural circuits persist into adulthood in the diseased brain, and that the underlying mechanisms for different autistic-like phenotypes have distinct properties."
}, {
    "id": "oai:dspace.mit.edu:1721.1/120621",
    "title": "Computational foundations of human social intelligence",
    "abstract": "This thesis develops formal computational cognitive models of the social intelligence underlying human cooperation and morality. Human social intelligence is uniquely powerful. We collaborate with others to accomplish together what none of us could do on our own; we share the benefits of collaboration fairly and trust others to do the same. Even young children work and play collaboratively, guided by normative principles, and with a sophistication unparalleled in other animal species. Here, I seek to understand these everyday feats of social intelligence in computational terms. What are the cognitive representations and processes that underlie these abilities and what are their origins? How can we apply these cognitive principles to build machines that have the capacity to understand, learn from, and cooperate with people? The overarching formal framework of this thesis is the integration of individually rational, hierarchical Bayesian models of learning, together with socially rational multi-agent and game-theoretic models of cooperation. I use this framework to probe cognitive questions across three time-scales: evolutionary, developmental, and in the moment. First, I investigate the evolutionary origins of the cognitive structures that enable cooperation and support social learning. I then describe how these structures are used to learn social and moral knowledge rapidly during development, leading to the accumulation of knowledge over generations. Finally I show how this knowledge is used and generalized in the moment, across an infinitude of possible situations. This framework is applied to a variety of cognitively challenging social inferences: determining the intentions of others, distinguishing who is friend or foe, and inferring the reputation of others all from just a single observation of behavior. It also answers how these inferences enable fair and reciprocal cooperation, the computation of moral permissibility, and moral learning. This framework predicts and explains human judgment and behavior measured in large-scale multi-person experiments. Together, these results shine light on how the scale and scope of human social behavior is ultimately grounded in the sophistication of our social intelligence.",
    "advisors": ["Joshua B. Tenenbaum"],
    "text": "Computational foundations of human social intelligence This thesis develops formal computational cognitive models of the social intelligence underlying human cooperation and morality. Human social intelligence is uniquely powerful. We collaborate with others to accomplish together what none of us could do on our own; we share the benefits of collaboration fairly and trust others to do the same. Even young children work and play collaboratively, guided by normative principles, and with a sophistication unparalleled in other animal species. Here, I seek to understand these everyday feats of social intelligence in computational terms. What are the cognitive representations and processes that underlie these abilities and what are their origins? How can we apply these cognitive principles to build machines that have the capacity to understand, learn from, and cooperate with people? The overarching formal framework of this thesis is the integration of individually rational, hierarchical Bayesian models of learning, together with socially rational multi-agent and game-theoretic models of cooperation. I use this framework to probe cognitive questions across three time-scales: evolutionary, developmental, and in the moment. First, I investigate the evolutionary origins of the cognitive structures that enable cooperation and support social learning. I then describe how these structures are used to learn social and moral knowledge rapidly during development, leading to the accumulation of knowledge over generations. Finally I show how this knowledge is used and generalized in the moment, across an infinitude of possible situations. This framework is applied to a variety of cognitively challenging social inferences: determining the intentions of others, distinguishing who is friend or foe, and inferring the reputation of others all from just a single observation of behavior. It also answers how these inferences enable fair and reciprocal cooperation, the computation of moral permissibility, and moral learning. This framework predicts and explains human judgment and behavior measured in large-scale multi-person experiments. Together, these results shine light on how the scale and scope of human social behavior is ultimately grounded in the sophistication of our social intelligence."
}, {
    "id": "oai:dspace.mit.edu:1721.1/120620",
    "title": "Using the language of thought",
    "abstract": "In this thesis, I develop and explore two novel models of how humans might be able to acquire high-level conceputal knowledge by performing probabilistic inference over a language of thought (Fodor 1975) - a space of symbolic and compositional mental representations sufficiently expressive to capture the meanings of human thoughts and utterances. These models and their associated learning algorithms are motivated by an attempt to provide an understanding of the algorithmic principles that might underlie a child's ability to search the haystack of sentences in her language of thought to find the needle that corresponds to any specific concept. The first model takes advantage of the compositionality inherent to LOT representations, framing concept acquisition as program induction in a functional programming language; the Exploration- Compression algorithm this model motivates iteratively builds a library of useful program fragments that, when composed, restructures the search space, making more useful programs shorter and easier to find. The second model, the Infinite Knowledge Base Model (IKM), frames concept learning as probabilistic inference over the space of relational knowledge bases; the algorithm I develop for learning in this model frames this inference problem as a state-space search over abductive proofs of the learner's observed data. This framing allows us to take advantage of powerful techniques from the heuristic search and classical planning literature to guide the learner. In the final part of this thesis, I explore the behavior of the IKM on several case studies of intuitive theories from the concept learning literature, and I discuss evidence for and against it with respect to other approaches to LOT models.",
    "advisors": ["Joshua B. Tenenbaum"],
    "text": "Using the language of thought In this thesis, I develop and explore two novel models of how humans might be able to acquire high-level conceputal knowledge by performing probabilistic inference over a language of thought (Fodor 1975) - a space of symbolic and compositional mental representations sufficiently expressive to capture the meanings of human thoughts and utterances. These models and their associated learning algorithms are motivated by an attempt to provide an understanding of the algorithmic principles that might underlie a child's ability to search the haystack of sentences in her language of thought to find the needle that corresponds to any specific concept. The first model takes advantage of the compositionality inherent to LOT representations, framing concept acquisition as program induction in a functional programming language; the Exploration- Compression algorithm this model motivates iteratively builds a library of useful program fragments that, when composed, restructures the search space, making more useful programs shorter and easier to find. The second model, the Infinite Knowledge Base Model (IKM), frames concept learning as probabilistic inference over the space of relational knowledge bases; the algorithm I develop for learning in this model frames this inference problem as a state-space search over abductive proofs of the learner's observed data. This framing allows us to take advantage of powerful techniques from the heuristic search and classical planning literature to guide the learner. In the final part of this thesis, I explore the behavior of the IKM on several case studies of intuitive theories from the concept learning literature, and I discuss evidence for and against it with respect to other approaches to LOT models."
}, {
    "id": "oai:dspace.mit.edu:1721.1/34564",
    "title": "Applications of empirical processes in learning theory : algorithmic stability and generalization bounds",
    "abstract": "This thesis studies two key properties of learning algorithms: their generalization ability and their stability with respect to perturbations. To analyze these properties, we focus on concentration inequalities and tools from empirical process theory. We obtain theoretical results and demonstrate their applications to machine learning. First, we show how various notions of stability upper- and lower-bound the bias and variance of several estimators of the expected performance for general learning algorithms. A weak stability condition is shown to be equivalent to consistency of empirical risk minimization. The second part of the thesis derives tight performance guarantees for greedy error minimization methods - a family of computationally tractable algorithms. In particular, we derive risk bounds for a greedy mixture density estimation procedure. We prove that, unlike what is suggested in the literature, the number of terms in the mixture is not a bias-variance trade-off for the performance. The third part of this thesis provides a solution to an open problem regarding the stability of Empirical Risk Minimization (ERM). This algorithm is of central importance in Learning Theory.",
    "advisors": ["Tomaso Poggio"],
    "text": "Applications of empirical processes in learning theory : algorithmic stability and generalization bounds This thesis studies two key properties of learning algorithms: their generalization ability and their stability with respect to perturbations. To analyze these properties, we focus on concentration inequalities and tools from empirical process theory. We obtain theoretical results and demonstrate their applications to machine learning. First, we show how various notions of stability upper- and lower-bound the bias and variance of several estimators of the expected performance for general learning algorithms. A weak stability condition is shown to be equivalent to consistency of empirical risk minimization. The second part of the thesis derives tight performance guarantees for greedy error minimization methods - a family of computationally tractable algorithms. In particular, we derive risk bounds for a greedy mixture density estimation procedure. We prove that, unlike what is suggested in the literature, the number of terms in the mixture is not a bias-variance trade-off for the performance. The third part of this thesis provides a solution to an open problem regarding the stability of Empirical Risk Minimization (ERM). This algorithm is of central importance in Learning Theory."
}, {
    "id": "oai:dspace.mit.edu:1721.1/39005",
    "title": "Eye-opening and control of visual synapse development in the mouse superior colliculus",
    "abstract": "The mammalian superior colliculus (SC) coordinates visual, somatosensory, and auditory stimuli to guide animal behavior. The superficial layers (sSC) receive visual information via two major afferent projections: 1) A direct retinal projection and 2) an indirect projection from Layer V visual cortex. The retinal projection reaches the rat sSC by embryonic day 16, is topographic, and refines to form a high resolution map of visual space early in development, before eye-opening in rodents (-P12-P14). The cortical projection is delayed by about eight days, just reaching the sSC around P4, and does not complete its topographic refinement until around the time of eye-opening. These afferents compete for synaptic space during a time when patterns of spontaneous and evoked activity are rapidly changing. I have used the mouse sSC as a model system to test the role of new activity patterns due to the initial onset of visual experience after eye-opening in visual synaptic development. I have described the organization of retinal and cortical afferents and the laminar organization of the mouse sSC in Chapter 3. Previous work demonstrated eye-opening (EO) induces the appearance of dendritic PSD-95 and LTP in the sSC within 2-4 hours.",
    "advisors": ["Martha Constantine-Paton"],
    "text": "Eye-opening and control of visual synapse development in the mouse superior colliculus The mammalian superior colliculus (SC) coordinates visual, somatosensory, and auditory stimuli to guide animal behavior. The superficial layers (sSC) receive visual information via two major afferent projections: 1) A direct retinal projection and 2) an indirect projection from Layer V visual cortex. The retinal projection reaches the rat sSC by embryonic day 16, is topographic, and refines to form a high resolution map of visual space early in development, before eye-opening in rodents (-P12-P14). The cortical projection is delayed by about eight days, just reaching the sSC around P4, and does not complete its topographic refinement until around the time of eye-opening. These afferents compete for synaptic space during a time when patterns of spontaneous and evoked activity are rapidly changing. I have used the mouse sSC as a model system to test the role of new activity patterns due to the initial onset of visual experience after eye-opening in visual synaptic development. I have described the organization of retinal and cortical afferents and the laminar organization of the mouse sSC in Chapter 3. Previous work demonstrated eye-opening (EO) induces the appearance of dendritic PSD-95 and LTP in the sSC within 2-4 hours."
}, {
    "id": "oai:dspace.mit.edu:1721.1/54623",
    "title": "A global framework for scene gist",
    "abstract": "Human observers are able to rapidly and accurately categorize natural scenes, but the representation mediating this feat is still unknown. Here we propose a framework of rapid scene categorization that does not segment a scene into objects and instead uses a vocabulary of global, ecological properties that describe spatial and functional aspects of scene space (such as navigability or mean depth). In Chapter 1, four experiments explore the human sensitivity to global properties for rapid scene categorization, as well as the computational sufficiency of these properties for predicting scene categories. Chapter 2 explores the time course of scene understanding, finding that global properties can be perceived with less image exposure than the computation of a scene's basic-level category. Finally, in Chapter 3, I explore aftereffects to adaptation to global properties, showing that repeated exposure to many global properties produces robust high-level aftereffects, thus providing evidence for the neural coding of these properties. Altogether, these results provide support for the hypothesis that rapid categorization of natural scenes may not be mediated primarily though objects and parts, but also through global properties of structure and affordance.",
    "advisors": ["Aude Oliva"],
    "text": "A global framework for scene gist Human observers are able to rapidly and accurately categorize natural scenes, but the representation mediating this feat is still unknown. Here we propose a framework of rapid scene categorization that does not segment a scene into objects and instead uses a vocabulary of global, ecological properties that describe spatial and functional aspects of scene space (such as navigability or mean depth). In Chapter 1, four experiments explore the human sensitivity to global properties for rapid scene categorization, as well as the computational sufficiency of these properties for predicting scene categories. Chapter 2 explores the time course of scene understanding, finding that global properties can be perceived with less image exposure than the computation of a scene's basic-level category. Finally, in Chapter 3, I explore aftereffects to adaptation to global properties, showing that repeated exposure to many global properties produces robust high-level aftereffects, thus providing evidence for the neural coding of these properties. Altogether, these results provide support for the hypothesis that rapid categorization of natural scenes may not be mediated primarily though objects and parts, but also through global properties of structure and affordance."
}, {
    "id": "oai:dspace.mit.edu:1721.1/8393",
    "title": "Dynamics and learning in recurrent neural networks",
    "abstract": "This thesis is a study of dynamics and learning in recurrent neural networks. Many computations of neural systems are carried out through a network of a large number of neurons. With massive feedback connections among these neurons, a study of its dynamics is necessary in order to understand the network's function. In this thesis, I aim at studying several recurrent network models and relating the dynamics with the networks' computation. For this purpose, three systems are studied and analyzed in detail: The first one is a network model for direction selectivity; the second one is a generalized network of Winner-Take-All; the third one is a model for integration in head-direction systems. One distinctive feature of neural systems is the ability of learning. The other part of my thesis is on learning in biologically motivated neural networks. Specifically, I study how the spike-time-dependent synaptic plasticity helps to stabilize persistent neural activities in the ocular motor integrator. I study the connections between back-propagation and contrastive-Hebbian learning, and show how backpropagation could be equivalently implemented by contrastive-Hebbian learning in a layered network. I also propose a learning rule governing synaptic plasticity in a network of spiking neurons and compare it with recent experimental results on spike-time-dependent plasticity.",
    "advisors": ["H. Sebastian Seung"],
    "text": "Dynamics and learning in recurrent neural networks This thesis is a study of dynamics and learning in recurrent neural networks. Many computations of neural systems are carried out through a network of a large number of neurons. With massive feedback connections among these neurons, a study of its dynamics is necessary in order to understand the network's function. In this thesis, I aim at studying several recurrent network models and relating the dynamics with the networks' computation. For this purpose, three systems are studied and analyzed in detail: The first one is a network model for direction selectivity; the second one is a generalized network of Winner-Take-All; the third one is a model for integration in head-direction systems. One distinctive feature of neural systems is the ability of learning. The other part of my thesis is on learning in biologically motivated neural networks. Specifically, I study how the spike-time-dependent synaptic plasticity helps to stabilize persistent neural activities in the ocular motor integrator. I study the connections between back-propagation and contrastive-Hebbian learning, and show how backpropagation could be equivalently implemented by contrastive-Hebbian learning in a layered network. I also propose a learning rule governing synaptic plasticity in a network of spiking neurons and compare it with recent experimental results on spike-time-dependent plasticity."
}, {
    "id": "oai:dspace.mit.edu:1721.1/46664",
    "title": "Eye-opening dependent elaboration and refinement of the cortical projection to the superficial superior colliculus in rats",
    "abstract": "The superior colliculus (SC) is a multi-layered midbrain structure responsible for multimodal integration and orienting behavior in mammals. The superficial layers of the SC (sSC) receive direct visual input from retinal ganglion cells (RGC) as well as input from pyramidal cells in layer V of the ipsilateral visual cortex (VC). The retinal input is refined well before eye opening (EO) and RGC axons arborize topographically to form an appropriate map of visual space. The projection from VC is still broad and unrefined at the time of EO, however. In both sSC and VC, physiological and biochemical evidence indicate considerable synaptic refinement in response to EO, which occurs naturally at the end of the second postnatal week. These studies use anterograde filling of corticocollicular axons in combination with controlled eyelid opening and reclosing paradigms to compare the corticocollicular projections of age-matched eye-opened and eye-sutured littermates. Reconstructions of individual corticocollicular axons in rat pups and statistically sampled arborization patterns across the colliculus at set times before and after controlled eye-lid opening, show that the onset of pattern vision is critical for the establishment of registration between the cortical and collicular maps of visual space. Moreover, if pattern vision is delayed by prolonging eye-lid closure the cortical projection withdraws to single axon cylinders. A latent plasticity remains, however; the corticocollicular axons can reestablish topologically appropriate arborization if eye opening occurs within at least a week of its normal occurrence.",
    "advisors": ["Martha Constantine-Paton"],
    "text": "Eye-opening dependent elaboration and refinement of the cortical projection to the superficial superior colliculus in rats The superior colliculus (SC) is a multi-layered midbrain structure responsible for multimodal integration and orienting behavior in mammals. The superficial layers of the SC (sSC) receive direct visual input from retinal ganglion cells (RGC) as well as input from pyramidal cells in layer V of the ipsilateral visual cortex (VC). The retinal input is refined well before eye opening (EO) and RGC axons arborize topographically to form an appropriate map of visual space. The projection from VC is still broad and unrefined at the time of EO, however. In both sSC and VC, physiological and biochemical evidence indicate considerable synaptic refinement in response to EO, which occurs naturally at the end of the second postnatal week. These studies use anterograde filling of corticocollicular axons in combination with controlled eyelid opening and reclosing paradigms to compare the corticocollicular projections of age-matched eye-opened and eye-sutured littermates. Reconstructions of individual corticocollicular axons in rat pups and statistically sampled arborization patterns across the colliculus at set times before and after controlled eye-lid opening, show that the onset of pattern vision is critical for the establishment of registration between the cortical and collicular maps of visual space. Moreover, if pattern vision is delayed by prolonging eye-lid closure the cortical projection withdraws to single axon cylinders. A latent plasticity remains, however; the corticocollicular axons can reestablish topologically appropriate arborization if eye opening occurs within at least a week of its normal occurrence."
}, {
    "id": "oai:dspace.mit.edu:1721.1/113953",
    "title": "Spiking and oscillatory correlates of visual short-term memory for multiple items",
    "abstract": "The richness of visual experience far exceeds our ability to remember what we have seen. However, it is unclear what neural mechanisms give rise to these limits to visual short-term memory capacity. Here, we measured neural activity in a change localization task, in which monkeys viewed two displays of multiple colored squares separated by a brief delay, and made a saccade to the square that changed color between displays. In chapter 2, we examine local field potentials in the lateral intraparietal area (LIP), frontal eye field, and lateral prefrontal cortex (PFC). At stimulus encoding, lower frequency oscillations decreased in power in proportion to the total number of stimuli presented, while higher frequency oscillations increased in power in proportion to the number of stimuli contralateral to the recording site. During the delay, lower frequency power instead increased with the number of contralateral stimuli, while higher frequency power was not modulated. We interpret these findings in terms of roles for low- and high-frequency oscillations in changing and maintaining cognitive state. In chapter 3, we compare spiking activity between LIP, PFC, and inferotemporal cortex (IT). Although the task required that the animal remember stimulus colors, activity in LIP and PFC primarily reflected the stimulus positions, while activity in IT primarily reflected color. In PFC, color information increased with the number of stimuli presented, while in IT, color information remained constant or decreased. Thus, IT was more strongly capacity-limited than PFC. Color selectivity during the delay was weak in all regions. However, in IT, activity at test stimulus presentation reflected the difference in square colors between the sample and test displays, while in PFC, activity primarily reflected the location of the changed square. Selectivity to these attributes was stronger on correct trials than incorrect trials. Our findings suggest a possible role for passive processes in IT in visual short-term memory.",
    "advisors": ["Earl K. Miller"],
    "text": "Spiking and oscillatory correlates of visual short-term memory for multiple items The richness of visual experience far exceeds our ability to remember what we have seen. However, it is unclear what neural mechanisms give rise to these limits to visual short-term memory capacity. Here, we measured neural activity in a change localization task, in which monkeys viewed two displays of multiple colored squares separated by a brief delay, and made a saccade to the square that changed color between displays. In chapter 2, we examine local field potentials in the lateral intraparietal area (LIP), frontal eye field, and lateral prefrontal cortex (PFC). At stimulus encoding, lower frequency oscillations decreased in power in proportion to the total number of stimuli presented, while higher frequency oscillations increased in power in proportion to the number of stimuli contralateral to the recording site. During the delay, lower frequency power instead increased with the number of contralateral stimuli, while higher frequency power was not modulated. We interpret these findings in terms of roles for low- and high-frequency oscillations in changing and maintaining cognitive state. In chapter 3, we compare spiking activity between LIP, PFC, and inferotemporal cortex (IT). Although the task required that the animal remember stimulus colors, activity in LIP and PFC primarily reflected the stimulus positions, while activity in IT primarily reflected color. In PFC, color information increased with the number of stimuli presented, while in IT, color information remained constant or decreased. Thus, IT was more strongly capacity-limited than PFC. Color selectivity during the delay was weak in all regions. However, in IT, activity at test stimulus presentation reflected the difference in square colors between the sample and test displays, while in PFC, activity primarily reflected the location of the changed square. Selectivity to these attributes was stronger on correct trials than incorrect trials. Our findings suggest a possible role for passive processes in IT in visual short-term memory."
}, {
    "id": "oai:dspace.mit.edu:1721.1/107869",
    "title": "Human induced pluripotent stem cell models of Rett Syndrome reveal deficits in early cortical development",
    "abstract": "Rett Syndrome (RTT) is a pervasive, X-linked neurodevelopmental disorder that predominantly affects girls. The clinical patient features of RTT are most commonly reported to emerge between the ages of 6-18 months and as such, RTT has largely been considered to be a postnatal disorder. The vast majority of cases are caused by sporadic mutations in the gene encoding methyl CpG-binding protein 2 (MeCP2), which is expressed in the brain during prenatal neurogenesis and continuously throughout adulthood. MeCP2 is a pleiotropic gene that functions as a complex, high-level transcriptional modulator. It both regulates and is regulated by coding genes and non-coding RNAs including microRNAs (miRNAs). The effects of MeCP2 are mediated by diverse signaling, transcriptional, and epigenetic mechanisms. Whereas the postnatal effects of MeCP2 have been widely studied, pre-symptomatic stages of RTT have yet to be thoroughly investigated. Recent evidence from our lab among others suggests a role for MeCP2 during prenatal neurogenesis that may contribute to the neuropathology observed later in life. We sought to characterize the course of neurogenesis in MeCP2-deficient human neurons with the use of induced pluripotent stem cells (iPSCs) derived from RTT patient skin samples. We generated a variety of monolayer and 3D neuronal models and found that RTT phenotypes are present at the earliest stages of brain development including neuroepithelial expansion, neural progenitor migration and differentiation, and later stages of membrane and synaptic physiological development. We established a link between MeCP2 and key microRNAs that are misregulated in RTT and lie upstream of signalling pathways that contribute to aberrant neuronal maturation in the absence of MeCP2. We have uncovered novel roles of MeCP2 in human neurogenesis. Whereas the processes that comprise early neural development were previously considered irrelevant to RTT pathology, the deficits we observed in neuronal differentiation, migration, and maturation are a crucial component to the larger picture of RTT pathogenesis and provide additional insight into the emergence of RTT patient symptoms.",
    "advisors": ["Mriganka Sur"],
    "text": "Human induced pluripotent stem cell models of Rett Syndrome reveal deficits in early cortical development Rett Syndrome (RTT) is a pervasive, X-linked neurodevelopmental disorder that predominantly affects girls. The clinical patient features of RTT are most commonly reported to emerge between the ages of 6-18 months and as such, RTT has largely been considered to be a postnatal disorder. The vast majority of cases are caused by sporadic mutations in the gene encoding methyl CpG-binding protein 2 (MeCP2), which is expressed in the brain during prenatal neurogenesis and continuously throughout adulthood. MeCP2 is a pleiotropic gene that functions as a complex, high-level transcriptional modulator. It both regulates and is regulated by coding genes and non-coding RNAs including microRNAs (miRNAs). The effects of MeCP2 are mediated by diverse signaling, transcriptional, and epigenetic mechanisms. Whereas the postnatal effects of MeCP2 have been widely studied, pre-symptomatic stages of RTT have yet to be thoroughly investigated. Recent evidence from our lab among others suggests a role for MeCP2 during prenatal neurogenesis that may contribute to the neuropathology observed later in life. We sought to characterize the course of neurogenesis in MeCP2-deficient human neurons with the use of induced pluripotent stem cells (iPSCs) derived from RTT patient skin samples. We generated a variety of monolayer and 3D neuronal models and found that RTT phenotypes are present at the earliest stages of brain development including neuroepithelial expansion, neural progenitor migration and differentiation, and later stages of membrane and synaptic physiological development. We established a link between MeCP2 and key microRNAs that are misregulated in RTT and lie upstream of signalling pathways that contribute to aberrant neuronal maturation in the absence of MeCP2. We have uncovered novel roles of MeCP2 in human neurogenesis. Whereas the processes that comprise early neural development were previously considered irrelevant to RTT pathology, the deficits we observed in neuronal differentiation, migration, and maturation are a crucial component to the larger picture of RTT pathogenesis and provide additional insight into the emergence of RTT patient symptoms."
}, {
    "id": "oai:dspace.mit.edu:1721.1/120619",
    "title": "Learning and inference with Wasserstein metrics",
    "abstract": "This thesis develops new approaches for three problems in machine learning, using tools from the study of optimal transport (or Wasserstein) distances between probability distributions. Optimal transport distances capture an intuitive notion of similarity between distributions, by incorporating the underlying geometry of the domain of the distributions. Despite their intuitive appeal, optimal transport distances are often difficult to apply in practice, as computing them requires solving a costly optimization problem. In each setting studied here, we describe a numerical method that overcomes this computational bottleneck and enables scaling to real data. In the first part, we consider the problem of multi-output learning in the presence of a metric on the output domain. We develop a loss function that measures the Wasserstein distance between the prediction and ground truth, and describe an efficient learning algorithm based on entropic regularization of the optimal transport problem. We additionally propose a novel extension of the Wasserstein distance from probability measures to unnormalized measures, which is applicable in settings where the ground truth is not naturally expressed as a probability distribution. We show statistical learning bounds for both the Wasserstein loss and its unnormalized counterpart. The Wasserstein loss can encourage smoothness of the predictions with respect to a chosen metric on the output space. We demonstrate this property on a real-data image tagging problem, outperforming a baseline that doesn't use the metric. In the second part, we consider the probabilistic inference problem for diffusion processes. Such processes model a variety of stochastic phenomena and appear often in continuous-time state space models. Exact inference for diffusion processes is generally intractable. In this work, we describe a novel approximate inference method, which is based on a characterization of the diffusion as following a gradient flow in a space of probability densities endowed with a Wasserstein metric. Existing methods for computing this Wasserstein gradient flow rely on discretizing the underlying domain of the diffusion, prohibiting their application to problems in more than several dimensions. In the current work, we propose a novel algorithm for computing a Wasserstein gradient flow that operates directly in a space of continuous functions, free of any underlying mesh. We apply our approximate gradient flow to the problem of filtering a diffusion, showing superior performance where standard filters struggle. Finally, we study the ecological inference problem, which is that of reasoning from aggregate measurements of a population to inferences about the individual behaviors of its members. This problem arises often when dealing with data from economics and political sciences, such as when attempting to infer the demographic breakdown of votes for each political party, given only the aggregate demographic and vote counts separately. Ecological inference is generally ill-posed, and requires prior information to distinguish a unique solution. We propose a novel, general framework for ecological inference that allows for a variety of priors and enables efficient computation of the most probable solution. Unlike previous methods, which rely on Monte Carlo estimates of the posterior, our inference procedure uses an efficient fixed point iteration that is linearly convergent. Given suitable prior information, our method can achieve more accurate inferences than existing methods. We additionally explore a sampling algorithm for estimating credible regions.",
    "advisors": ["Tomaso Poggio"],
    "text": "Learning and inference with Wasserstein metrics This thesis develops new approaches for three problems in machine learning, using tools from the study of optimal transport (or Wasserstein) distances between probability distributions. Optimal transport distances capture an intuitive notion of similarity between distributions, by incorporating the underlying geometry of the domain of the distributions. Despite their intuitive appeal, optimal transport distances are often difficult to apply in practice, as computing them requires solving a costly optimization problem. In each setting studied here, we describe a numerical method that overcomes this computational bottleneck and enables scaling to real data. In the first part, we consider the problem of multi-output learning in the presence of a metric on the output domain. We develop a loss function that measures the Wasserstein distance between the prediction and ground truth, and describe an efficient learning algorithm based on entropic regularization of the optimal transport problem. We additionally propose a novel extension of the Wasserstein distance from probability measures to unnormalized measures, which is applicable in settings where the ground truth is not naturally expressed as a probability distribution. We show statistical learning bounds for both the Wasserstein loss and its unnormalized counterpart. The Wasserstein loss can encourage smoothness of the predictions with respect to a chosen metric on the output space. We demonstrate this property on a real-data image tagging problem, outperforming a baseline that doesn't use the metric. In the second part, we consider the probabilistic inference problem for diffusion processes. Such processes model a variety of stochastic phenomena and appear often in continuous-time state space models. Exact inference for diffusion processes is generally intractable. In this work, we describe a novel approximate inference method, which is based on a characterization of the diffusion as following a gradient flow in a space of probability densities endowed with a Wasserstein metric. Existing methods for computing this Wasserstein gradient flow rely on discretizing the underlying domain of the diffusion, prohibiting their application to problems in more than several dimensions. In the current work, we propose a novel algorithm for computing a Wasserstein gradient flow that operates directly in a space of continuous functions, free of any underlying mesh. We apply our approximate gradient flow to the problem of filtering a diffusion, showing superior performance where standard filters struggle. Finally, we study the ecological inference problem, which is that of reasoning from aggregate measurements of a population to inferences about the individual behaviors of its members. This problem arises often when dealing with data from economics and political sciences, such as when attempting to infer the demographic breakdown of votes for each political party, given only the aggregate demographic and vote counts separately. Ecological inference is generally ill-posed, and requires prior information to distinguish a unique solution. We propose a novel, general framework for ecological inference that allows for a variety of priors and enables efficient computation of the most probable solution. Unlike previous methods, which rely on Monte Carlo estimates of the posterior, our inference procedure uses an efficient fixed point iteration that is linearly convergent. Given suitable prior information, our method can achieve more accurate inferences than existing methods. We additionally explore a sampling algorithm for estimating credible regions."
}, {
    "id": "oai:dspace.mit.edu:1721.1/49739",
    "title": "Perceptually inspired image estimation and enhancement",
    "abstract": "In this thesis, we present three image estimation and enhancement algorithms inspired by human vision. In the first part of the thesis, we propose an algorithm for mapping one image to another based on the statistics of a training set. Many vision problems can be cast as image mapping problems, such as, estimating reflectance from luminance, estimating shape from shading, separating signal and noise, etc. Such problems are typically under-constrained, and yet humans are remarkably good at solving them. Classic computational theories about the ability of the human visual system to solve such under-constrained problems attribute this feat to the use of some intuitive regularities of the world, e.g., surfaces tend to be piecewise constant. In recent years, there has been considerable interest in deriving more sophisticated statistical constraints from natural images, but because of the high-dimensional nature of images, representing and utilizing the learned models remains a challenge. Our techniques produce models that are very easy to store and to query. We show these techniques to be effective for a number of applications: removing noise from images, estimating a sharp image from a blurry one, decomposing an image into reflectance and illumination, and interpreting lightness illusions. In the second part of the thesis, we present an algorithm for compressing the dynamic range of an image while retaining important visual detail. The human visual system confronts a serious challenge with dynamic range, in that the physical world has an extremely high dynamic range, while neurons have low dynamic ranges.",
    "advisors": ["Edward H. Adelson"],
    "text": "Perceptually inspired image estimation and enhancement In this thesis, we present three image estimation and enhancement algorithms inspired by human vision. In the first part of the thesis, we propose an algorithm for mapping one image to another based on the statistics of a training set. Many vision problems can be cast as image mapping problems, such as, estimating reflectance from luminance, estimating shape from shading, separating signal and noise, etc. Such problems are typically under-constrained, and yet humans are remarkably good at solving them. Classic computational theories about the ability of the human visual system to solve such under-constrained problems attribute this feat to the use of some intuitive regularities of the world, e.g., surfaces tend to be piecewise constant. In recent years, there has been considerable interest in deriving more sophisticated statistical constraints from natural images, but because of the high-dimensional nature of images, representing and utilizing the learned models remains a challenge. Our techniques produce models that are very easy to store and to query. We show these techniques to be effective for a number of applications: removing noise from images, estimating a sharp image from a blurry one, decomposing an image into reflectance and illumination, and interpreting lightness illusions. In the second part of the thesis, we present an algorithm for compressing the dynamic range of an image while retaining important visual detail. The human visual system confronts a serious challenge with dynamic range, in that the physical world has an extremely high dynamic range, while neurons have low dynamic ranges."
}, {
    "id": "oai:dspace.mit.edu:1721.1/114075",
    "title": "Memory and locality in natural language",
    "abstract": "I explore the hypothesis that the universal properties of human languages can be explained in terms of efficient communication given fixed human information processing constraints. I argue that under short-term memory constraints, optimal languages should exhibit information locality: words that depend on each other, both in their interpretation and in their statistical distribution, should be close to each other in linear order. The informationtheoretic approach to natural language motivates a study of quantitative syntax in Chapter 2, focusing on word order flexibility. In Chapter 3, I show comprehensive corpus evidence from over 40 languages that word order in grammar and usage is shaped by working memory constraints in the form of dependency locality: a pressure for syntactically linked words to be close. In Chapter 4, I develop a new formal model of language processing cost, called noisy-context surprisal, based on rational inference over noisy memory representations. This model unifies surprisal and memory effects and derives dependency locality effects as a subset of information locality effects. I show that the new processing model also resolves a long-standing paradox in the psycholinguistic literature, structural forgetting, where the effects of memory appear to be language-dependent. In the conclusion I discuss connections to probabilistic grammars, endocentricity, duality of patterning, incremental planning, and deep reinforcement learning.",
    "advisors": ["Edward Gibson", "Roger Levy"],
    "text": "Memory and locality in natural language I explore the hypothesis that the universal properties of human languages can be explained in terms of efficient communication given fixed human information processing constraints. I argue that under short-term memory constraints, optimal languages should exhibit information locality: words that depend on each other, both in their interpretation and in their statistical distribution, should be close to each other in linear order. The informationtheoretic approach to natural language motivates a study of quantitative syntax in Chapter 2, focusing on word order flexibility. In Chapter 3, I show comprehensive corpus evidence from over 40 languages that word order in grammar and usage is shaped by working memory constraints in the form of dependency locality: a pressure for syntactically linked words to be close. In Chapter 4, I develop a new formal model of language processing cost, called noisy-context surprisal, based on rational inference over noisy memory representations. This model unifies surprisal and memory effects and derives dependency locality effects as a subset of information locality effects. I show that the new processing model also resolves a long-standing paradox in the psycholinguistic literature, structural forgetting, where the effects of memory appear to be language-dependent. In the conclusion I discuss connections to probabilistic grammars, endocentricity, duality of patterning, incremental planning, and deep reinforcement learning."
}, {
    "id": "oai:dspace.mit.edu:1721.1/120629",
    "title": "Emotion as information : inferring the unobserved causes of others' emotional expressions",
    "abstract": "Research in the domain of cognitive science has tended to neglect emotions. In my thesis, I take several steps to fill this gap by looking at people's representation of emotions, and its connection to other representations typically studied in cognitive science. I argue that people have an intuitive theory of emotion that is causally intertwined with their understanding of the physical and social world broadly. This intuitive theory allows us to use observed emotional cues as a window, to recover unobserved information about the world. I study these abilities in both adults and children, to gain insight into the most fundamental representations supporting such abilities. I also use computational models to capture the hierarchical, causal structure of this intuitive theory of emotion. In Study 1, I show that infants as young as 12-17 months can discriminate diverse within-valence emotional expressions elicited by funny, exciting, adorable, delicious, and sympathetic events, and map them onto their probable causes. In Study 2.1, I present that preschoolers can recover rich mental state information from observed emotional expressions. When the valence of someone's face changes between anticipated and actual outcomes, children by five gain insight into what she wants and believes about the world. Study 2.2 bridges theory of mind research, accounts of emotion attribution, and formal modeling, to provide a formal account of how people jointly infer beliefs and desires from emotional expressions. Study 3 tests children's understanding of social display rules. By middle childhood, children can use one person's emotional expressions regulated by a social context to infer the mental states of another. Altogether, these findings suggest that emotional cues provide a valuable entre into the unseen world. Not only adults, but also children, can use observed emotional expressions to infer their external causes and the internal mental states of other people. Although this intuitive theory of emotion may not necessarily mirror the actual processes of how emotions are generated, it supports rational inferences much of time, and it may be formed early in development. I see this work as bridging gaps across disciplines and helping advance the cognitive science of emotion understanding.",
    "advisors": ["Laura E. Schulz"],
    "text": "Emotion as information : inferring the unobserved causes of others' emotional expressions Research in the domain of cognitive science has tended to neglect emotions. In my thesis, I take several steps to fill this gap by looking at people's representation of emotions, and its connection to other representations typically studied in cognitive science. I argue that people have an intuitive theory of emotion that is causally intertwined with their understanding of the physical and social world broadly. This intuitive theory allows us to use observed emotional cues as a window, to recover unobserved information about the world. I study these abilities in both adults and children, to gain insight into the most fundamental representations supporting such abilities. I also use computational models to capture the hierarchical, causal structure of this intuitive theory of emotion. In Study 1, I show that infants as young as 12-17 months can discriminate diverse within-valence emotional expressions elicited by funny, exciting, adorable, delicious, and sympathetic events, and map them onto their probable causes. In Study 2.1, I present that preschoolers can recover rich mental state information from observed emotional expressions. When the valence of someone's face changes between anticipated and actual outcomes, children by five gain insight into what she wants and believes about the world. Study 2.2 bridges theory of mind research, accounts of emotion attribution, and formal modeling, to provide a formal account of how people jointly infer beliefs and desires from emotional expressions. Study 3 tests children's understanding of social display rules. By middle childhood, children can use one person's emotional expressions regulated by a social context to infer the mental states of another. Altogether, these findings suggest that emotional cues provide a valuable entre into the unseen world. Not only adults, but also children, can use observed emotional expressions to infer their external causes and the internal mental states of other people. Although this intuitive theory of emotion may not necessarily mirror the actual processes of how emotions are generated, it supports rational inferences much of time, and it may be formed early in development. I see this work as bridging gaps across disciplines and helping advance the cognitive science of emotion understanding."
}, {
    "id": "oai:dspace.mit.edu:1721.1/81731",
    "title": "Function follows form : how connectivity patterns govern neural responses",
    "abstract": "Connectivity restricts and defines the information that a network can process. It is the substance of information processing that underlies the patterns of functional activity in the brain. By combining diffusion-weighted imaging or DWI, with fMRI, we are able to non-invasively measure connectivity and neural responses in the same individuals and directly relate these two measures to one another. In Chapter 2, I first establish the proof-of-principle that anatomical connectivity alone can predict neural responses in cortex, specifically of face-selectivity in the fusiform gyrus. I then extend this novel approach to the rest of the brain and test whether connectivity can accurately predict neural responses to various visual categories in Chapter 3. Finally, in Chapter 4, I compare and contrast the resulting models, which are essentially networks of connectivity that are functionally-relevant to each visual category, and demonstrate the type of knowledge that can be uncovered by directly integrating structure and function.",
    "advisors": ["John D. E. Gabrieli"],
    "text": "Function follows form : how connectivity patterns govern neural responses Connectivity restricts and defines the information that a network can process. It is the substance of information processing that underlies the patterns of functional activity in the brain. By combining diffusion-weighted imaging or DWI, with fMRI, we are able to non-invasively measure connectivity and neural responses in the same individuals and directly relate these two measures to one another. In Chapter 2, I first establish the proof-of-principle that anatomical connectivity alone can predict neural responses in cortex, specifically of face-selectivity in the fusiform gyrus. I then extend this novel approach to the rest of the brain and test whether connectivity can accurately predict neural responses to various visual categories in Chapter 3. Finally, in Chapter 4, I compare and contrast the resulting models, which are essentially networks of connectivity that are functionally-relevant to each visual category, and demonstrate the type of knowledge that can be uncovered by directly integrating structure and function."
}, {
    "id": "oai:dspace.mit.edu:1721.1/65286",
    "title": "A role for Dopamine neuron NMDA receptors in learning and decision-making",
    "abstract": "Midbrain dopamine has demonstrated roles in locomotion, motivation, associative learning, habit formation, action selection and cognition. The many functions of dopamine can be attributed to the multiple projection targets of midbrain dopaminergic nuclei and to the multiple characteristic modes of dopamine neuron firing, tonic and phasic. Phasic transients of midbrain dopamine neurons are widely reported to signal errors conveying discrepancies between predicted and actual reward. Knocking out NMDARs in dopamine neurons has been shown to attenuate dopaminergic phasic firing providing a potential model for delineating the functions of tonic and phasic dopamine. In order to study the role of dopamine neuron NMDARs in rewardcontingent learning, we developed an auditory-cued binary choice task using complex auditory stimuli that lend themselves to efficient learning as well as morphing. We report that mice lacking NMDARs in dopamine neurons have a deficit in learning an auditory two-alternative choice task, in the absence of changes in response vigor. Dopamine neurons respond phasically to rewards as well as reward predictive cues. Updating the reward predictive value of cues is fundamental to shaping adaptive patterns of behavior and decision-making. Given the hypothesized role of dopamine in the updating of reward predictive cues, we were interested to see if an influence of reward history would be evident in the choices made by mice lacking dopamine neuron NMDARs. In an auditory-cued binary choice task, we find an influence of the difficulty of prior successes on subsequent decisions when mice are challenged with varying degrees of discrimination difficulty. In mice lacking dopamine neuron NMDARs, we find a lack of influence of prior decision difficulty. Our results identify a modulation of choices by prior decision difficulty in mice, and demonstrate the dopamine-dependent nature of this modulation. These findings are consistent with a role for dopamine neuron phasic firing in the trial-by-trial shaping of reward contingent learning.",
    "advisors": ["Susumu Tonegawa"],
    "text": "A role for Dopamine neuron NMDA receptors in learning and decision-making Midbrain dopamine has demonstrated roles in locomotion, motivation, associative learning, habit formation, action selection and cognition. The many functions of dopamine can be attributed to the multiple projection targets of midbrain dopaminergic nuclei and to the multiple characteristic modes of dopamine neuron firing, tonic and phasic. Phasic transients of midbrain dopamine neurons are widely reported to signal errors conveying discrepancies between predicted and actual reward. Knocking out NMDARs in dopamine neurons has been shown to attenuate dopaminergic phasic firing providing a potential model for delineating the functions of tonic and phasic dopamine. In order to study the role of dopamine neuron NMDARs in rewardcontingent learning, we developed an auditory-cued binary choice task using complex auditory stimuli that lend themselves to efficient learning as well as morphing. We report that mice lacking NMDARs in dopamine neurons have a deficit in learning an auditory two-alternative choice task, in the absence of changes in response vigor. Dopamine neurons respond phasically to rewards as well as reward predictive cues. Updating the reward predictive value of cues is fundamental to shaping adaptive patterns of behavior and decision-making. Given the hypothesized role of dopamine in the updating of reward predictive cues, we were interested to see if an influence of reward history would be evident in the choices made by mice lacking dopamine neuron NMDARs. In an auditory-cued binary choice task, we find an influence of the difficulty of prior successes on subsequent decisions when mice are challenged with varying degrees of discrimination difficulty. In mice lacking dopamine neuron NMDARs, we find a lack of influence of prior decision difficulty. Our results identify a modulation of choices by prior decision difficulty in mice, and demonstrate the dopamine-dependent nature of this modulation. These findings are consistent with a role for dopamine neuron phasic firing in the trial-by-trial shaping of reward contingent learning."
}, {
    "id": "oai:dspace.mit.edu:1721.1/106440",
    "title": "Lateral hypothalamic control of motivated behaviors through the midbrain dopamine system",
    "abstract": "The lateral hypothalamus and ventral tegmental area are two brain regions that have long been known to be involved in processing reward and the control of feeding behaviors. We continue work in this area by identifying the functional connectivity between these two regions, providing evidence that LH neurons projecting to the VTA encode conditioned responses, while LH neurons innervated by the VTA encode conditioned and unconditioned stimuli. Activation of the LH-VTA projection can increase compulsive sugar seeking, while inhibition of the projection can suppress this behavior without altering normal feeding due to hunger. We can separate this projection into the GABAergic and glutamatergic components, and we show that the GABAergic component plays a role in promoting feeding and social interaction by increasing motivation for consummatory behaviors, while the glutamatergic component largely plays a role in the suppression of these behaviors. Finally, we show that activation of the GABAergic component causes dopamine release downstream in the nucleus accumbens via disinhibition of VTA dopamine neurons through VTA GABA neurons. Together, these experiments have profoundly elucidated the functional roles of the individual circuit components of the greater mesolimbic dopamine system and provided potential targets for therapeutic intervention of overeating disorders and obesity..",
    "advisors": ["Kay M. Tye"],
    "text": "Lateral hypothalamic control of motivated behaviors through the midbrain dopamine system The lateral hypothalamus and ventral tegmental area are two brain regions that have long been known to be involved in processing reward and the control of feeding behaviors. We continue work in this area by identifying the functional connectivity between these two regions, providing evidence that LH neurons projecting to the VTA encode conditioned responses, while LH neurons innervated by the VTA encode conditioned and unconditioned stimuli. Activation of the LH-VTA projection can increase compulsive sugar seeking, while inhibition of the projection can suppress this behavior without altering normal feeding due to hunger. We can separate this projection into the GABAergic and glutamatergic components, and we show that the GABAergic component plays a role in promoting feeding and social interaction by increasing motivation for consummatory behaviors, while the glutamatergic component largely plays a role in the suppression of these behaviors. Finally, we show that activation of the GABAergic component causes dopamine release downstream in the nucleus accumbens via disinhibition of VTA dopamine neurons through VTA GABA neurons. Together, these experiments have profoundly elucidated the functional roles of the individual circuit components of the greater mesolimbic dopamine system and provided potential targets for therapeutic intervention of overeating disorders and obesity.."
}, {
    "id": "oai:dspace.mit.edu:1721.1/100872",
    "title": "Timing and hippocampal information processing",
    "abstract": "Timing is a key component in hippocampal encoding of space. I will discuss three lines of work related to this theme. First, I will describe the fine-timescale characteristics of single neurons in hippocampal subregion CAl, where theta oscillations organize groups of neurons into orderly sequences. While theta was once thought to be synchronized throughout CAl, it was recently shown instead to be offset in time along the long axis of the hippocampus. Considering distant pairs of neurons, our fundamental sequence spiking property may instead be systematically staggered by these offsets in the rhythms that pace them. I tested the impact of theta wave time offsets by recording place cell spike sequences from groups of neurons in distant parts of CAl, and found that place cell sequences more closely coordinate with each other than the underlying theta oscillations do. In regions that differ from one another by 13 milliseconds of theta delay, place cell sequences are typically aligned to within 5 milliseconds. This raises the possibility that theta wave offsets serve another purpose, perhaps timing the communication with brain areas connected to different parts of CAl, while compensatory mechanisms are in place to preserve the fine temporal alignment of place cell spatial information. Second, I will describe a tool for closed-loop experiments using information decoded from hippocampal ensembles. Place cell activity is typically extracted and analyzed only after an experiment has ended. But interrogating the timing of hippocampal information, enhancing or interfering with it, requires decoding that information immediately. I will discuss some of the difficulties and the eventual implementation of a system capable of sequence time-scale position decoding and then survey the future experimental applications.",
    "advisors": ["Matthew Wilson"],
    "text": "Timing and hippocampal information processing Timing is a key component in hippocampal encoding of space. I will discuss three lines of work related to this theme. First, I will describe the fine-timescale characteristics of single neurons in hippocampal subregion CAl, where theta oscillations organize groups of neurons into orderly sequences. While theta was once thought to be synchronized throughout CAl, it was recently shown instead to be offset in time along the long axis of the hippocampus. Considering distant pairs of neurons, our fundamental sequence spiking property may instead be systematically staggered by these offsets in the rhythms that pace them. I tested the impact of theta wave time offsets by recording place cell spike sequences from groups of neurons in distant parts of CAl, and found that place cell sequences more closely coordinate with each other than the underlying theta oscillations do. In regions that differ from one another by 13 milliseconds of theta delay, place cell sequences are typically aligned to within 5 milliseconds. This raises the possibility that theta wave offsets serve another purpose, perhaps timing the communication with brain areas connected to different parts of CAl, while compensatory mechanisms are in place to preserve the fine temporal alignment of place cell spatial information. Second, I will describe a tool for closed-loop experiments using information decoded from hippocampal ensembles. Place cell activity is typically extracted and analyzed only after an experiment has ended. But interrogating the timing of hippocampal information, enhancing or interfering with it, requires decoding that information immediately. I will discuss some of the difficulties and the eventual implementation of a system capable of sequence time-scale position decoding and then survey the future experimental applications."
}, {
    "id": "oai:dspace.mit.edu:1721.1/79186",
    "title": "Dynamics of dopamine signaling and network activity in the striatum during learning and motivated pursuit of goals",
    "abstract": "Learning to direct behaviors towards goals is a central function of all vertebrate nervous systems. Initial learning often involves an exploratory phase, in which actions are flexible and highly variable. With repeated successful experience, behaviors may be guided by cues in the environment that reliably predict the desired outcome, and eventually behaviors can be executed as crystallized action sequences, or \"habits\", which are relatively inflexible. Parallel circuits through the basal ganglia and their inputs from midbrain dopamine neurons are believed to make critical contributions to these phases of learning and behavioral execution. To explore the neural mechanisms underlying goal-directed learning and behavior, I have employed electrophysiological and electrochemical techniques to measure neural activity and dopamine release in networks of the striatum, the principle input nucleus of the basal ganglia as rats learned to pursue rewards in mazes. The electrophysiological recordings revealed training dependent dynamics in striatum local field potentials and coordinated neural firing that may differentially support both network rigidity and flexibility during pursuit of goals. Electrochemical measurements of real-time dopamine signaling during maze running revealed prolonged signaling changes that may contribute to motivating or guiding behavior. Pathological over or under-expression of these network states may contribute to symptoms experienced in a range of basal ganglia disorders, from Parkinson's disease to drug addiction.",
    "advisors": ["Ann M. Graybiel"],
    "text": "Dynamics of dopamine signaling and network activity in the striatum during learning and motivated pursuit of goals Learning to direct behaviors towards goals is a central function of all vertebrate nervous systems. Initial learning often involves an exploratory phase, in which actions are flexible and highly variable. With repeated successful experience, behaviors may be guided by cues in the environment that reliably predict the desired outcome, and eventually behaviors can be executed as crystallized action sequences, or \"habits\", which are relatively inflexible. Parallel circuits through the basal ganglia and their inputs from midbrain dopamine neurons are believed to make critical contributions to these phases of learning and behavioral execution. To explore the neural mechanisms underlying goal-directed learning and behavior, I have employed electrophysiological and electrochemical techniques to measure neural activity and dopamine release in networks of the striatum, the principle input nucleus of the basal ganglia as rats learned to pursue rewards in mazes. The electrophysiological recordings revealed training dependent dynamics in striatum local field potentials and coordinated neural firing that may differentially support both network rigidity and flexibility during pursuit of goals. Electrochemical measurements of real-time dopamine signaling during maze running revealed prolonged signaling changes that may contribute to motivating or guiding behavior. Pathological over or under-expression of these network states may contribute to symptoms experienced in a range of basal ganglia disorders, from Parkinson's disease to drug addiction."
}, {
    "id": "oai:dspace.mit.edu:1721.1/106432",
    "title": "A connectomic analysis of the directional selectivity circuit in the mouse retina",
    "abstract": "This thesis addresses the question of how direction selectivity (DS) arises in the mouse retina. DS has long been observed in retinal ganglion cells, and more recently confirmed in the starburst amacrine cell. Upstream retinal bipolar cells, however, have been shown to lac, indicating that the mechanism that gives rise to DS lies in the inner plexiform layer, where the axons of bipolar cells costratify with amacrine and ganglion cells. We reconstructed a region of the IPL and identified cell types within it, and have discovered a mechanism which may explain the origin of DS activity in the mammalian retina, which relies on what we call \"space-time wiring specificity.\" It has been suggested that a DS signal can arise from non-DS excitatory inputs if at least one among spatially segregated inputs transmits its signal with some delay, which we extend to consider also a difference in the degree to which the signal is sustained. Previously, it has been supposed that this delay occurs within the starburst amacrine cells' dendrites. We hypothesized an alternative, presynaptic mechanism. We observed that different bipolar cell types, which are believed to express different degrees of sustained activity, contact different regions of the starburst amacrine cell dendrite, giving rise to a space-time wiring specifity that should produce a DS signal. We additionally provide a model that predicts the strength of DS as a function of the spatial segregation of inputs and the temporal delay.",
    "advisors": ["H. Sebastian Seung"],
    "text": "A connectomic analysis of the directional selectivity circuit in the mouse retina This thesis addresses the question of how direction selectivity (DS) arises in the mouse retina. DS has long been observed in retinal ganglion cells, and more recently confirmed in the starburst amacrine cell. Upstream retinal bipolar cells, however, have been shown to lac, indicating that the mechanism that gives rise to DS lies in the inner plexiform layer, where the axons of bipolar cells costratify with amacrine and ganglion cells. We reconstructed a region of the IPL and identified cell types within it, and have discovered a mechanism which may explain the origin of DS activity in the mammalian retina, which relies on what we call \"space-time wiring specificity.\" It has been suggested that a DS signal can arise from non-DS excitatory inputs if at least one among spatially segregated inputs transmits its signal with some delay, which we extend to consider also a difference in the degree to which the signal is sustained. Previously, it has been supposed that this delay occurs within the starburst amacrine cells' dendrites. We hypothesized an alternative, presynaptic mechanism. We observed that different bipolar cell types, which are believed to express different degrees of sustained activity, contact different regions of the starburst amacrine cell dendrite, giving rise to a space-time wiring specifity that should produce a DS signal. We additionally provide a model that predicts the strength of DS as a function of the spatial segregation of inputs and the temporal delay."
}, {
    "id": "oai:dspace.mit.edu:1721.1/8186",
    "title": "From space to episodes : modeling memory formation in the hippocampal-neocortical system",
    "abstract": "This thesis describes the use of mathematical, statistical, and computational methods to analyze, in two paradigmatic areas, what the hippocampus and associated structures do, and how they do it. The first model explores the formation of place fields in the hippocampus. This model is constrained by hippocampal anatomy and physiology and data on the effects of environmental manipulations on the place cell representation. It is based on an attractor network model of area CA3 in which recurrent interactions create place cell representations from location- and direction-specific activity in the entorhinal cortex, all under neuromodulatory influence. In unfamiliar environments, mossy fiber inputs impose activity patterns on CA3, and recurrent collaterals and perforant path inputs are subject to graded Hebbian plasticity. Attractors are thus sculpted in CA3, and are associated with entorhinal activity patterns. In familiar environments, place fields are controlled by the way that perforant path inputs select amongst the attractors. Depending on training experience, the model generates place fields that are either directional or non-directional, and whose changes when the environment undergoes simple geometric transformations are in accordance with experimental data. Representations of multiple environments can be stored and recalled with little interference, and have the appropriate degrees of similarity in visually similar environments.",
    "advisors": ["Peter Dayan"],
    "text": "From space to episodes : modeling memory formation in the hippocampal-neocortical system This thesis describes the use of mathematical, statistical, and computational methods to analyze, in two paradigmatic areas, what the hippocampus and associated structures do, and how they do it. The first model explores the formation of place fields in the hippocampus. This model is constrained by hippocampal anatomy and physiology and data on the effects of environmental manipulations on the place cell representation. It is based on an attractor network model of area CA3 in which recurrent interactions create place cell representations from location- and direction-specific activity in the entorhinal cortex, all under neuromodulatory influence. In unfamiliar environments, mossy fiber inputs impose activity patterns on CA3, and recurrent collaterals and perforant path inputs are subject to graded Hebbian plasticity. Attractors are thus sculpted in CA3, and are associated with entorhinal activity patterns. In familiar environments, place fields are controlled by the way that perforant path inputs select amongst the attractors. Depending on training experience, the model generates place fields that are either directional or non-directional, and whose changes when the environment undergoes simple geometric transformations are in accordance with experimental data. Representations of multiple environments can be stored and recalled with little interference, and have the appropriate degrees of similarity in visually similar environments."
}, {
    "id": "oai:dspace.mit.edu:1721.1/16714",
    "title": "A Bayesian framework for concept learning",
    "abstract": "Human concept learning presents a version of the classic problem of induction, which is made particularly difficult by the combination of two requirements: the need to learn from a rich (i.e. nested and overlapping) vocabulary of possible concepts and the need to be able to generalize concepts reasonably from only a few positive examples. I begin this thesis by considering a simple number concept game as a concrete illustration of this ability. On this task, human learners can with reasonable confidence lock in on one out of a billion billion billion logically possible concepts, after seeing only four positive examples of the concept, and can generalize informatively after seeing just a single example. Neither of the two classic approaches to inductive inference hypothesis testing in a constrained space of possible rules and computing similarity to the observed examples can provide a complete picture of how people generalize concepts in even this simple setting. This thesis proposes a new computational framework for understanding how people learn concepts from examples, based on the principles of Bayesian inference. By imposing the constraints of a probabilistic model of the learning situation, the Bayesian learner can draw out much more information about a concept's extension from a given set of observed examples than either rule-based or similarity-based approaches do, and can use this information in a rational way to infer the probability that any new object is also an instance of the concept. There are three components of the Bayesian framework: a prior probability distribution over a hypothesis space of possible concepts; a likelihood function, which scores each hypothesis according to its probability of generating the observed examples; and the principle of hypothesis averaging, under which the learner computes the probability of generalizing a concept to new objects by averaging the predictions of all hypotheses weighted by their posterior probability (proportional to the product of their priors and likelihoods). The likelihood, under the assumption of randomly sampled positive examples, embodies the size principle for scoring hypotheses: smaller consistent hypotheses are more likely than larger hypotheses, and they become exponentially more likely as the number of observed examples increases. The principle of hypothesis averaging allows the Bayesian framework to accommodate both rule-like and similarity-like generalization behavior, depending on how peaked the posterior probability is. Together, the size principle plus hypothesis averaging predict a convergence from similarity-like generalization (due to a broad posterior distribution) after very few examples are observed to rule-like generalization (due to a sharply peaked posterior distribution) after sufficiently many examples have been observed. The main contributions of this thesis are as follows. First and foremost, I show how it is possible for people to learn and generalize concepts from just one or a few positive examples (Chapter 2).  Building on that understanding, I then present a series of case studies of simple concept learning situations where the Bayesian framework yields both qualitative and quantitative insights into the real behavior of human learners (Chapters 3-5). These cases each focus on a different learning domain. Chapter 3 looks at generalization in continuous feature spaces, a typical representation of objects in psychology and machine learning with the virtues of being analytically tractable and empirically accessible, but the downside of being highly abstract and artificial. Chapter 4 moves to the more natural domain of learning words for categories of objects and shows the relevance of the same phenomena and explanatory principles introduced in the more abstract setting of Chapters 1-3 for real-world learning tasks like this one. In each of these domains, both similarity-like and rule-like generalization emerge as special cases of the Bayesian framework in the limits of very few or very many examples, respectively. However, the transition from similarity to rules occurs much faster in the word learning domain than in the continuous feature space domain. I propose a Bayesian explanation of this difference in learning curves that places crucial importance on the density or sparsity of overlapping hypotheses in the learner's hypothesis space. To test this proposal, a third case study (Chapter 5) returns to the domain of number concepts, in which human learners possess a more complex body of prior knowledge that leads to a hypothesis space with both sparse and densely overlapping components. Here, the Bayesian theory predicts and human learners produce either rule-based or similarity-based generalization from a few examples, depending on the precise examples observed. I also discusses how several classic reasoning heuristics may be used to approximate the much more elaborate computations of Bayesian inference that this domain requires. In each of these case studies, I confront some of the classic questions of concept learning and induction: Is the acquisition of concepts driven mainly by pre-existing knowledge or the statistical force of our observations? Is generalization based primarily on abstract rules or similarity to exemplars? I argue that in almost all instances, the only reasonable answer to such questions is, Both. More importantly, I show how the Bayesian framework allows us to answer much more penetrating versions of these questions: How does prior knowledge interact with the observed examples to guide generalization? Why does generalization appear rule-based in some cases and similarity-based in others? Finally, Chapter 6 summarizes the major contributions in more detailed form and discusses how this work ts into the larger picture of contemporary research on human learning, thinking, and reasoning.",
    "advisors": ["Whitman A. Richards"],
    "text": "A Bayesian framework for concept learning Human concept learning presents a version of the classic problem of induction, which is made particularly difficult by the combination of two requirements: the need to learn from a rich (i.e. nested and overlapping) vocabulary of possible concepts and the need to be able to generalize concepts reasonably from only a few positive examples. I begin this thesis by considering a simple number concept game as a concrete illustration of this ability. On this task, human learners can with reasonable confidence lock in on one out of a billion billion billion logically possible concepts, after seeing only four positive examples of the concept, and can generalize informatively after seeing just a single example. Neither of the two classic approaches to inductive inference hypothesis testing in a constrained space of possible rules and computing similarity to the observed examples can provide a complete picture of how people generalize concepts in even this simple setting. This thesis proposes a new computational framework for understanding how people learn concepts from examples, based on the principles of Bayesian inference. By imposing the constraints of a probabilistic model of the learning situation, the Bayesian learner can draw out much more information about a concept's extension from a given set of observed examples than either rule-based or similarity-based approaches do, and can use this information in a rational way to infer the probability that any new object is also an instance of the concept. There are three components of the Bayesian framework: a prior probability distribution over a hypothesis space of possible concepts; a likelihood function, which scores each hypothesis according to its probability of generating the observed examples; and the principle of hypothesis averaging, under which the learner computes the probability of generalizing a concept to new objects by averaging the predictions of all hypotheses weighted by their posterior probability (proportional to the product of their priors and likelihoods). The likelihood, under the assumption of randomly sampled positive examples, embodies the size principle for scoring hypotheses: smaller consistent hypotheses are more likely than larger hypotheses, and they become exponentially more likely as the number of observed examples increases. The principle of hypothesis averaging allows the Bayesian framework to accommodate both rule-like and similarity-like generalization behavior, depending on how peaked the posterior probability is. Together, the size principle plus hypothesis averaging predict a convergence from similarity-like generalization (due to a broad posterior distribution) after very few examples are observed to rule-like generalization (due to a sharply peaked posterior distribution) after sufficiently many examples have been observed. The main contributions of this thesis are as follows. First and foremost, I show how it is possible for people to learn and generalize concepts from just one or a few positive examples (Chapter 2).  Building on that understanding, I then present a series of case studies of simple concept learning situations where the Bayesian framework yields both qualitative and quantitative insights into the real behavior of human learners (Chapters 3-5). These cases each focus on a different learning domain. Chapter 3 looks at generalization in continuous feature spaces, a typical representation of objects in psychology and machine learning with the virtues of being analytically tractable and empirically accessible, but the downside of being highly abstract and artificial. Chapter 4 moves to the more natural domain of learning words for categories of objects and shows the relevance of the same phenomena and explanatory principles introduced in the more abstract setting of Chapters 1-3 for real-world learning tasks like this one. In each of these domains, both similarity-like and rule-like generalization emerge as special cases of the Bayesian framework in the limits of very few or very many examples, respectively. However, the transition from similarity to rules occurs much faster in the word learning domain than in the continuous feature space domain. I propose a Bayesian explanation of this difference in learning curves that places crucial importance on the density or sparsity of overlapping hypotheses in the learner's hypothesis space. To test this proposal, a third case study (Chapter 5) returns to the domain of number concepts, in which human learners possess a more complex body of prior knowledge that leads to a hypothesis space with both sparse and densely overlapping components. Here, the Bayesian theory predicts and human learners produce either rule-based or similarity-based generalization from a few examples, depending on the precise examples observed. I also discusses how several classic reasoning heuristics may be used to approximate the much more elaborate computations of Bayesian inference that this domain requires. In each of these case studies, I confront some of the classic questions of concept learning and induction: Is the acquisition of concepts driven mainly by pre-existing knowledge or the statistical force of our observations? Is generalization based primarily on abstract rules or similarity to exemplars? I argue that in almost all instances, the only reasonable answer to such questions is, Both. More importantly, I show how the Bayesian framework allows us to answer much more penetrating versions of these questions: How does prior knowledge interact with the observed examples to guide generalization? Why does generalization appear rule-based in some cases and similarity-based in others? Finally, Chapter 6 summarizes the major contributions in more detailed form and discusses how this work ts into the larger picture of contemporary research on human learning, thinking, and reasoning."
}, {
    "id": "oai:dspace.mit.edu:1721.1/89864",
    "title": "Functional and computational analysis of RNA-binding proteins and their roles in cancer",
    "abstract": "This work is concerned with mRNA processing in mammalian cells and proceeds in two parts. In the first part, I introduce a computational framework for inferring the abundances of mRNA isoforms using high-throughput RNA sequencing data. This framework was applied to study the targets of the ubiquitous splicing factor hnRNP H in human cells. In the second part, I describe an experimental study of the Musashi (hnRNP-like) family of RNA-binding proteins in stem cells and cancer cells, which incorporates computational analyses that rely heavily on the framework developed in part one. In sum, this work provides a computational framework of general use in global analyses of RNA processing and its protein regulators, as well as functional insights into a family of poorly understood RNA-binding proteins. Several related analyses and techniques developed as part of the thesis are described in Appendix A-C. Appendix A describes a study of activity-dependent gene expression and mRNA processing in the mouse olfactory bulb. It uses computational techniques developed in part one of the thesis. Appendix B describes a technique for quantitative visualization of alternative splicing from RNA sequencing data and its integration into a genome browser. Appendix C describes a method for clonal analysis of neural stem cell growth and differentiation in culture using live imaging and `microdot' plates, developed as part of the work presented in part one of the thesis.",
    "advisors": ["Christopher B. Burge", "Rudolf Jaenisch"],
    "text": "Functional and computational analysis of RNA-binding proteins and their roles in cancer This work is concerned with mRNA processing in mammalian cells and proceeds in two parts. In the first part, I introduce a computational framework for inferring the abundances of mRNA isoforms using high-throughput RNA sequencing data. This framework was applied to study the targets of the ubiquitous splicing factor hnRNP H in human cells. In the second part, I describe an experimental study of the Musashi (hnRNP-like) family of RNA-binding proteins in stem cells and cancer cells, which incorporates computational analyses that rely heavily on the framework developed in part one. In sum, this work provides a computational framework of general use in global analyses of RNA processing and its protein regulators, as well as functional insights into a family of poorly understood RNA-binding proteins. Several related analyses and techniques developed as part of the thesis are described in Appendix A-C. Appendix A describes a study of activity-dependent gene expression and mRNA processing in the mouse olfactory bulb. It uses computational techniques developed in part one of the thesis. Appendix B describes a technique for quantitative visualization of alternative splicing from RNA sequencing data and its integration into a genome browser. Appendix C describes a method for clonal analysis of neural stem cell growth and differentiation in culture using live imaging and `microdot' plates, developed as part of the work presented in part one of the thesis."
}, {
    "id": "oai:dspace.mit.edu:1721.1/73696",
    "title": "Towards a unified account of face (and maybe object) processing",
    "abstract": "Faces are an important class of visual stimuli, and are thought to be processed differently from objects by the human visual system. Going beyond the false dichotomy of same versus different processing, it is more important to understand how exactly faces are processed similarly or differently from objects. However, even by itself, face processing is poorly understood. Various aspects of face processing, such as holistic, configural, and face-space processing, are investigated in relative isolation, and the relationships between these are unclear. Furthermore, face processing is characteristically affected by various stimulus transformations such as inversion, contrast reversal and spatial frequency filtering, but how or why is unclear. Most importantly, we do not understand even the basic mechanisms of face processing. We hypothesize that what makes face processing distinctive is the existence of large, coarse face templates. We test our hypothesis by modifying an existing model of object processing to utilize such templates, and find that our model can account for many face-related phenomena. Using small, fine face templates as a control, we find that our model displays object-like processing characteristics instead. Overall, we believe that we may have made the first steps towards achieving a unified account of face processing. In addition, results from our control suggest that face and object processing share fundamental computational mechanisms. Coupled with recent advances in brain recording techniques, our results mean that face recognition could form the \"tip of the spear\" for attacking and solving the problem of visual recognition.",
    "advisors": ["Tomaso A. Poggio"],
    "text": "Towards a unified account of face (and maybe object) processing Faces are an important class of visual stimuli, and are thought to be processed differently from objects by the human visual system. Going beyond the false dichotomy of same versus different processing, it is more important to understand how exactly faces are processed similarly or differently from objects. However, even by itself, face processing is poorly understood. Various aspects of face processing, such as holistic, configural, and face-space processing, are investigated in relative isolation, and the relationships between these are unclear. Furthermore, face processing is characteristically affected by various stimulus transformations such as inversion, contrast reversal and spatial frequency filtering, but how or why is unclear. Most importantly, we do not understand even the basic mechanisms of face processing. We hypothesize that what makes face processing distinctive is the existence of large, coarse face templates. We test our hypothesis by modifying an existing model of object processing to utilize such templates, and find that our model can account for many face-related phenomena. Using small, fine face templates as a control, we find that our model displays object-like processing characteristics instead. Overall, we believe that we may have made the first steps towards achieving a unified account of face processing. In addition, results from our control suggest that face and object processing share fundamental computational mechanisms. Coupled with recent advances in brain recording techniques, our results mean that face recognition could form the \"tip of the spear\" for attacking and solving the problem of visual recognition."
}, {
    "id": "oai:dspace.mit.edu:1721.1/100860",
    "title": "Modeling cognition with probabilistic programs : representations and algorithms",
    "abstract": "This thesis develops probabilistic programming as a productive metaphor for understanding cognition, both with respect to mental representations and the manipulation of such representations. In the first half of the thesis, I demonstrate the representational power of probabilistic programs in the domains of concept learning and social reasoning. I provide examples of richly structured concepts, defined in terms of systems of relations, subparts, and recursive embeddings, that are naturally expressed as programs and show initial experimental evidence that they match human generalization patterns. I then proceed to models of reasoning about reasoning, a domain where the expressive power of probabilistic programs is necessary to formalize our intuitive domain understanding due to the fact that, unlike previous formalisms, probabilistic programs allow conditioning to be represented in a model, not just applied to a model. I illustrate this insight with programs that model nested reasoning in game theory, artificial intelligence, and linguistics. In the second half, I develop three inference algorithms with the dual intent of showing how to efficiently compute the marginal distributions defined by probabilistic programs, and providing building blocks for process-level accounts of human cognition. First, I describe a Dynamic Programming algorithm for computing the marginal distribution of discrete probabilistic programs by compiling to systems of equations and show that it can make inference in models of \"reasoning about reasoning\" tractable by merging and reusing subcomputations. Second, I introduce the setting of amortized inference and show how learning inverse models lets us leverage samples generated by other inference algorithms to compile probabilistic models into fast recognition functions. Third, I develop a generic approach to coarse-to-fine inference in probabilistic programs and provide evidence that it can speed up inference in models with large state spaces that have appropriate hierarchical structure. Finally, I substantiate the claim that probabilistic programming is a productive metaphor by outlining new research questions that have been opened up by this line of investigation.",
    "advisors": ["Noah D. Goodman", "Joshua B. Tenenbaum"],
    "text": "Modeling cognition with probabilistic programs : representations and algorithms This thesis develops probabilistic programming as a productive metaphor for understanding cognition, both with respect to mental representations and the manipulation of such representations. In the first half of the thesis, I demonstrate the representational power of probabilistic programs in the domains of concept learning and social reasoning. I provide examples of richly structured concepts, defined in terms of systems of relations, subparts, and recursive embeddings, that are naturally expressed as programs and show initial experimental evidence that they match human generalization patterns. I then proceed to models of reasoning about reasoning, a domain where the expressive power of probabilistic programs is necessary to formalize our intuitive domain understanding due to the fact that, unlike previous formalisms, probabilistic programs allow conditioning to be represented in a model, not just applied to a model. I illustrate this insight with programs that model nested reasoning in game theory, artificial intelligence, and linguistics. In the second half, I develop three inference algorithms with the dual intent of showing how to efficiently compute the marginal distributions defined by probabilistic programs, and providing building blocks for process-level accounts of human cognition. First, I describe a Dynamic Programming algorithm for computing the marginal distribution of discrete probabilistic programs by compiling to systems of equations and show that it can make inference in models of \"reasoning about reasoning\" tractable by merging and reusing subcomputations. Second, I introduce the setting of amortized inference and show how learning inverse models lets us leverage samples generated by other inference algorithms to compile probabilistic models into fast recognition functions. Third, I develop a generic approach to coarse-to-fine inference in probabilistic programs and provide evidence that it can speed up inference in models with large state spaces that have appropriate hierarchical structure. Finally, I substantiate the claim that probabilistic programming is a productive metaphor by outlining new research questions that have been opened up by this line of investigation."
}, {
    "id": "oai:dspace.mit.edu:1721.1/62716",
    "title": "Brain circuits for the representation of subjective reward value",
    "abstract": "Successful interaction with the external world requires choosing appropriate actions in the context of available choices. Such decisions require the evaluation of the reward magnitude, or value, associated with each potential action. Delineating the neural circuits involved in this process remains an important goal in systems neuroscience. However, little is known about the neural circuits that compute, or represent, low level primary reward signals. We have combined quantitative psychophysical measures of subjective reward magnitude elicited by rewarding electrical brain stimulation, fMRI as a readout of whole-brain neural activity, and local inactivation of brain structures, to identify the neural representation of subjective reward magnitude. We find that multiple brain regions are activated by rewarding brain stimulation, but only two brain regions, the nucleus accumbens and the central and basolateral nucleus of the amygdala, exhibit patterns of activity levels that track the reward magnitude measured psychophysically, suggesting a role in the neural representation of reward magnitude. Furthermore, pharmacological silencing of the ventral tegmental area (VTA) disrupts reward-tracking behavior and increases stimulus-dependent activity in the nucleus accumbens and amygdala. Together these data suggest that ascending and descending pathways combine to produce a signal that ultimately guides behavior and is subject to modulation by VTA inputs.",
    "advisors": ["Alan Pradip Jasanoff"],
    "text": "Brain circuits for the representation of subjective reward value Successful interaction with the external world requires choosing appropriate actions in the context of available choices. Such decisions require the evaluation of the reward magnitude, or value, associated with each potential action. Delineating the neural circuits involved in this process remains an important goal in systems neuroscience. However, little is known about the neural circuits that compute, or represent, low level primary reward signals. We have combined quantitative psychophysical measures of subjective reward magnitude elicited by rewarding electrical brain stimulation, fMRI as a readout of whole-brain neural activity, and local inactivation of brain structures, to identify the neural representation of subjective reward magnitude. We find that multiple brain regions are activated by rewarding brain stimulation, but only two brain regions, the nucleus accumbens and the central and basolateral nucleus of the amygdala, exhibit patterns of activity levels that track the reward magnitude measured psychophysically, suggesting a role in the neural representation of reward magnitude. Furthermore, pharmacological silencing of the ventral tegmental area (VTA) disrupts reward-tracking behavior and increases stimulus-dependent activity in the nucleus accumbens and amygdala. Together these data suggest that ascending and descending pathways combine to produce a signal that ultimately guides behavior and is subject to modulation by VTA inputs."
}, {
    "id": "oai:dspace.mit.edu:1721.1/8024",
    "title": "From representation to recognition : MEG studies of face perception",
    "abstract": "Face recognition is one of the most important problems our visual system must solve. Here I used magnetoencephalography (MEG) in an effort to characterize the sequence of cognitive and neural processes underlying this remarkable ability. This work is designed to answer several questions. First, how long does it take for the human visual system to recognize a stimulus as a face? Second, what are the stages of processing in face perception? Finally, what is the nature of representations extracted at each of these stages? MEG provides an ideal tool for addressing these questions, as its high temporal resolution enables us to separately measure perceptual operations that may occur only a few tens of milliseconds apart from each other. Yet, unlike single-unit recording, it can be used in normal human subjects. Three new findings about human face recognition will be reported in this thesis. First, a face stimulus begins to be categorized as a face within 100 ms after stimulus onset in humans, substantially faster than previously thought. Second, face recognition occurs in two distinct stages: an initial stage at which the stimulus is categorized as a face, and a stage that occurs 70 ms later at which the individual identity of the face is extracted. Finally, the representations extracted at these two stages differ not only in specificity, but also in the aspects of a face represented at each stage.",
    "advisors": ["Nancy G. Kanwisher"],
    "text": "From representation to recognition : MEG studies of face perception Face recognition is one of the most important problems our visual system must solve. Here I used magnetoencephalography (MEG) in an effort to characterize the sequence of cognitive and neural processes underlying this remarkable ability. This work is designed to answer several questions. First, how long does it take for the human visual system to recognize a stimulus as a face? Second, what are the stages of processing in face perception? Finally, what is the nature of representations extracted at each of these stages? MEG provides an ideal tool for addressing these questions, as its high temporal resolution enables us to separately measure perceptual operations that may occur only a few tens of milliseconds apart from each other. Yet, unlike single-unit recording, it can be used in normal human subjects. Three new findings about human face recognition will be reported in this thesis. First, a face stimulus begins to be categorized as a face within 100 ms after stimulus onset in humans, substantially faster than previously thought. Second, face recognition occurs in two distinct stages: an initial stage at which the stimulus is categorized as a face, and a stage that occurs 70 ms later at which the individual identity of the face is extracted. Finally, the representations extracted at these two stages differ not only in specificity, but also in the aspects of a face represented at each stage."
}, {
    "id": "oai:dspace.mit.edu:1721.1/68421",
    "title": "The role of real-world size in object representation",
    "abstract": "Every object in the world has a physical size which is intrinsic to how we interact with it: we pick up small objects like coins with our fingers, we throw footballs and swing tennis rackets, we orient our body to bigger objects like chairs and tables and we navigate with respect to landmarks like fountains and buildings. Here I argue that the size of objects in the world is a basic property of object representation with both behavioral and neural consequences. Specifically, I suggest that objects have a canonical visual size based on their real-world size (Chapter 2), and that we automatically access real-world size information when we recognize an object (Chapter 3). Further, I present evidence that there are neural consequences of realworld size for the large-scale organization of object knowledge in ventral visual cortex (Chapter 4). Specifically, there are regions with differential selectivity for big and small objects, that span from along the dorsal and lateral surfaces of occipito-temporal cortex in a mirrored organization. Finally, I suggest that the empirical findings can be coherently explained by thinking about the experience of an observer situated in a three-dimensional world. This work provides testable predictions about retinal size biases in visual experience, and an approach in which to understand the neural representation of any object in the world.",
    "advisors": ["Aude Oliva"],
    "text": "The role of real-world size in object representation Every object in the world has a physical size which is intrinsic to how we interact with it: we pick up small objects like coins with our fingers, we throw footballs and swing tennis rackets, we orient our body to bigger objects like chairs and tables and we navigate with respect to landmarks like fountains and buildings. Here I argue that the size of objects in the world is a basic property of object representation with both behavioral and neural consequences. Specifically, I suggest that objects have a canonical visual size based on their real-world size (Chapter 2), and that we automatically access real-world size information when we recognize an object (Chapter 3). Further, I present evidence that there are neural consequences of realworld size for the large-scale organization of object knowledge in ventral visual cortex (Chapter 4). Specifically, there are regions with differential selectivity for big and small objects, that span from along the dorsal and lateral surfaces of occipito-temporal cortex in a mirrored organization. Finally, I suggest that the empirical findings can be coherently explained by thinking about the experience of an observer situated in a three-dimensional world. This work provides testable predictions about retinal size biases in visual experience, and an approach in which to understand the neural representation of any object in the world."
}, {
    "id": "oai:dspace.mit.edu:1721.1/62715",
    "title": "The hemo-neural hypothesis : effects of vasodilation on astrocytes in mammalian neocortex",
    "abstract": "Astrocytes play an important role in regulating neuronal activity and local brain states, in part by serving as intermediaries between neurons and vasculature. We postulate that neurons and astrocytes are sensitive to biophysical conditions in their local environment, in addition to their participation in traditional signaling networks with other neurons. Mechanically sensitive astrocytic endfeet ensheathe cerebral blood vessels, which change size in order to regulate blood flow. We found that changes in local biophysical state caused by mechanical perturbations exerted through blood vessels can depolarize astrocytes and some neurons in slice. To test the hemoneural hypothesis in vivo, we developed a means of inducing dilation using the SUR2B receptor agonist pinacidil, which is specific to vascular smooth muscle. It was important to ascertain that pinacidil had no direct effect on astrocytes or neurons, and we confirmed this in whole cell recordings in cortical slices. We then used two-photon imaging to visualize astrocytic calcium dynamics in vivo while manipulating vasodilation in vivo. Pinacidil caused a 10-20% dilation in most vessels, a degree of dilation of similar magnitude to those naturally evoked by persistent sensory stimulation (e.g. in fMRI studies). We found that increases in pial arteriole diameter could occasionally evoke traveling calcium waves in astrocytes. We also saw consistently slow increases (which took tens of seconds to onset, and persisted for minutes) in astrocytic calcium levels at both endfeet and soma in cortical layer 1, corresponding to vessel dilation. When vessels partially reconstricted due to pinacidil washout, calcium levels also showed a relative decrease. At short time scales (from 0.5 - 5 seconds) we saw strong correlations (>0.5) between small fluctuations in astrocytic calcium levels (1-3%) and vessel diameter (1-3%). Fluctuations in vessel diameter predicted similar fluctuations in astrocytic calcium, as often and as strongly as the reverse, suggesting feedback regulation between vascular diameter and astrocytic calcium activation levels.",
    "text": "The hemo-neural hypothesis : effects of vasodilation on astrocytes in mammalian neocortex Astrocytes play an important role in regulating neuronal activity and local brain states, in part by serving as intermediaries between neurons and vasculature. We postulate that neurons and astrocytes are sensitive to biophysical conditions in their local environment, in addition to their participation in traditional signaling networks with other neurons. Mechanically sensitive astrocytic endfeet ensheathe cerebral blood vessels, which change size in order to regulate blood flow. We found that changes in local biophysical state caused by mechanical perturbations exerted through blood vessels can depolarize astrocytes and some neurons in slice. To test the hemoneural hypothesis in vivo, we developed a means of inducing dilation using the SUR2B receptor agonist pinacidil, which is specific to vascular smooth muscle. It was important to ascertain that pinacidil had no direct effect on astrocytes or neurons, and we confirmed this in whole cell recordings in cortical slices. We then used two-photon imaging to visualize astrocytic calcium dynamics in vivo while manipulating vasodilation in vivo. Pinacidil caused a 10-20% dilation in most vessels, a degree of dilation of similar magnitude to those naturally evoked by persistent sensory stimulation (e.g. in fMRI studies). We found that increases in pial arteriole diameter could occasionally evoke traveling calcium waves in astrocytes. We also saw consistently slow increases (which took tens of seconds to onset, and persisted for minutes) in astrocytic calcium levels at both endfeet and soma in cortical layer 1, corresponding to vessel dilation. When vessels partially reconstricted due to pinacidil washout, calcium levels also showed a relative decrease. At short time scales (from 0.5 - 5 seconds) we saw strong correlations (>0.5) between small fluctuations in astrocytic calcium levels (1-3%) and vessel diameter (1-3%). Fluctuations in vessel diameter predicted similar fluctuations in astrocytic calcium, as often and as strongly as the reverse, suggesting feedback regulation between vascular diameter and astrocytic calcium activation levels."
}, {
    "id": "oai:dspace.mit.edu:1721.1/29989",
    "title": "Practical probabilistic inference",
    "abstract": "The design and use of expert systems for medical diagnosis remains an attractive goal. One such system, the Quick Medical Reference, Decision Theoretic (QMR-DT), is based on a Bayesian network. This very large-scale network models the appearance and manifestation of disease and has approximately 600 unobservable nodes and 4000 observable nodes that represent, respectively, the presence and measurable manifestation of disease in a patient. Exact inference of posterior distributions over the disease nodes is extremely intractable using generic algorithms. Inference can be made much more efficient by exploiting the QMR-DT's unique structure. Indeed, tailor-made inference algorithms for the QMR-DT efficiently generate exact disease posterior marginals for some diagnostic problems and accurate approximate posteriors for others. In this thesis, I identify a risk with using the QMR-DT disease posteriors for medical diagnosis. Specifically, I show that patients and physicians conspire to preferentially report findings that suggest the presence of disease. Because the QMR-DT does not contain an explicit model of this reporting bias, its disease posteriors may not be useful for diagnosis. Correcting these posteriors requires augmenting the QMR-DT with additional variables and dependencies that model the diagnostic procedure. I introduce the diagnostic QMR-DT (dQMR-DT), a Bayesian network containing both the QMR-DT and a simple model of the diagnostic procedure. Using diagnostic problems sampled from the dQMR-DT, I show the danger of doing diagnosis using disease posteriors from the unaugmented QMR-DT.",
    "advisors": ["Whitman Richards"],
    "text": "Practical probabilistic inference The design and use of expert systems for medical diagnosis remains an attractive goal. One such system, the Quick Medical Reference, Decision Theoretic (QMR-DT), is based on a Bayesian network. This very large-scale network models the appearance and manifestation of disease and has approximately 600 unobservable nodes and 4000 observable nodes that represent, respectively, the presence and measurable manifestation of disease in a patient. Exact inference of posterior distributions over the disease nodes is extremely intractable using generic algorithms. Inference can be made much more efficient by exploiting the QMR-DT's unique structure. Indeed, tailor-made inference algorithms for the QMR-DT efficiently generate exact disease posterior marginals for some diagnostic problems and accurate approximate posteriors for others. In this thesis, I identify a risk with using the QMR-DT disease posteriors for medical diagnosis. Specifically, I show that patients and physicians conspire to preferentially report findings that suggest the presence of disease. Because the QMR-DT does not contain an explicit model of this reporting bias, its disease posteriors may not be useful for diagnosis. Correcting these posteriors requires augmenting the QMR-DT with additional variables and dependencies that model the diagnostic procedure. I introduce the diagnostic QMR-DT (dQMR-DT), a Bayesian network containing both the QMR-DT and a simple model of the diagnostic procedure. Using diagnostic problems sampled from the dQMR-DT, I show the danger of doing diagnosis using disease posteriors from the unaugmented QMR-DT."
}, {
    "id": "oai:dspace.mit.edu:1721.1/107868",
    "title": "4D mapping of network-specific pathological propagation in Alzheimer's disease",
    "abstract": "Alzheimer's disease (AD) causes a devastating loss of memory and cognition for which there is no cure. Without effective treatments that slow or reverse the course of the disease, the rapidly aging population will require astronomical investment from society to care for the increasing numbers of AD patients. Additionally, the financial and emotional burden on families of affected individuals will be profound. Traditional approaches to the study of AD use either biochemical assays to probe cellular pathophysiology or non-invasive imaging platforms to investigate brain-wide network alterations. Though decades of research using these tools have advanced the field significantly, our increased understanding of AD has not led to successful interventions. One reason for this impediment may be that the tools used in neither approach can achieve the spatial and temporal precision necessary to study the consequences of molecular insults across the brain over time. In this thesis, I capitalize on recent advances in tissue processing technologies to gain a network-level perspective on the molecular and cellular progression of AD. First, I present optimized methods for in situ proteomic phenotyping of large-volume tissue specimens. Then, I use the techniques to map amyloid-beta (A[beta]) aggregates at the whole-brain scale across disease stages in a mouse model of AD. The spatially-unbiased, temporally-precise map demonstrates hierarchical susceptibility of increasingly large, memory-related brain networks to A[beta] deposition. Importantly, the 4D nature of the map reveals that subcortical nodes and white matter tracts of the Papez memory circuit exhibit unique, early vulnerability to A[beta] aggregates. Finally, using large-volume labeling approaches, I confirm the molecular findings by showing disease-specific A[beta] aggregation in human samples from the early hub regions. Together, this data unites desperate observations of network-level deficits and identifies critical locations of early A[beta] deposition in the brain. By linking molecular and network observations, I begin to provide biological explanations for the clinical manifestation of AD. This perspective can guide earlier patient identification and refine experimental approaches to developing cognitively efficacious treatments. These discoveries emphasize the necessity of multi-level investigations in neuroscience research and highlight the potential impacts of tools that enable researchers to bridge the gap.",
    "advisors": ["Li-Huei Tsai"],
    "text": "4D mapping of network-specific pathological propagation in Alzheimer's disease Alzheimer's disease (AD) causes a devastating loss of memory and cognition for which there is no cure. Without effective treatments that slow or reverse the course of the disease, the rapidly aging population will require astronomical investment from society to care for the increasing numbers of AD patients. Additionally, the financial and emotional burden on families of affected individuals will be profound. Traditional approaches to the study of AD use either biochemical assays to probe cellular pathophysiology or non-invasive imaging platforms to investigate brain-wide network alterations. Though decades of research using these tools have advanced the field significantly, our increased understanding of AD has not led to successful interventions. One reason for this impediment may be that the tools used in neither approach can achieve the spatial and temporal precision necessary to study the consequences of molecular insults across the brain over time. In this thesis, I capitalize on recent advances in tissue processing technologies to gain a network-level perspective on the molecular and cellular progression of AD. First, I present optimized methods for in situ proteomic phenotyping of large-volume tissue specimens. Then, I use the techniques to map amyloid-beta (A[beta]) aggregates at the whole-brain scale across disease stages in a mouse model of AD. The spatially-unbiased, temporally-precise map demonstrates hierarchical susceptibility of increasingly large, memory-related brain networks to A[beta] deposition. Importantly, the 4D nature of the map reveals that subcortical nodes and white matter tracts of the Papez memory circuit exhibit unique, early vulnerability to A[beta] aggregates. Finally, using large-volume labeling approaches, I confirm the molecular findings by showing disease-specific A[beta] aggregation in human samples from the early hub regions. Together, this data unites desperate observations of network-level deficits and identifies critical locations of early A[beta] deposition in the brain. By linking molecular and network observations, I begin to provide biological explanations for the clinical manifestation of AD. This perspective can guide earlier patient identification and refine experimental approaches to developing cognitively efficacious treatments. These discoveries emphasize the necessity of multi-level investigations in neuroscience research and highlight the potential impacts of tools that enable researchers to bridge the gap."
}, {
    "id": "oai:dspace.mit.edu:1721.1/120871",
    "title": "Building a state space for song learning",
    "abstract": "Song learning circuitry is thought to operate using a unique representation of each moment within each song syllable. Distinct timestamps for each moment in the song have been observed in the premotor cortical nucleus HVC, where neurons burst in sparse sequences. However, such sparse sequences are not present in very young birds, which sing highly variable syllables of random lengths. Furthermore, young birds learn by imitating a tutor song, and it was previously unclear precisely how the experience of hearing a tutor might shape auditory, motor, and evaluation pathways in the songbird brain. My thesis presents a framework for how these pathways may assemble during early learning, using simple neural mechanisms. I start with a neural network model for how premotor sequences may grow and split. This model predicts that the sequence-generating nucleus HVC would receive rhythmically patterned training inputs. I found such a signal when I recorded neurons that project to HVC. When juvenile birds sing, these neurons burst at the beginning of each syllable, and when the birds listen to a tutor, neurons burst at the rhythm of the tutor's song. Bursts marking the beginning of every tutor syllable could seed chains of sequential activity in HVC that could be used to generate the bird's own song imitation. I next used functional calcium imaging to characterize HVC sequences before and after tutor exposure. Analysis of these datasets led us to develop a new method for unsupervised detection of neural sequences. Using this method, I was able to observe neural sequences even prior to tutor exposure. Some of these sequences could be tracked as new syllables emerged after tutor exposure, and some sequences appeared to become coupled to the new syllables. In light of my new data, I expand on previous models of song learning to form a detailed hypothesis for how simple neural processes may perform song learning from start to finish.",
    "advisors": ["Michale S. Fee"],
    "text": "Building a state space for song learning Song learning circuitry is thought to operate using a unique representation of each moment within each song syllable. Distinct timestamps for each moment in the song have been observed in the premotor cortical nucleus HVC, where neurons burst in sparse sequences. However, such sparse sequences are not present in very young birds, which sing highly variable syllables of random lengths. Furthermore, young birds learn by imitating a tutor song, and it was previously unclear precisely how the experience of hearing a tutor might shape auditory, motor, and evaluation pathways in the songbird brain. My thesis presents a framework for how these pathways may assemble during early learning, using simple neural mechanisms. I start with a neural network model for how premotor sequences may grow and split. This model predicts that the sequence-generating nucleus HVC would receive rhythmically patterned training inputs. I found such a signal when I recorded neurons that project to HVC. When juvenile birds sing, these neurons burst at the beginning of each syllable, and when the birds listen to a tutor, neurons burst at the rhythm of the tutor's song. Bursts marking the beginning of every tutor syllable could seed chains of sequential activity in HVC that could be used to generate the bird's own song imitation. I next used functional calcium imaging to characterize HVC sequences before and after tutor exposure. Analysis of these datasets led us to develop a new method for unsupervised detection of neural sequences. Using this method, I was able to observe neural sequences even prior to tutor exposure. Some of these sequences could be tracked as new syllables emerged after tutor exposure, and some sequences appeared to become coupled to the new syllables. In light of my new data, I expand on previous models of song learning to form a detailed hypothesis for how simple neural processes may perform song learning from start to finish."
}, {
    "id": "oai:dspace.mit.edu:1721.1/61877",
    "title": "Activity-dependent integration and plasticity of new neurons during postnatal neurogenesis",
    "abstract": "Most neurons are born during the embryonic period to become the building blocks for a variety of brain circuits. However, two brain regions only start to assemble during the postnatal period. Both brain areas, olfactory bulb and dentate gyrus, mainly accommodate the integration of new neurons during the postnatal period, and continuously receive new neurons throughout animals' life. In this thesis, I used the rat olfactory bulb (OB) as a model system to address two important issues regarding the integration and plasticity of new neurons generated during the postnatal period. The first feature of postnatal neurogenesis is that when new neurons arrive and integrate into an adult OB, only half of neurons can ultimately survive. However, what form of activity pattern determines the survival of new neurons remains unclear. Using NaChBac sodium channels to selectively alter the intrinsic excitability of new neurons in vivo, this manipulation reveals that neuronal survival critically depends on the level of membrane depolarization. Once neurons integrate and survive in the brain circuits, neurons have the capability of monitoring their activity level and adaptively maintain their membrane excitability within the operational range. How they achieve the long-term stability of membrane excitability remains unclear. By altering the resting membrane potential of individual neurons in vivo, OB granule neurons are found to use a subthreshold parameter, resting membrane potential, to guide the compensatory changes of intrinsic ion channels and synaptic receptors. In summary, studies from this thesis have revealed the cellular mechanisms underlying neuronal survival in an in vivo brain circuit. I also uncover a novel form of homeostatic computation by which granule neurons preferentially use the subthreshold membrane potential response rather than spiking rates as a set point.",
    "advisors": ["Carlos Lois"],
    "text": "Activity-dependent integration and plasticity of new neurons during postnatal neurogenesis Most neurons are born during the embryonic period to become the building blocks for a variety of brain circuits. However, two brain regions only start to assemble during the postnatal period. Both brain areas, olfactory bulb and dentate gyrus, mainly accommodate the integration of new neurons during the postnatal period, and continuously receive new neurons throughout animals' life. In this thesis, I used the rat olfactory bulb (OB) as a model system to address two important issues regarding the integration and plasticity of new neurons generated during the postnatal period. The first feature of postnatal neurogenesis is that when new neurons arrive and integrate into an adult OB, only half of neurons can ultimately survive. However, what form of activity pattern determines the survival of new neurons remains unclear. Using NaChBac sodium channels to selectively alter the intrinsic excitability of new neurons in vivo, this manipulation reveals that neuronal survival critically depends on the level of membrane depolarization. Once neurons integrate and survive in the brain circuits, neurons have the capability of monitoring their activity level and adaptively maintain their membrane excitability within the operational range. How they achieve the long-term stability of membrane excitability remains unclear. By altering the resting membrane potential of individual neurons in vivo, OB granule neurons are found to use a subthreshold parameter, resting membrane potential, to guide the compensatory changes of intrinsic ion channels and synaptic receptors. In summary, studies from this thesis have revealed the cellular mechanisms underlying neuronal survival in an in vivo brain circuit. I also uncover a novel form of homeostatic computation by which granule neurons preferentially use the subthreshold membrane potential response rather than spiking rates as a set point."
}, {
    "id": "oai:dspace.mit.edu:1721.1/45336",
    "title": "Quantifying uncertainty in computational neuroscience with Bayesian statistical inference",
    "abstract": "Two key fields of computational neuroscience involve, respectively, the analysis of experimental recordings to understand the functional properties of neurons, and modeling how neurons and networks process sensory information in order to represent the environment. In both of these endeavors, it is crucial to understand and quantify uncertainty - when describing how the brain itself draws conclusions about the physical world, and when the experimenter interprets neuronal data. Bayesian modeling and inference methods provide many advantages for doing so. Three projects are presented that illustrate the advantages of the Bayesian approach. In the first, Markov chain Monte Carlo (MCMC) sampling methods were used to answer a range of scientific questions that arise in the analysis of physiological data from tuning curve experiments; in addition, a software toolbox is described that makes these methods widely accessible. In the second project, the model developed in the first project was extended to describe the detailed dynamics of orientation tuning in neurons in cat primary visual cortex. Using more sophisticated sampling-based inference methods, this model was applied to answer specific scientific questions about the tuning properties of a recorded population. The final project uses a Bayesian model to provide a normative explanation of sensory adaptation phenomena. The model was able to explain a range of detailed physiological adaptation phenomena.",
    "advisors": ["Mriganka Sur"],
    "text": "Quantifying uncertainty in computational neuroscience with Bayesian statistical inference Two key fields of computational neuroscience involve, respectively, the analysis of experimental recordings to understand the functional properties of neurons, and modeling how neurons and networks process sensory information in order to represent the environment. In both of these endeavors, it is crucial to understand and quantify uncertainty - when describing how the brain itself draws conclusions about the physical world, and when the experimenter interprets neuronal data. Bayesian modeling and inference methods provide many advantages for doing so. Three projects are presented that illustrate the advantages of the Bayesian approach. In the first, Markov chain Monte Carlo (MCMC) sampling methods were used to answer a range of scientific questions that arise in the analysis of physiological data from tuning curve experiments; in addition, a software toolbox is described that makes these methods widely accessible. In the second project, the model developed in the first project was extended to describe the detailed dynamics of orientation tuning in neurons in cat primary visual cortex. Using more sophisticated sampling-based inference methods, this model was applied to answer specific scientific questions about the tuning properties of a recorded population. The final project uses a Bayesian model to provide a normative explanation of sensory adaptation phenomena. The model was able to explain a range of detailed physiological adaptation phenomena."
}, {
    "id": "oai:dspace.mit.edu:1721.1/79141",
    "title": "Regulation of synaptic function and plasticity by cyclin-dependent kinase 5",
    "abstract": "The neuronal serine/threonine kinase cyclin-dependent kinase 5 (Cdk5) is activated by its regulatory subunit, p35, to post-translationally modify substrates through phosphorylation. In this thesis, I provide several lines of evidence that Cdk5 plays a critical role in synaptic function and plasticity. First, we characterized the function of Cdk5 in learning and memory by region-specific Cdk5 ablation. From multiple Cdk5 conditional knockout mouse models, we determined that Cdk5 is essential for memory formation and synaptic plasticity. Loss of Cdk5 in the hippocampus disrupts the cAMP pathway due to increased phosphodiesterase proteins. This dysregulation of cAMP signaling can be attenuated by a phosphodiesterase inhibitor to restore levels of protein phosphorylation, synaptic plasticity, and memory. Moreover, forebrain-specific deletion of Cdk5 affected multiple aspects of behavior that can partially be rescued by lithium treatment. We next identified the N-type calcium channels as a presynaptic substrate of Cdk5. We described how Cdk5-mediated phosphorylation of the N-type calcium channel increased calcium influx and channel open probability. This in turn enhanced the association of the N-type calcium channel with the active zone protein RIM1, which impacted vesicle docking and neurotransmission. Finally, we identified the postsynaptic density protein Shank3 as a Cdk5 substrate and observed that Cdk5-mediated phosphorylation of Shank3 plays a critical role in maintaining dendritic spine morphology and synaptic plasticity. Our collective results demonstrate a central role for Cdk5 in regulating both presynaptic and postsynaptic functions and provide better insight into how specific targets of Cdk5 can impact a general mechanism underlying synaptic transmission, synaptic plasticity, and cognitive function.",
    "advisors": ["Li-Huei Tsai"],
    "text": "Regulation of synaptic function and plasticity by cyclin-dependent kinase 5 The neuronal serine/threonine kinase cyclin-dependent kinase 5 (Cdk5) is activated by its regulatory subunit, p35, to post-translationally modify substrates through phosphorylation. In this thesis, I provide several lines of evidence that Cdk5 plays a critical role in synaptic function and plasticity. First, we characterized the function of Cdk5 in learning and memory by region-specific Cdk5 ablation. From multiple Cdk5 conditional knockout mouse models, we determined that Cdk5 is essential for memory formation and synaptic plasticity. Loss of Cdk5 in the hippocampus disrupts the cAMP pathway due to increased phosphodiesterase proteins. This dysregulation of cAMP signaling can be attenuated by a phosphodiesterase inhibitor to restore levels of protein phosphorylation, synaptic plasticity, and memory. Moreover, forebrain-specific deletion of Cdk5 affected multiple aspects of behavior that can partially be rescued by lithium treatment. We next identified the N-type calcium channels as a presynaptic substrate of Cdk5. We described how Cdk5-mediated phosphorylation of the N-type calcium channel increased calcium influx and channel open probability. This in turn enhanced the association of the N-type calcium channel with the active zone protein RIM1, which impacted vesicle docking and neurotransmission. Finally, we identified the postsynaptic density protein Shank3 as a Cdk5 substrate and observed that Cdk5-mediated phosphorylation of Shank3 plays a critical role in maintaining dendritic spine morphology and synaptic plasticity. Our collective results demonstrate a central role for Cdk5 in regulating both presynaptic and postsynaptic functions and provide better insight into how specific targets of Cdk5 can impact a general mechanism underlying synaptic transmission, synaptic plasticity, and cognitive function."
}, {
    "id": "oai:dspace.mit.edu:1721.1/39004",
    "title": "Learning about dynamic objects and recognizing static form",
    "abstract": "The effects of observed object motion on object perception are examined in two sets of studies. The first section of the thesis provides a thorough examination of various untested aspects of the basic \"temporal association\" hypothesis, which suggests that object motion provides a principled basis for linking distinct images together if they appear within small time intervals. Using familiar and unfamiliar objects undergoing various forms of non-rigid motion, I ask how well this simple hypothesis predicts behavior in change detection and categorization tasks. The results favor a modified version of the hypothesis which operates over a population of units, such that increases in generalization also produce increases in image sensitivity. The observed effects of long-term knowledge concerning object appearance and expected patterns of motion also force additional modifications of the initial hypothesis to incorporate interactions between learned predictions and recent experience. Specifically, the tendency to alter patterns of generalization following dynamic exposure appears to be contingent on the stability of the direction of movement through appearance space.",
    "advisors": ["Pawan Sinha"],
    "text": "Learning about dynamic objects and recognizing static form The effects of observed object motion on object perception are examined in two sets of studies. The first section of the thesis provides a thorough examination of various untested aspects of the basic \"temporal association\" hypothesis, which suggests that object motion provides a principled basis for linking distinct images together if they appear within small time intervals. Using familiar and unfamiliar objects undergoing various forms of non-rigid motion, I ask how well this simple hypothesis predicts behavior in change detection and categorization tasks. The results favor a modified version of the hypothesis which operates over a population of units, such that increases in generalization also produce increases in image sensitivity. The observed effects of long-term knowledge concerning object appearance and expected patterns of motion also force additional modifications of the initial hypothesis to incorporate interactions between learned predictions and recent experience. Specifically, the tendency to alter patterns of generalization following dynamic exposure appears to be contingent on the stability of the direction of movement through appearance space."
}, {
    "id": "oai:dspace.mit.edu:1721.1/97788",
    "title": "On the nature and origin of intuitive theories : learning, physics and psychology",
    "abstract": "This thesis develops formal computational models of intuitive theories, in particular intuitive physics and intuitive psychology, which form the basis of commonsense reasoning. The overarching formal framework is that of hierarchical Bayesian models, which see the mind as having domain-specific hypotheses about how the world works. The work first extends models of intuitive psychology to include higher-level social utilities, arguing against a pure 'classifier' view. Second, the work extends models of intuitive physics by introducing a ontological hierarchy of physics concepts, and examining how well people can reason about novel dynamic displays. I then examine the question of learning intuitive theories in general, arguing that an algorithmic approach based on stochastic search can address several puzzles of learning, including the 'chicken and egg' problem of concept learning. Finally, I argue the need for a joint theory-space for reasoning about intuitive physics and intuitive psychology, and provide such a simplified space in the form of a generative model for a novel domain called Lineland. Taken together, these results forge links between formal modeling, intuitive theories, and cognitive development.",
    "advisors": ["Joshua B. Tenenbaum"],
    "text": "On the nature and origin of intuitive theories : learning, physics and psychology This thesis develops formal computational models of intuitive theories, in particular intuitive physics and intuitive psychology, which form the basis of commonsense reasoning. The overarching formal framework is that of hierarchical Bayesian models, which see the mind as having domain-specific hypotheses about how the world works. The work first extends models of intuitive psychology to include higher-level social utilities, arguing against a pure 'classifier' view. Second, the work extends models of intuitive physics by introducing a ontological hierarchy of physics concepts, and examining how well people can reason about novel dynamic displays. I then examine the question of learning intuitive theories in general, arguing that an algorithmic approach based on stochastic search can address several puzzles of learning, including the 'chicken and egg' problem of concept learning. Finally, I argue the need for a joint theory-space for reasoning about intuitive physics and intuitive psychology, and provide such a simplified space in the form of a generative model for a novel domain called Lineland. Taken together, these results forge links between formal modeling, intuitive theories, and cognitive development."
}, {
    "id": "oai:dspace.mit.edu:1721.1/62718",
    "title": "Using neural population decoding to understand high level visual processing",
    "abstract": "The field of neuroscience has the potential to address profound questions including explaining how neural activity enables complex behaviors and conscious experience. However, currently the field is a long way from understanding these issues, and progress has been slow. One of the main problems holding back the pace of discovery is that it is still unclear how to interpret neural activity once it has been recorded. This lack of understanding has led to many different data analysis methods, which makes it difficult to evaluate the validity and importance of many reported results. If a clearer understanding of how to interpret neural data existed, it should be much easier to answer other questions about how the brain functions. In this thesis I describe how to use a data analysis method called 'neural population decoding' to analyze data in a way that is potentially more relevant for understanding neural information processing. By applying this method in novel ways to data from several vision experiments, I am able to make several new discoveries, including the fact that abstract category information is coded in the inferior temporal cortex (ITC) and prefrontal cortex (PFC) by dynamic patterns of neural activity, and that when a monkey attends to an object in a cluttered display, the pattern of ITC activity returns to a state that is similar to when the attended object is presented alone. These findings are not only interesting for insights that they give into the content and coding of information in high level visual areas, but they also demonstrate the benefits of using neural population decoding to analyze data. Thus, the methods developed in this thesis should enable more rapid progress toward an algorithmic level understanding of vision and information processing in other neural systems.",
    "advisors": ["Tomaso Poggio"],
    "text": "Using neural population decoding to understand high level visual processing The field of neuroscience has the potential to address profound questions including explaining how neural activity enables complex behaviors and conscious experience. However, currently the field is a long way from understanding these issues, and progress has been slow. One of the main problems holding back the pace of discovery is that it is still unclear how to interpret neural activity once it has been recorded. This lack of understanding has led to many different data analysis methods, which makes it difficult to evaluate the validity and importance of many reported results. If a clearer understanding of how to interpret neural data existed, it should be much easier to answer other questions about how the brain functions. In this thesis I describe how to use a data analysis method called 'neural population decoding' to analyze data in a way that is potentially more relevant for understanding neural information processing. By applying this method in novel ways to data from several vision experiments, I am able to make several new discoveries, including the fact that abstract category information is coded in the inferior temporal cortex (ITC) and prefrontal cortex (PFC) by dynamic patterns of neural activity, and that when a monkey attends to an object in a cluttered display, the pattern of ITC activity returns to a state that is similar to when the attended object is presented alone. These findings are not only interesting for insights that they give into the content and coding of information in high level visual areas, but they also demonstrate the benefits of using neural population decoding to analyze data. Thus, the methods developed in this thesis should enable more rapid progress toward an algorithmic level understanding of vision and information processing in other neural systems."
}, {
    "id": "oai:dspace.mit.edu:1721.1/114101",
    "title": "National technological and military prestige heavily influenced the development of early United States space policy",
    "abstract": "The purpose of this thesis is to discuss the relationship between U.S. space policy and: 1) national technological prestige 2) military superiority 3) and ultimately the political competition between the United States and the Soviet Union. The paper will focus primarily on national and military prestige, while briefly touching on the price tag of the early space advancements. It will concentrate on the early days of space exploration. The thesis will examine the impact the Russian satellite, Sputnik, had on the American people and the reaction of the United States. The thesis will also look at the beginning of the Apollo program and the decision to send man to the Moon. In conclusion, the thesis will look at a possible future for the United States space program and analyze the decision of America's leaders to abandon ambitious endeavors since the Apollo Moon landing.",
    "advisors": ["Richard P. Binzel"],
    "text": "National technological and military prestige heavily influenced the development of early United States space policy The purpose of this thesis is to discuss the relationship between U.S. space policy and: 1) national technological prestige 2) military superiority 3) and ultimately the political competition between the United States and the Soviet Union. The paper will focus primarily on national and military prestige, while briefly touching on the price tag of the early space advancements. It will concentrate on the early days of space exploration. The thesis will examine the impact the Russian satellite, Sputnik, had on the American people and the reaction of the United States. The thesis will also look at the beginning of the Apollo program and the decision to send man to the Moon. In conclusion, the thesis will look at a possible future for the United States space program and analyze the decision of America's leaders to abandon ambitious endeavors since the Apollo Moon landing."
}, {
    "id": "oai:dspace.mit.edu:1721.1/114375",
    "title": "U-Pb geochronology of the Acasta Gneiss Complex in Northwest Canada",
    "abstract": "The Acasta Gneiss Complex in Northwest Canada contains the oldest dated rocks in the world. The gneisses range in age from 4.03-3.6 Ga, as determined by U-Pb dating of zircons (Bowring and Williams 1999). U-Pb dating of xenocrystic cores in these zircons indicates a cryptic record of continental crust older than 4.0 Ga. In this study, zircons were selected and characterized from thirteen samples of Acasta Gneisses. Many of the zircons contain xenocrystic cores mantled by younger domains. U-Pb geochronological data were collected using laser ablation inductively coupled plasma mass spectrometery (LA-ICPMS). Twelve of the samples show evidence for two distinct crystallization events, one which formed the cores and another in which the mantle domain overgrew the cores. The oldest cores were dated at >4.0 Ga. This provides additional evidence for pre-4.0 Ga crust formation in the late Hadean.",
    "advisors": ["Samuel Bowring"],
    "text": "U-Pb geochronology of the Acasta Gneiss Complex in Northwest Canada The Acasta Gneiss Complex in Northwest Canada contains the oldest dated rocks in the world. The gneisses range in age from 4.03-3.6 Ga, as determined by U-Pb dating of zircons (Bowring and Williams 1999). U-Pb dating of xenocrystic cores in these zircons indicates a cryptic record of continental crust older than 4.0 Ga. In this study, zircons were selected and characterized from thirteen samples of Acasta Gneisses. Many of the zircons contain xenocrystic cores mantled by younger domains. U-Pb geochronological data were collected using laser ablation inductively coupled plasma mass spectrometery (LA-ICPMS). Twelve of the samples show evidence for two distinct crystallization events, one which formed the cores and another in which the mantle domain overgrew the cores. The oldest cores were dated at >4.0 Ga. This provides additional evidence for pre-4.0 Ga crust formation in the late Hadean."
}, {
    "id": "oai:dspace.mit.edu:1721.1/117445",
    "title": "Low rare earth element concentration impact glass from the K/T Boundary at Beloc",
    "abstract": "This paper seeks to describe an impact glass from the K/T boundary layer at Beloc that is depleted in rare earth elements (REE) relative to continental crust. It is widely agreed that a large bolide struck the Yucatan Peninsula roughly 65 Ma spreading a worldwide iridium anomaly. However, there is only one case of a piece of the impactor being found (Kyte et al., 1995). Impact glass from the K/T boundary at Beloc, Haiti has been widely researched with several types of glass documented. So far, all of the documented glass exhibits a crustal REE compositional pattern. In this study, REE composition from two glass types from the same K/T boundary layer sample are examined using a laser ablation inductively coupled plasma mass spectrometer (LA-ICP-MS). One glass type exhibits crustal REE composition which is enriched normalized to chondrite. The other glass type, however, displays a meteoritic REE composition signature. REE patterns for this glass are nearly flat when plotted normalized to chondrite with an average La/Sm ratio of 1.51 compared to 2.69 for crustal glass. Major element data were obtained through electron microprobe analysis and displays a composition that differs from one glass type to the other. Results suggest chondritic parent material from the Chicxulub impactor.",
    "advisors": ["Samuel Bowring", "Asish Basu"],
    "text": "Low rare earth element concentration impact glass from the K/T Boundary at Beloc This paper seeks to describe an impact glass from the K/T boundary layer at Beloc that is depleted in rare earth elements (REE) relative to continental crust. It is widely agreed that a large bolide struck the Yucatan Peninsula roughly 65 Ma spreading a worldwide iridium anomaly. However, there is only one case of a piece of the impactor being found (Kyte et al., 1995). Impact glass from the K/T boundary at Beloc, Haiti has been widely researched with several types of glass documented. So far, all of the documented glass exhibits a crustal REE compositional pattern. In this study, REE composition from two glass types from the same K/T boundary layer sample are examined using a laser ablation inductively coupled plasma mass spectrometer (LA-ICP-MS). One glass type exhibits crustal REE composition which is enriched normalized to chondrite. The other glass type, however, displays a meteoritic REE composition signature. REE patterns for this glass are nearly flat when plotted normalized to chondrite with an average La/Sm ratio of 1.51 compared to 2.69 for crustal glass. Major element data were obtained through electron microprobe analysis and displays a composition that differs from one glass type to the other. Results suggest chondritic parent material from the Chicxulub impactor."
}, {
    "id": "oai:dspace.mit.edu:1721.1/114105",
    "title": "Constraints on passive margin escarpment evolution from river basin reorganization in Brazil",
    "abstract": "Escarpments are present on passive margins around the world, but their evolution is poorly understood. Some geologists interpret escarpments as stationary features, whereas others have argued that they are retreating inland faster than a kilometer per Myr. I investigate Brazilian escarpments by determining whether or not the river networks on either side of the escarpment are in erosional equilibrium with each other. My approach is based on the premise that rivers on opposite sides of a stationary escarpment would be eroding at the same rate (erosional equilibrium), whereas rivers on opposite sides of a mobile escarpment would be eroding at different rates (erosional disequilibrium). I use a recently developed technique called chi mapping to assess the erosional disequilibrium of river networks along the Brazilian escarpments. For comparison, I also compile erosion rates on either side of the escarpments from cosmogenic \"Be measurements in the existing literature, and use these to calculate recent retreat rates of the escarpments, which fall between 4 and 40 m/Myr. I determine that chi mapping and cosmogenic erosion rates agree on the direction of escarpment movement, but disagree on the magnitudes of the retreat rates. I also estimate the percentage of drainage area exchanged by stream capture as the escarpment drainage divide moves across the landscape. Using two different estimation methods, I find that less than 40% of drainage area is exchanged by stream capture, making divide migration the dominant mechanism for drainage basin reorganization. If my estimates of recent escarpment retreat rates are representative of long-term rates, the Brazilian escarpments have retreated up to 5 km since their formation during the Cretaceous rifting event. My analysis shows that the topographic retreat of the Brazilian escarpments could have been driven by drainage basin disequilibrium resulting in divide migration.",
    "advisors": ["J. Taylor Perron"],
    "text": "Constraints on passive margin escarpment evolution from river basin reorganization in Brazil Escarpments are present on passive margins around the world, but their evolution is poorly understood. Some geologists interpret escarpments as stationary features, whereas others have argued that they are retreating inland faster than a kilometer per Myr. I investigate Brazilian escarpments by determining whether or not the river networks on either side of the escarpment are in erosional equilibrium with each other. My approach is based on the premise that rivers on opposite sides of a stationary escarpment would be eroding at the same rate (erosional equilibrium), whereas rivers on opposite sides of a mobile escarpment would be eroding at different rates (erosional disequilibrium). I use a recently developed technique called chi mapping to assess the erosional disequilibrium of river networks along the Brazilian escarpments. For comparison, I also compile erosion rates on either side of the escarpments from cosmogenic \"Be measurements in the existing literature, and use these to calculate recent retreat rates of the escarpments, which fall between 4 and 40 m/Myr. I determine that chi mapping and cosmogenic erosion rates agree on the direction of escarpment movement, but disagree on the magnitudes of the retreat rates. I also estimate the percentage of drainage area exchanged by stream capture as the escarpment drainage divide moves across the landscape. Using two different estimation methods, I find that less than 40% of drainage area is exchanged by stream capture, making divide migration the dominant mechanism for drainage basin reorganization. If my estimates of recent escarpment retreat rates are representative of long-term rates, the Brazilian escarpments have retreated up to 5 km since their formation during the Cretaceous rifting event. My analysis shows that the topographic retreat of the Brazilian escarpments could have been driven by drainage basin disequilibrium resulting in divide migration."
}, {
    "id": "oai:dspace.mit.edu:1721.1/114135",
    "title": "Feasibility of using cloud top altimetry for estimating tropical cyclone intensity estimation",
    "abstract": "This project explores whether cloud top altimetry can be used as an accurate and reliable means of estimating the intensity of tropical cyclones. Professor Kerry A. Emanuel developed the theory that is under investigation. His theory aims to calculate the peak surface wind speed in hurricanes using only three parameters, all of which can be collected from satellite imagery: cloud top height, sea surface temperature and cloud top temperature. Cloud top heights for selected hurricanes were obtained from the ICESat, and points were identified where the ICESat may have traversed the hurricanes. These points were compared with IR images to confirm the intersection of the ICESat track and the hurricane tracks. Out of 18 hurricanes examined, four provided feasible points to test this new technique. Two of these points were from hurricanes that were at the end stage of their life cycle; these two data points were discarded. Data from the two usable data points were compared to the recorded wind speeds from Unisys. It seems that the new method is overestimating the maximum surface wind speed by less than 10%. Two data points are insufficient for conclusively validating this technique. However, this project has established a viable method for gathering and analyzing altimetry data, providing a basis for further testing of the theory.",
    "advisors": ["Kerry A. Emanuel"],
    "text": "Feasibility of using cloud top altimetry for estimating tropical cyclone intensity estimation This project explores whether cloud top altimetry can be used as an accurate and reliable means of estimating the intensity of tropical cyclones. Professor Kerry A. Emanuel developed the theory that is under investigation. His theory aims to calculate the peak surface wind speed in hurricanes using only three parameters, all of which can be collected from satellite imagery: cloud top height, sea surface temperature and cloud top temperature. Cloud top heights for selected hurricanes were obtained from the ICESat, and points were identified where the ICESat may have traversed the hurricanes. These points were compared with IR images to confirm the intersection of the ICESat track and the hurricane tracks. Out of 18 hurricanes examined, four provided feasible points to test this new technique. Two of these points were from hurricanes that were at the end stage of their life cycle; these two data points were discarded. Data from the two usable data points were compared to the recorded wind speeds from Unisys. It seems that the new method is overestimating the maximum surface wind speed by less than 10%. Two data points are insufficient for conclusively validating this technique. However, this project has established a viable method for gathering and analyzing altimetry data, providing a basis for further testing of the theory."
}, {
    "id": "oai:dspace.mit.edu:1721.1/114108",
    "title": "A statistical approach to predicting snowfall using the SPF",
    "abstract": "A snowfall potential probability density function is introduced that uses an alternate method of statistical analysis in predicting snowfall by using concepts of normal probability distributions. The snowfall potential function (hereby referred to as the SPF) assumes certain identifiers are associated with snowfall of varying intensities. The conceptual relation of each identifier with snowfall is explained and the statistical association of each identifier to the SPF is determined by a correlation coefficient and the relative strength of that particular identifier with respect to the expected value (the mean). The intensity of the snowfall over an area is estimated by calculating the SPF's overall deviation from some expected value, where any given SPF value can estimate a snowfall value. The framework for the SPF is explained using a simple model. The correlation coefficients for several identifiers are calculated and an example of an application of the SPF is demonstrated. Further hypotheses are given as to how the SPF could ultimately be used to provide possible higher-accuracy snowfall forecasts through the development of time-dependent functions for each identifier and the assigning of specific functional forms for the SPF based on region of analysis, storm type (i.e.: coastal, Alberta clipper), and storm track. 2",
    "advisors": ["R. Alan Plumb"],
    "text": "A statistical approach to predicting snowfall using the SPF A snowfall potential probability density function is introduced that uses an alternate method of statistical analysis in predicting snowfall by using concepts of normal probability distributions. The snowfall potential function (hereby referred to as the SPF) assumes certain identifiers are associated with snowfall of varying intensities. The conceptual relation of each identifier with snowfall is explained and the statistical association of each identifier to the SPF is determined by a correlation coefficient and the relative strength of that particular identifier with respect to the expected value (the mean). The intensity of the snowfall over an area is estimated by calculating the SPF's overall deviation from some expected value, where any given SPF value can estimate a snowfall value. The framework for the SPF is explained using a simple model. The correlation coefficients for several identifiers are calculated and an example of an application of the SPF is demonstrated. Further hypotheses are given as to how the SPF could ultimately be used to provide possible higher-accuracy snowfall forecasts through the development of time-dependent functions for each identifier and the assigning of specific functional forms for the SPF based on region of analysis, storm type (i.e.: coastal, Alberta clipper), and storm track. 2"
}, {
    "id": "oai:dspace.mit.edu:1721.1/114137",
    "title": "QX Pup : the fascinating yolk of the Rotten Egg Nebula",
    "abstract": "QX Pup is a known Mira variable at the core of the Rotten Egg Nebula that has not been studied in detail since its discovery in 1983. In this study, four years of photometric data in V and I and two years of photometric data in R and B are analyzed. A period of T = 535.4  8 days, and a magnitude drop of [Delta]mI , = 2.2  0.69 are measured in I, and and phase shifts between the the other three filters and I are determined to be [o]R = 6  40 days, [o]B = 66  64 days, #, [o]v = 16  86 days. These results are used to speculate about the possibility of a light-echo off the Rotten Egg Nebula and the conditions on Earth-like planets around Mira variables.",
    "advisors": ["Richard P. Binzel", "Amanda S. Bosh"],
    "text": "QX Pup : the fascinating yolk of the Rotten Egg Nebula QX Pup is a known Mira variable at the core of the Rotten Egg Nebula that has not been studied in detail since its discovery in 1983. In this study, four years of photometric data in V and I and two years of photometric data in R and B are analyzed. A period of T = 535.4  8 days, and a magnitude drop of [Delta]mI , = 2.2  0.69 are measured in I, and and phase shifts between the the other three filters and I are determined to be [o]R = 6  40 days, [o]B = 66  64 days, #, [o]v = 16  86 days. These results are used to speculate about the possibility of a light-echo off the Rotten Egg Nebula and the conditions on Earth-like planets around Mira variables."
}, {
    "id": "oai:dspace.mit.edu:1721.1/114118",
    "title": "Methods for the study of virus adsorption to metal oxide in order to improve ceramic water filters",
    "abstract": "As of 2002, 1.1 billion people lacked access to clean water, causing several million deahts each year from highly-infectious enteric diseases. It has been recognized that an appropriate way of addressing this problem may be to enhance the effectiveness and the usage of house-hold water treatment and storage (HWTS) systems. Ceramic filters are examples of HWTS systems. Ceramic filters decrease the concentration of bacteria by pore size filtration but are not able to filter the nanometer-size viruses. It is proposed that adding an adequate amount of metal oxides to the clay before firing the filters would allow the ceramic to adsorb the viruses present in the water. This thesis takes two steps towards evaluating this proposition. It demonstrates the microbiological methods needed to assess the presence of viruses in water and to carry out experiments with bacteriophages, used as model viruses. It also presents the theory necessary to understand, measure and model virus adsorption to surfaces.",
    "advisors": ["Susan Murcott"],
    "text": "Methods for the study of virus adsorption to metal oxide in order to improve ceramic water filters As of 2002, 1.1 billion people lacked access to clean water, causing several million deahts each year from highly-infectious enteric diseases. It has been recognized that an appropriate way of addressing this problem may be to enhance the effectiveness and the usage of house-hold water treatment and storage (HWTS) systems. Ceramic filters are examples of HWTS systems. Ceramic filters decrease the concentration of bacteria by pore size filtration but are not able to filter the nanometer-size viruses. It is proposed that adding an adequate amount of metal oxides to the clay before firing the filters would allow the ceramic to adsorb the viruses present in the water. This thesis takes two steps towards evaluating this proposition. It demonstrates the microbiological methods needed to assess the presence of viruses in water and to carry out experiments with bacteriophages, used as model viruses. It also presents the theory necessary to understand, measure and model virus adsorption to surfaces."
}, {
    "id": "oai:dspace.mit.edu:1721.1/114138",
    "title": "Stable isotope and organic biomarker analysis of the late Proterozoic Coppercap formation in the MacKenzie Mountains",
    "abstract": "Sulfur and carbon stable isotope ratios and organic biomarker abundance were performed on drill core samples from the Coppercap Formation of the Coates Lake Group in the Windermere Supergroup of the MacKenzie Mountains to reconstruct an environmental condition proceeding the first Neoproterozoic Snowball Earth event. The Coppercap Formation directly underlies the Rapitan Group, Sturtian glacigenic deposits, and represents a depositional environment existing directly before the Cryogenian glacial episodes. Based on aryl isoprenoids, n-propyl cholestane, isopropyl cholestane, total organic carbon, carbonate mineral analysis, [delta]34S from pyrite, [delta]13Corganic and [delta]13Ccarbonate values, environmental conditions of the Coppercap Formation were reconstructed. The Coppercap Formation was found to be a shallow euxinic marine basin with purple and green sulfur bacteria microbial mats. This suggest persistent hydrogen sulfide rich waters in the shallow photic zone persisted until the Sturtian.",
    "advisors": ["Shuhei Ono"],
    "text": "Stable isotope and organic biomarker analysis of the late Proterozoic Coppercap formation in the MacKenzie Mountains Sulfur and carbon stable isotope ratios and organic biomarker abundance were performed on drill core samples from the Coppercap Formation of the Coates Lake Group in the Windermere Supergroup of the MacKenzie Mountains to reconstruct an environmental condition proceeding the first Neoproterozoic Snowball Earth event. The Coppercap Formation directly underlies the Rapitan Group, Sturtian glacigenic deposits, and represents a depositional environment existing directly before the Cryogenian glacial episodes. Based on aryl isoprenoids, n-propyl cholestane, isopropyl cholestane, total organic carbon, carbonate mineral analysis, [delta]34S from pyrite, [delta]13Corganic and [delta]13Ccarbonate values, environmental conditions of the Coppercap Formation were reconstructed. The Coppercap Formation was found to be a shallow euxinic marine basin with purple and green sulfur bacteria microbial mats. This suggest persistent hydrogen sulfide rich waters in the shallow photic zone persisted until the Sturtian."
}, {
    "id": "oai:dspace.mit.edu:1721.1/114132",
    "title": "Killer asteroids : feasibility of using the IRTF to track near-earth objects",
    "abstract": "The possibility of an asteroid or comet impact necessitates the tracking and cataloging of all such objects which could potentially impact Earth. Currently, no comprehensive catalog of Near Earth Objects (NEOs) exists which contains information on the physical properties of the objects. Spectroscopic observation of NEOs must be carried out in an efficient and timely manner in order to determine the physical properties of NEOs for this catalog. The cumulative fractions of objects visible at certain magnitudes were examined and compared for the NEOs discovered in 2005 at discovery, those at the first quarter moon following discovery, and all known NEOs in 1, 3, 5, and 10 year forecasted surveys to determine the best combination of Infrared Telescope Facility (IRTF) instrumentation, telescope observation time, and survey length. This thesis finds that the IRTF instrumentation should be improved to at least 19.5 to spectroscopically observe 57% of the objects discovered in 2005. Furthermore, spectroscopic observation of the objects should not occur at the first quarter moon immediately after discovery, as is currently the case, because as much as ~15% of the objects discovered in 2005 cannot be observed at this time. As survey length is increased, the fraction of objects that can be observed at the IRTF's current limiting magnitude also increase; thus it is best to conduct the survey as long as possible. Additionally, spectroscopic observation of the objects should be carried out every 7 days in order to gather the most information. Lastly, it is best to spectroscopically observe the objects within 7 days of discovery because the objects are generally discovered when they are at their closest possible approach to the Earth.",
    "advisors": ["Richard P. Binzel"],
    "text": "Killer asteroids : feasibility of using the IRTF to track near-earth objects The possibility of an asteroid or comet impact necessitates the tracking and cataloging of all such objects which could potentially impact Earth. Currently, no comprehensive catalog of Near Earth Objects (NEOs) exists which contains information on the physical properties of the objects. Spectroscopic observation of NEOs must be carried out in an efficient and timely manner in order to determine the physical properties of NEOs for this catalog. The cumulative fractions of objects visible at certain magnitudes were examined and compared for the NEOs discovered in 2005 at discovery, those at the first quarter moon following discovery, and all known NEOs in 1, 3, 5, and 10 year forecasted surveys to determine the best combination of Infrared Telescope Facility (IRTF) instrumentation, telescope observation time, and survey length. This thesis finds that the IRTF instrumentation should be improved to at least 19.5 to spectroscopically observe 57% of the objects discovered in 2005. Furthermore, spectroscopic observation of the objects should not occur at the first quarter moon immediately after discovery, as is currently the case, because as much as ~15% of the objects discovered in 2005 cannot be observed at this time. As survey length is increased, the fraction of objects that can be observed at the IRTF's current limiting magnitude also increase; thus it is best to conduct the survey as long as possible. Additionally, spectroscopic observation of the objects should be carried out every 7 days in order to gather the most information. Lastly, it is best to spectroscopically observe the objects within 7 days of discovery because the objects are generally discovered when they are at their closest possible approach to the Earth."
}, {
    "id": "oai:dspace.mit.edu:1721.1/115466",
    "title": "Trade and the environment : the political economy of CO emission leakage with analysis of the steel and oil sands industries",
    "abstract": "Introduction: In 2007, scientists and governmental officials from around the world contributed to the United Nations-authorized Intergovernmental Panel on Climate Change's (IPCC) Fourth Assessment Report. Through peer-reviewed scientific research and governmental review, the IPCC came to the conclusion that \"warming of the climate system is unequivocal,\" and that \"most of the observed increase in global average temperatures since the mid-20th century is very likely due to the observed increase in anthropogenic greenhouse gas concentrations.\" The IPCC Fourth Assessment states that humans have \"more likely than not\" contributed to the phenomena of more frequent \"warm spells/heat waves,\" larger \"area[s] affected by droughts,\" more \"intense tropical cyclones.. .and heavy precipitation events,\" and \"extreme high sea level[s].\" Citing \"high agreement\" and \"much evidence,\" the IPCC states that \"with current climate change mitigation policies and related sustainable development practices, global GHG [greenhouse gas] emissions will continue to grow over the next few decades.\" (Intergovernmental Panel on Climate Change, 2007)",
    "advisors": ["Kenneth A. Oye", "Ronald G. Prinn"],
    "text": "Trade and the environment : the political economy of CO emission leakage with analysis of the steel and oil sands industries Introduction: In 2007, scientists and governmental officials from around the world contributed to the United Nations-authorized Intergovernmental Panel on Climate Change's (IPCC) Fourth Assessment Report. Through peer-reviewed scientific research and governmental review, the IPCC came to the conclusion that \"warming of the climate system is unequivocal,\" and that \"most of the observed increase in global average temperatures since the mid-20th century is very likely due to the observed increase in anthropogenic greenhouse gas concentrations.\" The IPCC Fourth Assessment states that humans have \"more likely than not\" contributed to the phenomena of more frequent \"warm spells/heat waves,\" larger \"area[s] affected by droughts,\" more \"intense tropical cyclones.. .and heavy precipitation events,\" and \"extreme high sea level[s].\" Citing \"high agreement\" and \"much evidence,\" the IPCC states that \"with current climate change mitigation policies and related sustainable development practices, global GHG [greenhouse gas] emissions will continue to grow over the next few decades.\" (Intergovernmental Panel on Climate Change, 2007)"
}, {
    "id": "oai:dspace.mit.edu:1721.1/114321",
    "title": "A preliminary analysis of the lipid contents of a biofilm from a Yellowstone hydrothermal pool",
    "abstract": "The hot springs at Yellowstone National Park, such as Obsidian Pool, have been extensively studied as a source for previously unidentified microorganisms. Most of the previous studies focused on the genomic diversity of these environments, but recent interest in environmental samples for the study of lipid biomarkers has extended to these hot springs as well. In this preliminary study of Obsidian Pool, I used a modified Bligh and Dyer extraction process to create a total lipid extract (TLE), which was then further separated and processed by an acetone precipitation, mild alkaline methanolysis and column chromatography to isolate different fractions of the TLE. Compounds were then identified by gas chromatography-mass spectrometry (GCMS). The mild alkaline methanolysis allowed for the identification of decanoic acid methyl ester, a fatty acid methyl ester. From the column chromatography, only the saturated hydrocarbon fraction yielded interesting compounds; these were a series of n-alkanes (n-C, n-C, and n-C - n-C), two branched alkanes (5-methyloctodecane and 4-methylheptadecane), an isoprenoid (dibiphytane), a series of branched aliphatic alkanes with a quaternary substituted carbon atom (5,5 diethylalkanes), and a variety of hopanoids and steroids. These compounds indicate a strong community of bacterial and archaeal species in Obsidian pool. Some compounds that were indicative of eukarya were also identified, although a more in-depth study is necessary to determine whether these signals were from the pool or from external inputs. The lipids identified in this study indicate that the genomic diversity established by previous studies is reflected in the lipid diversity of Obsidian Pool. However, more study is required to fully categorize the lipids in the pool.",
    "advisors": ["Roger E. Summons"],
    "text": "A preliminary analysis of the lipid contents of a biofilm from a Yellowstone hydrothermal pool The hot springs at Yellowstone National Park, such as Obsidian Pool, have been extensively studied as a source for previously unidentified microorganisms. Most of the previous studies focused on the genomic diversity of these environments, but recent interest in environmental samples for the study of lipid biomarkers has extended to these hot springs as well. In this preliminary study of Obsidian Pool, I used a modified Bligh and Dyer extraction process to create a total lipid extract (TLE), which was then further separated and processed by an acetone precipitation, mild alkaline methanolysis and column chromatography to isolate different fractions of the TLE. Compounds were then identified by gas chromatography-mass spectrometry (GCMS). The mild alkaline methanolysis allowed for the identification of decanoic acid methyl ester, a fatty acid methyl ester. From the column chromatography, only the saturated hydrocarbon fraction yielded interesting compounds; these were a series of n-alkanes (n-C, n-C, and n-C - n-C), two branched alkanes (5-methyloctodecane and 4-methylheptadecane), an isoprenoid (dibiphytane), a series of branched aliphatic alkanes with a quaternary substituted carbon atom (5,5 diethylalkanes), and a variety of hopanoids and steroids. These compounds indicate a strong community of bacterial and archaeal species in Obsidian pool. Some compounds that were indicative of eukarya were also identified, although a more in-depth study is necessary to determine whether these signals were from the pool or from external inputs. The lipids identified in this study indicate that the genomic diversity established by previous studies is reflected in the lipid diversity of Obsidian Pool. However, more study is required to fully categorize the lipids in the pool."
}, {
    "id": "oai:dspace.mit.edu:1721.1/117442",
    "title": "A 3-D tomographic survey of compound chondrules in two carbonaceous chondrites : Acfer 139 and Renazzo",
    "abstract": "Compound chondrules (CCs) are two or more chondrules fused or mechanically joined together. Studies of compound chondrules have been conducted in order to understand the chondrule formation process, including the environment and precursor materials. Chondrule formation is still not currently understood and much doubt still remains as to whether the chondrule formation process is nebular and/or planetary However, our evidence suggests that a primary and secondary process are responsible for CCs: (1) semi-molten collisions among individual (I) chondrules (Gooding and Keil, 1981) and (2) Parent-body impacts that cause jostling, fracturing and compaction (Wasson et al., 1995). By understanding chondrule collisions and impacts we can gain greater knowledge into what was happening during early solar system formation, including mean interparticle distances and velocities (Gooding and Keil, 1981). This thesis introduces x-ray tomography as a new method of measuring and surveying CCs; and explores the advantages and limitations of this method. A 3-D tomographic study of two CR carbonaceous meteorites, Acfer139 and Renazzo, reveals a higher frequency of compound chondrules in CR chondrites than in ordinary chondrites. Previous two-dimensional studies of compound chondrules are reexamined and two new categories, touching and aggregate compounds, are also introduced. Our reexamination suggests a 9.6% frequency of compound chondrules (excluding the new categories), twice more than the value of 4% found by Gooding and Keil (1981) and almost 4 times the value of 2.4% found by Wasson et al. (1995) in their studies of ordinary chondrites. The implications of a higher compound chondrule frequency on chondrule formation are also examined.",
    "advisors": ["Timothy L. Grove", "Denton S. Ebel"],
    "text": "A 3-D tomographic survey of compound chondrules in two carbonaceous chondrites : Acfer 139 and Renazzo Compound chondrules (CCs) are two or more chondrules fused or mechanically joined together. Studies of compound chondrules have been conducted in order to understand the chondrule formation process, including the environment and precursor materials. Chondrule formation is still not currently understood and much doubt still remains as to whether the chondrule formation process is nebular and/or planetary However, our evidence suggests that a primary and secondary process are responsible for CCs: (1) semi-molten collisions among individual (I) chondrules (Gooding and Keil, 1981) and (2) Parent-body impacts that cause jostling, fracturing and compaction (Wasson et al., 1995). By understanding chondrule collisions and impacts we can gain greater knowledge into what was happening during early solar system formation, including mean interparticle distances and velocities (Gooding and Keil, 1981). This thesis introduces x-ray tomography as a new method of measuring and surveying CCs; and explores the advantages and limitations of this method. A 3-D tomographic study of two CR carbonaceous meteorites, Acfer139 and Renazzo, reveals a higher frequency of compound chondrules in CR chondrites than in ordinary chondrites. Previous two-dimensional studies of compound chondrules are reexamined and two new categories, touching and aggregate compounds, are also introduced. Our reexamination suggests a 9.6% frequency of compound chondrules (excluding the new categories), twice more than the value of 4% found by Gooding and Keil (1981) and almost 4 times the value of 2.4% found by Wasson et al. (1995) in their studies of ordinary chondrites. The implications of a higher compound chondrule frequency on chondrule formation are also examined."
}, {
    "id": "oai:dspace.mit.edu:1721.1/114374",
    "title": "Paleomagnetism conglomerate test on Archean conglomerate rock from Jack Hills, Australia",
    "abstract": "A paleomagnetism study known as a conglomerate test was run on an Archean sandstone conglomerate rock to determine if the sample contained a remnant magnetization from the time of its formation. Twenty-nine clasts from a thin section of the sample were thermally demagnetized up to a temperature of 395C. The heating revealed two components of magnetization which were unblocked at low and mid temperatures, revealing a magnetic mineralogy of Pyrrhotite.Eight matrix samples were heated to a temperature of 650C which revealed two components of magnetization at mid and high temperatures, providing evidence for a magnetic mineralogy of Hematite.The direction of measured magnetic moment of the clasts were statistically similar, indicating that the rock failed the conglomerate test and was remagnetized after the rock formed.",
    "advisors": ["Benjamin Weiss"],
    "text": "Paleomagnetism conglomerate test on Archean conglomerate rock from Jack Hills, Australia A paleomagnetism study known as a conglomerate test was run on an Archean sandstone conglomerate rock to determine if the sample contained a remnant magnetization from the time of its formation. Twenty-nine clasts from a thin section of the sample were thermally demagnetized up to a temperature of 395C. The heating revealed two components of magnetization which were unblocked at low and mid temperatures, revealing a magnetic mineralogy of Pyrrhotite.Eight matrix samples were heated to a temperature of 650C which revealed two components of magnetization at mid and high temperatures, providing evidence for a magnetic mineralogy of Hematite.The direction of measured magnetic moment of the clasts were statistically similar, indicating that the rock failed the conglomerate test and was remagnetized after the rock formed."
}, {
    "id": "oai:dspace.mit.edu:1721.1/115778",
    "title": "The relationship between Centaur activity and ring formation",
    "abstract": "Introduction: Centaurs are small bodies whose orbits lie between those of Jupiter and Neptune (Gehrels, 1999). They are thought to be transition objects that originate in the Kuiper belt and occupy the cis-Neptunian region before potentially becoming Jupiter-family or other short-period comets (Dones, Levison, & Duncan, 1996). Their short dynamical lifetimes are on the order of 106 years (Horner, Evans, & Bailey, 2004) due to their unstable, planet-crossing orbits (Horner, Evans, Bailey, & Asher, 2003). Some Centaurs have been observed to be active, and the bodies in the population of active Centaurs have perihelion distances that are statistically smaller than the median perihelion distance for all Centaurs, suggesting that Centaur activity is thermal in nature (Jewitt, 2009). Centaur activity may be observed through changes in the brightness of an object such as those exhibited by the Centaur Chiron (Parker et al., 1997). The presence of a coma around a Centaur may also provide evidence of activity, and dust comae have been detected around several bodies including Chiron (Meech & Belton, 1989; Luu & Jewitt, 1990) and Echeclus (Choi, Weissman, & Polishook, 2006). In addition to comae, other structures have been observed around Centaurs, such as the ring system that was discovered around Chariklo during a stellar occultation (Braga-Ribas et al., 2014). A symmetric feature was observed around Chiron during an occultation (Ruprecht et al., 2015), and some interpret this feature to be possible ring material (Ortiz et al., 2015). Similarly, the trans-Neptunian dwarf planet Haumea was revealed to have a ring during a stellar occultation (Ortiz et al., 2017). The collisional spreading time of Chariklo's rings was calculated to be on the order of 101 years, which is short in comparison to the estimated Centaur lifetime of approximately 106 years (Pan & Wu, 2016), yet Centaur rings are still observed despite this contradiction. Shepherd satellites may serve to increase the lifetime of a Centaur's rings (Pan & Wu, 2016) and maintain distinct ring edges such as those observed in Chariklo's ring system (Charnoz, Canup, Crida, Dones, 2017). Moreover, Centaur activity could supply material to an already present ring system, thus prolonging its lifetime. This study explores the potential connection between Centaur activity and Centaur ring systems by using the N-body integrator REBOUND to model outburst particle interactions and distributions.",
    "advisors": ["Amanda S. Bosh", "Margaret Pan"],
    "text": "The relationship between Centaur activity and ring formation Introduction: Centaurs are small bodies whose orbits lie between those of Jupiter and Neptune (Gehrels, 1999). They are thought to be transition objects that originate in the Kuiper belt and occupy the cis-Neptunian region before potentially becoming Jupiter-family or other short-period comets (Dones, Levison, & Duncan, 1996). Their short dynamical lifetimes are on the order of 106 years (Horner, Evans, & Bailey, 2004) due to their unstable, planet-crossing orbits (Horner, Evans, Bailey, & Asher, 2003). Some Centaurs have been observed to be active, and the bodies in the population of active Centaurs have perihelion distances that are statistically smaller than the median perihelion distance for all Centaurs, suggesting that Centaur activity is thermal in nature (Jewitt, 2009). Centaur activity may be observed through changes in the brightness of an object such as those exhibited by the Centaur Chiron (Parker et al., 1997). The presence of a coma around a Centaur may also provide evidence of activity, and dust comae have been detected around several bodies including Chiron (Meech & Belton, 1989; Luu & Jewitt, 1990) and Echeclus (Choi, Weissman, & Polishook, 2006). In addition to comae, other structures have been observed around Centaurs, such as the ring system that was discovered around Chariklo during a stellar occultation (Braga-Ribas et al., 2014). A symmetric feature was observed around Chiron during an occultation (Ruprecht et al., 2015), and some interpret this feature to be possible ring material (Ortiz et al., 2015). Similarly, the trans-Neptunian dwarf planet Haumea was revealed to have a ring during a stellar occultation (Ortiz et al., 2017). The collisional spreading time of Chariklo's rings was calculated to be on the order of 101 years, which is short in comparison to the estimated Centaur lifetime of approximately 106 years (Pan & Wu, 2016), yet Centaur rings are still observed despite this contradiction. Shepherd satellites may serve to increase the lifetime of a Centaur's rings (Pan & Wu, 2016) and maintain distinct ring edges such as those observed in Chariklo's ring system (Charnoz, Canup, Crida, Dones, 2017). Moreover, Centaur activity could supply material to an already present ring system, thus prolonging its lifetime. This study explores the potential connection between Centaur activity and Centaur ring systems by using the N-body integrator REBOUND to model outburst particle interactions and distributions."
}, {
    "id": "oai:dspace.mit.edu:1721.1/114120",
    "title": "Establishing an unambiguous connection between grain size and style of sediment transport in the Lower Niobrara River, Nebraska, USA",
    "abstract": "The transport of sediment is often separated into two components, bedload and suspended load. This division is important because bedload has a dominant control on channel morphology while suspended load dominates the formation of overbank deposits. Experimental data has related the style of sediment transport to mean flow conditions and bed topography. However direct application of this method in natural, sandy rivers is difficult due to large variabilities in flow. We propose a method for determining local flow conditions using the distribution of grain sizes traveling in the water column. Local shear velocity is found by fitting the Rouse equation for suspended sediment transport to measured sediment concentrations. Empirical criteria for distinguishing between suspended load and bedload are used to determine the fraction of sediment traveling in each respective mode. Application of this method to the Niobrara River, Nebraska, shows that -80 % of the sediment is traveling as suspended load, ~ 20 % is traveling in a transitional mode between bedload and suspended load and less than 1 % is traveling as pure bedload. We establish an unambiguous connection between grain size and the style of sediment transport and highlight the importance of the transitional transport mode in natural systems.",
    "advisors": ["David Mohrig"],
    "text": "Establishing an unambiguous connection between grain size and style of sediment transport in the Lower Niobrara River, Nebraska, USA The transport of sediment is often separated into two components, bedload and suspended load. This division is important because bedload has a dominant control on channel morphology while suspended load dominates the formation of overbank deposits. Experimental data has related the style of sediment transport to mean flow conditions and bed topography. However direct application of this method in natural, sandy rivers is difficult due to large variabilities in flow. We propose a method for determining local flow conditions using the distribution of grain sizes traveling in the water column. Local shear velocity is found by fitting the Rouse equation for suspended sediment transport to measured sediment concentrations. Empirical criteria for distinguishing between suspended load and bedload are used to determine the fraction of sediment traveling in each respective mode. Application of this method to the Niobrara River, Nebraska, shows that -80 % of the sediment is traveling as suspended load, ~ 20 % is traveling in a transitional mode between bedload and suspended load and less than 1 % is traveling as pure bedload. We establish an unambiguous connection between grain size and the style of sediment transport and highlight the importance of the transitional transport mode in natural systems."
}, {
    "id": "oai:dspace.mit.edu:1721.1/114337",
    "title": "The Day Nui Con Voi mylonitic belt in Southwestern China and Its implications for the early Cenozoic extrusion of Indochina",
    "abstract": "The early Cenozoic India-Asia collision resulted in the extrusion of large crustal fragments southeast from the Eastern Himalayan syntaxis, with large shear zones at their boundaries that could have accommodated displacements of hundreds to perhaps a thousand kilometers. Along the northeastern edge of the Indochina extruded fragment, the belt of mylonitic metamorphic rocks generally referred to as the Ailao Shan/Red River shear zone forms the extrusion boundary. This shear zone actually consists of at least two belts, the Ailao Shan and the Day Nui Con Voi, which are separated by a narrow belt of unmetamorphosed Triassic sedimentary rocks. In the Chinese extension of the Day Nui Con Voi, the presence of sillimanite and garnet indicates the shear zone formed at amphibolite grade, and the mylonitic fabric defined by muscovite and biotite indicate left-lateral shearing. Ar/Ar cooling ages indicate the metamorphic rocks reached the cooling temperature of muscovite and biotite 26.07  0.20 to 32.46  0.25 Ma, ages that match those in the Day Nui Con Voi in north Vietnam. These data come from both the core orthogneiss of the shear zone as well as a narrow carapace of metasedimentary rocks of unknown age. Both rock units form an antiform in southern China that plunges below Triassic sedimentary rocks of South China. These relations show that: 1) the Day Nui Con Voi in China is the direct continuation of the same belt in north Vietnam, 2) the Day Nui Con Voi does not directly connect with the Ailao Shan shear zone, 3) the Day Nui Con Voi shear zone has a structural (?) cover of South China Mesozoic sedimentary rocks, 4) structural relations limit the amount of late stage left-lateral shear on the Indochina boundary, and 5) the structural relations require a more complex history for the shear zone along the NE boundary of the extruded Indochina crustal fragment than proposed by all earlier workers.",
    "advisors": ["B. Clark Burchfiel"],
    "text": "The Day Nui Con Voi mylonitic belt in Southwestern China and Its implications for the early Cenozoic extrusion of Indochina The early Cenozoic India-Asia collision resulted in the extrusion of large crustal fragments southeast from the Eastern Himalayan syntaxis, with large shear zones at their boundaries that could have accommodated displacements of hundreds to perhaps a thousand kilometers. Along the northeastern edge of the Indochina extruded fragment, the belt of mylonitic metamorphic rocks generally referred to as the Ailao Shan/Red River shear zone forms the extrusion boundary. This shear zone actually consists of at least two belts, the Ailao Shan and the Day Nui Con Voi, which are separated by a narrow belt of unmetamorphosed Triassic sedimentary rocks. In the Chinese extension of the Day Nui Con Voi, the presence of sillimanite and garnet indicates the shear zone formed at amphibolite grade, and the mylonitic fabric defined by muscovite and biotite indicate left-lateral shearing. Ar/Ar cooling ages indicate the metamorphic rocks reached the cooling temperature of muscovite and biotite 26.07  0.20 to 32.46  0.25 Ma, ages that match those in the Day Nui Con Voi in north Vietnam. These data come from both the core orthogneiss of the shear zone as well as a narrow carapace of metasedimentary rocks of unknown age. Both rock units form an antiform in southern China that plunges below Triassic sedimentary rocks of South China. These relations show that: 1) the Day Nui Con Voi in China is the direct continuation of the same belt in north Vietnam, 2) the Day Nui Con Voi does not directly connect with the Ailao Shan shear zone, 3) the Day Nui Con Voi shear zone has a structural (?) cover of South China Mesozoic sedimentary rocks, 4) structural relations limit the amount of late stage left-lateral shear on the Indochina boundary, and 5) the structural relations require a more complex history for the shear zone along the NE boundary of the extruded Indochina crustal fragment than proposed by all earlier workers."
}, {
    "id": "oai:dspace.mit.edu:1721.1/114333",
    "title": "Local and expert knowledge in experienced mining communities : the case of a proposed uranium mine in Crownpoint",
    "abstract": "Public access to science is an essential environmental justice component of any mining development. Both limited public access to professional scientific knowledge and little acknowledgment by professionals of the contributions of local knowledge hinder discussion of proposed mines. A proposed uranium mine in Crownpoint, New Mexico, a predominantly Navajo community, presents a case for studying the role of expert and local knowledge in the individual's perception of the risks and benefits associated with the mine. Interviews, supplemented with numerous Nuclear Regulatory Commission documents and other articles of the public record, were used to understand how people developed their personal understanding of the trade-offs of mining uranium in their town. This research reveals that family experiences and personal observations are correlated with individual perception of risk, but the perception of uncertainty is related the group of experts available to the individual. The results suggest that individuals in such communities should have access to a range of experts and that local knowledge and experiences should be taken into account when journalists, industry representatives and government officials translate expert knowledge for public consumption.",
    "advisors": ["JoAnn Carmin", "Samuel Browring"],
    "text": "Local and expert knowledge in experienced mining communities : the case of a proposed uranium mine in Crownpoint Public access to science is an essential environmental justice component of any mining development. Both limited public access to professional scientific knowledge and little acknowledgment by professionals of the contributions of local knowledge hinder discussion of proposed mines. A proposed uranium mine in Crownpoint, New Mexico, a predominantly Navajo community, presents a case for studying the role of expert and local knowledge in the individual's perception of the risks and benefits associated with the mine. Interviews, supplemented with numerous Nuclear Regulatory Commission documents and other articles of the public record, were used to understand how people developed their personal understanding of the trade-offs of mining uranium in their town. This research reveals that family experiences and personal observations are correlated with individual perception of risk, but the perception of uncertainty is related the group of experts available to the individual. The results suggest that individuals in such communities should have access to a range of experts and that local knowledge and experiences should be taken into account when journalists, industry representatives and government officials translate expert knowledge for public consumption."
}, {
    "id": "oai:dspace.mit.edu:1721.1/114339",
    "title": "Determining the concentration and source of lead in chocolate using lead isotopes",
    "abstract": "Single-origin dark chocolate samples derived from cocoa grown in developing countries from around the world were analyzed to determine their lead concentrations and the isotopic composition of the lead. The lead isotope ratios were compared with published data from aerosols and volcanic rocks nearest to the cocoa growing regions. Samples from different countries and manufacturers were compared, and we conclude that the source of lead depends on the country of origin and not the manufacturer. Chocolates grown in the Northern Hemisphere usually had lead isotope ratios that matched the global atmospheric lead isotopic signature from the Northern Hemisphere. Chocolates grown in the Southern Hemisphere did not match the global signature, but rather more closely matched the lead isotopic signature from volcanic rocks in their respective countries, and had a lower average lead concentration than chocolates from the Northern Hemisphere. Soils from Venezuela were also analyzed, and confirmed the conclusion that atmospheric lead is the predominant source of bioavailable lead. Many of the chocolates also had lead concentrations below the limit of 0.1 ppm set by the FDA; however, one manufacturer, Dagoba, consistently had lead concentrations above the limit. The percent of cocoa in each chocolate bar was also compared with the lead concentrations, concluding that the concentration of lead is not necessarily dependent on the amount of cocoa in the bar.",
    "advisors": ["Samuel Bowring"],
    "text": "Determining the concentration and source of lead in chocolate using lead isotopes Single-origin dark chocolate samples derived from cocoa grown in developing countries from around the world were analyzed to determine their lead concentrations and the isotopic composition of the lead. The lead isotope ratios were compared with published data from aerosols and volcanic rocks nearest to the cocoa growing regions. Samples from different countries and manufacturers were compared, and we conclude that the source of lead depends on the country of origin and not the manufacturer. Chocolates grown in the Northern Hemisphere usually had lead isotope ratios that matched the global atmospheric lead isotopic signature from the Northern Hemisphere. Chocolates grown in the Southern Hemisphere did not match the global signature, but rather more closely matched the lead isotopic signature from volcanic rocks in their respective countries, and had a lower average lead concentration than chocolates from the Northern Hemisphere. Soils from Venezuela were also analyzed, and confirmed the conclusion that atmospheric lead is the predominant source of bioavailable lead. Many of the chocolates also had lead concentrations below the limit of 0.1 ppm set by the FDA; however, one manufacturer, Dagoba, consistently had lead concentrations above the limit. The percent of cocoa in each chocolate bar was also compared with the lead concentrations, concluding that the concentration of lead is not necessarily dependent on the amount of cocoa in the bar."
}, {
    "id": "oai:dspace.mit.edu:1721.1/114098",
    "title": "Optical analysis of the Wallace Astrophysical Observatory 24-inch and Magellan I 6.5-meter telescopes",
    "abstract": "The goal of this thesis is to propose a layout for the Wallace Astrophysical Observatory (WAO) 24-inch optical system that would utilize the full potential of the telescope and a new CCD imaging instrument. An optical analysis of the 6.5-meter Magellan I telescope was first performed to determine the optimal mounting location for the Raymond and Beverly Sackler Magellan Instant Camera (MAGIC). The analysis method used for the Magellan I model was then applied to the WAO 24-inch telescope. The results of the optical analysis of the WAO 24-inch model suggest that the optimal layout would follow a Cassegrain model with a focal ratio of between 15.3 and 16.6 to obtain image sizes of approximately 1.0 arcsecond or less over a field of up to 26.4 arcminutes in diameter.",
    "advisors": ["James L. Elliot", "David J. Osip"],
    "text": "Optical analysis of the Wallace Astrophysical Observatory 24-inch and Magellan I 6.5-meter telescopes The goal of this thesis is to propose a layout for the Wallace Astrophysical Observatory (WAO) 24-inch optical system that would utilize the full potential of the telescope and a new CCD imaging instrument. An optical analysis of the 6.5-meter Magellan I telescope was first performed to determine the optimal mounting location for the Raymond and Beverly Sackler Magellan Instant Camera (MAGIC). The analysis method used for the Magellan I model was then applied to the WAO 24-inch telescope. The results of the optical analysis of the WAO 24-inch model suggest that the optimal layout would follow a Cassegrain model with a focal ratio of between 15.3 and 16.6 to obtain image sizes of approximately 1.0 arcsecond or less over a field of up to 26.4 arcminutes in diameter."
}, {
    "id": "oai:dspace.mit.edu:1721.1/114113",
    "title": "Determining land use change and desertification in China using remote sensing data",
    "abstract": "Desertification, the spread of desert-like conditions in arid or semiarid areas due to human influence or to climatic change, affects most arable land in arid and semi-arid China. This project provides an analysis of desertification in northeastern arid and semi-arid China to determine its spatial distribution, severity, and causes. It locates areas of desertification and identifies and ranks in order of importance their anthropogenic and climatological causes. It especially focuses on the savanna transition zone west of Beijing to see if climate factors or increasing population density can be correlated to land cover change. GIS (Geographic Information Systems) software is used to recognize locations of rapid land cover change. Statistical tests, such as unbalanced multi-way ANOVA, determine if climatic or anthropogenic factors can predict if an area is undergoing rapid land cover change. The climate and population data is resampled to an uniform 0.5' scale and converted into qualitative, data before statistical testing. This project tests if land cover change, a more difficult indicator to measure, can be predicted by analyzing trends in vegetation, precipitation, temperature, wind and population. Desertification is more likely and more severe in climates with low precipitation. Areas with low population density tend to have less severe land degradation than areas with medium or high density; this may be due to more intense land use in high population areas.",
    "advisors": ["Dennis McLaughlin"],
    "text": "Determining land use change and desertification in China using remote sensing data Desertification, the spread of desert-like conditions in arid or semiarid areas due to human influence or to climatic change, affects most arable land in arid and semi-arid China. This project provides an analysis of desertification in northeastern arid and semi-arid China to determine its spatial distribution, severity, and causes. It locates areas of desertification and identifies and ranks in order of importance their anthropogenic and climatological causes. It especially focuses on the savanna transition zone west of Beijing to see if climate factors or increasing population density can be correlated to land cover change. GIS (Geographic Information Systems) software is used to recognize locations of rapid land cover change. Statistical tests, such as unbalanced multi-way ANOVA, determine if climatic or anthropogenic factors can predict if an area is undergoing rapid land cover change. The climate and population data is resampled to an uniform 0.5' scale and converted into qualitative, data before statistical testing. This project tests if land cover change, a more difficult indicator to measure, can be predicted by analyzing trends in vegetation, precipitation, temperature, wind and population. Desertification is more likely and more severe in climates with low precipitation. Areas with low population density tend to have less severe land degradation than areas with medium or high density; this may be due to more intense land use in high population areas."
}, {
    "id": "oai:dspace.mit.edu:1721.1/114136",
    "title": "Comparison of PSF fitting methods for determining centroids of stars",
    "abstract": "This paper shows a comparison of four fitting models used in order to calculate the centroids for twelve stars. The offsets for each coordinate are calculated with respect to the mean of the coordinates for varying aperture sizes. The computed offsets are then compared to determine if there was any effect from the magnitude of the star or the star's position on the CCD chip. An appropriate aperture size of 12 pixels is chosen in order to compare each method. It is determined that a magnitude effect exists, though it is very small and results in an approximate difference between residuals of between 0.02 and 0.04 pixels, which for most methods is within the fitting error. For the position on the chip effect, the vectors of the x and y residuals are produced in vector plots in order to demonstrate the directional tendency each fitting method had dependent on chip position. Each model has some dependency on position on the CCD chip, with 0.033 pixels as the largest variation between models. Therefore, for accuracy less than 0.033 pixels any of these models can be used for fitting a PSF to a star. However, if greater accuracy is needed more steps need to be taken in order to determine the best PSF fit.",
    "advisors": ["Richard P. Binzel"],
    "text": "Comparison of PSF fitting methods for determining centroids of stars This paper shows a comparison of four fitting models used in order to calculate the centroids for twelve stars. The offsets for each coordinate are calculated with respect to the mean of the coordinates for varying aperture sizes. The computed offsets are then compared to determine if there was any effect from the magnitude of the star or the star's position on the CCD chip. An appropriate aperture size of 12 pixels is chosen in order to compare each method. It is determined that a magnitude effect exists, though it is very small and results in an approximate difference between residuals of between 0.02 and 0.04 pixels, which for most methods is within the fitting error. For the position on the chip effect, the vectors of the x and y residuals are produced in vector plots in order to demonstrate the directional tendency each fitting method had dependent on chip position. Each model has some dependency on position on the CCD chip, with 0.033 pixels as the largest variation between models. Therefore, for accuracy less than 0.033 pixels any of these models can be used for fitting a PSF to a star. However, if greater accuracy is needed more steps need to be taken in order to determine the best PSF fit."
}, {
    "id": "oai:dspace.mit.edu:1721.1/115039",
    "title": "Using machine learning for hydrocarbon prospecting in Reconcavo Basin, Brazil",
    "abstract": "Machine Learning techniques are being widely used in Social Sciences to find connections amongst various variables. Machine Learning connects features across different fields that do not seem to have known mathematical relationships with each other. In natural resource prospecting, machine learning can be applied to connect geochemical, geophysical, and geological variables. However, the biggest challenge in machine learning remains obtaining the data to train the ML algorithms. Here, we have applied machine learning on data extracted from maps via image processing. While the overall accuracy of prediction remains as low as 33% at this stage, we see places where the algorithm can be improved and the accuracy increased.",
    "advisors": ["Bradford Hager"],
    "text": "Using machine learning for hydrocarbon prospecting in Reconcavo Basin, Brazil Machine Learning techniques are being widely used in Social Sciences to find connections amongst various variables. Machine Learning connects features across different fields that do not seem to have known mathematical relationships with each other. In natural resource prospecting, machine learning can be applied to connect geochemical, geophysical, and geological variables. However, the biggest challenge in machine learning remains obtaining the data to train the ML algorithms. Here, we have applied machine learning on data extracted from maps via image processing. While the overall accuracy of prediction remains as low as 33% at this stage, we see places where the algorithm can be improved and the accuracy increased."
}, {
    "id": "oai:dspace.mit.edu:1721.1/114338",
    "title": "Predicted and observed free-air gravity anomalies for delamination models of the formation of the Siberian Flood Basalts",
    "abstract": "The mechanism responsible for the formation of the Siberian Flood Basalts (SFB) has yet to be discovered and adequately quantified. One theory proposes that thinning of the lithosphere due to delamination triggered the eruption. This model is characterized by a drip of denser material within the mantle, and because it involves a density-driven process, calculations of predicted gravity at the surface can be used to test the model. Temperature, composition, and stress output from the delamination model presented in Elkins-Tanton (2007) were used to calculate predicted gravity measurements at the surface. These predictions were then compared to gravity observations of the SFB, focusing on the potential eruptive center at Noril'sk. Model runs in both Cartesian and axisymmetric coordinates were analyzed, and each run predicted a negative anomaly over the site of the drip with a magnitude ranging from 20 to 50 mGal. In the observations, an average radial gravity profile centered on Noril'sk also contained a slight negative anomaly at the center, suggesting partial agreement with the delamination theory. Because the amplitude of the observed gravity anomaly is substantially smaller than the predicted amplitude, the qualitative agreement is encouraging, but not definitive.",
    "advisors": ["Bradford H. Hager"],
    "text": "Predicted and observed free-air gravity anomalies for delamination models of the formation of the Siberian Flood Basalts The mechanism responsible for the formation of the Siberian Flood Basalts (SFB) has yet to be discovered and adequately quantified. One theory proposes that thinning of the lithosphere due to delamination triggered the eruption. This model is characterized by a drip of denser material within the mantle, and because it involves a density-driven process, calculations of predicted gravity at the surface can be used to test the model. Temperature, composition, and stress output from the delamination model presented in Elkins-Tanton (2007) were used to calculate predicted gravity measurements at the surface. These predictions were then compared to gravity observations of the SFB, focusing on the potential eruptive center at Noril'sk. Model runs in both Cartesian and axisymmetric coordinates were analyzed, and each run predicted a negative anomaly over the site of the drip with a magnitude ranging from 20 to 50 mGal. In the observations, an average radial gravity profile centered on Noril'sk also contained a slight negative anomaly at the center, suggesting partial agreement with the delamination theory. Because the amplitude of the observed gravity anomaly is substantially smaller than the predicted amplitude, the qualitative agreement is encouraging, but not definitive."
}, {
    "id": "oai:dspace.mit.edu:1721.1/114327",
    "title": "Refinements and improvements to a phenomenological model for the jet opening angles of gamma-ray bursts",
    "abstract": "Long duration gamma-ray bursts (GRBs) are thought to originate from the core collapse of massive, rapidly rotating stars - events called \"hypernovae.\" In this thesis, we improve upon a phenomenological model to determine [theta], the jet opening angle of GRBs. We assume that hypernova progenitors are massive stars in binary systems. We calculate [theta] by equating two expressions for the probability of a given GRB being detected - one based on the geometry of the beaming model and the other based on the observed and expected rates of long duration GRBs. These expressions give [theta] as a function of several key physical parameters. We estimate these parameters, perform a Monte Carlo simulation, and obtain the most probable value of [theta] for both single and double jet GRB models. In contrast to previous work, we allow the minimum mass of star-forming galaxies to vary between 10Mo and 10Mo, and we calculate the galactic number density separately for three subtypes of spiral galaxies. For single jet and double jet models, we find that [theta] = 2.8+.-.. deg and [theta] = 1.9+.-.. deg respectively. These results are somewhat lower than the results obtained in the earlier stages of this project [15, 16], but are in agreement with values inferred from the observed properties of GRBs [4]. Our results therefore support the assumption that massive binary stars are the progenitors of hypernovae that produce long-duration GRBs.",
    "advisors": ["Paul C. Joss"],
    "text": "Refinements and improvements to a phenomenological model for the jet opening angles of gamma-ray bursts Long duration gamma-ray bursts (GRBs) are thought to originate from the core collapse of massive, rapidly rotating stars - events called \"hypernovae.\" In this thesis, we improve upon a phenomenological model to determine [theta], the jet opening angle of GRBs. We assume that hypernova progenitors are massive stars in binary systems. We calculate [theta] by equating two expressions for the probability of a given GRB being detected - one based on the geometry of the beaming model and the other based on the observed and expected rates of long duration GRBs. These expressions give [theta] as a function of several key physical parameters. We estimate these parameters, perform a Monte Carlo simulation, and obtain the most probable value of [theta] for both single and double jet GRB models. In contrast to previous work, we allow the minimum mass of star-forming galaxies to vary between 10Mo and 10Mo, and we calculate the galactic number density separately for three subtypes of spiral galaxies. For single jet and double jet models, we find that [theta] = 2.8+.-.. deg and [theta] = 1.9+.-.. deg respectively. These results are somewhat lower than the results obtained in the earlier stages of this project [15, 16], but are in agreement with values inferred from the observed properties of GRBs [4]. Our results therefore support the assumption that massive binary stars are the progenitors of hypernovae that produce long-duration GRBs."
}, {
    "id": "oai:dspace.mit.edu:1721.1/114119",
    "title": "Ozone chemistry during global glaciations : a possible climate feedback",
    "abstract": "A theory for changes in ozone chemistry during late Proterozoic global glaciations is developed. The possible significance of temperature, humidity, nitrogen oxides, reactive chlorine, lightning frequency, surface deposition and albedo as altered constraints on ozone processes is discussed. An elementary box model is developed by the author to make first order judgments regarding the significance of chemistry changes on ozone concentrations and its climactic effect. A one dimensional photochemical-transport model (Kasting, 1995) was used to more precisely determine the effects of global glaciations on ozone concentrations up to 5 hPa in several latitude bands. Reduced NO availability in the stratosphere seems to dominate ozone's response (positive anomalies) in the stratosphere. Low temperatures, low humidity, reduced lighting frequency and altered chlorine and nitrogen chemistry collectively reduce ozone presence in the troposphere, however the overall sign of the tropospheric ozone anomaly depends heavily on poorly characterized deposition rates. With output from the one-dimensional photochemistry model, a time-varying ozone concentration field was assembled for the entire planet and used in snowball runs of the General Circulation Model (NCAR Community Atmosphere Model 3.0). These runs were compared to a controlled snowball run with a modern ozone field to discern the climactic significance of altered ozone. Results suggest that ozone concentrations during global glaciations might directly produce global average surface radiation anomalies of -1.5 ~ 1.5 W/m , resulting in global average surface temperature anomalies of -0.5 ~ 0.5K. Magnitude and sign uncertainties result from poorly known deposition rates for ozone over frozen surfaces and the simplicity of the modeling technique. The indirect effect of increasing stratospheric ozone, i.e. a reduction in atmospheric oxidative capacity, may result in positive anomalies of other green house gasses and is discussed as an area for further research.",
    "advisors": ["R. Alan Plumb"],
    "text": "Ozone chemistry during global glaciations : a possible climate feedback A theory for changes in ozone chemistry during late Proterozoic global glaciations is developed. The possible significance of temperature, humidity, nitrogen oxides, reactive chlorine, lightning frequency, surface deposition and albedo as altered constraints on ozone processes is discussed. An elementary box model is developed by the author to make first order judgments regarding the significance of chemistry changes on ozone concentrations and its climactic effect. A one dimensional photochemical-transport model (Kasting, 1995) was used to more precisely determine the effects of global glaciations on ozone concentrations up to 5 hPa in several latitude bands. Reduced NO availability in the stratosphere seems to dominate ozone's response (positive anomalies) in the stratosphere. Low temperatures, low humidity, reduced lighting frequency and altered chlorine and nitrogen chemistry collectively reduce ozone presence in the troposphere, however the overall sign of the tropospheric ozone anomaly depends heavily on poorly characterized deposition rates. With output from the one-dimensional photochemistry model, a time-varying ozone concentration field was assembled for the entire planet and used in snowball runs of the General Circulation Model (NCAR Community Atmosphere Model 3.0). These runs were compared to a controlled snowball run with a modern ozone field to discern the climactic significance of altered ozone. Results suggest that ozone concentrations during global glaciations might directly produce global average surface radiation anomalies of -1.5 ~ 1.5 W/m , resulting in global average surface temperature anomalies of -0.5 ~ 0.5K. Magnitude and sign uncertainties result from poorly known deposition rates for ozone over frozen surfaces and the simplicity of the modeling technique. The indirect effect of increasing stratospheric ozone, i.e. a reduction in atmospheric oxidative capacity, may result in positive anomalies of other green house gasses and is discussed as an area for further research."
}, {
    "id": "oai:dspace.mit.edu:1721.1/114336",
    "title": "A comparison of kinematic and dynamic schemes for calculating long-range atmospheric trajectories",
    "abstract": "Two numerical models, one kinematic and one dynamic, were created and compared in their ability to predict trajectories of atmospheric parcels over eight days. While kinematic models are more widely used due to their accuracy, dynamic models can be used pedagogically to visualize the balance of forces in the atmosphere. The kinematic model used gridded wind speed data from the Global Forecast System (GFS) to predict parcel flow, while the dynamic model calculated wind speeds from advection equations using geopotential height fields from GFS. The trajectories of ensembles of parcels were simulated from five launch locations. The spread of parcels from each location was calculated along with the deviation from reference trajectories. The dynamic model performed comparably to the kinematic model, despite the presence of inertial oscillations in some computed trajectories at mid- and high- latitudes which are likely to be physically unrealistic. The dynamic model was more sensitive to changes in spatial resolution than the kinematic model. Dynamic trajectory models were shown to be accurate enough to be used as a tool to visualize the interplay of forces acting in the atmosphere.",
    "advisors": ["Glenn R. Flierl"],
    "text": "A comparison of kinematic and dynamic schemes for calculating long-range atmospheric trajectories Two numerical models, one kinematic and one dynamic, were created and compared in their ability to predict trajectories of atmospheric parcels over eight days. While kinematic models are more widely used due to their accuracy, dynamic models can be used pedagogically to visualize the balance of forces in the atmosphere. The kinematic model used gridded wind speed data from the Global Forecast System (GFS) to predict parcel flow, while the dynamic model calculated wind speeds from advection equations using geopotential height fields from GFS. The trajectories of ensembles of parcels were simulated from five launch locations. The spread of parcels from each location was calculated along with the deviation from reference trajectories. The dynamic model performed comparably to the kinematic model, despite the presence of inertial oscillations in some computed trajectories at mid- and high- latitudes which are likely to be physically unrealistic. The dynamic model was more sensitive to changes in spatial resolution than the kinematic model. Dynamic trajectory models were shown to be accurate enough to be used as a tool to visualize the interplay of forces acting in the atmosphere."
}, {
    "id": "oai:dspace.mit.edu:1721.1/58189",
    "title": "Microstructural study of two-phase marbles in simple shear",
    "abstract": "Microstructural and textural observations have been conducted on synthetic calcite with 20 wt% quartz deformed in simple shear using transmission electron microscopy and selected-area diffraction. The marbles were deformed at 873, 973, and 1073 K at a stress of 305, 222, and 127 MPa, respectively, and a strain rate of 10 -4 s -1. The microstructure, shape-preferred orientation (SPO), grain aspect ratios, lattice-preferred orientation (LPO), dislocation densities, and grain sizes were compared to the results of other studies on similar carbonates deformed in triaxial loading, torsion, and simple shear. Microstructures are consistent with other marbles at similar temperatures and stresses, with the only major difference in grain size. The SPO and aspect ratios differ from the theoretical calculations, but are consistent with other marbles. This SPO and aspect ratio is consistent with grains behaving as high-viscosity particles with low-viscosity boundaries. Loading conditions appear to affect the strain at which recrystallization starts, with evidence for new grains at a strain of 3 in this study, compared to minimum strains of at least 4 for others. Dislocation densities are 3.5 x 10 13 m -2 , 8 x 10 13 m -2, and 1.3 x 1014 m -2 for the samples at 873, 973, and 1073 K, respectively, and when inserted into a paleopiezometer, the predicted stresses are 347, 257, and 156 MPa, respectively, which is in good agreement with the applied conditions.",
    "advisors": ["J. Brian Evans", "Samuel M. Allen"],
    "text": "Microstructural study of two-phase marbles in simple shear Microstructural and textural observations have been conducted on synthetic calcite with 20 wt% quartz deformed in simple shear using transmission electron microscopy and selected-area diffraction. The marbles were deformed at 873, 973, and 1073 K at a stress of 305, 222, and 127 MPa, respectively, and a strain rate of 10 -4 s -1. The microstructure, shape-preferred orientation (SPO), grain aspect ratios, lattice-preferred orientation (LPO), dislocation densities, and grain sizes were compared to the results of other studies on similar carbonates deformed in triaxial loading, torsion, and simple shear. Microstructures are consistent with other marbles at similar temperatures and stresses, with the only major difference in grain size. The SPO and aspect ratios differ from the theoretical calculations, but are consistent with other marbles. This SPO and aspect ratio is consistent with grains behaving as high-viscosity particles with low-viscosity boundaries. Loading conditions appear to affect the strain at which recrystallization starts, with evidence for new grains at a strain of 3 in this study, compared to minimum strains of at least 4 for others. Dislocation densities are 3.5 x 10 13 m -2 , 8 x 10 13 m -2, and 1.3 x 1014 m -2 for the samples at 873, 973, and 1073 K, respectively, and when inserted into a paleopiezometer, the predicted stresses are 347, 257, and 156 MPa, respectively, which is in good agreement with the applied conditions."
}, {
    "id": "oai:dspace.mit.edu:1721.1/114130",
    "title": "Sizing the X-ray spectral resolution limits of the REgolith X-ray Imaging Spectrometer (REXIS) instrument at Asteroid 1999RQ36",
    "abstract": "The REgolith X-ray Imaging Spectrometer (REXIS), a Charge-Coupled Device (CCD)-based coded aperture soft X-ray (0.3-7.5 keV) telescope for remote geochemical X-ray Fluorescence (XRF) spectrometry, will be flying on board the Origins Spectral Interpretations Resource Identification Security Regolith Explorer (OSIRIS-REx) asteroid sample return mission that will be visiting the asteroid 1999 RQ36 and sending a sample back to Earth. REXIS will detect elemental XRF lines and produce a histogram of results as the spacecraft orbits the asteroid as well as produce a global map of elemental abundance ratios. The accuracy requirement for measuring the global ratios of elements and the spectral resolution requirement for discriminating unique XRF lines from each other have been set in place. The correct interpretation of X-ray measurements from the surface of 1999 RQ36 is limited by properties that are intrinsic to the CCD detector, CCID-41, that has been chosen for REXIS. This thesis study outlines student experimentation and results that were conducted on the CCID-41 detector to gauge the intrinsic detector noise as a function of detector temperature. Further, the widening of spectral lines on the resultant histogram was also equated as a function of detector temperature. Members of the REXIS Team built a spectral resolution model to investigate both the widening of spectral lines as a function of detector temperature and the accuracy of the measurement of elemental abundance line ratios as a function of detector temperature. Data from the student laboratory experimentation suggested that the detector temperature remain at or below -75C to minimize intrinsic noise properties. Data from the computational analyses of the spectral resolution model suggest consistent results that the detector temperature remains at or below -55C to remain within the established REXIS requirements. The combination of these three results leads to the author's recommendation that a detector temperature requirement be set that the temperature of the CCD detector onboard REXIS shall not exceed -55C and that a detector temperature goal be set that the temperature of the CCD detector shall not exceed -75C.",
    "advisors": ["Sara Seager"],
    "text": "Sizing the X-ray spectral resolution limits of the REgolith X-ray Imaging Spectrometer (REXIS) instrument at Asteroid 1999RQ36 The REgolith X-ray Imaging Spectrometer (REXIS), a Charge-Coupled Device (CCD)-based coded aperture soft X-ray (0.3-7.5 keV) telescope for remote geochemical X-ray Fluorescence (XRF) spectrometry, will be flying on board the Origins Spectral Interpretations Resource Identification Security Regolith Explorer (OSIRIS-REx) asteroid sample return mission that will be visiting the asteroid 1999 RQ36 and sending a sample back to Earth. REXIS will detect elemental XRF lines and produce a histogram of results as the spacecraft orbits the asteroid as well as produce a global map of elemental abundance ratios. The accuracy requirement for measuring the global ratios of elements and the spectral resolution requirement for discriminating unique XRF lines from each other have been set in place. The correct interpretation of X-ray measurements from the surface of 1999 RQ36 is limited by properties that are intrinsic to the CCD detector, CCID-41, that has been chosen for REXIS. This thesis study outlines student experimentation and results that were conducted on the CCID-41 detector to gauge the intrinsic detector noise as a function of detector temperature. Further, the widening of spectral lines on the resultant histogram was also equated as a function of detector temperature. Members of the REXIS Team built a spectral resolution model to investigate both the widening of spectral lines as a function of detector temperature and the accuracy of the measurement of elemental abundance line ratios as a function of detector temperature. Data from the student laboratory experimentation suggested that the detector temperature remain at or below -75C to minimize intrinsic noise properties. Data from the computational analyses of the spectral resolution model suggest consistent results that the detector temperature remains at or below -55C to remain within the established REXIS requirements. The combination of these three results leads to the author's recommendation that a detector temperature requirement be set that the temperature of the CCD detector onboard REXIS shall not exceed -55C and that a detector temperature goal be set that the temperature of the CCD detector shall not exceed -75C."
}, {
    "id": "oai:dspace.mit.edu:1721.1/114094",
    "title": "Air quality applications of extreme value theory : return levels of extreme ozone events in Chicago and surrounding areas",
    "abstract": "To quantify the effects of the NO, SIP call in urban and rural locales, surface ozone data from the Air Quality System is analyzed. Methods from extreme value theory are applied to calculate and compare 20-year return levels at 5 urban and 17 rural/suburban sites in Illinois based upon maximum daily 8-. hour average ozone concentrations from summer (JJA) for two periods (1992- 2002 and 2003-2013) and a threshold of 70 (ppb). Between the two periods, 21 out of 22 sites experienced a decrease in 20-year return levels. The magnitude of these decreases does not indicate a strong correlation between population density and air quality improvements, however, further analysis is required.",
    "advisors": ["Noelle Eckley Selin"],
    "text": "Air quality applications of extreme value theory : return levels of extreme ozone events in Chicago and surrounding areas To quantify the effects of the NO, SIP call in urban and rural locales, surface ozone data from the Air Quality System is analyzed. Methods from extreme value theory are applied to calculate and compare 20-year return levels at 5 urban and 17 rural/suburban sites in Illinois based upon maximum daily 8-. hour average ozone concentrations from summer (JJA) for two periods (1992- 2002 and 2003-2013) and a threshold of 70 (ppb). Between the two periods, 21 out of 22 sites experienced a decrease in 20-year return levels. The magnitude of these decreases does not indicate a strong correlation between population density and air quality improvements, however, further analysis is required."
}, {
    "id": "oai:dspace.mit.edu:1721.1/90659",
    "title": "A machine learning model of Manhattan air pollution at high spatial resolution",
    "abstract": "A machine-learning model was created to predict air pollution at high spatial resolution in Manhattan, New York using taxi trip data. Urban air pollution increases morbidity and mortality through respiratory and cardiovascular impacts, and understanding and predicting it is a significant public health challenge. A neural network NARX model was created in MATLAB for each cell on a 250m square grid laid over Manhattan, for a total of 907 individual models across the city, for PM2 .5 , CO, NO2 , 03, and SO 2. In addition to standard meteorological inputs, data describing the distance and time traveled by taxis within each grid cell was used in the models. The models generally performed well, with mean R2 values between .62 (SO 2) and .86 (03), comparable to or better than previous models at this spatial scale. The model is computationally efficient enough to be run in real-time to aid citizens' and public health officials' decisions, and its efficacy suggests that taxi data is a valuable additional input to previous neural network pollution models.",
    "advisors": ["Marguerite Nyhan"],
    "text": "A machine learning model of Manhattan air pollution at high spatial resolution A machine-learning model was created to predict air pollution at high spatial resolution in Manhattan, New York using taxi trip data. Urban air pollution increases morbidity and mortality through respiratory and cardiovascular impacts, and understanding and predicting it is a significant public health challenge. A neural network NARX model was created in MATLAB for each cell on a 250m square grid laid over Manhattan, for a total of 907 individual models across the city, for PM2 .5 , CO, NO2 , 03, and SO 2. In addition to standard meteorological inputs, data describing the distance and time traveled by taxis within each grid cell was used in the models. The models generally performed well, with mean R2 values between .62 (SO 2) and .86 (03), comparable to or better than previous models at this spatial scale. The model is computationally efficient enough to be run in real-time to aid citizens' and public health officials' decisions, and its efficacy suggests that taxi data is a valuable additional input to previous neural network pollution models."
}, {
    "id": "oai:dspace.mit.edu:1721.1/114332",
    "title": "An analysis of a continuum X-ray Diffraction/Fluorescence instrument",
    "abstract": "Scientists at NASA's Goddard Space Flight Center have developed a Combined X-ray Diffraction/Fluorescence (CXRDF) instrument. CXRDF performs simultaneous chemical and structural analysis of an unprepared sample, making it ideal for planetary mineral identification. In an effort to analyze the effectiveness of CXRDF, samples were chosen from a list of minerals that are important in the debate about the origin of the outcrops at Meridiani Planum on Mars. These samples were run on both CXRDF and a laboratory X-ray diffractometer. The datasets were compared, looking at peak identification, d-spacing resolution, and whether the instruments could definitively identify each sample. CXRDF successfully measured the d-spacings for each mineral, and the chemical analysis data were very valuable. However, for CXRDF to be able to definitively identify minerals, its d-spacing range and resolution will need to be improved, in addition to its data analysis software.",
    "advisors": ["Sang-Heon Shim"],
    "text": "An analysis of a continuum X-ray Diffraction/Fluorescence instrument Scientists at NASA's Goddard Space Flight Center have developed a Combined X-ray Diffraction/Fluorescence (CXRDF) instrument. CXRDF performs simultaneous chemical and structural analysis of an unprepared sample, making it ideal for planetary mineral identification. In an effort to analyze the effectiveness of CXRDF, samples were chosen from a list of minerals that are important in the debate about the origin of the outcrops at Meridiani Planum on Mars. These samples were run on both CXRDF and a laboratory X-ray diffractometer. The datasets were compared, looking at peak identification, d-spacing resolution, and whether the instruments could definitively identify each sample. CXRDF successfully measured the d-spacings for each mineral, and the chemical analysis data were very valuable. However, for CXRDF to be able to definitively identify minerals, its d-spacing range and resolution will need to be improved, in addition to its data analysis software."
}, {
    "id": "oai:dspace.mit.edu:1721.1/114121",
    "title": "Controlling factors on Mesozoic and Cenozoic metamorphism and deformation in the Maria Fold and Thrust Belt and Colorado River Extensional Corridor, Southeastern California and Western Arizona",
    "abstract": "The Maria Fold and Thrust Belt (MFTB) and Colorado River Extensional Corridor (CREC) were the sites of atypically extreme compression in Mesozoic time and extension in Cenozoic time, respectively. The orientations of these deformational structures are at odds with the Sevier and Laramide thrust belts and the Basin and Range Extensional Province surrounding these areas, a fact that remains largely unexplained. Data pertaining to metamorphic grade, deformational structures, and plutonism are compiled and reported in order to characterize compression and metamorphism. Field data on the 18.6 Ma Peach Spring Tuff are collected and presented and data on cooling ages are compiled in order to characterize extension. It is suggested that high metamorphic temperatures and ductile compressional structures are related to Late Cretaceous S-type plutonism; furthermore, it is suggested that later extension is related to earlier metamorphism and compression. It is demonstrated that the spread in attitudes of the Peach Spring Tuff correlates well with the degree of post- 18.6 Ma extension. Finally, a favored model is presented for the Mesozoic-Cenozoic evolution of the MFTB and CREC.",
    "advisors": ["Oliver Jagoutz"],
    "text": "Controlling factors on Mesozoic and Cenozoic metamorphism and deformation in the Maria Fold and Thrust Belt and Colorado River Extensional Corridor, Southeastern California and Western Arizona The Maria Fold and Thrust Belt (MFTB) and Colorado River Extensional Corridor (CREC) were the sites of atypically extreme compression in Mesozoic time and extension in Cenozoic time, respectively. The orientations of these deformational structures are at odds with the Sevier and Laramide thrust belts and the Basin and Range Extensional Province surrounding these areas, a fact that remains largely unexplained. Data pertaining to metamorphic grade, deformational structures, and plutonism are compiled and reported in order to characterize compression and metamorphism. Field data on the 18.6 Ma Peach Spring Tuff are collected and presented and data on cooling ages are compiled in order to characterize extension. It is suggested that high metamorphic temperatures and ductile compressional structures are related to Late Cretaceous S-type plutonism; furthermore, it is suggested that later extension is related to earlier metamorphism and compression. It is demonstrated that the spread in attitudes of the Peach Spring Tuff correlates well with the degree of post- 18.6 Ma extension. Finally, a favored model is presented for the Mesozoic-Cenozoic evolution of the MFTB and CREC."
}, {
    "id": "oai:dspace.mit.edu:1721.1/114330",
    "title": "Stable isotope probing of hyperthermophilic filamentous microbial communities in Octopus Spring, Yellowstone National Park",
    "abstract": "Stabe isotope probe (SIP) incubation studies were performed using C-labeled carbon substrates on hyperthermophilic filamentous streamer communities inhabiting Octopus Spring in Yellowstone National Park. Biomass was removed from the outflow stream and incubated at near-in situ conditions with labeled bicarbonate, formate, or acetate. Lipids from the biomass were extracted and analyzed using gas chromatography-mass spectroscopy (GC-MS) and gas chromatography-isotope ratio monitoring-mass spectroscopy(GC-IRMS). We observed incorporation of C-labeled acetate into the total biomass, archaeal lipids, and a small number of bacterial lipids, but no incorporation of labeled formate or bicarbonate. During 67 hours of incubation, 0.060 [mu]g of labeled acetate was incorporated by the archaeal and bacterial community. The lack of acetate incorporation by most bacteria, or formate and bicarbonate incorporation by any community member may reflect rates of carbon turnover, the carbon acquisition pathway used, or inhibition under experimental conditions,",
    "advisors": ["Roger Summons"],
    "text": "Stable isotope probing of hyperthermophilic filamentous microbial communities in Octopus Spring, Yellowstone National Park Stabe isotope probe (SIP) incubation studies were performed using C-labeled carbon substrates on hyperthermophilic filamentous streamer communities inhabiting Octopus Spring in Yellowstone National Park. Biomass was removed from the outflow stream and incubated at near-in situ conditions with labeled bicarbonate, formate, or acetate. Lipids from the biomass were extracted and analyzed using gas chromatography-mass spectroscopy (GC-MS) and gas chromatography-isotope ratio monitoring-mass spectroscopy(GC-IRMS). We observed incorporation of C-labeled acetate into the total biomass, archaeal lipids, and a small number of bacterial lipids, but no incorporation of labeled formate or bicarbonate. During 67 hours of incubation, 0.060 [mu]g of labeled acetate was incorporated by the archaeal and bacterial community. The lack of acetate incorporation by most bacteria, or formate and bicarbonate incorporation by any community member may reflect rates of carbon turnover, the carbon acquisition pathway used, or inhibition under experimental conditions,"
}, {
    "id": "oai:dspace.mit.edu:1721.1/114111",
    "title": "H107[alpha] recombination-line emission, 4800-MHz and 1666-MHz continuum emission in the HII region RCW38",
    "abstract": "We present results from observations of H107[alpha] recombination-line emission and the related 4800 MHz continuum emission of the HII region RCW 38 using the Australia Telescope Compact Array. We find the continuum emission to be concentrated in a ring-like structure with the 05 star, IRS2, approximately centered in the cavity within the ring. The temperature of the ionized gas ranges from 5200 to 7500 K and the emission is optically thin. The H107[alpha] line emission appears to be confined within the continuum ring. We also find the continuum ring to encircle the peak in the diffuse X-ray gas. The radio continuum emission matches closely to NIR observations with a bright western ridge containing the peak in the 10[mu]m emission known as IRS1 (Frogel et al. 1974) apparent in both observations. From calculations of continuum and line parameters, we estimate the spectral type of the ionizing source for the region to be an 05/06 star which is consistent with the spectral type of IRS2.",
    "advisors": ["Richard P.Binzel"],
    "text": "H107[alpha] recombination-line emission, 4800-MHz and 1666-MHz continuum emission in the HII region RCW38 We present results from observations of H107[alpha] recombination-line emission and the related 4800 MHz continuum emission of the HII region RCW 38 using the Australia Telescope Compact Array. We find the continuum emission to be concentrated in a ring-like structure with the 05 star, IRS2, approximately centered in the cavity within the ring. The temperature of the ionized gas ranges from 5200 to 7500 K and the emission is optically thin. The H107[alpha] line emission appears to be confined within the continuum ring. We also find the continuum ring to encircle the peak in the diffuse X-ray gas. The radio continuum emission matches closely to NIR observations with a bright western ridge containing the peak in the 10[mu]m emission known as IRS1 (Frogel et al. 1974) apparent in both observations. From calculations of continuum and line parameters, we estimate the spectral type of the ionizing source for the region to be an 05/06 star which is consistent with the spectral type of IRS2."
}, {
    "id": "oai:dspace.mit.edu:1721.1/114376",
    "title": "A paleomagnetic study of the angrite Sahara 99555",
    "abstract": "Sahara 99555 (SAH 99555) is the oldest dated angrite sample, a rare type of meteorite, and is only ~2 Myr younger than the age of the solar system (1, 2). SAH 99555 shows no post-cooling brecciation or weathering from the parent body, and does not display signs of significant terrestrial weathering. Therefore, paleomagnetic experiments were conducted for the first time on SAH 99555 to discover if it contains a primary paleomagnetism and then to determine a paleointensity estimate and its ferromagnetic mineralogy. Our studies show that the primary ferromagnetic mineral in SAH 99555 is magnetite and there are also some accessory ferromagnetic minerals including titanomagnetite and hematite. The natural remanent magnetization (NRM) of SAH 99555 appears to have a low-coercivity component probably from a collector's hand magnet or the Earth's field as well as a high-coercivity component, similar to D'Orbigny, another angrite. The paleointensity measurements of the high-coercivity component of SAH 99555,5 agree with the paleointensity estimates made for other angrites (D'Orbigny and A-881371) by Weiss et al. (2008) (4). Therefore, it appears that SAH 99555 does record a primary paleofield from when it was on the angrite parent body, which would be the oldest known paleomagnetic record yet identified in a planetary rock. Further studies are suggested to verify these conclusions, which make SAH 99555 one more key data point in the understanding of the early solar system.",
    "advisors": ["Benjamin Weiss"],
    "text": "A paleomagnetic study of the angrite Sahara 99555 Sahara 99555 (SAH 99555) is the oldest dated angrite sample, a rare type of meteorite, and is only ~2 Myr younger than the age of the solar system (1, 2). SAH 99555 shows no post-cooling brecciation or weathering from the parent body, and does not display signs of significant terrestrial weathering. Therefore, paleomagnetic experiments were conducted for the first time on SAH 99555 to discover if it contains a primary paleomagnetism and then to determine a paleointensity estimate and its ferromagnetic mineralogy. Our studies show that the primary ferromagnetic mineral in SAH 99555 is magnetite and there are also some accessory ferromagnetic minerals including titanomagnetite and hematite. The natural remanent magnetization (NRM) of SAH 99555 appears to have a low-coercivity component probably from a collector's hand magnet or the Earth's field as well as a high-coercivity component, similar to D'Orbigny, another angrite. The paleointensity measurements of the high-coercivity component of SAH 99555,5 agree with the paleointensity estimates made for other angrites (D'Orbigny and A-881371) by Weiss et al. (2008) (4). Therefore, it appears that SAH 99555 does record a primary paleofield from when it was on the angrite parent body, which would be the oldest known paleomagnetic record yet identified in a planetary rock. Further studies are suggested to verify these conclusions, which make SAH 99555 one more key data point in the understanding of the early solar system."
}, {
    "id": "oai:dspace.mit.edu:1721.1/114326",
    "title": "Lower Charles River bathymetry : 108 years of fresh water",
    "abstract": "The Lower Charles River has been a heavily utilized urban river that runs between Cambridge and Boston in Massachusetts. The recreational usage of the river is dependent on adequate water depths and there have been no definitive prior studies on the sedimentation rate of the Lower Charles River. The river transitioned from tidal to a freshwater basin in 1908 and the study area for historical comparisons was from the old Charles River Dam to the Boston University Bridge. This study surveyed the river, digitized three prior surveys that spanned 114 years, calculated volumes and depth distributions for each survey, and estimated sedimentation rates from fits to the volumes over time. The average sedimentation rate is estimated as 5-10 mm/year, which implies 1.8-3.5 feet sedimentation since 1908. Sedimentation rates and distributions are necessary to develop comprehensive management plans for the river.",
    "advisors": ["Taylor Perron"],
    "text": "Lower Charles River bathymetry : 108 years of fresh water The Lower Charles River has been a heavily utilized urban river that runs between Cambridge and Boston in Massachusetts. The recreational usage of the river is dependent on adequate water depths and there have been no definitive prior studies on the sedimentation rate of the Lower Charles River. The river transitioned from tidal to a freshwater basin in 1908 and the study area for historical comparisons was from the old Charles River Dam to the Boston University Bridge. This study surveyed the river, digitized three prior surveys that spanned 114 years, calculated volumes and depth distributions for each survey, and estimated sedimentation rates from fits to the volumes over time. The average sedimentation rate is estimated as 5-10 mm/year, which implies 1.8-3.5 feet sedimentation since 1908. Sedimentation rates and distributions are necessary to develop comprehensive management plans for the river."
}, {
    "id": "oai:dspace.mit.edu:1721.1/114322",
    "title": "Comets in the near-Earth object population",
    "abstract": "Because the lifespan of near-Earth objects (NEOs) is shorter than the age of the solar system, these objects originated elsewhere and they must have a source of re-supply. We seek to determine what fraction of the NEO population consists of dormant or extinct comets. We identify comet candidates among NEOs using three criteria: the Jovian Tisserand parameter Tj < 3, comet nucleus-like spectral parameters (generally linear spectra which correspond to C, D, or P taxonomic types), and low (<0.075) albedos. Out of 31 objects we sample having Tj < 3, we find 17 objects or approximately 55% also satisfy these comet candidate criteria. Bias corrected discovery statistics (Stuart 2003, Ph.D. thesis; Stuart & Binzel 2004, Icarus 170, 295) estimate 30% of the entire NEO population resides in orbits having a value of Tj < 3. Combining these two factors suggests that approximately 16% of the total NEO population has both dynamical and physical properties consistent with a cometary origin.",
    "advisors": ["Richard P. Binzel"],
    "text": "Comets in the near-Earth object population Because the lifespan of near-Earth objects (NEOs) is shorter than the age of the solar system, these objects originated elsewhere and they must have a source of re-supply. We seek to determine what fraction of the NEO population consists of dormant or extinct comets. We identify comet candidates among NEOs using three criteria: the Jovian Tisserand parameter Tj < 3, comet nucleus-like spectral parameters (generally linear spectra which correspond to C, D, or P taxonomic types), and low (<0.075) albedos. Out of 31 objects we sample having Tj < 3, we find 17 objects or approximately 55% also satisfy these comet candidate criteria. Bias corrected discovery statistics (Stuart 2003, Ph.D. thesis; Stuart & Binzel 2004, Icarus 170, 295) estimate 30% of the entire NEO population resides in orbits having a value of Tj < 3. Combining these two factors suggests that approximately 16% of the total NEO population has both dynamical and physical properties consistent with a cometary origin."
}, {
    "id": "oai:dspace.mit.edu:1721.1/114324",
    "title": "A coupled atmosphere-ocean model of thermohaline circulation, including wind-driven gyre circulation with an analytical solution",
    "abstract": "A parameter representing circulation due to wind forcing is added to the thermohaline circulation model of Marotzke (1996). The model consists of four boxes and is governed by a system of two differential equations governing the temperature and salinity differences between high latitude ocean and low latitude ocean boxes. The modified model is solved numerically for equilibrium solutions, and then solved analytically by the method of Krasovskiy and Stone (1998). At the maximum strength of wind-forced circulation studied, v = 5 x 10- s-, a stable thermal mode equilibrium temperature difference of 25 K is calculated. Once v reaches a critical value, which is within the range of physically reasonable values, the stable haline mode equlibrium and unstable thermal mode equilibrium are no longer observed. It is concluded that strong wind-forced circulation suppresses the thermal mode equilibrium, but that more research is necessary to determine the degree to which this effect is present in the real world.",
    "advisors": ["Peter H. Stone"],
    "text": "A coupled atmosphere-ocean model of thermohaline circulation, including wind-driven gyre circulation with an analytical solution A parameter representing circulation due to wind forcing is added to the thermohaline circulation model of Marotzke (1996). The model consists of four boxes and is governed by a system of two differential equations governing the temperature and salinity differences between high latitude ocean and low latitude ocean boxes. The modified model is solved numerically for equilibrium solutions, and then solved analytically by the method of Krasovskiy and Stone (1998). At the maximum strength of wind-forced circulation studied, v = 5 x 10- s-, a stable thermal mode equilibrium temperature difference of 25 K is calculated. Once v reaches a critical value, which is within the range of physically reasonable values, the stable haline mode equlibrium and unstable thermal mode equilibrium are no longer observed. It is concluded that strong wind-forced circulation suppresses the thermal mode equilibrium, but that more research is necessary to determine the degree to which this effect is present in the real world."
}, {
    "id": "oai:dspace.mit.edu:1721.1/114125",
    "title": "Ice and the apparent variation of GPS station positions for Alaska",
    "abstract": "Most GPS stations in Alaska show apparent seasonal variations on the order of one centimeter. The majority of these stations move downwards throughout the winter, a motion which is in phase with regional snowfall and has been attributed to hydrological loading (Fu et. al., 2012). The range of phases across the state, however, spans half the year and does not correlate with snow patterns. Six stations show discontinuous seasonal variations on the order of 1-10cm. After examining the geography and windspeed at these sites, we conclude that at least three of them will accumulate rime ice during the winter, and that this ice can cause apparent upwards motions of the stations. This hypothesis is supported by seasonal variations in the stations' multipath values, which indicate increased signal scattering during the winter.",
    "advisors": ["Thomas Herring"],
    "text": "Ice and the apparent variation of GPS station positions for Alaska Most GPS stations in Alaska show apparent seasonal variations on the order of one centimeter. The majority of these stations move downwards throughout the winter, a motion which is in phase with regional snowfall and has been attributed to hydrological loading (Fu et. al., 2012). The range of phases across the state, however, spans half the year and does not correlate with snow patterns. Six stations show discontinuous seasonal variations on the order of 1-10cm. After examining the geography and windspeed at these sites, we conclude that at least three of them will accumulate rime ice during the winter, and that this ice can cause apparent upwards motions of the stations. This hypothesis is supported by seasonal variations in the stations' multipath values, which indicate increased signal scattering during the winter."
}, {
    "id": "oai:dspace.mit.edu:1721.1/114124",
    "title": "First autonomous telescope at Wallace Observatory : impact and preliminary results",
    "abstract": "The construction and characterization of an autonomous telescope began in Fall 2014 at the MIT George R. Wallace, Jr. Astrophysical Observatory. An 11-inch Cassegrain Telescope was assembled in a 10-foot Technical Innovations ProDome. This telescope, the Small AUtonomous Robotic Optical Nightwatcher (SAURON), has the potential to autonomously collect photometric images. Data were taken on T-And0-15785, an eclipsing binary star, in order to test and characterize the system. The out-of-ecliptic R magnitude of T-AndO-15785 was found to be 13.487  0.016. The magnitude changes for the primary and secondary eclipses were found to be 0.72  0.036 and 0.62  0.031 R magnitudes respectively. The telescope, SAURON, is currently able to locate targets and collect data robotically with an observer monitoring from afar.",
    "advisors": ["Richard Binzel", "Michael Person"],
    "text": "First autonomous telescope at Wallace Observatory : impact and preliminary results The construction and characterization of an autonomous telescope began in Fall 2014 at the MIT George R. Wallace, Jr. Astrophysical Observatory. An 11-inch Cassegrain Telescope was assembled in a 10-foot Technical Innovations ProDome. This telescope, the Small AUtonomous Robotic Optical Nightwatcher (SAURON), has the potential to autonomously collect photometric images. Data were taken on T-And0-15785, an eclipsing binary star, in order to test and characterize the system. The out-of-ecliptic R magnitude of T-AndO-15785 was found to be 13.487  0.016. The magnitude changes for the primary and secondary eclipses were found to be 0.72  0.036 and 0.62  0.031 R magnitudes respectively. The telescope, SAURON, is currently able to locate targets and collect data robotically with an observer monitoring from afar."
}, {
    "id": "oai:dspace.mit.edu:1721.1/114107",
    "title": "Mercury emissions inventories in the Lake Superior states",
    "abstract": "Mercury pollution can cause harmful impacts on human health and the environment, a concern that is magnified in marine environments like the Great Lakes. While there are many local, national, and global efforts to track emissions, one key complication to accurately estimating atmospheric mercury is the disagreement between the two national EPA inventories, the National Emissions Inventory (NEI) and Toxics Release Inventory (TRI), which differ in both location and magnitude of emissions. By comparing the NEI and TRI datasets from 2008 and 2011 in the states bordering Lake Superior, this study aims to uncover potential biases in each inventory and determine when a given inventory is more accurate. Additionally, year-to-year TRI emission totals since 2000 are studied to produce a more precise visualization of mercury trends in the Lake Superior Basin. The most notable difference between the two inventories was the absence of mining in TRI, one of the most significant sectors in NEI for both years studied. The utilities sector, however, showed more agreement between the two inventories. The relationship between the NEI and TRI numbers for the facilities within the utilities sector that were found in both datasets was found to be TRI = (1.206)NEI, matching the results from a previous study. Additionally, the study of the yearly TRI data from 2000 to 2014 showed that while average emissions per facility have been declining since 2000, particularly in the manufacturing sector, there is a surprising degree of variability in yearly totals than expected, exposing a potential topic of future research.",
    "advisors": ["Noelle Selin"],
    "text": "Mercury emissions inventories in the Lake Superior states Mercury pollution can cause harmful impacts on human health and the environment, a concern that is magnified in marine environments like the Great Lakes. While there are many local, national, and global efforts to track emissions, one key complication to accurately estimating atmospheric mercury is the disagreement between the two national EPA inventories, the National Emissions Inventory (NEI) and Toxics Release Inventory (TRI), which differ in both location and magnitude of emissions. By comparing the NEI and TRI datasets from 2008 and 2011 in the states bordering Lake Superior, this study aims to uncover potential biases in each inventory and determine when a given inventory is more accurate. Additionally, year-to-year TRI emission totals since 2000 are studied to produce a more precise visualization of mercury trends in the Lake Superior Basin. The most notable difference between the two inventories was the absence of mining in TRI, one of the most significant sectors in NEI for both years studied. The utilities sector, however, showed more agreement between the two inventories. The relationship between the NEI and TRI numbers for the facilities within the utilities sector that were found in both datasets was found to be TRI = (1.206)NEI, matching the results from a previous study. Additionally, the study of the yearly TRI data from 2000 to 2014 showed that while average emissions per facility have been declining since 2000, particularly in the manufacturing sector, there is a surprising degree of variability in yearly totals than expected, exposing a potential topic of future research."
}, {
    "id": "oai:dspace.mit.edu:1721.1/114096",
    "title": "Geochronological constraints on the Trinity diamictite in Newfoundland : Implications for Ediacaran glaciation",
    "abstract": "The Avalon terrane in Newfoundland includes the Ediacaran Gaskiers Formation, which has been associated with a Snowball glaciation event. The complicated regional stratigraphy and lack of precise geochronological constraints has made it difficult to determine the spatial and temporal extent of the Gaskiers glaciation. Recent recognition of a diamictite facies on the nearby Bonavista Peninsula correlative with the Gaskiers diamictite has allowed for new, high-precision geochronological constraints on the Gaskiers glaciation and constrains the duration of the event to less than 390 320 kyr. The Snowball Earth hypothesis requires that glaciation continued for several millions of years so that CO2 could build up to high enough levels in the atmosphere for catastrophic deglaciation; the short duration of the Gaskiers event makes it unlikely to have been a Snowball event. Further geochronological studies are needed to determine whether the Gaskiers glaciation was a discrete event or if it was a glacial maximum in a longer Ediacaran ice age.",
    "advisors": ["Kristin Bergmann"],
    "text": "Geochronological constraints on the Trinity diamictite in Newfoundland : Implications for Ediacaran glaciation The Avalon terrane in Newfoundland includes the Ediacaran Gaskiers Formation, which has been associated with a Snowball glaciation event. The complicated regional stratigraphy and lack of precise geochronological constraints has made it difficult to determine the spatial and temporal extent of the Gaskiers glaciation. Recent recognition of a diamictite facies on the nearby Bonavista Peninsula correlative with the Gaskiers diamictite has allowed for new, high-precision geochronological constraints on the Gaskiers glaciation and constrains the duration of the event to less than 390 320 kyr. The Snowball Earth hypothesis requires that glaciation continued for several millions of years so that CO2 could build up to high enough levels in the atmosphere for catastrophic deglaciation; the short duration of the Gaskiers event makes it unlikely to have been a Snowball event. Further geochronological studies are needed to determine whether the Gaskiers glaciation was a discrete event or if it was a glacial maximum in a longer Ediacaran ice age."
}, {
    "id": "oai:dspace.mit.edu:1721.1/114099",
    "title": "An examination of trace element concentrations across calcite/aragonite transitions in a Madagascan stalagmite",
    "abstract": "Calcite and aragonite speleothems have been used for the past few decades to provide paleoclimate information, but in speleothems of mixed mineralogy the signal of mineral changes can complicate understanding of climate signals also present. This mineral signal must therefore be examined and controlled for. In this work, ICP-MS (inductively-coupled plasma mass spectrometry) analysis of trace element (strontium and magnesium) incorporation into calcium carbonate provides additional evidence for a shift in CaCO3 polymorphs identified via XRD (xray diffraction) analysis of speleothem AB-2 from Anjohibe Cave, Madgascar. The ICP-MS Sr/Ca data qualitatively supports the Sr/Ca data collected by the XRF core scanner, exhibiting a sharp decrease in value across the identified mineral transition. A corresponding increase in Mg/Ca revealed by the ICP-MS provides further evidence for a change from aragonite to calcite at this location. This mineralogical change occurred between 870-880 CE (+/- 13 years) is nearly concurrent with a shift in [delta]13C isotopes that was identified in previous work on this speleothem and attributed to an ecological shift in dominant photosynthetic pathways (Bums et al 2016). A second, control section was identified as pure calcite by XRD analysis, but revealed Mg/Ca and Sr/Ca ratios characteristic of mineralogical transitions, suggesting that there may have been layers of aragonite that have recrystallized to calcite since deposition.",
    "advisors": ["David McGee"],
    "text": "An examination of trace element concentrations across calcite/aragonite transitions in a Madagascan stalagmite Calcite and aragonite speleothems have been used for the past few decades to provide paleoclimate information, but in speleothems of mixed mineralogy the signal of mineral changes can complicate understanding of climate signals also present. This mineral signal must therefore be examined and controlled for. In this work, ICP-MS (inductively-coupled plasma mass spectrometry) analysis of trace element (strontium and magnesium) incorporation into calcium carbonate provides additional evidence for a shift in CaCO3 polymorphs identified via XRD (xray diffraction) analysis of speleothem AB-2 from Anjohibe Cave, Madgascar. The ICP-MS Sr/Ca data qualitatively supports the Sr/Ca data collected by the XRF core scanner, exhibiting a sharp decrease in value across the identified mineral transition. A corresponding increase in Mg/Ca revealed by the ICP-MS provides further evidence for a change from aragonite to calcite at this location. This mineralogical change occurred between 870-880 CE (+/- 13 years) is nearly concurrent with a shift in [delta]13C isotopes that was identified in previous work on this speleothem and attributed to an ecological shift in dominant photosynthetic pathways (Bums et al 2016). A second, control section was identified as pure calcite by XRD analysis, but revealed Mg/Ca and Sr/Ca ratios characteristic of mineralogical transitions, suggesting that there may have been layers of aragonite that have recrystallized to calcite since deposition."
}, {
    "id": "oai:dspace.mit.edu:1721.1/114102",
    "title": "Analysis of Pluto's light curve to detect volatile transport",
    "abstract": "Changes in the volatile distribution on Pluto's surface and in its atmosphere are expected to occur over its orbital path due to varying surface insolation[14]. To investigate these changes, a model was created to synthesize light curves of Pluto, given the viewing geometry and surface albedo distribution. Using an initial surface albedo distribution based on images taken by New Horizons, changes in the light curve mean magnitudes and amplitudes over time were compared to the smallest magnitude changes detectable by a variety of telescopes. The model predicts that yearly observations on a large ground-based telescope, such as the 6.5-meter Magellan telescopes, could observe magnitude changes due to both changes in viewing geometry and surface albedo changes. The model can be compared to future observations to estimate how much surface albedo change is necessary to produce the observed light curves, and can therefore be used to link observational data to physical changes on Pluto's surface and the methods of volatile transport responsible for those changes.",
    "advisors": ["Amanda Bosh"],
    "text": "Analysis of Pluto's light curve to detect volatile transport Changes in the volatile distribution on Pluto's surface and in its atmosphere are expected to occur over its orbital path due to varying surface insolation[14]. To investigate these changes, a model was created to synthesize light curves of Pluto, given the viewing geometry and surface albedo distribution. Using an initial surface albedo distribution based on images taken by New Horizons, changes in the light curve mean magnitudes and amplitudes over time were compared to the smallest magnitude changes detectable by a variety of telescopes. The model predicts that yearly observations on a large ground-based telescope, such as the 6.5-meter Magellan telescopes, could observe magnitude changes due to both changes in viewing geometry and surface albedo changes. The model can be compared to future observations to estimate how much surface albedo change is necessary to produce the observed light curves, and can therefore be used to link observational data to physical changes on Pluto's surface and the methods of volatile transport responsible for those changes."
}, {
    "id": "oai:dspace.mit.edu:1721.1/114373",
    "title": "A global and tropical quasi-decadal oscillation of the atmosphere and Ocean",
    "abstract": "An oscillatory, quasi-periodic signal with a period of around 10 years was found in radiosonde- and satellite-measured datasets of lower stratospheric temperature. Power spectrum analysis and Fourier decomposition were used to characterize the temporal and vertical manifestations of the signal, while EOF analyses were used to analyze its spatial characteristics. The oscillation was found to be unrelated to the solar activity cycle, while it displayed coherence with similar oscillatory signals in ENSO, PDO and AMO indices, as well as with a quasi-decadal signal in SST data. Finally, the quasi-decadal signal in lower stratospheric temperature was found to have a small but measurable contribution to the signal of tropical cyclone potential intensity in the Atlantic MDR.",
    "advisors": ["Kerry Emanuel"],
    "text": "A global and tropical quasi-decadal oscillation of the atmosphere and Ocean An oscillatory, quasi-periodic signal with a period of around 10 years was found in radiosonde- and satellite-measured datasets of lower stratospheric temperature. Power spectrum analysis and Fourier decomposition were used to characterize the temporal and vertical manifestations of the signal, while EOF analyses were used to analyze its spatial characteristics. The oscillation was found to be unrelated to the solar activity cycle, while it displayed coherence with similar oscillatory signals in ENSO, PDO and AMO indices, as well as with a quasi-decadal signal in SST data. Finally, the quasi-decadal signal in lower stratospheric temperature was found to have a small but measurable contribution to the signal of tropical cyclone potential intensity in the Atlantic MDR."
}, {
    "id": "oai:dspace.mit.edu:1721.1/114097",
    "title": "Does the theory of parameterized conveciton apply to layered mantle convection?",
    "abstract": "There are numerous models for convection of the Earth's mantle, the end cases of which are whole-mantle convection and layered convection. Heat flow is an important consideration in the evalution of these models. Simple thermal evolution models based on boundary layer theory have in the past been used to look at these models. However, insufficient attention has been paid to how well the theory applies. This was particularly uncertain for the case of layered convection with a radiogenically enriched lower mantle. I modified the finite-element code ConMan to include exponentially decaying internal heating so that the radiogenic isotopes in the lower layer would be accurately represented, and compared the experimental results of a one-layer case and a two-layer case to the theoretical solutions for those cases from boundary layer theory. It turns out that boundary layer theory does indeed seem to be accurate for the case of a two-layered convecting system with a radiogenic lower layer that produces exponentially decaying internal heating.",
    "advisors": ["Bradford H. Hager"],
    "text": "Does the theory of parameterized conveciton apply to layered mantle convection? There are numerous models for convection of the Earth's mantle, the end cases of which are whole-mantle convection and layered convection. Heat flow is an important consideration in the evalution of these models. Simple thermal evolution models based on boundary layer theory have in the past been used to look at these models. However, insufficient attention has been paid to how well the theory applies. This was particularly uncertain for the case of layered convection with a radiogenically enriched lower mantle. I modified the finite-element code ConMan to include exponentially decaying internal heating so that the radiogenic isotopes in the lower layer would be accurately represented, and compared the experimental results of a one-layer case and a two-layer case to the theoretical solutions for those cases from boundary layer theory. It turns out that boundary layer theory does indeed seem to be accurate for the case of a two-layered convecting system with a radiogenic lower layer that produces exponentially decaying internal heating."
}, {
    "id": "oai:dspace.mit.edu:1721.1/117912",
    "title": "Methane and carbon dioxide cycling in soils of the Harvard Forest",
    "abstract": "Soil is Earth's largest terrestrial carbon pool (Oertel et al., 2016) and can act as a net source of greenhouse gases (GHG). However, if organic material accumulates in soils faster than it is converted to CO2 by cellular respiration, soil becomes a smaller GHG source and even has the potential to become a GHG sink. Not much is known about factors that drive soil to be a source or a sink of GHG. Soil temperature and moisture have both been shown to correlate with CH4 emissions and temperature has been shown to correlate with CO 2 emissions (Jacinthe et al., 2015). Currently these relationships are not well constrained, particularly in upland soils, which are soils found at elevations between 100 and 500 m (Carating et al., 2014). Soil from the Harvard Forest was collected and used in two in-lab flux experiments to constrain the effect that soil moisture has on i.) the rate of CH4 and CO2 production/consumption and ii.) the fraction of injected CH4 that is oxidized to CO2 by soil microbes. The first experiment involved injecting vials containing soil samples with CH4 , taking an initial measurement with a residual gas analyzer (RGA), incubating for three days, and taking final measurements using the RGA. The results of this experiment indicated that cellular respiration is an important carbon source in these soils, with more CO2 coming from cellular respiration than from the oxidation of CH4. The second experiment involved injecting vials containing soil samples with CH4 and 14CH4 as a tracer, incubating for six days, and analyzing CO2 from each sample using a scintillation counter. This experiment showed a weak trend indicating that increased soil moisture may result in decreased CH4 oxidation. Results showed that decays per minute from the samples were lower than in a control. These results indicated that not all CO 2 from each sample was successfully captured and analyzed using the methods here. So while the trend may hold true, it should be supported by reconducting the experiment using a more reliable means of CO2 capture. The unexpected results from both experiments indicated that there is still much to be learned about the reactions that occur in these soils and how to perfect laboratory methods to study them.",
    "advisors": ["Colette Heald"],
    "text": "Methane and carbon dioxide cycling in soils of the Harvard Forest Soil is Earth's largest terrestrial carbon pool (Oertel et al., 2016) and can act as a net source of greenhouse gases (GHG). However, if organic material accumulates in soils faster than it is converted to CO2 by cellular respiration, soil becomes a smaller GHG source and even has the potential to become a GHG sink. Not much is known about factors that drive soil to be a source or a sink of GHG. Soil temperature and moisture have both been shown to correlate with CH4 emissions and temperature has been shown to correlate with CO 2 emissions (Jacinthe et al., 2015). Currently these relationships are not well constrained, particularly in upland soils, which are soils found at elevations between 100 and 500 m (Carating et al., 2014). Soil from the Harvard Forest was collected and used in two in-lab flux experiments to constrain the effect that soil moisture has on i.) the rate of CH4 and CO2 production/consumption and ii.) the fraction of injected CH4 that is oxidized to CO2 by soil microbes. The first experiment involved injecting vials containing soil samples with CH4 , taking an initial measurement with a residual gas analyzer (RGA), incubating for three days, and taking final measurements using the RGA. The results of this experiment indicated that cellular respiration is an important carbon source in these soils, with more CO2 coming from cellular respiration than from the oxidation of CH4. The second experiment involved injecting vials containing soil samples with CH4 and 14CH4 as a tracer, incubating for six days, and analyzing CO2 from each sample using a scintillation counter. This experiment showed a weak trend indicating that increased soil moisture may result in decreased CH4 oxidation. Results showed that decays per minute from the samples were lower than in a control. These results indicated that not all CO 2 from each sample was successfully captured and analyzed using the methods here. So while the trend may hold true, it should be supported by reconducting the experiment using a more reliable means of CO2 capture. The unexpected results from both experiments indicated that there is still much to be learned about the reactions that occur in these soils and how to perfect laboratory methods to study them."
}, {
    "id": "oai:dspace.mit.edu:1721.1/114380",
    "title": "The Mercurian magma ocean, first crust, and implications for planetary formation mechanisms",
    "abstract": "The size of the Mercurian core and the low ferrous iron bearing silicate content of its crust offer constraints on formation models for the planet. Here we consider a bulk composition that allows endogenous formation of the planet's large core, and by processing the mantle through a magma ocean, would produce a low-iron crust. More Earth-like bulk compositions require silicate removal, perhaps by a giant impact, to create the planet's large core fraction. The earliest crusts expected in a giant impact scenario are discussed in comparison to the endogenous model. We find that the endogenous model can produce a large core with either a plagioclase flotation crust or a low-iron magmatic crust. For the giant impact model, in the absence of a plagioclase flotation crust, the impact may be constrained to occur within about 300,000 years of the planet's initial fractionating magma ocean, at which time the giant impact can remove most of the silicate iron oxide budget of the planet before gravitational overturn carries it into the deep planetary interior. Thus a specific bulk composition is required to make Mercury endogenously, but specific timing of events is required to make it exogenously through giant impact. Measurements taken by the MESSENGER mission, when compared to predictions given here, may help resolve Mercury's formation process.",
    "advisors": ["Linda T. Elkins-Tanton"],
    "text": "The Mercurian magma ocean, first crust, and implications for planetary formation mechanisms The size of the Mercurian core and the low ferrous iron bearing silicate content of its crust offer constraints on formation models for the planet. Here we consider a bulk composition that allows endogenous formation of the planet's large core, and by processing the mantle through a magma ocean, would produce a low-iron crust. More Earth-like bulk compositions require silicate removal, perhaps by a giant impact, to create the planet's large core fraction. The earliest crusts expected in a giant impact scenario are discussed in comparison to the endogenous model. We find that the endogenous model can produce a large core with either a plagioclase flotation crust or a low-iron magmatic crust. For the giant impact model, in the absence of a plagioclase flotation crust, the impact may be constrained to occur within about 300,000 years of the planet's initial fractionating magma ocean, at which time the giant impact can remove most of the silicate iron oxide budget of the planet before gravitational overturn carries it into the deep planetary interior. Thus a specific bulk composition is required to make Mercury endogenously, but specific timing of events is required to make it exogenously through giant impact. Measurements taken by the MESSENGER mission, when compared to predictions given here, may help resolve Mercury's formation process."
}, {
    "id": "oai:dspace.mit.edu:1721.1/117447",
    "title": "Transit timing variations of the exoplanet K2-25b",
    "abstract": "Transit light curves of the exoplanet K2-25b were studied to examine the possibility of transit timing variations (TTVs) in the system, which could imply the presence of a perturbing planet. Observations of K2-25b transits were taken using 14-inch and 24-inch telescopes at Wallace Astrophysical Observatory. Two transit light curves were fit using an MCMC implementation to find the orbital period, planetary radius, and semi-major axis. A new period calculation yielded an orbital period of 3.48457 +/-0.00004, consistent with the period of 3.484552 +0.000044/-0.000036 from Mann et al. 2016. No significant variations were found in the midtimes of the new transit observations when comparing them to the midtime originally published in Mann et al. 2016. Future observations will require smaller uncertainties to meaningfully constrain the mass and period of potential perturbing planets. Signal-to-noise ratio calculations showed that telescopes over approximately 2.2 meters in diameter have better potential to detect small TTVs.",
    "advisors": ["Amanda Bosh"],
    "text": "Transit timing variations of the exoplanet K2-25b Transit light curves of the exoplanet K2-25b were studied to examine the possibility of transit timing variations (TTVs) in the system, which could imply the presence of a perturbing planet. Observations of K2-25b transits were taken using 14-inch and 24-inch telescopes at Wallace Astrophysical Observatory. Two transit light curves were fit using an MCMC implementation to find the orbital period, planetary radius, and semi-major axis. A new period calculation yielded an orbital period of 3.48457 +/-0.00004, consistent with the period of 3.484552 +0.000044/-0.000036 from Mann et al. 2016. No significant variations were found in the midtimes of the new transit observations when comparing them to the midtime originally published in Mann et al. 2016. Future observations will require smaller uncertainties to meaningfully constrain the mass and period of potential perturbing planets. Signal-to-noise ratio calculations showed that telescopes over approximately 2.2 meters in diameter have better potential to detect small TTVs."
}, {
    "id": "oai:dspace.mit.edu:1721.1/114359",
    "title": "Modeling sea surface height in the Gulf of Mexico",
    "abstract": "A model was created to form synthetic plots of sea surface height (SSH) from monthly SSH statistics in the Gulf of Mexico generated from satellite laser altimetry data. SSH is a signal of the upper ocean mixed layer heat content and is an input for hurricane intensity models. A significant ocean feature in the Gulf of Mexico is the Loop Current (LC) which sheds warm eddies into the Gulf of Mexico at irregular intervals, which adds to the variability in monthly SSH readings beyond seasonal change. Satellite laser altimetry data was used from October 14th 1992 to May 23rd 2007. The SSH data included an area of the Gulf of Mexico (16N-30N latitude, 80W-100W longitude) with a resolution of 1/3 by 1/3 on a Mercator grid. Monthly SSH averages, variances, and covariances were created from a total of 763 samples, which allowed for approximately 65 samples per month. Once monthly SSH averages, variances, and covariances were made, synthetic plots were made by using a Karhunen-Love transform, the Singular Variable Decomposition of the SSH monthly covariance, and random vector composed of random numbers in a Gaussian distribution. Differences in synthetic SSH plots compared to individual SSH observations could vary greatly; the average of all synthetic SSH plot nodes differed by no more than plus or minus 10 cm. The difference between observed and synthetic SSH variance was no more than 400 cm. The large differences occurred in the in the eddy shedding region of the LC. To assess the effectiveness of the model, the synthetic SSH model will need to be used in a hurricane intensity model.",
    "advisors": ["Kerry Emanuel"],
    "text": "Modeling sea surface height in the Gulf of Mexico A model was created to form synthetic plots of sea surface height (SSH) from monthly SSH statistics in the Gulf of Mexico generated from satellite laser altimetry data. SSH is a signal of the upper ocean mixed layer heat content and is an input for hurricane intensity models. A significant ocean feature in the Gulf of Mexico is the Loop Current (LC) which sheds warm eddies into the Gulf of Mexico at irregular intervals, which adds to the variability in monthly SSH readings beyond seasonal change. Satellite laser altimetry data was used from October 14th 1992 to May 23rd 2007. The SSH data included an area of the Gulf of Mexico (16N-30N latitude, 80W-100W longitude) with a resolution of 1/3 by 1/3 on a Mercator grid. Monthly SSH averages, variances, and covariances were created from a total of 763 samples, which allowed for approximately 65 samples per month. Once monthly SSH averages, variances, and covariances were made, synthetic plots were made by using a Karhunen-Love transform, the Singular Variable Decomposition of the SSH monthly covariance, and random vector composed of random numbers in a Gaussian distribution. Differences in synthetic SSH plots compared to individual SSH observations could vary greatly; the average of all synthetic SSH plot nodes differed by no more than plus or minus 10 cm. The difference between observed and synthetic SSH variance was no more than 400 cm. The large differences occurred in the in the eddy shedding region of the LC. To assess the effectiveness of the model, the synthetic SSH model will need to be used in a hurricane intensity model."
}, {
    "id": "oai:dspace.mit.edu:1721.1/114361",
    "title": "Modeling the sorting of sediments on delta tops",
    "abstract": "Sediment sorting in fluvial rivers produces great variation in the grain-size of deposit over the length of a river. Knowledge of the pattern of sediment deposition may help to shed light on the processes that sort by grain size. A simple geometric model was developed to predict the location of the transition between coarse and fine sediment in a bimodal sediment distribution and tested against flume-created deposits with a variety of flow and depositional conditions. The models agreed well with the overall form of plane-bed clinoform deposits but not with rippled-bed clinoform deposits, thus a new model for predicting transitions in rippled-bed deposits is needed.",
    "advisors": ["John B. Southard"],
    "text": "Modeling the sorting of sediments on delta tops Sediment sorting in fluvial rivers produces great variation in the grain-size of deposit over the length of a river. Knowledge of the pattern of sediment deposition may help to shed light on the processes that sort by grain size. A simple geometric model was developed to predict the location of the transition between coarse and fine sediment in a bimodal sediment distribution and tested against flume-created deposits with a variety of flow and depositional conditions. The models agreed well with the overall form of plane-bed clinoform deposits but not with rippled-bed clinoform deposits, thus a new model for predicting transitions in rippled-bed deposits is needed."
}, {
    "id": "oai:dspace.mit.edu:1721.1/114358",
    "title": "Investigation of regional variation in Lunar crater morphometry from (Lunar Orbiter Laser Altimeter) LOLA observations",
    "abstract": "The advent of global Digital Elevation Models of the lunar surface, obtained from the Lunar Orbiter Laser Altimeter (LOLA), has allowed for a quantitative assessment of crater morphometry. 351 simple and complex craters in the Mare Serenitatis, far side highlands, near side highlands, and South Pole-Aitken basin are decomposed into 50 elevation profiles, from which key geometric crater properties are extracted. The geometric properties and their respective standard variation, such as height-to-diameter ratios, and average elevation profile are compared on a global level to investigate regional differences in terrain rheology and study the transition between the simple and complex crater regime. Furthermore, the relationship between known degradation mechanisms and crater morphometry is discussed, as well as the current state of quantitative methods to assess crater degradation. The resulting regional differences observed in crater morphometry are explained in the context of lunar geologic history. Finally, the addition of other crater geometric properties in future quantitative assessments will broaden the study of crater morphometry, and improvements to current methods are necessary to conclusively define degradation states in terms of quantitative factors.",
    "advisors": ["Maria T. Zuber"],
    "text": "Investigation of regional variation in Lunar crater morphometry from (Lunar Orbiter Laser Altimeter) LOLA observations The advent of global Digital Elevation Models of the lunar surface, obtained from the Lunar Orbiter Laser Altimeter (LOLA), has allowed for a quantitative assessment of crater morphometry. 351 simple and complex craters in the Mare Serenitatis, far side highlands, near side highlands, and South Pole-Aitken basin are decomposed into 50 elevation profiles, from which key geometric crater properties are extracted. The geometric properties and their respective standard variation, such as height-to-diameter ratios, and average elevation profile are compared on a global level to investigate regional differences in terrain rheology and study the transition between the simple and complex crater regime. Furthermore, the relationship between known degradation mechanisms and crater morphometry is discussed, as well as the current state of quantitative methods to assess crater degradation. The resulting regional differences observed in crater morphometry are explained in the context of lunar geologic history. Finally, the addition of other crater geometric properties in future quantitative assessments will broaden the study of crater morphometry, and improvements to current methods are necessary to conclusively define degradation states in terms of quantitative factors."
}, {
    "id": "oai:dspace.mit.edu:1721.1/114320",
    "title": "Spin directions of asteroids : lightcurve analysis of Koronis family members 158 Koronis and 720 Bohlinia",
    "abstract": "Clusters of asteroids within the main-belt are referred to as dynamical families because they are believed to have originated as the result of collisional destructions of large parent bodies. Family members are the remnants of the parent body break-up and often retain some of the parent body's original rotational information. Previous studies have indicated that the Koronis dynamical family may have been relatively recently formed due to the non-random nature of the orientation of its members spin vectors. This project was undertaken to contribute to the rotational data on Koronis family members in order to better understand the unusual properties observed. The goal of this project was to determine the directions of spin of Koronis family members 158 Koronis and 720 Bohlinia. Observations were made at the MIT Wallace Astrophysical Observatory in Westford, Massachusetts during 1999. Both of the targets were determined to have a retrograde sense of rotation, which is in agreement with the previously known sense of rotation of Koronis family member 243 Ida. In addition, the synodic period of 158 Koronis was confirmed by these observations, and a new value for the sidereal period of 720 Bohlinia was found to be 8.920  0.005 hours. During the course of the observations, an uncataloged asteroid was discovered and has since been assigned the designation 1999 QQ2 by the IAU Minor Planet Center.",
    "advisors": ["Richard P. Binzel"],
    "text": "Spin directions of asteroids : lightcurve analysis of Koronis family members 158 Koronis and 720 Bohlinia Clusters of asteroids within the main-belt are referred to as dynamical families because they are believed to have originated as the result of collisional destructions of large parent bodies. Family members are the remnants of the parent body break-up and often retain some of the parent body's original rotational information. Previous studies have indicated that the Koronis dynamical family may have been relatively recently formed due to the non-random nature of the orientation of its members spin vectors. This project was undertaken to contribute to the rotational data on Koronis family members in order to better understand the unusual properties observed. The goal of this project was to determine the directions of spin of Koronis family members 158 Koronis and 720 Bohlinia. Observations were made at the MIT Wallace Astrophysical Observatory in Westford, Massachusetts during 1999. Both of the targets were determined to have a retrograde sense of rotation, which is in agreement with the previously known sense of rotation of Koronis family member 243 Ida. In addition, the synodic period of 158 Koronis was confirmed by these observations, and a new value for the sidereal period of 720 Bohlinia was found to be 8.920  0.005 hours. During the course of the observations, an uncataloged asteroid was discovered and has since been assigned the designation 1999 QQ2 by the IAU Minor Planet Center."
}, {
    "id": "oai:dspace.mit.edu:1721.1/114103",
    "title": "Records of Great Basin precipitation during MIS 11 from two Lehman Cave stalagmites",
    "abstract": "Trace elements (Mg/Ca and Sr/Ca) and stable isotopes ([delta]13C and [delta]18O) were measured in two coeval stalagmites, LC3 and BT1, from the Lehman Caves, Nevada. BT1 spans 388 to 384 ka. The LC3 record is split into two parts due to a hiatus observed through fluorescent imaging. The section prior to the hiatus spans 411 ka to 402 ka. The post-hiatus section of the stalagmite has a single age of 383 ka; an age model cannot be constructed for this part of the record. These stalagmites span Marine Isotope Stage 11 (MIS 11), a long interglacial that occurred around 424-374 ka. Comparison with more recent stalagmite records has shown prior calcite precipitation to be the dominant control on Mg/Ca and [delta]13C in the cave. The trace element and stable isotope records obtained point to an arid climate in the Great Basin during MIS 11.",
    "advisors": ["David McGee"],
    "text": "Records of Great Basin precipitation during MIS 11 from two Lehman Cave stalagmites Trace elements (Mg/Ca and Sr/Ca) and stable isotopes ([delta]13C and [delta]18O) were measured in two coeval stalagmites, LC3 and BT1, from the Lehman Caves, Nevada. BT1 spans 388 to 384 ka. The LC3 record is split into two parts due to a hiatus observed through fluorescent imaging. The section prior to the hiatus spans 411 ka to 402 ka. The post-hiatus section of the stalagmite has a single age of 383 ka; an age model cannot be constructed for this part of the record. These stalagmites span Marine Isotope Stage 11 (MIS 11), a long interglacial that occurred around 424-374 ka. Comparison with more recent stalagmite records has shown prior calcite precipitation to be the dominant control on Mg/Ca and [delta]13C in the cave. The trace element and stable isotope records obtained point to an arid climate in the Great Basin during MIS 11."
}, {
    "id": "oai:dspace.mit.edu:1721.1/65599",
    "title": "Changes in atmospheric eddy length with the seasonal cycle and global warming",
    "abstract": "A recent article by Kidston et al. [8] demonstrates that the length of atmospheric eddies increases in simulations of future global warming. This thesis expands on Kidston et al.'s work with additional studies of eddy length in the NCEP2 reanalysis (a model-data synthesis that reconstructs past atmospheric circulation) and general circulation models (GCMs) from the Coupled Model Intercomparison Project phase 3. Eddy lengths are compared to computed values of the Rossby radius and the Rhines scale, which have been hypothesized to set the eddy length. The GCMs reproduce the seasonal variation in the eddy lengths seen in the reanalysis. To explore the effect of latent heating on the eddies, a modification to the static stability is used to calculate an effective Rossby radius. The effective Rossby radius is an improvement over the traditional dry Rossby radius in predicting the seasonal cycle of northern hemisphere eddy length, if the height scale used for calculation of the Rossby radius is the depth of the free troposphere. There is no improvement if the scale height is used instead of the free troposphere depth. However, both Rossby radii and the Rhines scale fail to explain the weaker seasonal cycle in southern hemisphere eddy length. In agreement with Kidson et al., the GCMs robustly project an increase in eddy length as the climate warms. The Rossby radii and Rhines scale are also generally projected to increase. Although it is not possible to state with confidence what process ultimately controls atmospheric eddy lengths, taken as a whole the results of this study increase confidence in the projection of future increases in eddy length.",
    "advisors": ["Paul A. O'Gorman"],
    "text": "Changes in atmospheric eddy length with the seasonal cycle and global warming A recent article by Kidston et al. [8] demonstrates that the length of atmospheric eddies increases in simulations of future global warming. This thesis expands on Kidston et al.'s work with additional studies of eddy length in the NCEP2 reanalysis (a model-data synthesis that reconstructs past atmospheric circulation) and general circulation models (GCMs) from the Coupled Model Intercomparison Project phase 3. Eddy lengths are compared to computed values of the Rossby radius and the Rhines scale, which have been hypothesized to set the eddy length. The GCMs reproduce the seasonal variation in the eddy lengths seen in the reanalysis. To explore the effect of latent heating on the eddies, a modification to the static stability is used to calculate an effective Rossby radius. The effective Rossby radius is an improvement over the traditional dry Rossby radius in predicting the seasonal cycle of northern hemisphere eddy length, if the height scale used for calculation of the Rossby radius is the depth of the free troposphere. There is no improvement if the scale height is used instead of the free troposphere depth. However, both Rossby radii and the Rhines scale fail to explain the weaker seasonal cycle in southern hemisphere eddy length. In agreement with Kidson et al., the GCMs robustly project an increase in eddy length as the climate warms. The Rossby radii and Rhines scale are also generally projected to increase. Although it is not possible to state with confidence what process ultimately controls atmospheric eddy lengths, taken as a whole the results of this study increase confidence in the projection of future increases in eddy length."
}, {
    "id": "oai:dspace.mit.edu:1721.1/117910",
    "title": "Investigation of the heterogeneous ice nucleation potential of sea spray aerosol",
    "abstract": "Bubble bursting at the ocean surface generates smaller film-burst particles and larger jet drop particles that differ in composition. The chemical composition of sea spray aerosols is an important parameter for the evaluation of their impact on the global climate system. This study investigates the role of particle chemistry on the heterogeneous ice nucleation potential of laboratory-generated sea spray aerosols. Cultures of Procholorococcus, a highly abundant marine phytoplankton species, were used as a model source of organic sea spray aerosols. Results show that smaller particles generated from the lysed Procholorococcus cultures were organically enriched and effectively activated as ice nucleating particles at warmer temperatures and lower supersaturations than larger particles. The role of chemical composition in the activation of the particles was studied by measuring the nucleation abilities of single component organic molecules that mimic proteins, lipids, and carbohydrates in Procholorococcus. Amylopectin, agarose, and aspartic acid exhibited nucleation behaviors similar to particles generated from Procholorococcus cultures. Therefore, carbohydrates and proteins with numerous and well-ordered hydrophilic functional groups may determine the ice nucleation potential of organic sea spray aerosols.",
    "advisors": ["Daniel J. Cziczo"],
    "text": "Investigation of the heterogeneous ice nucleation potential of sea spray aerosol Bubble bursting at the ocean surface generates smaller film-burst particles and larger jet drop particles that differ in composition. The chemical composition of sea spray aerosols is an important parameter for the evaluation of their impact on the global climate system. This study investigates the role of particle chemistry on the heterogeneous ice nucleation potential of laboratory-generated sea spray aerosols. Cultures of Procholorococcus, a highly abundant marine phytoplankton species, were used as a model source of organic sea spray aerosols. Results show that smaller particles generated from the lysed Procholorococcus cultures were organically enriched and effectively activated as ice nucleating particles at warmer temperatures and lower supersaturations than larger particles. The role of chemical composition in the activation of the particles was studied by measuring the nucleation abilities of single component organic molecules that mimic proteins, lipids, and carbohydrates in Procholorococcus. Amylopectin, agarose, and aspartic acid exhibited nucleation behaviors similar to particles generated from Procholorococcus cultures. Therefore, carbohydrates and proteins with numerous and well-ordered hydrophilic functional groups may determine the ice nucleation potential of organic sea spray aerosols."
}, {
    "id": "oai:dspace.mit.edu:1721.1/114116",
    "title": "A digital path to environmental education",
    "abstract": "Digital technology has become completely engrained in the process of scientific research. Grade schools are beginning to incorporate computers and other digital devices into the learning process as tools for learning other subjects, including science. Educators have experimented with a variety of programs that incorporate computers, such as digital tutors, simulations of natural systems, and interactive simulations. This study aimed to demonstrate that handheld computers are a useful resource in problem-based learning environments. A \"mini-curriculum\", focused on making handheld computer-aided measurements of the key environmental parameters in marine estuaries, was designed and taught to eleven students from the Palmetto Ridge High School in Naples, Florida over a two week period. Students' reactions to both the curriculum and the computers were overwhelmingly positive. This proof-of-concept trial indicates that handheld computers have the potential to be a very useful tool in problem-based learning.",
    "advisors": ["Kip Hodges"],
    "text": "A digital path to environmental education Digital technology has become completely engrained in the process of scientific research. Grade schools are beginning to incorporate computers and other digital devices into the learning process as tools for learning other subjects, including science. Educators have experimented with a variety of programs that incorporate computers, such as digital tutors, simulations of natural systems, and interactive simulations. This study aimed to demonstrate that handheld computers are a useful resource in problem-based learning environments. A \"mini-curriculum\", focused on making handheld computer-aided measurements of the key environmental parameters in marine estuaries, was designed and taught to eleven students from the Palmetto Ridge High School in Naples, Florida over a two week period. Students' reactions to both the curriculum and the computers were overwhelmingly positive. This proof-of-concept trial indicates that handheld computers have the potential to be a very useful tool in problem-based learning."
}, {
    "id": "oai:dspace.mit.edu:1721.1/114131",
    "title": "Characterizing fault structure and general morphology of the Tensleep Sandstone of Teapot Dome, Wyoming as it relates to industrial carbon sequestration",
    "abstract": "Consistent data demonstrates a rise in global atmospheric concentrations of carbon, in the form of carbon dioxide. A large portion of the current atmospheric concentration is due to emissions from the burning of fossil fuels, which humans use for energy consumption. Many experts believe that of all the mechanisms in which carbon dioxide emissions can be mitigated, sequestering carbon dioxide, specifically in geologic reservoirs, is among the most promising of all approaches. This paper examines a fault structure in the specific geologic reservoir known as NPR-3, or the Teapot Dome oilfield. Using seismic modeling and subsurface modeling software packages to interpret seismic data of the region, geologic features and faults are mapped. These maps provide valuable characterization information useful to an overall evaluation of the effectiveness of geological storage of carbon sequestration in the Teapot Dome site.",
    "advisors": ["David Mohrig"],
    "text": "Characterizing fault structure and general morphology of the Tensleep Sandstone of Teapot Dome, Wyoming as it relates to industrial carbon sequestration Consistent data demonstrates a rise in global atmospheric concentrations of carbon, in the form of carbon dioxide. A large portion of the current atmospheric concentration is due to emissions from the burning of fossil fuels, which humans use for energy consumption. Many experts believe that of all the mechanisms in which carbon dioxide emissions can be mitigated, sequestering carbon dioxide, specifically in geologic reservoirs, is among the most promising of all approaches. This paper examines a fault structure in the specific geologic reservoir known as NPR-3, or the Teapot Dome oilfield. Using seismic modeling and subsurface modeling software packages to interpret seismic data of the region, geologic features and faults are mapped. These maps provide valuable characterization information useful to an overall evaluation of the effectiveness of geological storage of carbon sequestration in the Teapot Dome site."
}, {
    "id": "oai:dspace.mit.edu:1721.1/114354",
    "title": "Magmatic processes that generated the rim andesites of Medicine Lake Volcano, N. California",
    "abstract": "This paper characterizes the compositionally distinctive high-Na andesite lavas at Medicine Lake Volcano that erupted at ~100 ka and that built most of Medicine Lake's caldera. These high-Na lavas define a compositional trend that formed by fractional crystallization in a shallow magma chamber (~ 4 to 8 km). Petrologic evidence indicates pre-eruptive H20 contents of 2 to 4 wt.% H20 over a temperature range of 1070 to 900 C. Oxygen fugacity recorded in coexisting spinel and rhombohedral oxides varies from NNO (Nickel - Nickel Oxide) to NNO 0.7 log units. Experiments performed at 1 kbar - H20 saturated conditions at the NNO buffer on a primitive andesite reproduce most of the major element compositional variability exhibited in the high-Na lavas.",
    "advisors": ["Timothy Grove"],
    "text": "Magmatic processes that generated the rim andesites of Medicine Lake Volcano, N. California This paper characterizes the compositionally distinctive high-Na andesite lavas at Medicine Lake Volcano that erupted at ~100 ka and that built most of Medicine Lake's caldera. These high-Na lavas define a compositional trend that formed by fractional crystallization in a shallow magma chamber (~ 4 to 8 km). Petrologic evidence indicates pre-eruptive H20 contents of 2 to 4 wt.% H20 over a temperature range of 1070 to 900 C. Oxygen fugacity recorded in coexisting spinel and rhombohedral oxides varies from NNO (Nickel - Nickel Oxide) to NNO 0.7 log units. Experiments performed at 1 kbar - H20 saturated conditions at the NNO buffer on a primitive andesite reproduce most of the major element compositional variability exhibited in the high-Na lavas."
}, {
    "id": "oai:dspace.mit.edu:1721.1/114346",
    "title": "Characterization of a 3D printed pumped counterflow virtual impactor and an aerodynamic lens concentrator",
    "abstract": "Atmospheric aerosols have an important role in cloud formation and, by extension, in the overall climate system. Field studies are required to refine the uncertainty associated with the net radiative effect of atmospheric aerosols. Two pre-existing cloud sampling devices, the pumped counterflow virtual impactor (PCVI) and aerodynamic lens concentrator (ADL), were modelled using computer aided design software and printed using stereolithography printing. These devices were compared against their industrial counterparts. The printed PCVI was proven to be as effective as the industrial PCVI in a smaller working range. The printed concentrator effectively concentrated particles, but at a lower concentration factor than the industrial concentrator. This study revealed potential for further refinement in design features for both devices and it served as an essential pre-study for future field campaigns that will use these 3D printed devices.",
    "advisors": ["Daniel Cziczo"],
    "text": "Characterization of a 3D printed pumped counterflow virtual impactor and an aerodynamic lens concentrator Atmospheric aerosols have an important role in cloud formation and, by extension, in the overall climate system. Field studies are required to refine the uncertainty associated with the net radiative effect of atmospheric aerosols. Two pre-existing cloud sampling devices, the pumped counterflow virtual impactor (PCVI) and aerodynamic lens concentrator (ADL), were modelled using computer aided design software and printed using stereolithography printing. These devices were compared against their industrial counterparts. The printed PCVI was proven to be as effective as the industrial PCVI in a smaller working range. The printed concentrator effectively concentrated particles, but at a lower concentration factor than the industrial concentrator. This study revealed potential for further refinement in design features for both devices and it served as an essential pre-study for future field campaigns that will use these 3D printed devices."
}, {
    "id": "oai:dspace.mit.edu:1721.1/114127",
    "title": "Understanding the spatial distribution of the Southern Hemisphere near-surface westerlies and Its trends",
    "abstract": "In some experiments in which idealized general circulation models (GCMS) are used to study the tropospheric response to stratospheric perturbations, the tropospheric response is much stronger and longer-lived compared to observations (e.g., Polvani and Kushner 2002, Kushner and Polvani 2004). Chan and Plumb (2009) found that those experiments which exhibited particularly long tropospheric decorrelation times (and, by the fluctuation dissipation theorem, much stronger annular mode responses) were marked by bimodality in the distribution of the latitude of surface zonal-mean zonal winds. Here, ERA-Interim and NCEP/NCAR reanalysis data are used to establish if this bimodality exists in the Southern Hemisphere (SH) near-surface winds, which would predict the existence of an additional mode of tropospheric variability exhibiting stronger and longer-lived responses than what has previously been observed. Histograms of the latitudinal position of maximum near-surface zonal-mean zonal winds turn up no convincing evidence of jet bimodality, although they do reveal an interesting - but probably spurious trimodality in the NCEP/NCAR June-August 850-hPa distribution of jet latitude. A climatology of wintertime zonal winds reveals that there is a time-mean split jet over the South Pacific Ocean; furthermore, empirical orthogonal function analysis reveals that, over the South Pacific, the dominant mode of wintertime zonal wind variability is a splitting and un splitting of the jet. Ultimately, both the climatological split jet and its variability are determined not to be evidence of jet bimodality. The temporal trends in the distribution of near-surface jet latitude are also examined. Stratospheric ozone depletion has been implicated in surface circulation changes in the SH high latitudes; one of these changes has been a poleward shift of the jet in austral summer. In this thesis, it is found that a poleward shift of the December-February distribution of jet latitude has taken place from the pre-ozone-hole to ozone-hole eras, consistent with previous findings. The novel result is that there has also been a poleward shift of this distribution in May, which is consistent with a secondary maximum in ozone depletion near the tropopause in April-May as observed by other authors (Thompson et al. 2011), and would imply the occurrence of troposphere-stratosphere coupling in late fall. An in-depth investigation of these May zonal wind trends will be pursued in future work.",
    "advisors": ["R. Alan Plumb"],
    "text": "Understanding the spatial distribution of the Southern Hemisphere near-surface westerlies and Its trends In some experiments in which idealized general circulation models (GCMS) are used to study the tropospheric response to stratospheric perturbations, the tropospheric response is much stronger and longer-lived compared to observations (e.g., Polvani and Kushner 2002, Kushner and Polvani 2004). Chan and Plumb (2009) found that those experiments which exhibited particularly long tropospheric decorrelation times (and, by the fluctuation dissipation theorem, much stronger annular mode responses) were marked by bimodality in the distribution of the latitude of surface zonal-mean zonal winds. Here, ERA-Interim and NCEP/NCAR reanalysis data are used to establish if this bimodality exists in the Southern Hemisphere (SH) near-surface winds, which would predict the existence of an additional mode of tropospheric variability exhibiting stronger and longer-lived responses than what has previously been observed. Histograms of the latitudinal position of maximum near-surface zonal-mean zonal winds turn up no convincing evidence of jet bimodality, although they do reveal an interesting - but probably spurious trimodality in the NCEP/NCAR June-August 850-hPa distribution of jet latitude. A climatology of wintertime zonal winds reveals that there is a time-mean split jet over the South Pacific Ocean; furthermore, empirical orthogonal function analysis reveals that, over the South Pacific, the dominant mode of wintertime zonal wind variability is a splitting and un splitting of the jet. Ultimately, both the climatological split jet and its variability are determined not to be evidence of jet bimodality. The temporal trends in the distribution of near-surface jet latitude are also examined. Stratospheric ozone depletion has been implicated in surface circulation changes in the SH high latitudes; one of these changes has been a poleward shift of the jet in austral summer. In this thesis, it is found that a poleward shift of the December-February distribution of jet latitude has taken place from the pre-ozone-hole to ozone-hole eras, consistent with previous findings. The novel result is that there has also been a poleward shift of this distribution in May, which is consistent with a secondary maximum in ozone depletion near the tropopause in April-May as observed by other authors (Thompson et al. 2011), and would imply the occurrence of troposphere-stratosphere coupling in late fall. An in-depth investigation of these May zonal wind trends will be pursued in future work."
}, {
    "id": "oai:dspace.mit.edu:1721.1/114353",
    "title": "The Green's function for the diffusion coefficient",
    "abstract": "The scattering diffusion coefficient between two points can theoretically be extracted from a random distribution of sources. An improved ability to measure the diffusion coefficient of the Earth's crust would simplify the process of characterizing the fracture network for applications in geothermal energy. This has the potential to make geothermal wells more economical to make, more efficient to operate, and longer lived. Previous work has shown the diffusion coefficient can be extracted from synthetic datasets in both one dimension and three dimensions using seismic interferometry. This paper attempts to recover the diffusion coefficient for a realistic source distribution taken from a microseismic dataset from a geothermal field in Indonesia. This dataset did not have an ideal distribution of sources, so the estimated diffusion coefficient did not match the expected value. A better estimate of the expected diffusion coefficient and an improved dataset with sources more evenly distributed in all directions around the receivers would likely give a better result.",
    "advisors": ["Alison Malcolm"],
    "text": "The Green's function for the diffusion coefficient The scattering diffusion coefficient between two points can theoretically be extracted from a random distribution of sources. An improved ability to measure the diffusion coefficient of the Earth's crust would simplify the process of characterizing the fracture network for applications in geothermal energy. This has the potential to make geothermal wells more economical to make, more efficient to operate, and longer lived. Previous work has shown the diffusion coefficient can be extracted from synthetic datasets in both one dimension and three dimensions using seismic interferometry. This paper attempts to recover the diffusion coefficient for a realistic source distribution taken from a microseismic dataset from a geothermal field in Indonesia. This dataset did not have an ideal distribution of sources, so the estimated diffusion coefficient did not match the expected value. A better estimate of the expected diffusion coefficient and an improved dataset with sources more evenly distributed in all directions around the receivers would likely give a better result."
}, {
    "id": "oai:dspace.mit.edu:1721.1/114368",
    "title": "An overview of the volcano-tectonic hazards of Portland, Oregon, and an assessment of emergency preparedness",
    "abstract": "Portland, Oregon, lies within an active tectonic margin, which puts the city at risk to hazards from earthquakes and volcanic eruptions. The young Juan de Fuca microplate is subducting under North America, introducing not only arc magmatism into the overlying plate, but also interplate and intraplate seismicity related to the subduction zone. Large crustal earthquakes are also probable in Portland because of the oblique strike-slip Portland Hills Fault zone. These hazards create risk to Portland residents and infrastructure because of pre-existing vulnerabilities. Much of Portland's downtown area, including the government and business districts, is at risk of ground shaking infrastructure damage, liquefaction and landslides due to earthquakes. Additionally, the city is within 110 km of three active Cascadia stratovolcanoes, two of which pose hazards from tephra and lahars. Though the city is under the umbrella of four emergency response plans-city, county, state and federal-there are critical gaps in mitigation strategies, emergency exercises and community education and outreach. Portland cannot prevent earthquakes or volcanic eruptions, but the city can reduce its vulnerability to these hazards.",
    "advisors": ["Stphane Rondenay"],
    "text": "An overview of the volcano-tectonic hazards of Portland, Oregon, and an assessment of emergency preparedness Portland, Oregon, lies within an active tectonic margin, which puts the city at risk to hazards from earthquakes and volcanic eruptions. The young Juan de Fuca microplate is subducting under North America, introducing not only arc magmatism into the overlying plate, but also interplate and intraplate seismicity related to the subduction zone. Large crustal earthquakes are also probable in Portland because of the oblique strike-slip Portland Hills Fault zone. These hazards create risk to Portland residents and infrastructure because of pre-existing vulnerabilities. Much of Portland's downtown area, including the government and business districts, is at risk of ground shaking infrastructure damage, liquefaction and landslides due to earthquakes. Additionally, the city is within 110 km of three active Cascadia stratovolcanoes, two of which pose hazards from tephra and lahars. Though the city is under the umbrella of four emergency response plans-city, county, state and federal-there are critical gaps in mitigation strategies, emergency exercises and community education and outreach. Portland cannot prevent earthquakes or volcanic eruptions, but the city can reduce its vulnerability to these hazards."
}, {
    "id": "oai:dspace.mit.edu:1721.1/114122",
    "title": "A near-ultraviolet spectroscopic survey of B-type asteroids",
    "abstract": "This study aimed to evaluate the presence of spectral slope variations of B-type asteroids in the near-ultraviolet wavelength range and further compare variations to those found in the near-infrared (de Leon et al., 2012) and infrared (All-Lagoa et al., 2013). New observations of 19 B-type asteroids were obtained using the Telescopio Nazionale Galileo (TNG) and additional observations were collected on the William Herschel Telescope (WHT) and Isaac Newton Telescope (INT). After identifying appropriate solar analogs for spectral reduction, it was found that 1) not all asteroids are B-types as classified by the M4AST online tool (Popescu et al., 2012), and 2) spectral slope variations were present amongst the B-type asteroids. These spectral slope variations could not be traced to the use of certain solar analogs or differences in airmass during observations. Furthermore, these variations were in good agreement spectral slope variations of carbonaceous chondrites, particularly in the near-UV region. These results support the work of de Leon et al. (2012) and Alf-Lagoa et al. (2013) in identifying spectral slope variations and contributing to a three-part survey of B-type asteroids across different wavelengths.",
    "advisors": ["Richard P. Binzel"],
    "text": "A near-ultraviolet spectroscopic survey of B-type asteroids This study aimed to evaluate the presence of spectral slope variations of B-type asteroids in the near-ultraviolet wavelength range and further compare variations to those found in the near-infrared (de Leon et al., 2012) and infrared (All-Lagoa et al., 2013). New observations of 19 B-type asteroids were obtained using the Telescopio Nazionale Galileo (TNG) and additional observations were collected on the William Herschel Telescope (WHT) and Isaac Newton Telescope (INT). After identifying appropriate solar analogs for spectral reduction, it was found that 1) not all asteroids are B-types as classified by the M4AST online tool (Popescu et al., 2012), and 2) spectral slope variations were present amongst the B-type asteroids. These spectral slope variations could not be traced to the use of certain solar analogs or differences in airmass during observations. Furthermore, these variations were in good agreement spectral slope variations of carbonaceous chondrites, particularly in the near-UV region. These results support the work of de Leon et al. (2012) and Alf-Lagoa et al. (2013) in identifying spectral slope variations and contributing to a three-part survey of B-type asteroids across different wavelengths."
}, {
    "id": "oai:dspace.mit.edu:1721.1/114344",
    "title": "Imaging of the Hellenic subduction zone by seismic tomography",
    "abstract": "The Hellenic subduction zone is a complicated tectonic boundary, along which transitions in the nature of subducted material are believed to occur. The objective of this study was to better constrain the subsurface geometry of the Hellenic subduction zone by increasing the resolution of an existing tomographic model of the region. Increase in resolution is important for understanding the effects of inferred transitions in subducted material at depth. Nonlinear inversion of P-wave travel times was used on a global dataset expanded by temporary array data collected in southern Greece. Results show a vertically continuous slab, with a break at the depth 200-400 km detected only in the NW portion of the system. At depth above 400 kin, there is a lateral discontinuity marked by the Central Hellenic Shear Zone and Kephalonia Transform Fault, with slab more pronounced in southern part of the system. Our study supports the hypothesis of the change in subduction mode between northern and southern part of Hellenic arc in late Pliocene.",
    "advisors": ["Stphane Rondenay"],
    "text": "Imaging of the Hellenic subduction zone by seismic tomography The Hellenic subduction zone is a complicated tectonic boundary, along which transitions in the nature of subducted material are believed to occur. The objective of this study was to better constrain the subsurface geometry of the Hellenic subduction zone by increasing the resolution of an existing tomographic model of the region. Increase in resolution is important for understanding the effects of inferred transitions in subducted material at depth. Nonlinear inversion of P-wave travel times was used on a global dataset expanded by temporary array data collected in southern Greece. Results show a vertically continuous slab, with a break at the depth 200-400 km detected only in the NW portion of the system. At depth above 400 kin, there is a lateral discontinuity marked by the Central Hellenic Shear Zone and Kephalonia Transform Fault, with slab more pronounced in southern part of the system. Our study supports the hypothesis of the change in subduction mode between northern and southern part of Hellenic arc in late Pliocene."
}, {
    "id": "oai:dspace.mit.edu:1721.1/114128",
    "title": "A test for a see-saw oscillation between the Amazon and Congo basins using regional climate modeling",
    "abstract": "The Amazon and Congo basins represent two of the three largest regions of rainfall found on the globe. Eltahir et al. (2004) have proposed the existence of a \"see-saw\" oscillation between these two basins, where a reduction of rainfall in one region is marked by an increase in the other. This inverse relationship has been observed both directly by Eltahir et al., using satellite data of regional rainfall (Simpson et al. 1988), and indirectly through changes in river flow measurements (Amarasekera et al. 1977) during the last century. However, little work has been done to study this see-saw effect through the use of climate models. Using a regional climate model (RegCM), the appearance of the oscillation was tested by converting rainforest area in one basin into two different land types, simulating drought-like climate conditions to induce additional rainfall in the other basin. In total, one control run and four land-modified runs were simulated for this experiment. The effects of these conditions were modeled over a one-year period (1980). It was found that in some cases, reduction of rainfall in one basin resulted in increased rainfall in small areas of the other; however, over the entirety of both basins, evidence of the see-saw hypothesis was not simulated. Several factors may have contributed to this result, including the limitations associated with using a regional model, as well as the initial conditions set for the five climate simulations.",
    "advisors": ["Elfatih Eltahir"],
    "text": "A test for a see-saw oscillation between the Amazon and Congo basins using regional climate modeling The Amazon and Congo basins represent two of the three largest regions of rainfall found on the globe. Eltahir et al. (2004) have proposed the existence of a \"see-saw\" oscillation between these two basins, where a reduction of rainfall in one region is marked by an increase in the other. This inverse relationship has been observed both directly by Eltahir et al., using satellite data of regional rainfall (Simpson et al. 1988), and indirectly through changes in river flow measurements (Amarasekera et al. 1977) during the last century. However, little work has been done to study this see-saw effect through the use of climate models. Using a regional climate model (RegCM), the appearance of the oscillation was tested by converting rainforest area in one basin into two different land types, simulating drought-like climate conditions to induce additional rainfall in the other basin. In total, one control run and four land-modified runs were simulated for this experiment. The effects of these conditions were modeled over a one-year period (1980). It was found that in some cases, reduction of rainfall in one basin resulted in increased rainfall in small areas of the other; however, over the entirety of both basins, evidence of the see-saw hypothesis was not simulated. Several factors may have contributed to this result, including the limitations associated with using a regional model, as well as the initial conditions set for the five climate simulations."
}, {
    "id": "oai:dspace.mit.edu:1721.1/114347",
    "title": "Rotational lightcurve analysis of binary Asteroid (22) Kalliope/Linus",
    "abstract": "Binary asteroids have been insightful to scientists in recent years in their quest to better understand the Solar System in its early stage. Observing a mutual event between a primary and its moon can yield the sizes of the objects in units of the semi-major axis a. When the linear dimensions of the orbit can be known, Kepler's Third Law allows for a solution of the mass. As an example, because the absolute linear scale of (22) Kalliope/Linus is known [1], one can determine the component sizes and reduce error bars on the mass and density of this M-type asteroid. Since the bulk composition is known from spectral data, the porosity of the asteroid can be calculated. Knowing the porosity of the asteroid can give scientists a better understanding of its formation and dynamical evolution. Binary object (22) Kalliope/Linus is a classic example of a system for which this technique can yield valuable results. An observing campaign involving five observers resulted in twenty-eight nights of data. The data were used to create rotational lightcurves, which were scanned for signatures of mutual events.",
    "advisors": ["Richard Binzel"],
    "text": "Rotational lightcurve analysis of binary Asteroid (22) Kalliope/Linus Binary asteroids have been insightful to scientists in recent years in their quest to better understand the Solar System in its early stage. Observing a mutual event between a primary and its moon can yield the sizes of the objects in units of the semi-major axis a. When the linear dimensions of the orbit can be known, Kepler's Third Law allows for a solution of the mass. As an example, because the absolute linear scale of (22) Kalliope/Linus is known [1], one can determine the component sizes and reduce error bars on the mass and density of this M-type asteroid. Since the bulk composition is known from spectral data, the porosity of the asteroid can be calculated. Knowing the porosity of the asteroid can give scientists a better understanding of its formation and dynamical evolution. Binary object (22) Kalliope/Linus is a classic example of a system for which this technique can yield valuable results. An observing campaign involving five observers resulted in twenty-eight nights of data. The data were used to create rotational lightcurves, which were scanned for signatures of mutual events."
}, {
    "id": "oai:dspace.mit.edu:1721.1/114140",
    "title": "Hillslope evolution in response to lateral base level migration",
    "abstract": "Hillslopes evolve in response to base level change, sediment production, and sediment transport. Many previous studies have focused on hillslopes undergoing vertical base level migration due to tectonic forcing and bedrock incision. Many geomorphic features, however, are characterized by lateral hillslope retreat and have not been adequately studied. Here I adapt a theory of linear diffusive hillslope evolution to relate the velocity of lateral hillslope retreat to the steady-state hillslope form. A case study in a Florida sapping network, in which headward migration of seepage faces in a sandy soil sets the base level for the surrounding hillslopes, provides numerous opportunities to test the analytical model by direct measurement. Measurements of hillslopes in the Florida sapping network found quantitative agreement between the predicted and observed hillslope morphology. An expected relationship between geometric drainage area and channel growth velocity was not borne out in the data, but the distribution of measured v/K ratios is consistent with what I expect based on my preferential sampling of slow-moving gently-sloped heads. Several explanations are given to explain why the expected relationship with drainage area is not observed, and suggestions for future work based on these findings is offered.",
    "advisors": ["Taylor Perron"],
    "text": "Hillslope evolution in response to lateral base level migration Hillslopes evolve in response to base level change, sediment production, and sediment transport. Many previous studies have focused on hillslopes undergoing vertical base level migration due to tectonic forcing and bedrock incision. Many geomorphic features, however, are characterized by lateral hillslope retreat and have not been adequately studied. Here I adapt a theory of linear diffusive hillslope evolution to relate the velocity of lateral hillslope retreat to the steady-state hillslope form. A case study in a Florida sapping network, in which headward migration of seepage faces in a sandy soil sets the base level for the surrounding hillslopes, provides numerous opportunities to test the analytical model by direct measurement. Measurements of hillslopes in the Florida sapping network found quantitative agreement between the predicted and observed hillslope morphology. An expected relationship between geometric drainage area and channel growth velocity was not borne out in the data, but the distribution of measured v/K ratios is consistent with what I expect based on my preferential sampling of slow-moving gently-sloped heads. Several explanations are given to explain why the expected relationship with drainage area is not observed, and suggestions for future work based on these findings is offered."
}, {
    "id": "oai:dspace.mit.edu:1721.1/114123",
    "title": "Variable star photometry in a secondary school curriculum",
    "abstract": "The author proved that photometry of variable stars can be performed by anyone using the shoestring budget of only a digital camera along with a laptop. Extrinsic variable star Algol was observed using a 14\" telescope as well as CCD and had its light curve plotted. In direct comparison, V474 Mon was observed using only a low cost $200 digital camera. Armed with a laptop for data analysis, the author plotted its light curve. Lastly, the whole process of research astronomy was applied to a classroom final project setting. Future work includes expanding this thesis into a full semester long astronomy course for high school students.",
    "advisors": ["Richard P. Binzel"],
    "text": "Variable star photometry in a secondary school curriculum The author proved that photometry of variable stars can be performed by anyone using the shoestring budget of only a digital camera along with a laptop. Extrinsic variable star Algol was observed using a 14\" telescope as well as CCD and had its light curve plotted. In direct comparison, V474 Mon was observed using only a low cost $200 digital camera. Armed with a laptop for data analysis, the author plotted its light curve. Lastly, the whole process of research astronomy was applied to a classroom final project setting. Future work includes expanding this thesis into a full semester long astronomy course for high school students."
}, {
    "id": "oai:dspace.mit.edu:1721.1/117909",
    "title": "Development of a leaching procedure for isotopic study of metal/silicate partitioning experiments",
    "abstract": "The ratio of 238U/ 235U has long been assumed to be constant and equal to 137.88. However, recent research has found that uranium fractionation occurs in a variety of environments, especially reducing environments. Fractionation in metal/silicate systems could be a contributor to Earth's geodynamo heating and affect Pb-Pb geochronology. Sixteen experimental iron/silicate samples were separated magnetically into iron and silicate fractions. Each fraction was leached with 2.5 M HCl, 10 M HCl, 10 M HCl at a higher temperature, and HF and the uranium released in each step was measured by mass spectrometry. The depleted nature of the uranium used to create the samples precludes high-precision isotope fractionation assessments. However, the effectiveness of the leaching procedure was examined for potential use on future samples. The U release pattern in silicate fractions was not consistent between all of the samples, and major element concentration measurements will allow identification of the phase being digested in each step. The contamination from silicate and graphite in the metal fractions prevents uranium from being accurately measured.",
    "advisors": ["Timothy Grove"],
    "text": "Development of a leaching procedure for isotopic study of metal/silicate partitioning experiments The ratio of 238U/ 235U has long been assumed to be constant and equal to 137.88. However, recent research has found that uranium fractionation occurs in a variety of environments, especially reducing environments. Fractionation in metal/silicate systems could be a contributor to Earth's geodynamo heating and affect Pb-Pb geochronology. Sixteen experimental iron/silicate samples were separated magnetically into iron and silicate fractions. Each fraction was leached with 2.5 M HCl, 10 M HCl, 10 M HCl at a higher temperature, and HF and the uranium released in each step was measured by mass spectrometry. The depleted nature of the uranium used to create the samples precludes high-precision isotope fractionation assessments. However, the effectiveness of the leaching procedure was examined for potential use on future samples. The U release pattern in silicate fractions was not consistent between all of the samples, and major element concentration measurements will allow identification of the phase being digested in each step. The contamination from silicate and graphite in the metal fractions prevents uranium from being accurately measured."
}, {
    "id": "oai:dspace.mit.edu:1721.1/101344",
    "title": "Hydrographic structure of overflow water passing through the Denmark Strait",
    "abstract": "Denmark Strait Overflow Water (DSOW) constitutes the densest portion of North Atlantic Deep Water, which feeds the lower limb of the Atlantic Meridional Overturning Circulation (AMOC). As such, it is critical to understand how DSOW is transferred from the upstream basins in the Nordic Seas, across the Greenland-Scotland Ridge, and to the North Atlantic Ocean. The goal of this study is to characterize the hydrographic structure of the different DSOW constituents at the sill before the water descends into the Irminger Sea using temperature and salinity (T/S) data from 111 shipboard crossings in the vicinity of the sill, collected between 1990 and 2012. The individual realizations indicate that weakly stratified \"boluses\" of DSOW frequent the sill and contribute the densest water to the overflow. This study also characterizes the structure, size, and location of the boluses and relates them to the T/S modes found at the sill. Lastly, historical hydrographic data from the Nordic Seas are used to make inferences regarding the origin of the boluses.",
    "advisors": ["Robert S. Pickart"],
    "text": "Hydrographic structure of overflow water passing through the Denmark Strait Denmark Strait Overflow Water (DSOW) constitutes the densest portion of North Atlantic Deep Water, which feeds the lower limb of the Atlantic Meridional Overturning Circulation (AMOC). As such, it is critical to understand how DSOW is transferred from the upstream basins in the Nordic Seas, across the Greenland-Scotland Ridge, and to the North Atlantic Ocean. The goal of this study is to characterize the hydrographic structure of the different DSOW constituents at the sill before the water descends into the Irminger Sea using temperature and salinity (T/S) data from 111 shipboard crossings in the vicinity of the sill, collected between 1990 and 2012. The individual realizations indicate that weakly stratified \"boluses\" of DSOW frequent the sill and contribute the densest water to the overflow. This study also characterizes the structure, size, and location of the boluses and relates them to the T/S modes found at the sill. Lastly, historical hydrographic data from the Nordic Seas are used to make inferences regarding the origin of the boluses."
}, {
    "id": "oai:dspace.mit.edu:1721.1/42280",
    "title": "The interaction of two coastal plumes and its effect on the transport of Alexandrium fundyense",
    "abstract": "Harmful algal blooms (HABs) of A. fundyense, more commonly known as \"red tides\", are a serious economic and public health concern in the Gulf of Maine. Until recently, there was very little known about the mechanisms regulating the observed spatial and temporal distributions of A. fundyense in this region. In the beginning of this work a review of previous research on A. fundyense and the mechanisms controlling their spatial and temporal distributions in the Gulf of Maine is presented. One of the major conclusions that can be drawn from previous work is that a thorough understanding of the interactions between river plumes is essential to our understanding of this problem. The rest of this thesis intends to contribute to the understanding of these plume interactions and their effect on the transport of A. fundyense. Mixing between two interacting river plumes with various buoyancies is investigated through laboratory experiments. These experiments indicate that under these idealized conditions, there was little mixing between the plumes after their initial interaction. A numerical model is used to explore the effects of river mouth size and flux variations on the interaction between two plumes. It is shown that based on river mouth geometry and flow rates the effect of the southern plume on the path of the northern plume can be predicted. In the final section a simple NP model is coupled with the physical model to explore the possible effects of river plume interaction on the distribution of A. fundyense. Based on our modeled results, it appears as if the southern river under certain conditions could temporarily act as a shield preventing A. fundyense from reaching the coast but that this was not a permanent state.",
    "advisors": ["Glenn R. Flierl"],
    "text": "The interaction of two coastal plumes and its effect on the transport of Alexandrium fundyense Harmful algal blooms (HABs) of A. fundyense, more commonly known as \"red tides\", are a serious economic and public health concern in the Gulf of Maine. Until recently, there was very little known about the mechanisms regulating the observed spatial and temporal distributions of A. fundyense in this region. In the beginning of this work a review of previous research on A. fundyense and the mechanisms controlling their spatial and temporal distributions in the Gulf of Maine is presented. One of the major conclusions that can be drawn from previous work is that a thorough understanding of the interactions between river plumes is essential to our understanding of this problem. The rest of this thesis intends to contribute to the understanding of these plume interactions and their effect on the transport of A. fundyense. Mixing between two interacting river plumes with various buoyancies is investigated through laboratory experiments. These experiments indicate that under these idealized conditions, there was little mixing between the plumes after their initial interaction. A numerical model is used to explore the effects of river mouth size and flux variations on the interaction between two plumes. It is shown that based on river mouth geometry and flow rates the effect of the southern plume on the path of the northern plume can be predicted. In the final section a simple NP model is coupled with the physical model to explore the possible effects of river plume interaction on the distribution of A. fundyense. Based on our modeled results, it appears as if the southern river under certain conditions could temporarily act as a shield preventing A. fundyense from reaching the coast but that this was not a permanent state."
}, {
    "id": "oai:dspace.mit.edu:1721.1/98676",
    "title": "Tropical cyclone precipitation risk in the Southern United States",
    "abstract": "This thesis works to evaluate the new rainfall algorithm that is used to simulate longterm tropical cyclone precipitation (TCP) climatology throughout the southeastern United States. The TCP climatology is based on a fleet of synthetic tropical cyclones developed using National Center for Atmospheric Research/National Centers for Environmental Prediction reanalysis data from 1980 to 2010 and the Coupled Hurricane Intensity Prediction System (CHIPS) model. The climatology is compared to hourly rainfall estimates from the WSR-88D Next Generation Weather Radar (NEXRAD-II) system. In general the synthetic TCP estimates show good agreement with radar-based observations. The rainfall algorithm appears to perform better at coastal locations versus inland ones, and in general has better agreement in the eastern locations considered in this study. In addition, the spatial dependence of radar rainfall estimates was addressed, and in general more extreme TCP-events exhibited a greater degree of event total precipitation variation at grid box-scale. Finally, preliminary work incorporating streamflow measurements as a metric for assessing TCP risk using the synthetic rainfall climatology was begun. Correlation between both grid box-specific and basin-average radar-based event TCP and surface streamflow measurements (from the U.S. Geological Survey National Water Information System) varied greatly, and was generally moderate, and future work should incorporate more thorough streamflow modeling in order to evaluate these comparisons.",
    "advisors": ["Kerry A. Emanuel"],
    "text": "Tropical cyclone precipitation risk in the Southern United States This thesis works to evaluate the new rainfall algorithm that is used to simulate longterm tropical cyclone precipitation (TCP) climatology throughout the southeastern United States. The TCP climatology is based on a fleet of synthetic tropical cyclones developed using National Center for Atmospheric Research/National Centers for Environmental Prediction reanalysis data from 1980 to 2010 and the Coupled Hurricane Intensity Prediction System (CHIPS) model. The climatology is compared to hourly rainfall estimates from the WSR-88D Next Generation Weather Radar (NEXRAD-II) system. In general the synthetic TCP estimates show good agreement with radar-based observations. The rainfall algorithm appears to perform better at coastal locations versus inland ones, and in general has better agreement in the eastern locations considered in this study. In addition, the spatial dependence of radar rainfall estimates was addressed, and in general more extreme TCP-events exhibited a greater degree of event total precipitation variation at grid box-scale. Finally, preliminary work incorporating streamflow measurements as a metric for assessing TCP risk using the synthetic rainfall climatology was begun. Correlation between both grid box-specific and basin-average radar-based event TCP and surface streamflow measurements (from the U.S. Geological Survey National Water Information System) varied greatly, and was generally moderate, and future work should incorporate more thorough streamflow modeling in order to evaluate these comparisons."
}, {
    "id": "oai:dspace.mit.edu:1721.1/98675",
    "title": "Assessing impact of the sulfate aerosol first indirect effect on tropical cyclone activity",
    "abstract": "Tropical cyclones (TCs) are among the most expensive and lethal geophysical hazards. Studies suggest that the intensity of TCs will increase due to the thermodynamic effects of anthropogenic greenhouse gas input. In contrast, while aerosols are shown to have an overall cooling effect on global climate, their impact on TCs is not yet well-understood. This paper explores the influence of the sulfate aerosol first indirect effect (AIE) on Atlantic hurricane intensity and genesis. I use a single-column radiative convective model that incorporates the first AIE (aerosol enhancement of cloud reflectivity) through parameterization of cloud droplet number, radius, and optical depth. Cloud droplet number is parameterized using an empirical scheme, while the radius is determined from cloud liquid water content and number concentration moments, and the optical depth scheme is embedded in the original single-column model. The model is run with both the IGAC/SPARC Chemistry Climate Model Initiative (CCMI) historical simulations of sulfate concentrations over the hurricane main development region during hurricane peak season (August-October) and a self-generated inventory of sulfate concentrations based on realistic vertical variability in sulfate levels. The model was run to radiative-convective equilibrium (RCE), then rerun under weak temperature gradient mode (WTG). Runs successfully produce the Twomey or first indirect effect, which states that increased aerosols will increase cloud droplet number concentration, decrease the effective cloud droplet radius, and increase the cloud optical depth. The net effect is increased reflection of radiation from the atmosphere, which theoretically cools the Earth, decreasing the potential intensity and genesis potential of TCs. While model runs produce the expected changes in cloud properties, cloud cover is not sufficient for sulfate concentrations to have a substantial impact on hurricane activity via the AIE when the model is run to RCE. The WTG mode is then implemented with the goal of producing low-lying stratocumulus clouds to increase total cloud cover, but the single-column WTG scheme was not able to produce stratocumulus that did not also produce an overly strong negative feedback. Using the single-column model, one can demonstrate the indirect effect of sulfate aerosols on cloud reflectivity and that sufficient cloud cover is needed to produce a noticeable cooling and change in expected hurricane behavior. A further study of the subject could include parameterization of the poorly-understood cold or mixed-phase clouds, which can include characterization of additional aerosol types. In addition, a two-dimensional model has greater capacity to model phenomena such as low-lying stratocumulus, which could produce a more substantial ambient effect.",
    "advisors": ["Kerry A. Emanuel"],
    "text": "Assessing impact of the sulfate aerosol first indirect effect on tropical cyclone activity Tropical cyclones (TCs) are among the most expensive and lethal geophysical hazards. Studies suggest that the intensity of TCs will increase due to the thermodynamic effects of anthropogenic greenhouse gas input. In contrast, while aerosols are shown to have an overall cooling effect on global climate, their impact on TCs is not yet well-understood. This paper explores the influence of the sulfate aerosol first indirect effect (AIE) on Atlantic hurricane intensity and genesis. I use a single-column radiative convective model that incorporates the first AIE (aerosol enhancement of cloud reflectivity) through parameterization of cloud droplet number, radius, and optical depth. Cloud droplet number is parameterized using an empirical scheme, while the radius is determined from cloud liquid water content and number concentration moments, and the optical depth scheme is embedded in the original single-column model. The model is run with both the IGAC/SPARC Chemistry Climate Model Initiative (CCMI) historical simulations of sulfate concentrations over the hurricane main development region during hurricane peak season (August-October) and a self-generated inventory of sulfate concentrations based on realistic vertical variability in sulfate levels. The model was run to radiative-convective equilibrium (RCE), then rerun under weak temperature gradient mode (WTG). Runs successfully produce the Twomey or first indirect effect, which states that increased aerosols will increase cloud droplet number concentration, decrease the effective cloud droplet radius, and increase the cloud optical depth. The net effect is increased reflection of radiation from the atmosphere, which theoretically cools the Earth, decreasing the potential intensity and genesis potential of TCs. While model runs produce the expected changes in cloud properties, cloud cover is not sufficient for sulfate concentrations to have a substantial impact on hurricane activity via the AIE when the model is run to RCE. The WTG mode is then implemented with the goal of producing low-lying stratocumulus clouds to increase total cloud cover, but the single-column WTG scheme was not able to produce stratocumulus that did not also produce an overly strong negative feedback. Using the single-column model, one can demonstrate the indirect effect of sulfate aerosols on cloud reflectivity and that sufficient cloud cover is needed to produce a noticeable cooling and change in expected hurricane behavior. A further study of the subject could include parameterization of the poorly-understood cold or mixed-phase clouds, which can include characterization of additional aerosol types. In addition, a two-dimensional model has greater capacity to model phenomena such as low-lying stratocumulus, which could produce a more substantial ambient effect."
}, {
    "id": "oai:dspace.mit.edu:1721.1/42997",
    "title": "Plagioclase preferred orientation in the layered mylonites : evaluation of flow laws for the lower crust",
    "abstract": "We evaluate the applicability of plagioclase and gabbro flow laws by comparing predicted and observed deformation mechanisms in gabbroic shear zones. Gabbros and layered gabbro mylonites were collected from the Southwest Indian Ridge (SWIR), ODP Hole 735B. Deformation temperatures are constrained by two-pyroxene thermometry, stress is estimated from grain size, and deformation mechanisms are analyzed by microstructure and the presence or absence of a lattice preferred orientation (LPO). Our analyses indicate that mylonite layers deformed at a strain rate in the range of 1012 to 101 s-1, while coarse-grained gabbro deformed at a strain rate of approximately 10-14 to 1013 s-1. Plagioclase in pure plagioclase mylonite layers exhibit strong LPOs indicating they deform by dislocation creep. Plagioclase grain size in mixed plagioclase-pyroxene mylonite layers is finer than in pure plagioclase layers, and depends on the size and proportion of pyroxenes. Progressive mixing of pyroxene and plagioclase within gabbro mylonite layers is accompanied by weakening of the LPO indicating that phase mixing promotes a transition to diffusion creep processes that involve grain boundary sliding. Our results indicate that experimental flow laws are accurate at geologic strain rates, although the strain rate for diffusion creep of fine-grained gabbro may be underestimated. At the conditions estimated for the SWIR crust, our calculations suggest that strain localization leads to a factor of two to four decrease in lower crustal viscosity. Even so, the viscosity of lower gabbroic crust is predicted to be similar to that of dry upper mantle.",
    "advisors": ["Greg Hirth"],
    "text": "Plagioclase preferred orientation in the layered mylonites : evaluation of flow laws for the lower crust We evaluate the applicability of plagioclase and gabbro flow laws by comparing predicted and observed deformation mechanisms in gabbroic shear zones. Gabbros and layered gabbro mylonites were collected from the Southwest Indian Ridge (SWIR), ODP Hole 735B. Deformation temperatures are constrained by two-pyroxene thermometry, stress is estimated from grain size, and deformation mechanisms are analyzed by microstructure and the presence or absence of a lattice preferred orientation (LPO). Our analyses indicate that mylonite layers deformed at a strain rate in the range of 1012 to 101 s-1, while coarse-grained gabbro deformed at a strain rate of approximately 10-14 to 1013 s-1. Plagioclase in pure plagioclase mylonite layers exhibit strong LPOs indicating they deform by dislocation creep. Plagioclase grain size in mixed plagioclase-pyroxene mylonite layers is finer than in pure plagioclase layers, and depends on the size and proportion of pyroxenes. Progressive mixing of pyroxene and plagioclase within gabbro mylonite layers is accompanied by weakening of the LPO indicating that phase mixing promotes a transition to diffusion creep processes that involve grain boundary sliding. Our results indicate that experimental flow laws are accurate at geologic strain rates, although the strain rate for diffusion creep of fine-grained gabbro may be underestimated. At the conditions estimated for the SWIR crust, our calculations suggest that strain localization leads to a factor of two to four decrease in lower crustal viscosity. Even so, the viscosity of lower gabbroic crust is predicted to be similar to that of dry upper mantle."
}, {
    "id": "oai:dspace.mit.edu:1721.1/69474",
    "title": "Organic geochemistry and stable isotope constraints on Precambrian biogeochemical processes",
    "abstract": "Details of the biogeochemical cycles and the dominant mechanisms present in Precambrian remain heavily debated topics. The events of the Late Proterozoic onset to glaciations and what types of early life existed in the Archean are two of the many provoking topics within the Precambrian. We set out to improve the understanding of these geologic intervals by examining stable isotopic signatures and molecular fossils (biomarkers) in Late Proterozoic and Mesoarchean ages sedimentary rocks in Northwestern Territories, Canada and Pilbara, Western Australia, respectively. This thesis presents sulfur, carbon, oxygen and nitrogen stable isotopic data along with distribution of steranes and hopanes biomarkers. Geochemical data is analyzed in the context of elucidating the key biological and environmental factors involved in the Mesoarchean marine biosphere and the Late Proterozoic onset of glaciations. Stable isotopic analysis of the Gorge Creek Group in Pilbara, Western Australia reveals organisms capable of microbial sulfur disproportionation were likely the dominant biological players in Mesoarchean deep-ocean sulfur cycling. Biomarker and isotopic proxies of the Coppercap Formation reveal diverse biological activity directly prior to the Sturtian Glaciation with communities of green and purple sulfur bacteria as well as methanotrophs and cyanobacteria. Possible environmental implications of these communities co-existing are explained in context of changes in ocean chemistry and the diversification of eukaryotic life.",
    "advisors": ["Shuhei Ono"],
    "text": "Organic geochemistry and stable isotope constraints on Precambrian biogeochemical processes Details of the biogeochemical cycles and the dominant mechanisms present in Precambrian remain heavily debated topics. The events of the Late Proterozoic onset to glaciations and what types of early life existed in the Archean are two of the many provoking topics within the Precambrian. We set out to improve the understanding of these geologic intervals by examining stable isotopic signatures and molecular fossils (biomarkers) in Late Proterozoic and Mesoarchean ages sedimentary rocks in Northwestern Territories, Canada and Pilbara, Western Australia, respectively. This thesis presents sulfur, carbon, oxygen and nitrogen stable isotopic data along with distribution of steranes and hopanes biomarkers. Geochemical data is analyzed in the context of elucidating the key biological and environmental factors involved in the Mesoarchean marine biosphere and the Late Proterozoic onset of glaciations. Stable isotopic analysis of the Gorge Creek Group in Pilbara, Western Australia reveals organisms capable of microbial sulfur disproportionation were likely the dominant biological players in Mesoarchean deep-ocean sulfur cycling. Biomarker and isotopic proxies of the Coppercap Formation reveal diverse biological activity directly prior to the Sturtian Glaciation with communities of green and purple sulfur bacteria as well as methanotrophs and cyanobacteria. Possible environmental implications of these communities co-existing are explained in context of changes in ocean chemistry and the diversification of eukaryotic life."
}, {
    "id": "oai:dspace.mit.edu:1721.1/59650",
    "title": "Depth and orbital tuning : a new chronology of glaciation and nonlinear orbital climate change",
    "abstract": "It is suggested that orbital tuning casts a false light upon the chronology of glaciation and the understanding of the climatic response to orbital variations. By developing a new age-model, independent of orbital assumptions, a significant non-linear response to orbital forcing becomes evident in the [delta] 18 0 record. The new age-model also indicates glacial terminations two through eight are 8,000 years older than the orbitally based estimates. A simple obliquity threshold model is presented which reproduces the timing, amplitude, and observed non-linearities of the  [delta] 18 0 record; and supports the plausibility of the new age-model and the inferred non-linear climatic response.",
    "advisors": ["Carl Wunsch"],
    "text": "Depth and orbital tuning : a new chronology of glaciation and nonlinear orbital climate change It is suggested that orbital tuning casts a false light upon the chronology of glaciation and the understanding of the climatic response to orbital variations. By developing a new age-model, independent of orbital assumptions, a significant non-linear response to orbital forcing becomes evident in the [delta] 18 0 record. The new age-model also indicates glacial terminations two through eight are 8,000 years older than the orbitally based estimates. A simple obliquity threshold model is presented which reproduces the timing, amplitude, and observed non-linearities of the  [delta] 18 0 record; and supports the plausibility of the new age-model and the inferred non-linear climatic response."
}, {
    "id": "oai:dspace.mit.edu:1721.1/53533",
    "title": "The response of a point source in a liquid layer overlying a liquid half space",
    "abstract": "The response to a harmonic point source in a liquid layer overlying a liquid half space is computed as a function of frequency. Included are the contributions form all normal modes that occur, and the branch-line integral representing the refraction arrival. The value of the refraction arrival is given in terms of the complex error function. The effect of different velocity and density contrasts are considered, and the effect of source depth and the distance to the receiver are investigated. The results giving the behavior of the magnitude of the branch line show that it is much larger at the mode cutoffs than at other values of frequency. The total amplitude of the response shows a regular oscillation in the frequency range in which two modes are present, and somewhat irregular high and low values over the range in which three modes are present. This behavior reflects the difference in amplitude at frequencies for which modes reinforce or interfere with each other.",
    "advisors": ["Stephen M. Simpson, Jr"],
    "text": "The response of a point source in a liquid layer overlying a liquid half space The response to a harmonic point source in a liquid layer overlying a liquid half space is computed as a function of frequency. Included are the contributions form all normal modes that occur, and the branch-line integral representing the refraction arrival. The value of the refraction arrival is given in terms of the complex error function. The effect of different velocity and density contrasts are considered, and the effect of source depth and the distance to the receiver are investigated. The results giving the behavior of the magnitude of the branch line show that it is much larger at the mode cutoffs than at other values of frequency. The total amplitude of the response shows a regular oscillation in the frequency range in which two modes are present, and somewhat irregular high and low values over the range in which three modes are present. This behavior reflects the difference in amplitude at frequencies for which modes reinforce or interfere with each other."
}, {
    "id": "oai:dspace.mit.edu:1721.1/70783",
    "title": "Geochemical properties of the Beni Bousera (N. Morocco) peridotites : a field and laboratory approach to understanding melt infiltration and extraction in an orogenic peridotite massif",
    "abstract": "The Beni Bousera ultramafic massif is a tectonically emplaced body of upper mantle material that is exposed over 72 km2 in the Betic-Rif-Tell orogenic belt of northern Morocco. The massif is composed primarily of spinel lherzolite, although meter-scale domains of harzburgite, dunite, and veins and layers of pyroxenite are relatively common. A combined field and laboratory-based investigation of Beni Bousera has yielded a new dataset comprised of whole rock and mineral chemistry data that is used to suggest the massif is more heterogeneous than previously interpreted, and that \"secondary\" peridotites and dunites were formed by different petrogenic processes than the remainder of the massif. Trends in the CaO-MgO-Al 203-SiO 2 system and trace element patterns suggest partial melting of the peridotite played a significant role in the evolution of the massif, as has been previously suggested for other orogenic peridotites. Heterogeneous zones - regions throughout the massif with diverse groups of peridotites and pyroxenites - are believed to be relict melt transport channels. Peridotites and pyroxenites found in these zones show evidence of interaction with a relatively depleted melt (high Cr, low Al and Ti in mineral phases), and textures that suggest the secondary precipitation of neoblastic pyroxenes along matrix olivine grain boundaries. Rare earth element concentrations of clinopyroxene from two peridotite groups suggest equilibration in the garnet stability field, and temperatures calculated from dysprosium concentrations are usually higher than those determined from two-pyroxene thermometry. Linear major element patterns versus MgO and whole rock REE compositions of Beni Bousera dunites suggest that they cannot have formed via partial melting alone. The data collected here also suggest that the massif has experienced several mantle P/T conditions, and continuously equilibrated during exhumation and emplacement into the crust. A comparison with the geometry of the melt transport domain at Ronda, another Mediterranean peridotite body, suggests that although Beni Bousera and Ronda may be related in age of exhumation, melt may have been transported via different pathways in the two massifs.",
    "advisors": ["Oliver Jagoutz"],
    "text": "Geochemical properties of the Beni Bousera (N. Morocco) peridotites : a field and laboratory approach to understanding melt infiltration and extraction in an orogenic peridotite massif The Beni Bousera ultramafic massif is a tectonically emplaced body of upper mantle material that is exposed over 72 km2 in the Betic-Rif-Tell orogenic belt of northern Morocco. The massif is composed primarily of spinel lherzolite, although meter-scale domains of harzburgite, dunite, and veins and layers of pyroxenite are relatively common. A combined field and laboratory-based investigation of Beni Bousera has yielded a new dataset comprised of whole rock and mineral chemistry data that is used to suggest the massif is more heterogeneous than previously interpreted, and that \"secondary\" peridotites and dunites were formed by different petrogenic processes than the remainder of the massif. Trends in the CaO-MgO-Al 203-SiO 2 system and trace element patterns suggest partial melting of the peridotite played a significant role in the evolution of the massif, as has been previously suggested for other orogenic peridotites. Heterogeneous zones - regions throughout the massif with diverse groups of peridotites and pyroxenites - are believed to be relict melt transport channels. Peridotites and pyroxenites found in these zones show evidence of interaction with a relatively depleted melt (high Cr, low Al and Ti in mineral phases), and textures that suggest the secondary precipitation of neoblastic pyroxenes along matrix olivine grain boundaries. Rare earth element concentrations of clinopyroxene from two peridotite groups suggest equilibration in the garnet stability field, and temperatures calculated from dysprosium concentrations are usually higher than those determined from two-pyroxene thermometry. Linear major element patterns versus MgO and whole rock REE compositions of Beni Bousera dunites suggest that they cannot have formed via partial melting alone. The data collected here also suggest that the massif has experienced several mantle P/T conditions, and continuously equilibrated during exhumation and emplacement into the crust. A comparison with the geometry of the melt transport domain at Ronda, another Mediterranean peridotite body, suggests that although Beni Bousera and Ronda may be related in age of exhumation, melt may have been transported via different pathways in the two massifs."
}, {
    "id": "oai:dspace.mit.edu:1721.1/90650",
    "title": "An electrodynamic balance (EDB) for extraterrestrial cloud formation studies",
    "abstract": "Ice clouds scatter and absorb solar radiation, affecting atmospheric and surface temperatures (Gettelman et al., 2012). On Mars, where ice contained in clouds makes up a large portion of total atmospheric water vapor, ice clouds also alter the planetary water budget (Maltagliati et al., 2011; Rafkin et al., 2013). Thus, it is important for climate models to be able to accurately predict the conditions under which ice clouds can form. Typical Martian temperatures at cloud-formation height range from -150-200 K (Trainer, Toon, & Tolbert, 2009). Heterogeneous deposition nucleation is thought to be the dominant freezing mechanism on Mars due to the abundance of mineral dust to serve as ice nuclei (IN) (Mdittanen et al., 2005). The parameters for such nucleation are not well characterized at such low temperatures (Trainer et al., 2009). Previous experimental studies have investigated the relative humidity required for deposition nucleation within the Martian temperature range. However, most studies took place on bulk aerosol samples, did not use mineral dusts analogous to Martian dust, or were constrained by particle lifetime and temperature limits. In this project, we re-purpose a single-particle instrument and set it up to perform experiments for more precise ice nucleation data under Martian atmospheric conditions. We use an electrodynamic balance (EDB) to levitate individual particles with diameters around 10 pm. We calculate the size of the particle and changes in size based on the holding voltages. The system can be cooled to 200 K in its current configuration, and the relative humidity and atmospheric constituents can be controlled by adding gas. To test the EDB, we perform validation experiments. We investigate deliquescence and efflorescence on salts at room temperature and 0 'C. We modify the cooling system, thermocouples, and relative humidity sensors and begin freezing experiments with Arizona Test Dust (ATD) and with Mojave Mars Simulant (MMS) dust. We investigate water uptake on MMS particles and find it to be non-hygroscopic but wettable, uptaking monolayers of water between 65-95% relative humidity. From 200 K to 220 K, MMS does not nucleate up to 115% RHice, suggesting that higher supersaturations are needed for ice clouds to form; some Martian cloud modelers should revisit the critical supersaturation parameterization. Future work will improve the EDB and use it to examine phase functions and light scattering.",
    "advisors": ["Daniel J. Cziczo"],
    "text": "An electrodynamic balance (EDB) for extraterrestrial cloud formation studies Ice clouds scatter and absorb solar radiation, affecting atmospheric and surface temperatures (Gettelman et al., 2012). On Mars, where ice contained in clouds makes up a large portion of total atmospheric water vapor, ice clouds also alter the planetary water budget (Maltagliati et al., 2011; Rafkin et al., 2013). Thus, it is important for climate models to be able to accurately predict the conditions under which ice clouds can form. Typical Martian temperatures at cloud-formation height range from -150-200 K (Trainer, Toon, & Tolbert, 2009). Heterogeneous deposition nucleation is thought to be the dominant freezing mechanism on Mars due to the abundance of mineral dust to serve as ice nuclei (IN) (Mdittanen et al., 2005). The parameters for such nucleation are not well characterized at such low temperatures (Trainer et al., 2009). Previous experimental studies have investigated the relative humidity required for deposition nucleation within the Martian temperature range. However, most studies took place on bulk aerosol samples, did not use mineral dusts analogous to Martian dust, or were constrained by particle lifetime and temperature limits. In this project, we re-purpose a single-particle instrument and set it up to perform experiments for more precise ice nucleation data under Martian atmospheric conditions. We use an electrodynamic balance (EDB) to levitate individual particles with diameters around 10 pm. We calculate the size of the particle and changes in size based on the holding voltages. The system can be cooled to 200 K in its current configuration, and the relative humidity and atmospheric constituents can be controlled by adding gas. To test the EDB, we perform validation experiments. We investigate deliquescence and efflorescence on salts at room temperature and 0 'C. We modify the cooling system, thermocouples, and relative humidity sensors and begin freezing experiments with Arizona Test Dust (ATD) and with Mojave Mars Simulant (MMS) dust. We investigate water uptake on MMS particles and find it to be non-hygroscopic but wettable, uptaking monolayers of water between 65-95% relative humidity. From 200 K to 220 K, MMS does not nucleate up to 115% RHice, suggesting that higher supersaturations are needed for ice clouds to form; some Martian cloud modelers should revisit the critical supersaturation parameterization. Future work will improve the EDB and use it to examine phase functions and light scattering."
}, {
    "id": "oai:dspace.mit.edu:1721.1/53181",
    "title": "Seismic discontinuities and order estimation using wavelets : a receiver function approach",
    "abstract": "In this thesis, I explore the use of non-linear wavelet techniques to estimate the order and scale of velocity discontinuties in the mantle transition zone through waveform analysis of Pds converted waves. The converted phases are isolated through a single station/multiple event receiver function technique which uses a wavelet deconvolution and denoising known as WaRD. It is an edge-preserving damped least squares solution with a small water level and subsequent wavelet thresholding. The deconvolved data is then imaged through an imaging technique which maps the conversions to the depth domain. The Pds phases are then isolated through a windowing and weighting, and then matched to a fractional order spline using a greedy matching pursuit algorithm. The data for this study consists of 2 Australian stations, CAN (Geoscope) and WRAB (IRIS), and 5 Japanese stations (JIZ, SGN, TKA, TMR, and TYM) from the F-Net array (formerly Freesia). CAN and WRAB are located in a relatively quiet continental tectonic setting, while the Japanese stations are in a more complex subduction zone environment. TKA (southern Japan) and TMR (northern Japan) are each thought to be underlain by a single subducting slab. JIZ, SGN, and TYM are located in central Japan where the Pacific and Philippine plates meet, and the subduction zone is thought to be very complex, with 2 slabs intersecting directly below these stations. Order and scale estimates for both Pds phases were obtained for CAN, WRAB, and SGN, and only P410s and P660 estimates were obtained for JIZ and TYM, respectively. Signal complexity in the image stacks prevented the determination of order estimates in either Pds phase for TKA and TMR. Order and scale estimates for the 410km discontinuity range between 0.325-0.450, and 18-35, respectively. Estimates for the order and scale of the 660km discontinuity range between 0.225-0.325 and 23-31, respectively. The order estimates for the P410s at CAN and WRAB were lower (0.325) than the estimates at JIZ and SGN (0.400-0.450), while the order estimates for the P660s at CAN and WRAB were higher (0.325-0.350) than the estimates for SGN and TYM (0.225-0.275). The results are consistent with a mixture type model in which the shape of the velocity discontinuity is a cusp-like feature and is caused by a critical density of one mineral phase with another. The ability to determine the order and scale and possible lateral variations could have major implications for the current views of discontinuities in the mantle transition zone.",
    "advisors": ["Robert van der Hilst"],
    "text": "Seismic discontinuities and order estimation using wavelets : a receiver function approach In this thesis, I explore the use of non-linear wavelet techniques to estimate the order and scale of velocity discontinuties in the mantle transition zone through waveform analysis of Pds converted waves. The converted phases are isolated through a single station/multiple event receiver function technique which uses a wavelet deconvolution and denoising known as WaRD. It is an edge-preserving damped least squares solution with a small water level and subsequent wavelet thresholding. The deconvolved data is then imaged through an imaging technique which maps the conversions to the depth domain. The Pds phases are then isolated through a windowing and weighting, and then matched to a fractional order spline using a greedy matching pursuit algorithm. The data for this study consists of 2 Australian stations, CAN (Geoscope) and WRAB (IRIS), and 5 Japanese stations (JIZ, SGN, TKA, TMR, and TYM) from the F-Net array (formerly Freesia). CAN and WRAB are located in a relatively quiet continental tectonic setting, while the Japanese stations are in a more complex subduction zone environment. TKA (southern Japan) and TMR (northern Japan) are each thought to be underlain by a single subducting slab. JIZ, SGN, and TYM are located in central Japan where the Pacific and Philippine plates meet, and the subduction zone is thought to be very complex, with 2 slabs intersecting directly below these stations. Order and scale estimates for both Pds phases were obtained for CAN, WRAB, and SGN, and only P410s and P660 estimates were obtained for JIZ and TYM, respectively. Signal complexity in the image stacks prevented the determination of order estimates in either Pds phase for TKA and TMR. Order and scale estimates for the 410km discontinuity range between 0.325-0.450, and 18-35, respectively. Estimates for the order and scale of the 660km discontinuity range between 0.225-0.325 and 23-31, respectively. The order estimates for the P410s at CAN and WRAB were lower (0.325) than the estimates at JIZ and SGN (0.400-0.450), while the order estimates for the P660s at CAN and WRAB were higher (0.325-0.350) than the estimates for SGN and TYM (0.225-0.275). The results are consistent with a mixture type model in which the shape of the velocity discontinuity is a cusp-like feature and is caused by a critical density of one mineral phase with another. The ability to determine the order and scale and possible lateral variations could have major implications for the current views of discontinuities in the mantle transition zone."
}, {
    "id": "oai:dspace.mit.edu:1721.1/84911",
    "title": "Effects of subsurface fracture interactions on surface deformation",
    "abstract": "Although the surface deformation resulting from the opening of a single fracture in a layered elastic half-space resembles the observed deformation at the InSalah site, it seems unlikely that only a single fracture is involved. This raises the question of how interaction among multiple fractures affects surface deformation. Finite element modeling is used to build a 3D model of a reservoir with multiple fractures. The interacting cracks and fractures give this model a more complicated stress state, and so any surface deformation would be different from that of a model with a single fracture. Geodetic monitoring of large-scale CO 2 sequestration provides a potentially powerful and cost-effective tool for interrogating reservoir structure and processes. For example, InSAR observations at the InSalah, Algeria sequestration site have mapped the surface deformation above an active reservoir, and helped delineate the effects of CO2 storage. The impact of interactions on individual fractures and the qualitative changes in the surface displacement and stress fields are considered and the importance of orientation, position and fracture area is investigated. It was found that when the crack locations are biased towards stacked parallel arrangements, then the shielding effect of interactions dominates, meaning that the overall stiffness of a representative volume increases. When collinear interactions dominate then the overall stiffness is reduced. These effects are then used to find a volume average and a continuum description of a solid with effective elastic properties. In this way a volume of fractured rock can be replaced with a representative volume with elastic properties that approximate the interaction effects.",
    "advisors": ["Bradford Hager"],
    "text": "Effects of subsurface fracture interactions on surface deformation Although the surface deformation resulting from the opening of a single fracture in a layered elastic half-space resembles the observed deformation at the InSalah site, it seems unlikely that only a single fracture is involved. This raises the question of how interaction among multiple fractures affects surface deformation. Finite element modeling is used to build a 3D model of a reservoir with multiple fractures. The interacting cracks and fractures give this model a more complicated stress state, and so any surface deformation would be different from that of a model with a single fracture. Geodetic monitoring of large-scale CO 2 sequestration provides a potentially powerful and cost-effective tool for interrogating reservoir structure and processes. For example, InSAR observations at the InSalah, Algeria sequestration site have mapped the surface deformation above an active reservoir, and helped delineate the effects of CO2 storage. The impact of interactions on individual fractures and the qualitative changes in the surface displacement and stress fields are considered and the importance of orientation, position and fracture area is investigated. It was found that when the crack locations are biased towards stacked parallel arrangements, then the shielding effect of interactions dominates, meaning that the overall stiffness of a representative volume increases. When collinear interactions dominate then the overall stiffness is reduced. These effects are then used to find a volume average and a continuum description of a solid with effective elastic properties. In this way a volume of fractured rock can be replaced with a representative volume with elastic properties that approximate the interaction effects."
}, {
    "id": "oai:dspace.mit.edu:1721.1/53161",
    "title": "Diapycnal mixing transience and the meridional overturning circulation",
    "abstract": "Diapycnal mixing of ocean waters is crucial to the dynamics and associated heat transport of the meridional overturning circulation, yet uncertainty exists regarding the distribution and physical mechanisms of this mixing. This study uses a highly-idealized, single-hemisphere model of buoyancy-forced flow to examine the examine the effects of the transience of diapycnal mixing on the MOC. The strength of the MOC was found to be insensitive to mixing transience when mixing occurred uniformly on basin boundaries. For mixing that was highly localized in space, a ten-fold increase in transience, as compared with the time-invariant control, resulted in a decrease by about 20% of MOC mass and heat transport. The degree of sensitivity in the highly localized case is likely to be a strong function of the surface restoring timescale for temperature. The circulation dynamics associated with transient mixing displayed large-scale, complex oscillations that increased in amplitude with the transience of mixing. Attempts to quantify the relationship between mixing transience, MOC strength, and the power expended in mixing were inconclusive and merit further investigation.",
    "advisors": ["Kerry Emanuel"],
    "text": "Diapycnal mixing transience and the meridional overturning circulation Diapycnal mixing of ocean waters is crucial to the dynamics and associated heat transport of the meridional overturning circulation, yet uncertainty exists regarding the distribution and physical mechanisms of this mixing. This study uses a highly-idealized, single-hemisphere model of buoyancy-forced flow to examine the examine the effects of the transience of diapycnal mixing on the MOC. The strength of the MOC was found to be insensitive to mixing transience when mixing occurred uniformly on basin boundaries. For mixing that was highly localized in space, a ten-fold increase in transience, as compared with the time-invariant control, resulted in a decrease by about 20% of MOC mass and heat transport. The degree of sensitivity in the highly localized case is likely to be a strong function of the surface restoring timescale for temperature. The circulation dynamics associated with transient mixing displayed large-scale, complex oscillations that increased in amplitude with the transience of mixing. Attempts to quantify the relationship between mixing transience, MOC strength, and the power expended in mixing were inconclusive and merit further investigation."
}, {
    "id": "oai:dspace.mit.edu:1721.1/84910",
    "title": "Constraining the average fill densities of Mars' lowlands and fluvial erosion of Titan's polar regions.",
    "abstract": "Other than Earth, Mars and Titan are the only bodies in our Solar System where we have observed widespread fluvial activity. In this thesis I present two approaches for constraining the extent of multiple resurfacing processes in order to gain insight into the early history of Mars and Titan. One of the most distinctive features of the Martian surface is the dichotomy between the heavily cratered southern highlands and the relatively smooth northern lowlands. The northern lowlands appear smooth because many of the craters in the north have been partially or completely buried beneath volcanic and sedimentary fill of unknown relative proportions. In Chapter 1, we use the Mars Orbiter Laser Altimeter (MOLA) topography data, the Mars Reconnaissance Orbiter (MRO) gravity model and a Wiener filter to map these buried craters and estimate minimum fill thickness and volume as well as maximum fill density. The overall trend observed for the northern lowlands is more sedimentation near the dichotomy and less sedimentation further north and near the Tharsis region, which is consistent with the geology of the region. Titan has few impact craters, suggesting that its surface is geologically young. In Chapter 2 we evaluate whether fluvial erosion has caused significant resurfacing by estimating the cumulative erosion around the margins of polar lakes. Images of drowned fluvial features around the lake margins, where elevated levels of hydrocarbon liquids appear to have partly flooded fluvial valleys, allow us to map topographic contours that trace the fluvially dissected topography. We then used a numerical landscape evolution model to calibrate a relationship between contour sinuosity, which reflects the extent of fluvial valley incision, and cumulative erosion. We find that cumulative fluvial erosion around the margins of Titan's polar lakes, including Ligeia Mare, Kraken Mare, and Punga Mare in the north and Ontario Lacus in the south, ranges from 4% to 31% of the initial relief. Additional model simulations show that this amount of fluvial erosion does not render craters invisible at the resolution of currently available imagery, suggesting that fluvial erosion is not the only major resurfacing mechanism operating in Titan's polar regions.",
    "advisors": ["Maria T. Zuber"],
    "text": "Constraining the average fill densities of Mars' lowlands and fluvial erosion of Titan's polar regions. Other than Earth, Mars and Titan are the only bodies in our Solar System where we have observed widespread fluvial activity. In this thesis I present two approaches for constraining the extent of multiple resurfacing processes in order to gain insight into the early history of Mars and Titan. One of the most distinctive features of the Martian surface is the dichotomy between the heavily cratered southern highlands and the relatively smooth northern lowlands. The northern lowlands appear smooth because many of the craters in the north have been partially or completely buried beneath volcanic and sedimentary fill of unknown relative proportions. In Chapter 1, we use the Mars Orbiter Laser Altimeter (MOLA) topography data, the Mars Reconnaissance Orbiter (MRO) gravity model and a Wiener filter to map these buried craters and estimate minimum fill thickness and volume as well as maximum fill density. The overall trend observed for the northern lowlands is more sedimentation near the dichotomy and less sedimentation further north and near the Tharsis region, which is consistent with the geology of the region. Titan has few impact craters, suggesting that its surface is geologically young. In Chapter 2 we evaluate whether fluvial erosion has caused significant resurfacing by estimating the cumulative erosion around the margins of polar lakes. Images of drowned fluvial features around the lake margins, where elevated levels of hydrocarbon liquids appear to have partly flooded fluvial valleys, allow us to map topographic contours that trace the fluvially dissected topography. We then used a numerical landscape evolution model to calibrate a relationship between contour sinuosity, which reflects the extent of fluvial valley incision, and cumulative erosion. We find that cumulative fluvial erosion around the margins of Titan's polar lakes, including Ligeia Mare, Kraken Mare, and Punga Mare in the north and Ontario Lacus in the south, ranges from 4% to 31% of the initial relief. Additional model simulations show that this amount of fluvial erosion does not render craters invisible at the resolution of currently available imagery, suggesting that fluvial erosion is not the only major resurfacing mechanism operating in Titan's polar regions."
}, {
    "id": "oai:dspace.mit.edu:1721.1/47847",
    "title": "Comparison of wind stress algorithms, datasets and oceanic power input",
    "abstract": "If the ocean is in a statistically steady state, energy balance is a strong constraint, suggesting that the energy input into the world ocean is dissipated simultaneously at the same rate. Energy conservation is one of the most important principles in the natural world. However, the study of energy balance in the oceanic circulation has long been overlooked. Mink and Winch (1998) proposed that energy is needed to maintain the meridional overturning circulation and they also concluded that the wind energy input into the world ocean constitute the most important part. Since then, many estimates on the wind energy input have been given with a focus on different time and spatial scales. It is well known that it is the air-sea momentum flux (wind stress) that actually drives the ocean circulation, especially the upper layer circulation. Due to the difficulties of directly measuring the wind stress, different algorithms were proposed to relate the wind stress with the wind velocity and other related atmospheric and oceanic variables. Different algorithms in fact produce quite different wind stresses, which may leads to spurious estimates in the wind energy input into the world ocean. The thesis is organized as follows. In chapter 1, we try to understand the difference of four bulk algorithms, and conclude that different bulk algorithms may yield the wind energy input differences of 20%. Comparison of 4 different wind stress dataset were presented in Chapter 2. However, we do not determine which product is the best. In Chapter 3, a simple numerical experiment was executed and some preliminary estimate on the effects of introducing the wind stress dependence on the oceanic surface velocity were given. The ECCO data computation, however, does not produce the results as expected and some explanations are given.",
    "advisors": ["Carl Wunsch"],
    "text": "Comparison of wind stress algorithms, datasets and oceanic power input If the ocean is in a statistically steady state, energy balance is a strong constraint, suggesting that the energy input into the world ocean is dissipated simultaneously at the same rate. Energy conservation is one of the most important principles in the natural world. However, the study of energy balance in the oceanic circulation has long been overlooked. Mink and Winch (1998) proposed that energy is needed to maintain the meridional overturning circulation and they also concluded that the wind energy input into the world ocean constitute the most important part. Since then, many estimates on the wind energy input have been given with a focus on different time and spatial scales. It is well known that it is the air-sea momentum flux (wind stress) that actually drives the ocean circulation, especially the upper layer circulation. Due to the difficulties of directly measuring the wind stress, different algorithms were proposed to relate the wind stress with the wind velocity and other related atmospheric and oceanic variables. Different algorithms in fact produce quite different wind stresses, which may leads to spurious estimates in the wind energy input into the world ocean. The thesis is organized as follows. In chapter 1, we try to understand the difference of four bulk algorithms, and conclude that different bulk algorithms may yield the wind energy input differences of 20%. Comparison of 4 different wind stress dataset were presented in Chapter 2. However, we do not determine which product is the best. In Chapter 3, a simple numerical experiment was executed and some preliminary estimate on the effects of introducing the wind stress dependence on the oceanic surface velocity were given. The ECCO data computation, however, does not produce the results as expected and some explanations are given."
}, {
    "id": "oai:dspace.mit.edu:1721.1/69239",
    "title": "The response of the Red Sea to a strong wind jet near the Tokar Gap in summer",
    "abstract": "Remote sensing and in situ observations are used to investigate the ocean response to the Tokar Wind Jet in the Red Sea. The wind jet blows down the pressure gradient through the Tokar Gap on the Sudanese coast, at about 18N, during the summer monsoon season. It disturbs the prevailing along-sea (southeastward) winds with strong cross-sea (northeastward) winds that can last from days to weeks and reach amplitudes of 20-25 m/s. By comparing scatterometer winds with along-track and gridded sea level anomaly observations, it is shown that an intense dipolar eddy spins up in less than seven days in response to the wind jet. The eddy pair has a horizontal scale of 140 km. Maximum ocean surface velocities can reach 1 m/s and eddy currents extend at least 200 m into the water column. The eddy currents appear to cover the width of the sea, providing a pathway for rapid transport of marine organisms and other drifting material from one coast to the other. Interannual variability in the strength of the dipole is closely matched with variability in the strength of the wind jet. The dipole is observed to be quasi-stationary, although there is some evidence for slow eastward propagation-simulation of the dipole in an idealized high-resolution numerical model suggests that this is the result of self-advection. These and other recent in situ observations in the Red Sea show that the upper ocean currents are dominated by mesoscale eddies rather than by a slow overturning circulation.",
    "advisors": ["Amy Bower"],
    "text": "The response of the Red Sea to a strong wind jet near the Tokar Gap in summer Remote sensing and in situ observations are used to investigate the ocean response to the Tokar Wind Jet in the Red Sea. The wind jet blows down the pressure gradient through the Tokar Gap on the Sudanese coast, at about 18N, during the summer monsoon season. It disturbs the prevailing along-sea (southeastward) winds with strong cross-sea (northeastward) winds that can last from days to weeks and reach amplitudes of 20-25 m/s. By comparing scatterometer winds with along-track and gridded sea level anomaly observations, it is shown that an intense dipolar eddy spins up in less than seven days in response to the wind jet. The eddy pair has a horizontal scale of 140 km. Maximum ocean surface velocities can reach 1 m/s and eddy currents extend at least 200 m into the water column. The eddy currents appear to cover the width of the sea, providing a pathway for rapid transport of marine organisms and other drifting material from one coast to the other. Interannual variability in the strength of the dipole is closely matched with variability in the strength of the wind jet. The dipole is observed to be quasi-stationary, although there is some evidence for slow eastward propagation-simulation of the dipole in an idealized high-resolution numerical model suggests that this is the result of self-advection. These and other recent in situ observations in the Red Sea show that the upper ocean currents are dominated by mesoscale eddies rather than by a slow overturning circulation."
}, {
    "id": "oai:dspace.mit.edu:1721.1/59657",
    "title": "Abyssal mixing from bottom boundary effects in Mid-Atlantic Ridge flank canyons",
    "abstract": "This paper begins to explore a previously neglected mechanism for abyssal ocean mixing using bottom boundary layer dynamics. Abyssal mixing and the associated upward buoyancy fluxes are necessary to balance the sinking of dense waters at high latitudes and to close the global overturning circulation. Previous studies have concentrated on the hypothesis that the primary mechanism for this mixing is breaking internal waves generated by tidal flows over rough topography. However, intriguing observations, particularly from the Brazil Basin Tracer Release Experiment, suggest that mixing in the flank canyons of the Mid-Atlantic Ridge generated when strong mean flows interact with the many sills and constrictions within the canyons may represent a dynamically important amount of abyssal mixing. The energy pathways and mechanisms of this mixing are much less clear than in the case of breaking internal waves. This study attempts to clarify this by suggesting an analogy with an idealized diffusive boundary layer over a sloping bottom. This boundary layer is characterized by up-slope flows powered by the buoyancy flux in the fluid far from the boundary. Here we explore the energy budget of the boundary layer, and find that the diffusive boundary layer provides flows that are generally consistent with those observed in submarine canyons. In addition, we derive the vertical velocity in the far-field fluid, analogous to an Ekman pumping velocity, that these boundary layers can induce when the bottom slope is not constant. Finally, we present both theoretical and numerical models of exchange flows between the bottom boundary and the far-field flow when the bottom slope is not constant. These exchange flows provide a mechanism by which boundary-driven mixing can affect the overall stratification and buoyancy fluxes of the basin interior.",
    "advisors": ["Lawrence J. Pratt"],
    "text": "Abyssal mixing from bottom boundary effects in Mid-Atlantic Ridge flank canyons This paper begins to explore a previously neglected mechanism for abyssal ocean mixing using bottom boundary layer dynamics. Abyssal mixing and the associated upward buoyancy fluxes are necessary to balance the sinking of dense waters at high latitudes and to close the global overturning circulation. Previous studies have concentrated on the hypothesis that the primary mechanism for this mixing is breaking internal waves generated by tidal flows over rough topography. However, intriguing observations, particularly from the Brazil Basin Tracer Release Experiment, suggest that mixing in the flank canyons of the Mid-Atlantic Ridge generated when strong mean flows interact with the many sills and constrictions within the canyons may represent a dynamically important amount of abyssal mixing. The energy pathways and mechanisms of this mixing are much less clear than in the case of breaking internal waves. This study attempts to clarify this by suggesting an analogy with an idealized diffusive boundary layer over a sloping bottom. This boundary layer is characterized by up-slope flows powered by the buoyancy flux in the fluid far from the boundary. Here we explore the energy budget of the boundary layer, and find that the diffusive boundary layer provides flows that are generally consistent with those observed in submarine canyons. In addition, we derive the vertical velocity in the far-field fluid, analogous to an Ekman pumping velocity, that these boundary layers can induce when the bottom slope is not constant. Finally, we present both theoretical and numerical models of exchange flows between the bottom boundary and the far-field flow when the bottom slope is not constant. These exchange flows provide a mechanism by which boundary-driven mixing can affect the overall stratification and buoyancy fluxes of the basin interior."
}, {
    "id": "oai:dspace.mit.edu:1721.1/40977",
    "title": "Studies of climate variability in a simple coupled model",
    "abstract": "The mechanisms of variability of a coupled atmosphere-ocean model are investigated through the study of two coupled configurations: an aquaplanet in which gyres are absent, and an aquaplanet in which a ridge extending from pole to pole supports gyres. Empirical Orthogonal Functions (EOFs) are used to explore the main features of variability exhibited by extended integrations of both configurations. In the aquaplanet a decadal variability is observed in the atmosphere and the ocean. Stochastic driving of the annular modes in the atmosphere generates an anomalous Sea Surface Temperature (SST) dipole through latent heat fluxes and Ekman pumping. A feedback of this SST dipole on the atmosphere enables a damping slow enough for anomalies to persist over decadal time scales. This air-sea feedback combined with a slow advection of the anomalies by mean ocean currents result in the observed decadal oscillation. A simple stochastic model captures the essence of this mechanism. In the ridge decadal variability is absent but centennial variability is observed in the atmosphere and the ocean. Stochastic driving of the annular modes in the atmosphere generates a weak SST tripole due to latent heat fluxes. The weak amplitude of this tripole prevents the existence of any significant air-sea feedback, implies a stronger damping than in the aquaplanet, and ultimately results in the absence of oscillations. The classic stochastic model of Hasselmann [19] explains the evolution of the SST anomaly through time. Within a delay of one year stochastic atmospheric variability additionally generates a baroclinic streamfunction as well as baroclinic Rossby waves at the eastern boundary of the basin. The former is slowly advected by the mean flow while the latter propagates towards the western boundary, inducing a feedback on the atmosphere with a delay of sixty years. A simple model is found to capture the essence of this mechanism. The results of the aquaplanet and the ridge are used to interpret the Drake, a third configuration in which a band of land extends from the North Pole to the line of -45' of latitude. In the northern hemisphere of the Drake mean state and variability are similar to the ones observed in the ridge. The observed centennial oscillation would correspond to a decadal oscillation in the Atlantic. In the southern hemisphere of the Drake, mean state and variability have elements of both the ridge and the aquaplanet.",
    "advisors": ["John Marshall"],
    "text": "Studies of climate variability in a simple coupled model The mechanisms of variability of a coupled atmosphere-ocean model are investigated through the study of two coupled configurations: an aquaplanet in which gyres are absent, and an aquaplanet in which a ridge extending from pole to pole supports gyres. Empirical Orthogonal Functions (EOFs) are used to explore the main features of variability exhibited by extended integrations of both configurations. In the aquaplanet a decadal variability is observed in the atmosphere and the ocean. Stochastic driving of the annular modes in the atmosphere generates an anomalous Sea Surface Temperature (SST) dipole through latent heat fluxes and Ekman pumping. A feedback of this SST dipole on the atmosphere enables a damping slow enough for anomalies to persist over decadal time scales. This air-sea feedback combined with a slow advection of the anomalies by mean ocean currents result in the observed decadal oscillation. A simple stochastic model captures the essence of this mechanism. In the ridge decadal variability is absent but centennial variability is observed in the atmosphere and the ocean. Stochastic driving of the annular modes in the atmosphere generates a weak SST tripole due to latent heat fluxes. The weak amplitude of this tripole prevents the existence of any significant air-sea feedback, implies a stronger damping than in the aquaplanet, and ultimately results in the absence of oscillations. The classic stochastic model of Hasselmann [19] explains the evolution of the SST anomaly through time. Within a delay of one year stochastic atmospheric variability additionally generates a baroclinic streamfunction as well as baroclinic Rossby waves at the eastern boundary of the basin. The former is slowly advected by the mean flow while the latter propagates towards the western boundary, inducing a feedback on the atmosphere with a delay of sixty years. A simple model is found to capture the essence of this mechanism. The results of the aquaplanet and the ridge are used to interpret the Drake, a third configuration in which a band of land extends from the North Pole to the line of -45' of latitude. In the northern hemisphere of the Drake mean state and variability are similar to the ones observed in the ridge. The observed centennial oscillation would correspond to a decadal oscillation in the Atlantic. In the southern hemisphere of the Drake, mean state and variability have elements of both the ridge and the aquaplanet."
}, {
    "id": "oai:dspace.mit.edu:1721.1/33941",
    "title": "A stable isotope stratigraphy for the Axel Heiberg Fossil Forest and its application to Eocene climate",
    "abstract": "The Eocene era was a warm, climatically dynamic transitional period between the Paleocene greenhouse world and the Oligocene icehouse world. This study details carbon and hydrogen isotopic and biomarker analyses of samples of lignite (bulk fossil leaves), wood, paleosol, and resinite from the Middle to Late Eocene age fossil forest stratigraphy on Axel Heiberg Island, Nunavut, Canada. Bulk carbon isotopes show a record of frequent, large fluctuations on the scale of the Paleocene-Eocene Thermal Maximum benthic carbon excursion of [approx.] 2.6% (Zachos 1999). However, terrestrial flora are less sensitive to CO fluctuations given their capacity to regulate stomatal intake and the comparatively easy diffusion of CO in air. Resinites (-22.8 +/- 1.7%) are enriched relative to bulk lignite (-24.7 +/- 0.75%), and wood (-21.66 +/- 0.45%) is also enriched relative to bulk lignite. Both 1) a scenario of periodic methane hydrate pulses and 2) a scenario of fluctuating forest stand LAI (leaf area index) are not inconsistent with our data. Either mechanism could be responsible for large carbon isotope shifts. Higher plant input dominated the n-alkane signature. Compound-specific hydrogen isotopes in n-alkanes show a record of marked secular change, with isotopes becoming generally lighter over the time span of the stratigraphy, though punctuated by singular fluctuations as large as 32%.",
    "advisors": ["Roger Summons", "Julian Sachs"],
    "text": "A stable isotope stratigraphy for the Axel Heiberg Fossil Forest and its application to Eocene climate The Eocene era was a warm, climatically dynamic transitional period between the Paleocene greenhouse world and the Oligocene icehouse world. This study details carbon and hydrogen isotopic and biomarker analyses of samples of lignite (bulk fossil leaves), wood, paleosol, and resinite from the Middle to Late Eocene age fossil forest stratigraphy on Axel Heiberg Island, Nunavut, Canada. Bulk carbon isotopes show a record of frequent, large fluctuations on the scale of the Paleocene-Eocene Thermal Maximum benthic carbon excursion of [approx.] 2.6% (Zachos 1999). However, terrestrial flora are less sensitive to CO fluctuations given their capacity to regulate stomatal intake and the comparatively easy diffusion of CO in air. Resinites (-22.8 +/- 1.7%) are enriched relative to bulk lignite (-24.7 +/- 0.75%), and wood (-21.66 +/- 0.45%) is also enriched relative to bulk lignite. Both 1) a scenario of periodic methane hydrate pulses and 2) a scenario of fluctuating forest stand LAI (leaf area index) are not inconsistent with our data. Either mechanism could be responsible for large carbon isotope shifts. Higher plant input dominated the n-alkane signature. Compound-specific hydrogen isotopes in n-alkanes show a record of marked secular change, with isotopes becoming generally lighter over the time span of the stratigraphy, though punctuated by singular fluctuations as large as 32%."
}, {
    "id": "oai:dspace.mit.edu:1721.1/118026",
    "title": "Gene transfer history of carbon fixation proteins constrains marine cyanobacteria divergence times",
    "abstract": "Carboxysomes provide an avenue for narrowing the timing of evolutionary events in groups of cyanobacteria that are ecologically dominant in modem marine environments - groups that may have an integral role in oxygenating the Earth's atmosphere. Here I show that using concatenated phylogenies of carbon fixation proteins better informs the horizontal gene transfer event that brought carboxysomes from purple sulfur bacteria into marine cyanobacteria and that this gene history aids in constraining the evolutionary timing of carbon fixation. Genes encoding the proteins for the a-carboxysomal shell as well as RuBisCO and carbonic anhydrase are co-located on the genomes of various cyanobacteria in the Prochlorococcus and Synechococcus groups. Previous studies have shown that these genes were likely horizontally transferred together from Chromatiales (purple sulfur bacteria), a group of phototrophic Gammaproteobacteria. While many of these genes are highly conserved and thus yield poorly resolved phylogenies, their concatenation clarifies a shared evolutionary history. This work integrates gene transfer with molecular clock calibration methods to determine divergence times. Accordingly, I evaluate the relationship between atmospheric evolution and the ecology of important groups of phototrophs.",
    "advisors": ["Greg Fournier"],
    "text": "Gene transfer history of carbon fixation proteins constrains marine cyanobacteria divergence times Carboxysomes provide an avenue for narrowing the timing of evolutionary events in groups of cyanobacteria that are ecologically dominant in modem marine environments - groups that may have an integral role in oxygenating the Earth's atmosphere. Here I show that using concatenated phylogenies of carbon fixation proteins better informs the horizontal gene transfer event that brought carboxysomes from purple sulfur bacteria into marine cyanobacteria and that this gene history aids in constraining the evolutionary timing of carbon fixation. Genes encoding the proteins for the a-carboxysomal shell as well as RuBisCO and carbonic anhydrase are co-located on the genomes of various cyanobacteria in the Prochlorococcus and Synechococcus groups. Previous studies have shown that these genes were likely horizontally transferred together from Chromatiales (purple sulfur bacteria), a group of phototrophic Gammaproteobacteria. While many of these genes are highly conserved and thus yield poorly resolved phylogenies, their concatenation clarifies a shared evolutionary history. This work integrates gene transfer with molecular clock calibration methods to determine divergence times. Accordingly, I evaluate the relationship between atmospheric evolution and the ecology of important groups of phototrophs."
}, {
    "id": "oai:dspace.mit.edu:1721.1/57861",
    "title": "Linkages between Eurasian snow cover and Northern Hemisphere winter-time climate variability",
    "abstract": "Recently it has been shown that the Eurasian snow cover in the prior autumn (ESCSON) and the leading mode variability in the wintertime extratropical Northern Hemisphere (NH) atmospheric circulation have significant correlation. In this study, a linkage between the ESCSON and the following wintertime NH climate variability was investigated. Satellite data from the NOAA is used for snow cover, and NCEP/NCAR Reanalysis data are used for climate variables. The high latitudes sea-level pressure is quality-controlled by use of the IABP sea-level pressure dataset, which is derived from the buoy observations. Interannual variability of and association between ESCSON and winter climate variables were surveyed by use of linear statistical analysis techniques; Empirical Orthogonal Function (EOF) analysis, and correlation/regression analysis. The gravity current by the expansion of the cold, dense air over Siberia north- and westward remained one among the several possible mechanisms. The upper air mechanism may be active to connect the ESCSON and the leading mode of DJF surface pressure variability. It is also suggested that the DJF sea-level pressure variations associated with the ESCSON is considerably confined to the Atlantic side, and has only limited association with the linear trend and the Pacific side variations. Future work may include reexamination of the results using the possible, longer data of the observation. The mechanism connecting the ESCSON anomalies and the upper level circulation anomaly should be investigated further, for which one possible approach is analysis of the wave activity and energy propagation in the troposphere and stratosphere.",
    "advisors": ["Dara Entekhabi"],
    "text": "Linkages between Eurasian snow cover and Northern Hemisphere winter-time climate variability Recently it has been shown that the Eurasian snow cover in the prior autumn (ESCSON) and the leading mode variability in the wintertime extratropical Northern Hemisphere (NH) atmospheric circulation have significant correlation. In this study, a linkage between the ESCSON and the following wintertime NH climate variability was investigated. Satellite data from the NOAA is used for snow cover, and NCEP/NCAR Reanalysis data are used for climate variables. The high latitudes sea-level pressure is quality-controlled by use of the IABP sea-level pressure dataset, which is derived from the buoy observations. Interannual variability of and association between ESCSON and winter climate variables were surveyed by use of linear statistical analysis techniques; Empirical Orthogonal Function (EOF) analysis, and correlation/regression analysis. The gravity current by the expansion of the cold, dense air over Siberia north- and westward remained one among the several possible mechanisms. The upper air mechanism may be active to connect the ESCSON and the leading mode of DJF surface pressure variability. It is also suggested that the DJF sea-level pressure variations associated with the ESCSON is considerably confined to the Atlantic side, and has only limited association with the linear trend and the Pacific side variations. Future work may include reexamination of the results using the possible, longer data of the observation. The mechanism connecting the ESCSON anomalies and the upper level circulation anomaly should be investigated further, for which one possible approach is analysis of the wave activity and energy propagation in the troposphere and stratosphere."
}, {
    "id": "oai:dspace.mit.edu:1721.1/54506",
    "title": "Kinematically consistent, elastic block model for the eastern Mediterranean constrained by GPS measurements",
    "abstract": "I use a Global Positioning System (GPS) velocity field to constrain block models of the eastern Mediterranean and surrounding regions that account for the angular velocities of constituent blocks and elastic strain accumulation on block-bounding faults in the interseismic period. Kinematically consistent fault slip rates and locking depths are estimated by this method. Eleven blocks are considered, including the major plates, based largely on previous geodetic, seismic, and geologic studies: Eurasia (EU), Nubia (NU), Arabia (AR), Anatolia (AN), Caucasus (CA), South Aegea (AE), Central Greece (GR), North Aegea (NE), Southeast Aegea (SE), Macedonia (MA), and Adria (AD). Two models are presented, one in which the best-fitting locking depth for the entire region (-15 km) is used on all boundaries (Model A), and one in which shallower locking depths are used on the Marmara Fault, the Hellenic and Cyprus Arcs, and in the Greater Caucasus (Model B), based on a consideration of locally best-fitting locking depths. An additional block, Black Sea (BS), is postulated in a third model. The models are in fair to good agreement with the results of previous studies of plate motion, fault slip rates, seismic moment rates and paleomagnetic rotations. Notably, some block pairs in the Aegean region have Euler poles on, or near to, their common boundaries, in qualitative agreement with so-called pinned block models, e.g., for the transfer of slip from the right-lateral North Anatolian Fault system to a set of left-lateral and normal faults in central and northern Greece (McKenzie and Jackson, 1983; Taymaz et al., 1991a; Goldsworthy et al., 2002).",
    "advisors": ["Bradford H. Hager"],
    "text": "Kinematically consistent, elastic block model for the eastern Mediterranean constrained by GPS measurements I use a Global Positioning System (GPS) velocity field to constrain block models of the eastern Mediterranean and surrounding regions that account for the angular velocities of constituent blocks and elastic strain accumulation on block-bounding faults in the interseismic period. Kinematically consistent fault slip rates and locking depths are estimated by this method. Eleven blocks are considered, including the major plates, based largely on previous geodetic, seismic, and geologic studies: Eurasia (EU), Nubia (NU), Arabia (AR), Anatolia (AN), Caucasus (CA), South Aegea (AE), Central Greece (GR), North Aegea (NE), Southeast Aegea (SE), Macedonia (MA), and Adria (AD). Two models are presented, one in which the best-fitting locking depth for the entire region (-15 km) is used on all boundaries (Model A), and one in which shallower locking depths are used on the Marmara Fault, the Hellenic and Cyprus Arcs, and in the Greater Caucasus (Model B), based on a consideration of locally best-fitting locking depths. An additional block, Black Sea (BS), is postulated in a third model. The models are in fair to good agreement with the results of previous studies of plate motion, fault slip rates, seismic moment rates and paleomagnetic rotations. Notably, some block pairs in the Aegean region have Euler poles on, or near to, their common boundaries, in qualitative agreement with so-called pinned block models, e.g., for the transfer of slip from the right-lateral North Anatolian Fault system to a set of left-lateral and normal faults in central and northern Greece (McKenzie and Jackson, 1983; Taymaz et al., 1991a; Goldsworthy et al., 2002)."
}, {
    "id": "oai:dspace.mit.edu:1721.1/119987",
    "title": "Influence of eddy-eddy interactions and tropical wind variability on sudden stratospheric warming formation",
    "abstract": "This thesis investigates the effects of eddy-eddy interactions (EEI) and tropical wind variability on sudden stratospheric warming (SSW) formation in an idealized atmospheric GCM. Chapter 2 introduces a method to produce split and displacement SSWs in comparable amounts using either wavenumber 1 or 2 tropospheric heating perturbations. The results are compared to those obtained with wavenumber 2 topographic forcing. It is shown that the fraction of SSWs forced by anomalously strong tropospheric wave flux in the model is similar to that of SSWs in the observed atmosphere, but that the fractions for splits and displacements are different. Furthermore, a large fraction of SSWs occur without significant anomalous tropospheric wave flux, indicating that stratospheric transmission of climatological tropospheric wave flux plays an important role in SSW formation. Chapter 3 investigates the effects of EEI on SSW formation in the model by reproducing the model runs from Chapter 2 with EEI turned off in parts of the atmosphere. It is found that SSW frequencies can be strongly dependent on EEI throughout the atmosphere, but that EEI are required locally for splits and displacements to occur. Significant changes in SSW frequencies are obtained by turning off EEI locally, without changing the lower stratospheric wave forcing. Chapter 3 shows that while SSW formation can be considered a wave-mean flow interaction to first order, higher order processes are required to accurately reproduce both SSW frequencies and dynamics. The wavenumber 2 heating run used in Chapters 2 and 3 produce spontaneous tropical wind oscillations in the stratosphere. Chapter 4 identifies the source of these oscillations, and investigates the effects of the oscillations on the stratospheric polar vortex. Model runs with suppressed tropical wind variability are compared to the control run of Chapter 2. A slight increase in SSW frequency can be found in the model runs with suppressed tropical variability. It is found that upper stratospheric equatorial wind anomalies are strongly correlated with polar vortex strength, and hypothesized that westerly equatorial wind anomalies in the upper stratosphere can reinforce the conditions that lead to an anomalously strong polar vortex. A mechanism explaining this influence is presented.",
    "advisors": ["R. Alan Plumb"],
    "text": "Influence of eddy-eddy interactions and tropical wind variability on sudden stratospheric warming formation This thesis investigates the effects of eddy-eddy interactions (EEI) and tropical wind variability on sudden stratospheric warming (SSW) formation in an idealized atmospheric GCM. Chapter 2 introduces a method to produce split and displacement SSWs in comparable amounts using either wavenumber 1 or 2 tropospheric heating perturbations. The results are compared to those obtained with wavenumber 2 topographic forcing. It is shown that the fraction of SSWs forced by anomalously strong tropospheric wave flux in the model is similar to that of SSWs in the observed atmosphere, but that the fractions for splits and displacements are different. Furthermore, a large fraction of SSWs occur without significant anomalous tropospheric wave flux, indicating that stratospheric transmission of climatological tropospheric wave flux plays an important role in SSW formation. Chapter 3 investigates the effects of EEI on SSW formation in the model by reproducing the model runs from Chapter 2 with EEI turned off in parts of the atmosphere. It is found that SSW frequencies can be strongly dependent on EEI throughout the atmosphere, but that EEI are required locally for splits and displacements to occur. Significant changes in SSW frequencies are obtained by turning off EEI locally, without changing the lower stratospheric wave forcing. Chapter 3 shows that while SSW formation can be considered a wave-mean flow interaction to first order, higher order processes are required to accurately reproduce both SSW frequencies and dynamics. The wavenumber 2 heating run used in Chapters 2 and 3 produce spontaneous tropical wind oscillations in the stratosphere. Chapter 4 identifies the source of these oscillations, and investigates the effects of the oscillations on the stratospheric polar vortex. Model runs with suppressed tropical wind variability are compared to the control run of Chapter 2. A slight increase in SSW frequency can be found in the model runs with suppressed tropical variability. It is found that upper stratospheric equatorial wind anomalies are strongly correlated with polar vortex strength, and hypothesized that westerly equatorial wind anomalies in the upper stratosphere can reinforce the conditions that lead to an anomalously strong polar vortex. A mechanism explaining this influence is presented."
}, {
    "id": "oai:dspace.mit.edu:1721.1/59757",
    "title": "Exploring the distribution and physiological roles of bacterial membrane lipids in the marine environment",
    "abstract": "Lipids have a legacy in the geologic record extending back to the Archaean. Since the phylogenetic diversity of life is reflected in the structural diversity of biomolecules, lipid biomarkers that are shown to be diagnostic of certain organisms that carry out specific biochemical processes or that are demonstrated to have unique physiological roles can be used to trace the biogeochemical influence of bacteria in modern and ancient environments. In this thesis I explore the application of two classes of bacterial membrane lipids as biomarkers for marine biogeochemical processes in marine environments: ladderanes and hopanoids. Through the detection of ladderane lipids - biomarkers for anaerobic ammonium oxidizing (anammox) bacteria - I demonstrate the presence and distribution of anammox bacteria in a subterranean estuary. Through a survey of hopanoids in marine environments and cultured marine cyanobacteria I show that hopanoids are ubiquitous in the oceans and that their presence in ancient marine sediments could provide information about biogeochemical processes in past environments. Based on novel results demonstrating that hopanoids are resistant to extraction by non-ionic detergent, I propose that they may play a role in lipid ordering and the formation of putative lipid rafts in hopanoid-producing bacteria.",
    "advisors": ["Roger E. Summons"],
    "text": "Exploring the distribution and physiological roles of bacterial membrane lipids in the marine environment Lipids have a legacy in the geologic record extending back to the Archaean. Since the phylogenetic diversity of life is reflected in the structural diversity of biomolecules, lipid biomarkers that are shown to be diagnostic of certain organisms that carry out specific biochemical processes or that are demonstrated to have unique physiological roles can be used to trace the biogeochemical influence of bacteria in modern and ancient environments. In this thesis I explore the application of two classes of bacterial membrane lipids as biomarkers for marine biogeochemical processes in marine environments: ladderanes and hopanoids. Through the detection of ladderane lipids - biomarkers for anaerobic ammonium oxidizing (anammox) bacteria - I demonstrate the presence and distribution of anammox bacteria in a subterranean estuary. Through a survey of hopanoids in marine environments and cultured marine cyanobacteria I show that hopanoids are ubiquitous in the oceans and that their presence in ancient marine sediments could provide information about biogeochemical processes in past environments. Based on novel results demonstrating that hopanoids are resistant to extraction by non-ionic detergent, I propose that they may play a role in lipid ordering and the formation of putative lipid rafts in hopanoid-producing bacteria."
}, {
    "id": "oai:dspace.mit.edu:1721.1/69769",
    "title": "A radiocarbon method and multi-tracer approach to quantifying groundwater discharge to coastal waters",
    "abstract": "Groundwater discharge into estuaries and the coastal ocean is an important mechanism for the transport of dissolved chemical species to coastal waters. Because many dissolved species are present in groundwater in concentrations that are orders of magnitude higher than typical river concentrations, groundwater-borne nutrients and pollutants can have a substantial impact on the chemistry and biology of estuaries and the coastal ocean. However, direct fluxes of groundwater into the coastal ocean (submarine groundwater discharge, or SGD) can be difficult to quantify. Geochemical tracers of groundwater discharge can reflect the cumulative SGD flux from numerous small, widely dispersed, and perhaps ephemeral sources such as springs, seeps, and diffuse discharge. The natural radiocarbon content (A14C) of dissolved inorganic carbon (DIC) was developed as a tracer of fresh, terrestrially driven fluxes from confined aquifers. This A14C method was tested during five sampling periods from November 1999 to April 2002 in two small estuaries in southeastern North Carolina. In coastal North Carolina, fresh water artesian discharge is characterized by a low A14C signature acquired from the carbonate aquifer rock. Mixing models were used to evaluate the inputs from potential sources of DIC-A'4C to each estuary, including seawater, springs, fresh water stream inputs, and salt marsh respiration DIC additions. These calculations showed that artesian discharge dominated the total fresh water input to these estuaries during nearly all sampling periods.",
    "advisors": ["Daniel C. McCorkle"],
    "text": "A radiocarbon method and multi-tracer approach to quantifying groundwater discharge to coastal waters Groundwater discharge into estuaries and the coastal ocean is an important mechanism for the transport of dissolved chemical species to coastal waters. Because many dissolved species are present in groundwater in concentrations that are orders of magnitude higher than typical river concentrations, groundwater-borne nutrients and pollutants can have a substantial impact on the chemistry and biology of estuaries and the coastal ocean. However, direct fluxes of groundwater into the coastal ocean (submarine groundwater discharge, or SGD) can be difficult to quantify. Geochemical tracers of groundwater discharge can reflect the cumulative SGD flux from numerous small, widely dispersed, and perhaps ephemeral sources such as springs, seeps, and diffuse discharge. The natural radiocarbon content (A14C) of dissolved inorganic carbon (DIC) was developed as a tracer of fresh, terrestrially driven fluxes from confined aquifers. This A14C method was tested during five sampling periods from November 1999 to April 2002 in two small estuaries in southeastern North Carolina. In coastal North Carolina, fresh water artesian discharge is characterized by a low A14C signature acquired from the carbonate aquifer rock. Mixing models were used to evaluate the inputs from potential sources of DIC-A'4C to each estuary, including seawater, springs, fresh water stream inputs, and salt marsh respiration DIC additions. These calculations showed that artesian discharge dominated the total fresh water input to these estuaries during nearly all sampling periods."
}, {
    "id": "oai:dspace.mit.edu:1721.1/37981",
    "title": "A comparison of logging while drilling (LWD) and wireline acoustic measurements",
    "abstract": "The instruments used to measure borehole acoustic data can be classified as either wireline or logging while drilling (LWD). The wireline tool measures formation speeds after the borehole is drilled, and the LWD tool measures formation speeds while the borehole is drilled. This thesis focuses on comparing the data collected by these tools and how formation properties affect their measurements. LWD and wireline measurements taken from the same borehole are compared. Discrepancies in estimated shear and compressional velocities, as calculated by time semblance methods, were found between the two data sets. We modeled radially layered formations with increasing or decreasing radial velocity profile to estimate the acoustic measurement penetration for each tool. We reprocessed sections of the data using frequency semblance methods and compared with layered model results. We found that a frequency-domain analysis is feasible and reduces the overall difference between the LWD and wireline shear and compressional velocity estimates. The remaining discrepancy can be explained by the different radial depths of penetration of these two tools, which naturally leads to a difference in the velocity estimates when there is a radial gradient in the velocity profile.",
    "advisors": ["M. Nafi Toksz"],
    "text": "A comparison of logging while drilling (LWD) and wireline acoustic measurements The instruments used to measure borehole acoustic data can be classified as either wireline or logging while drilling (LWD). The wireline tool measures formation speeds after the borehole is drilled, and the LWD tool measures formation speeds while the borehole is drilled. This thesis focuses on comparing the data collected by these tools and how formation properties affect their measurements. LWD and wireline measurements taken from the same borehole are compared. Discrepancies in estimated shear and compressional velocities, as calculated by time semblance methods, were found between the two data sets. We modeled radially layered formations with increasing or decreasing radial velocity profile to estimate the acoustic measurement penetration for each tool. We reprocessed sections of the data using frequency semblance methods and compared with layered model results. We found that a frequency-domain analysis is feasible and reduces the overall difference between the LWD and wireline shear and compressional velocity estimates. The remaining discrepancy can be explained by the different radial depths of penetration of these two tools, which naturally leads to a difference in the velocity estimates when there is a radial gradient in the velocity profile."
}, {
    "id": "oai:dspace.mit.edu:1721.1/77785",
    "title": "Determining timescales of natural carbonation of peridotite in the Samail Ophiolite, Sultanate of Oman",
    "abstract": "Determining timescales of the formation and preservation of carbonate alteration products in mantle peridotite is important in order to better understand the role of this potentially important sink in the global carbon cycle and also to evaluate the feasibility of using artificially-enhanced, in situ formation of carbonates in peridotite to mitigate the buildup of anthropogenic CO emissions in the atmosphere. Timescales of natural carbonation of peridotite were investigated in the mantle layer of the Samail Ophiolite, Sultanate of Oman. Rates of ongoing, low-temperature CO uptake were estimated through C and Th dating of carbonate alteration products. Approximately 1-3 x 10 kg CO/yr is sequestered in Ca-rich surface travertines and approximately 10 kg CO/yr is sequestered in Mg-rich carbonate veins. Rates of CO removal were estimated through calculation of maximum erosion rates from cosmogenic 3He measurements in partially-serpentinized peridotite bedrock associated with carbonate alteration products. Maximum erosion rates for serpentinized peridotite bedrock are ~5 to 180 m/Myr (average: ~40 m/Myr), which removes at most 10-10 kg CO/yr through erosion of Mg-rich carbonate veins.",
    "advisors": ["Susan E. Humphris", "Kenneth W. W. Sims"],
    "text": "Determining timescales of natural carbonation of peridotite in the Samail Ophiolite, Sultanate of Oman Determining timescales of the formation and preservation of carbonate alteration products in mantle peridotite is important in order to better understand the role of this potentially important sink in the global carbon cycle and also to evaluate the feasibility of using artificially-enhanced, in situ formation of carbonates in peridotite to mitigate the buildup of anthropogenic CO emissions in the atmosphere. Timescales of natural carbonation of peridotite were investigated in the mantle layer of the Samail Ophiolite, Sultanate of Oman. Rates of ongoing, low-temperature CO uptake were estimated through C and Th dating of carbonate alteration products. Approximately 1-3 x 10 kg CO/yr is sequestered in Ca-rich surface travertines and approximately 10 kg CO/yr is sequestered in Mg-rich carbonate veins. Rates of CO removal were estimated through calculation of maximum erosion rates from cosmogenic 3He measurements in partially-serpentinized peridotite bedrock associated with carbonate alteration products. Maximum erosion rates for serpentinized peridotite bedrock are ~5 to 180 m/Myr (average: ~40 m/Myr), which removes at most 10-10 kg CO/yr through erosion of Mg-rich carbonate veins."
}, {
    "id": "oai:dspace.mit.edu:1721.1/57862",
    "title": "Petrological and rheological controls on volcanism to terrestrial planets",
    "abstract": "Through experimental petrology and geodynamic modeling, processes of melting under thick lithospheres on the Earth and the moon are investigated. Phase equilibrium experiments were carried out on Apollo 14B and 15C picritic glasses (Chapters 5 and 6) and on a Sierran high-potassium lava (Chapter 1). These, along with petrologic modeling of Cascades high alumina olivine tholeiites (Chapter 4), yield information on depths and pressures of melt generation and constraints on source composition. Geodynamic modeling of lithospheric thinning processes, including delamination under the Siberian flood basalts (Chapter 2), gravitational instabilities in the lunar magma ocean cumulates (Chapter 7), and thinning and convection due to giant meteorite impacts (Chapters 3 and 8), has lead to new models for melt production. These studies together show how lithospheric thinning and unusual mantle compositions can lead to melting without calling on unusual mantle potential temperatures, and can explain the volumes and durations of continental flood basalts and lunar mare basalts.",
    "advisors": ["Timothy L. Grove", "Bradford H. Hager"],
    "text": "Petrological and rheological controls on volcanism to terrestrial planets Through experimental petrology and geodynamic modeling, processes of melting under thick lithospheres on the Earth and the moon are investigated. Phase equilibrium experiments were carried out on Apollo 14B and 15C picritic glasses (Chapters 5 and 6) and on a Sierran high-potassium lava (Chapter 1). These, along with petrologic modeling of Cascades high alumina olivine tholeiites (Chapter 4), yield information on depths and pressures of melt generation and constraints on source composition. Geodynamic modeling of lithospheric thinning processes, including delamination under the Siberian flood basalts (Chapter 2), gravitational instabilities in the lunar magma ocean cumulates (Chapter 7), and thinning and convection due to giant meteorite impacts (Chapters 3 and 8), has lead to new models for melt production. These studies together show how lithospheric thinning and unusual mantle compositions can lead to melting without calling on unusual mantle potential temperatures, and can explain the volumes and durations of continental flood basalts and lunar mare basalts."
}, {
    "id": "oai:dspace.mit.edu:1721.1/98669",
    "title": "Theory and application of source independent full wavefield elastic converted phase seismic imaging and velocity analysis",
    "abstract": "The recorded seismic signal contains full information about the source that generated the seismic waves and the path along which the seismic waves travel and interfere with subsurface. However, source information is not an explicit part of the seismic record and thus is a large source of uncertainty in seismic imaging and velocity analysis applications. In this thesis, we develop source-independent methods for seismic imaging, seismic trace interpolation and velocity analysis using the interference between pure (PP and SS) and converted-phase (PS and SP) waves. For seismic imaging, we develop amplitude-balancing source-independent converted-phase seismic imaging conditions and introduce a concept of conversion ratio coefficients to provide a physical and mathematical foundation for source-independent converted-phase (SICP) imaging. For seismic trace interpolation, we develop a scheme for migration/de-migration to suppress migration-based artifacts due to sparse station deployments. For velocity analysis, we present first a source-independent space-lag domain Extended SICP imaging condition (ESICP-IC). Then, we mathematically derive an optimization scheme for source independent converted-phase wave equation migration velocity analysis (SICP-WEMVA). We investigate numerically the stability and convergence of SICP-ICs, SICP interpolation and SICP-WEMVA with synthetic data. Finally, using the developed methodologies, we investigate the subsurface structure of the Hengill geothermal area in Iceland using the abundant micro-seismic activity of the region. The constructed SICP seismic images show detailed subsurface structure of the Hengill area that is well correlated with previous seismic and resistivity studies. Also, we find that the amplitudes of the images are well correlated with a low resistivity region of the geothermal area. The reason for this correlation is not fully understood, but may provide an additional tool for investigation of the Hengill site.",
    "advisors": ["Alison E. Malcolm", "Michael C. Fehler"],
    "text": "Theory and application of source independent full wavefield elastic converted phase seismic imaging and velocity analysis The recorded seismic signal contains full information about the source that generated the seismic waves and the path along which the seismic waves travel and interfere with subsurface. However, source information is not an explicit part of the seismic record and thus is a large source of uncertainty in seismic imaging and velocity analysis applications. In this thesis, we develop source-independent methods for seismic imaging, seismic trace interpolation and velocity analysis using the interference between pure (PP and SS) and converted-phase (PS and SP) waves. For seismic imaging, we develop amplitude-balancing source-independent converted-phase seismic imaging conditions and introduce a concept of conversion ratio coefficients to provide a physical and mathematical foundation for source-independent converted-phase (SICP) imaging. For seismic trace interpolation, we develop a scheme for migration/de-migration to suppress migration-based artifacts due to sparse station deployments. For velocity analysis, we present first a source-independent space-lag domain Extended SICP imaging condition (ESICP-IC). Then, we mathematically derive an optimization scheme for source independent converted-phase wave equation migration velocity analysis (SICP-WEMVA). We investigate numerically the stability and convergence of SICP-ICs, SICP interpolation and SICP-WEMVA with synthetic data. Finally, using the developed methodologies, we investigate the subsurface structure of the Hengill geothermal area in Iceland using the abundant micro-seismic activity of the region. The constructed SICP seismic images show detailed subsurface structure of the Hengill area that is well correlated with previous seismic and resistivity studies. Also, we find that the amplitudes of the images are well correlated with a low resistivity region of the geothermal area. The reason for this correlation is not fully understood, but may provide an additional tool for investigation of the Hengill site."
}, {
    "id": "oai:dspace.mit.edu:1721.1/55331",
    "title": "Radiation and dissipation of internal waves generated by geostrophic motions impinging on small-scale topography",
    "abstract": "Observations and inverse models suggest that small-scale turbulent mixing is enhanced in the Southern Ocean in regions above rough topography. The enhancement extends 1 km above the topography suggesting that mixing is supported by breaking of gravity waves radiated from the ocean bottom. In other regions, gravity wave radiation by bottom topography has been primarily associated with the barotropic tide. In this study, we explore the alternative hypothesis that the enhanced mixing in the Southern Ocean is sustained by internal waves generated by geostrophic motions flowing over bottom topography. Weakly-nonlinear theory is used to describe the internal wave generation and the feedback of the waves on the zonally averaged flow. A major finding is that the waves generated at the ocean bottom at finite inverse Froude numbers drive vigorous inertial oscillations. The wave radiation and dissipation at equilibrium is therefore the result of both geostrophic flow and inertial oscillations and differs substantially from the classical lee wave problem. The theoretical predictions are tested versus two-dimensional and three-dimensional high resolution numerical simulations with parameters representative of the Drake Passage region. Theory and fully nonlinear numerical simulations are used to estimate internal wave radiation from LADCP, CTD and topography data from two regions in the Southern Ocean: Drake Passage and the Southeast Pacific. The results show that radiation and dissipation of internal waves generated by geostrophic motions reproduce the magnitude and distribution of dissipation measured in the region.",
    "advisors": ["Raffaele Ferrari"],
    "text": "Radiation and dissipation of internal waves generated by geostrophic motions impinging on small-scale topography Observations and inverse models suggest that small-scale turbulent mixing is enhanced in the Southern Ocean in regions above rough topography. The enhancement extends 1 km above the topography suggesting that mixing is supported by breaking of gravity waves radiated from the ocean bottom. In other regions, gravity wave radiation by bottom topography has been primarily associated with the barotropic tide. In this study, we explore the alternative hypothesis that the enhanced mixing in the Southern Ocean is sustained by internal waves generated by geostrophic motions flowing over bottom topography. Weakly-nonlinear theory is used to describe the internal wave generation and the feedback of the waves on the zonally averaged flow. A major finding is that the waves generated at the ocean bottom at finite inverse Froude numbers drive vigorous inertial oscillations. The wave radiation and dissipation at equilibrium is therefore the result of both geostrophic flow and inertial oscillations and differs substantially from the classical lee wave problem. The theoretical predictions are tested versus two-dimensional and three-dimensional high resolution numerical simulations with parameters representative of the Drake Passage region. Theory and fully nonlinear numerical simulations are used to estimate internal wave radiation from LADCP, CTD and topography data from two regions in the Southern Ocean: Drake Passage and the Southeast Pacific. The results show that radiation and dissipation of internal waves generated by geostrophic motions reproduce the magnitude and distribution of dissipation measured in the region."
}, {
    "id": "oai:dspace.mit.edu:1721.1/16824",
    "title": "Interannual variability of air-sea fluxes of carbon dioxide and oxygen",
    "abstract": "The currently observed increase in atmospheric CO2 due anthropogenic emissions is substantially slowed by natural processes that incorporate CO2 into the terrestrial biota and the ocean. Year-to-year changes in the CO2 growth rate that exceed variations in the fossil fuel source indicate a significant variability in these global CO2 sinks. However, the enormous complexity of the terrestrial and oceanic biogeochemical systems that absorb atmospheric CO2 makes these sinks extremely difficult to understand and precisely quantify. Many techniques, including the interpretation of the relative changes in atmospheric CO2 and O2/N2, ocean modeling, and atmospheric data inversions, have been employed to estimate the mean and variability of global CO2 sinks. However, uncertainty remains large. The goal of this thesis is to improve understanding of global CO2 sinks by considering (1) the error in the atmospheric O2/N2 partitioning method due to the neglect of interannual variability in the air-sea fluxes of 02, and (2) the interannual variability of the ocean CO2 sink.",
    "advisors": ["John C. Marshall"],
    "text": "Interannual variability of air-sea fluxes of carbon dioxide and oxygen The currently observed increase in atmospheric CO2 due anthropogenic emissions is substantially slowed by natural processes that incorporate CO2 into the terrestrial biota and the ocean. Year-to-year changes in the CO2 growth rate that exceed variations in the fossil fuel source indicate a significant variability in these global CO2 sinks. However, the enormous complexity of the terrestrial and oceanic biogeochemical systems that absorb atmospheric CO2 makes these sinks extremely difficult to understand and precisely quantify. Many techniques, including the interpretation of the relative changes in atmospheric CO2 and O2/N2, ocean modeling, and atmospheric data inversions, have been employed to estimate the mean and variability of global CO2 sinks. However, uncertainty remains large. The goal of this thesis is to improve understanding of global CO2 sinks by considering (1) the error in the atmospheric O2/N2 partitioning method due to the neglect of interannual variability in the air-sea fluxes of 02, and (2) the interannual variability of the ocean CO2 sink."
}, {
    "id": "oai:dspace.mit.edu:1721.1/58531",
    "title": "Rossby waves and two-dimensional turbulence in the presence of a large-scale zonal jet",
    "abstract": "This dissertation represents a theoretical, numerical, and observational study of barotropic waves and turbulence in an inhomogeneous background flow environment. The theoretical aspects of the work are simplified by restricting attention to the two-dimensional doublyperiodic beta-plane, and in nearly every respect to large-scale zonal flows which are barotropically stable (in a normal-mode sense). The role of the flow inhomogeneity is investigated by considering both nonlinear and linear theory of wave, mean-flow interaction; the key concept to emerge is that of induced spectral transfer of conserved wave quantities by the basic-state flow. Along the way, some new nonlinear conservation laws are derived. In the special case examined of a large-scale zonal jet, the wave enstrophy is approximately conserved in a fully nonlinear sense, and the wave, mean-flow interaction may be characterized as an induced spectral transfer of the wave enstrophy along lines of constant zonal wavenumber k. Because of the scale separation, the linear part of the interaction problem can be closed by applying WKB ray-tracing theory. The turbulent dynamics act to smooth the spectral gradients by irreversible mixing of wave enstrophy; their closure is less easily quantified. The theoretical ideas are tested by performing numerical simulation experiments of both the spin-down and forced-dissipative equilibrium variety. In particular, the nature of the wave, mean-flow interaction can be identified by examining the interaction terms as functions of the meridional wavenumber X for fixed k. In so doing one can determine the point at which irreversible nonlinear dynamics take over from reversible linear dynamics; while the latter are characterized by induced transfer of enstrophy along lines of constant k, the former operate by diffusing energy and enstrophy across such contours. Finally the ideas of the thesis are applied to atmospheric data, and the results used to interpret the observed nonlinear spectral fluxes of kinetic energy and of enstrophy, as well as the interaction between the stationary (viz. one-month time-mean) and transient flow components.",
    "advisors": ["Peter B. Rhines"],
    "text": "Rossby waves and two-dimensional turbulence in the presence of a large-scale zonal jet This dissertation represents a theoretical, numerical, and observational study of barotropic waves and turbulence in an inhomogeneous background flow environment. The theoretical aspects of the work are simplified by restricting attention to the two-dimensional doublyperiodic beta-plane, and in nearly every respect to large-scale zonal flows which are barotropically stable (in a normal-mode sense). The role of the flow inhomogeneity is investigated by considering both nonlinear and linear theory of wave, mean-flow interaction; the key concept to emerge is that of induced spectral transfer of conserved wave quantities by the basic-state flow. Along the way, some new nonlinear conservation laws are derived. In the special case examined of a large-scale zonal jet, the wave enstrophy is approximately conserved in a fully nonlinear sense, and the wave, mean-flow interaction may be characterized as an induced spectral transfer of the wave enstrophy along lines of constant zonal wavenumber k. Because of the scale separation, the linear part of the interaction problem can be closed by applying WKB ray-tracing theory. The turbulent dynamics act to smooth the spectral gradients by irreversible mixing of wave enstrophy; their closure is less easily quantified. The theoretical ideas are tested by performing numerical simulation experiments of both the spin-down and forced-dissipative equilibrium variety. In particular, the nature of the wave, mean-flow interaction can be identified by examining the interaction terms as functions of the meridional wavenumber X for fixed k. In so doing one can determine the point at which irreversible nonlinear dynamics take over from reversible linear dynamics; while the latter are characterized by induced transfer of enstrophy along lines of constant k, the former operate by diffusing energy and enstrophy across such contours. Finally the ideas of the thesis are applied to atmospheric data, and the results used to interpret the observed nonlinear spectral fluxes of kinetic energy and of enstrophy, as well as the interaction between the stationary (viz. one-month time-mean) and transient flow components."
}, {
    "id": "oai:dspace.mit.edu:1721.1/98672",
    "title": "The influence of core crystallization and mantle overturn on ancient dynamos",
    "abstract": "This dissertation contributes to three unresolved problems in planetary science regarding potential dynamo action in asteroids, the Moon, and Mars. First, we examine the physical processes active during asteroid core crystallization. We model inward crystallization as well as crystal settling and the compaction of a possible cumulate inner core. We then explore the potential strength and longevity of a dynamo in the planetesimal's early history. We find that cumulate inner core solidification would be capable of sustaining a dynamo during the bulk of solidification, but that there may be insufficient power for a dynamo in an inward dendritic solidification scenario. Next, we explore the origin of the magnetic field recorded in the lunar crust. Evidence suggests a core dynamo existed from 4.2 to 3.56 Ga, and possibly until near present day. Seismic measurements indicate the lunar core is partially solidified. Latent heat and concentrated light elements at the interface of a solidifying inner core could drive outer core convection. We demonstrate that core solidification can account for the observationally inferred duration of the lunar dynamo. However, it cannot explain the magnitude of the recorded magnetic field. A dynamo may also stop and restart due to heat flow fluctuations as convective vigor falls below the threshold for dynamo action. Finally, we examine the early history of the Martian mantle. The solidification of a magma ocean may result in an unstable density profile prone to overturn. A long-wavelength instability could play a role in the stark contrasts observed between the northern and southern hemispheres of Mars, including the dichotomy in crustal thickness and magnetization. However, we find that cumulate overturn in the Martian scenario would likely have occurred with short wavelengths. In an isoviscous model, thermal convection ensues rapidly after overturn; however, when viscosity is temperature dependent, compositional stability suppresses the onset of convection.",
    "advisors": ["Linda Elkins-Tanton"],
    "text": "The influence of core crystallization and mantle overturn on ancient dynamos This dissertation contributes to three unresolved problems in planetary science regarding potential dynamo action in asteroids, the Moon, and Mars. First, we examine the physical processes active during asteroid core crystallization. We model inward crystallization as well as crystal settling and the compaction of a possible cumulate inner core. We then explore the potential strength and longevity of a dynamo in the planetesimal's early history. We find that cumulate inner core solidification would be capable of sustaining a dynamo during the bulk of solidification, but that there may be insufficient power for a dynamo in an inward dendritic solidification scenario. Next, we explore the origin of the magnetic field recorded in the lunar crust. Evidence suggests a core dynamo existed from 4.2 to 3.56 Ga, and possibly until near present day. Seismic measurements indicate the lunar core is partially solidified. Latent heat and concentrated light elements at the interface of a solidifying inner core could drive outer core convection. We demonstrate that core solidification can account for the observationally inferred duration of the lunar dynamo. However, it cannot explain the magnitude of the recorded magnetic field. A dynamo may also stop and restart due to heat flow fluctuations as convective vigor falls below the threshold for dynamo action. Finally, we examine the early history of the Martian mantle. The solidification of a magma ocean may result in an unstable density profile prone to overturn. A long-wavelength instability could play a role in the stark contrasts observed between the northern and southern hemispheres of Mars, including the dichotomy in crustal thickness and magnetization. However, we find that cumulate overturn in the Martian scenario would likely have occurred with short wavelengths. In an isoviscous model, thermal convection ensues rapidly after overturn; however, when viscosity is temperature dependent, compositional stability suppresses the onset of convection."
}, {
    "id": "oai:dspace.mit.edu:1721.1/8013",
    "title": "A laboratory study of the friction behavior of granular materials",
    "abstract": "I report on laboratory experiments designed to investigate the microphysical processes that result in rate- and state-dependent friction behavior and experiments designed to match the boundary conditions used by numerical models of granular friction. The effect of relative humidity (RH from <5% to 100%) is investigated with velocity stepping tests (10-20 tm/s) and slide-hold-slide (SHS) tests (3-1000 s) on 3 mm thick layers of quartz powder, alumina powder, Westerly granite powder, and Westerly granite blocks sheared at 25 MPa normal stress. Powders are conditioned in situ under controlled RH to create new surface area before shearing. A transition from velocity-strengthening to velocity-weakening frictional behavior occurs as RH increases. Frictional healing is negligible at low humidity and increases with increasing RH for both materials. While the coefficient of sliding friction for powders is independent of humidity, bare surface data indicate that sliding friction decreases with increased RH. Normal stress vibrations in SHS tests can add compaction induced granular strengthening, but for constant normal force tests, chemically assisted healing mechanisms control the friction behavior. The chemically assisted contact junction processes can be reduced or turned off at low humidity at room temperature in quartz and alumina. Velocity stepping tests and SHS tests are also performed at different values of applied normal stress (5 to 45 MPa) after pre-conditioning at high normal stress (40 and 35 MPa) for powders and no pre-conditioning for bare surfaces. Time-dependent frictional healing decreases with increasing normal stress.",
    "advisors": ["Chris Marone"],
    "text": "A laboratory study of the friction behavior of granular materials I report on laboratory experiments designed to investigate the microphysical processes that result in rate- and state-dependent friction behavior and experiments designed to match the boundary conditions used by numerical models of granular friction. The effect of relative humidity (RH from <5% to 100%) is investigated with velocity stepping tests (10-20 tm/s) and slide-hold-slide (SHS) tests (3-1000 s) on 3 mm thick layers of quartz powder, alumina powder, Westerly granite powder, and Westerly granite blocks sheared at 25 MPa normal stress. Powders are conditioned in situ under controlled RH to create new surface area before shearing. A transition from velocity-strengthening to velocity-weakening frictional behavior occurs as RH increases. Frictional healing is negligible at low humidity and increases with increasing RH for both materials. While the coefficient of sliding friction for powders is independent of humidity, bare surface data indicate that sliding friction decreases with increased RH. Normal stress vibrations in SHS tests can add compaction induced granular strengthening, but for constant normal force tests, chemically assisted healing mechanisms control the friction behavior. The chemically assisted contact junction processes can be reduced or turned off at low humidity at room temperature in quartz and alumina. Velocity stepping tests and SHS tests are also performed at different values of applied normal stress (5 to 45 MPa) after pre-conditioning at high normal stress (40 and 35 MPa) for powders and no pre-conditioning for bare surfaces. Time-dependent frictional healing decreases with increasing normal stress."
}, {
    "id": "oai:dspace.mit.edu:1721.1/77788",
    "title": "Physiology of multiple sulfur isotope fractionation during microbial sulfate reduction",
    "abstract": "Microbial sulfate reduction (MSR) utilizes sulfate as an electron acceptor and produces sulfide that is depleted in heavy isotopes of sulfur relative to starting sulfate. The fractionation of S-isotopes is commonly used to trace the biogeochemical cycling of sulfur in nature, but a mechanistic understanding of factors that control the range of isotope fractionation is still lacking. This thesis investigates links between the physiology of sulfate reducing bacteria in pure cultures and multiple sulfur isotope (, , 34S, and S) fractionation during MSR in batch and continuous culture experiments. Experiments address the influence of nutrient and electron donor conditions, including organic carbon, nitrogen, and iron, in cultures of a newly isolated marine sulfate reducing bacterium (DMSS-1). An actively growing culture of DMSS-1 produced sulfide depleted in S by 6 to 66%o, depending on the availability and chemistry of organic electron donors. The magnitude of isotope effect correlated well with the cell specific sulfate reduction rate (csSRR), and the largest isotope effects occurred when cultures grew slowly on glucose, a recalcitrant organic substrate. These findings bridge the long-standing discrepancy between the upper limit for S-isotope effect in laboratory cultures and the corresponding observations in nature and indicate that the large (>46 %o) fractionation of S-isotopes does not unambiguously record the oxidative sulfurrecycling. When the availability of iron was limited, the increase in S-isotope fractionation was accompanied by a decrease in the cytochrome c content as well as csSRR. In contrast, growth in nitrogenlimited cultures increased both csSRR and S-isotope fractionation. The influence of individual enzymes and electron carriers involved in sulfate respiration on the fractionation of S-isotopes was also investigated in cultures of mutant strains of Desulfovibrio vulgaris Hildenborough. The mutant lacking Type I tetraheme cytochrome c fractionated S/S ratio 50% greater relative to the wild type. The increasing S-isotope fractionation accompanied the evolution of H2 in the headspace and the decreasing csSRR. These results further demonstrate that the flow of electrons to terminal reductases imparts the primary control on the magnitude of the fractionation of S-isotopes, suggested by culture experiments using DMSS-1.",
    "advisors": ["Tanja Bosak", "Shuhei Ono"],
    "text": "Physiology of multiple sulfur isotope fractionation during microbial sulfate reduction Microbial sulfate reduction (MSR) utilizes sulfate as an electron acceptor and produces sulfide that is depleted in heavy isotopes of sulfur relative to starting sulfate. The fractionation of S-isotopes is commonly used to trace the biogeochemical cycling of sulfur in nature, but a mechanistic understanding of factors that control the range of isotope fractionation is still lacking. This thesis investigates links between the physiology of sulfate reducing bacteria in pure cultures and multiple sulfur isotope (, , 34S, and S) fractionation during MSR in batch and continuous culture experiments. Experiments address the influence of nutrient and electron donor conditions, including organic carbon, nitrogen, and iron, in cultures of a newly isolated marine sulfate reducing bacterium (DMSS-1). An actively growing culture of DMSS-1 produced sulfide depleted in S by 6 to 66%o, depending on the availability and chemistry of organic electron donors. The magnitude of isotope effect correlated well with the cell specific sulfate reduction rate (csSRR), and the largest isotope effects occurred when cultures grew slowly on glucose, a recalcitrant organic substrate. These findings bridge the long-standing discrepancy between the upper limit for S-isotope effect in laboratory cultures and the corresponding observations in nature and indicate that the large (>46 %o) fractionation of S-isotopes does not unambiguously record the oxidative sulfurrecycling. When the availability of iron was limited, the increase in S-isotope fractionation was accompanied by a decrease in the cytochrome c content as well as csSRR. In contrast, growth in nitrogenlimited cultures increased both csSRR and S-isotope fractionation. The influence of individual enzymes and electron carriers involved in sulfate respiration on the fractionation of S-isotopes was also investigated in cultures of mutant strains of Desulfovibrio vulgaris Hildenborough. The mutant lacking Type I tetraheme cytochrome c fractionated S/S ratio 50% greater relative to the wild type. The increasing S-isotope fractionation accompanied the evolution of H2 in the headspace and the decreasing csSRR. These results further demonstrate that the flow of electrons to terminal reductases imparts the primary control on the magnitude of the fractionation of S-isotopes, suggested by culture experiments using DMSS-1."
}, {
    "id": "oai:dspace.mit.edu:1721.1/29929",
    "title": "Evolution of topography in glaciated mountain ranges",
    "abstract": "This thesis examines the response of alpine landscapes to the onset of glaciation. The basic approach is to compare fluvial and glacial landscapes, since it is the change from the former to the latter that accompanies climatic cooling. This allows a detailed evaluation of hypotheses relating climate change to tectonic processes in glaciated mountain belts. Fieldwork was carried out in the eastern Sierra Nevada, California, and the Sangre de Cristo Range, Colorado, alongside digital elevation model analyses in the western US, the Southern Alps of New Zealand, and the Himalaya of northwestern Pakistan. The evidence presented here suggests that the so-called \"chicken-and-egg\" hypothesis is overstated in its appeal to glacial erosion as a major source of relief production and subsequent peak uplift. Glaciers in the eastern Sierra Nevada and the western Sangre de Cristos have redistributed relief, but have produced only modest relief by enlarging drainage basins at the expense of low-relieftopography. Glaciers have lowered valley floors and ridgelines by similar amounts, limiting the amount of \"missing mass\" that can be generated, and causing a decrease in drainage basin relief.",
    "advisors": ["Kelin X. Whipple"],
    "text": "Evolution of topography in glaciated mountain ranges This thesis examines the response of alpine landscapes to the onset of glaciation. The basic approach is to compare fluvial and glacial landscapes, since it is the change from the former to the latter that accompanies climatic cooling. This allows a detailed evaluation of hypotheses relating climate change to tectonic processes in glaciated mountain belts. Fieldwork was carried out in the eastern Sierra Nevada, California, and the Sangre de Cristo Range, Colorado, alongside digital elevation model analyses in the western US, the Southern Alps of New Zealand, and the Himalaya of northwestern Pakistan. The evidence presented here suggests that the so-called \"chicken-and-egg\" hypothesis is overstated in its appeal to glacial erosion as a major source of relief production and subsequent peak uplift. Glaciers in the eastern Sierra Nevada and the western Sangre de Cristos have redistributed relief, but have produced only modest relief by enlarging drainage basins at the expense of low-relieftopography. Glaciers have lowered valley floors and ridgelines by similar amounts, limiting the amount of \"missing mass\" that can be generated, and causing a decrease in drainage basin relief."
}, {
    "id": "oai:dspace.mit.edu:1721.1/30290",
    "title": "The biogeochemistry and residual mean circulation of the southern ocean",
    "abstract": "I develop conceptual models of the biogeochemistry and physical circulation of the Southern Ocean in order to study the air-sea fluxes of trace gases and biological productivity and their potential changes over glacial-interglacial timescales. Mesoscale eddy transfers play a dominant role in the dynamical and tracer balances in the Antarctic Circumpolar Current, and the transport of tracers is driven by the residual mean circulation which is the net effect of the Eulerian mean circulation and the eddy-induced circulation. Using an idealized, zonally averaged model of the ACC, I illustrate the sensitivity of the uptake of transient tracers including CFC11, bomb-[Delta]C and anthropogenic CO to surface wind stress and buoyancy fluxes over the Southern Ocean. The model qualitatively reproduces observed distribution of CFC11 and bomb-[Delta]C , and a suite of sensitivity experiments illustrate the physical processes controlling the rates of the oceanic uptake of these tracers. The sensitivities of the uptake of CFC11 and bomb-[Delta]C are largely different because of the differences in their air-sea equilibration timescales. The uptake of CFC11 is mainly determined by the rates of physical transport in the ocean, and that of bomb-[Delta]C is mainly controlled by the air-sea gas transfer velocity. Anthropogenic CO falls in between these two cases, and the rate of anthropogenic CO uptake is affected by both processes. Biological productivity in the Southern Ocean is characterized with the circum- polar belt of elevated biological productivity, \"Antarctic Circumpolar Productivity Belt\".",
    "advisors": ["John C. Marshall"],
    "text": "The biogeochemistry and residual mean circulation of the southern ocean I develop conceptual models of the biogeochemistry and physical circulation of the Southern Ocean in order to study the air-sea fluxes of trace gases and biological productivity and their potential changes over glacial-interglacial timescales. Mesoscale eddy transfers play a dominant role in the dynamical and tracer balances in the Antarctic Circumpolar Current, and the transport of tracers is driven by the residual mean circulation which is the net effect of the Eulerian mean circulation and the eddy-induced circulation. Using an idealized, zonally averaged model of the ACC, I illustrate the sensitivity of the uptake of transient tracers including CFC11, bomb-[Delta]C and anthropogenic CO to surface wind stress and buoyancy fluxes over the Southern Ocean. The model qualitatively reproduces observed distribution of CFC11 and bomb-[Delta]C , and a suite of sensitivity experiments illustrate the physical processes controlling the rates of the oceanic uptake of these tracers. The sensitivities of the uptake of CFC11 and bomb-[Delta]C are largely different because of the differences in their air-sea equilibration timescales. The uptake of CFC11 is mainly determined by the rates of physical transport in the ocean, and that of bomb-[Delta]C is mainly controlled by the air-sea gas transfer velocity. Anthropogenic CO falls in between these two cases, and the rate of anthropogenic CO uptake is affected by both processes. Biological productivity in the Southern Ocean is characterized with the circum- polar belt of elevated biological productivity, \"Antarctic Circumpolar Productivity Belt\"."
}, {
    "id": "oai:dspace.mit.edu:1721.1/59648",
    "title": "Subsurface imaging with reverse vertical seismic profiles",
    "abstract": "This thesis presents imaging results from a 3D reverse vertical seismic profile (RVSP) dataset measured at a hydrocarbon bearing pinnacle reef in northern Michigan. The study presented many challenges in seismic data processing and imaging, as the survey geometry was unique in several ways. Reverse VSP, which uses seismic sources in a borehole and receivers on the earth's surface, is fairly rare. RVSP in 3D with a random distribution of surface geophones is unprecedented. At the time this data was collected, no commercially available processing tools existed to address this geometry, so a processing scheme had to be developed. The data processing sequence presented in this thesis, which includes amplitude corrections, first break picking, deconvolution, wavefield separation, and application of statics, takes advantage of the repeatible signature of the new downhole source (Paulsson et al., 1998). Since the data can be handled in common-receiver gathers instead of the usual common-source gathers, it can be treated like several single offset VSPs during the processing sequence. Issues related to the 3D geometry and the random distribution of the receiver array need not be addressed until the imaging step. The generalized Radon transform (GRT) migration method of Miller et al. (1987) provides a high resolution image of a portion of the target reef at 4600 feet (1400 meters) depth. The high resolution of the image is largely due to the downhole source, which generated a high powered signal at frequencies up to several hundred Hertz. Another factor in the high resolution of the image is the success of receiver consistent model-based Wiener deconvolution (Haldorsen et al., 1994), possible because the source signature was repeatable. Due to adverse conditions and power system failure, a large portion of the surface array did not record data.",
    "advisors": ["M. Nafi Toksz"],
    "text": "Subsurface imaging with reverse vertical seismic profiles This thesis presents imaging results from a 3D reverse vertical seismic profile (RVSP) dataset measured at a hydrocarbon bearing pinnacle reef in northern Michigan. The study presented many challenges in seismic data processing and imaging, as the survey geometry was unique in several ways. Reverse VSP, which uses seismic sources in a borehole and receivers on the earth's surface, is fairly rare. RVSP in 3D with a random distribution of surface geophones is unprecedented. At the time this data was collected, no commercially available processing tools existed to address this geometry, so a processing scheme had to be developed. The data processing sequence presented in this thesis, which includes amplitude corrections, first break picking, deconvolution, wavefield separation, and application of statics, takes advantage of the repeatible signature of the new downhole source (Paulsson et al., 1998). Since the data can be handled in common-receiver gathers instead of the usual common-source gathers, it can be treated like several single offset VSPs during the processing sequence. Issues related to the 3D geometry and the random distribution of the receiver array need not be addressed until the imaging step. The generalized Radon transform (GRT) migration method of Miller et al. (1987) provides a high resolution image of a portion of the target reef at 4600 feet (1400 meters) depth. The high resolution of the image is largely due to the downhole source, which generated a high powered signal at frequencies up to several hundred Hertz. Another factor in the high resolution of the image is the success of receiver consistent model-based Wiener deconvolution (Haldorsen et al., 1994), possible because the source signature was repeatable. Due to adverse conditions and power system failure, a large portion of the surface array did not record data."
}, {
    "id": "oai:dspace.mit.edu:1721.1/55159",
    "title": "Halogenated 1'-methyl-1,2'-bipyrroles (MBPs) in the Norwestern Atlantic",
    "abstract": "Halogenated 1'-methyl-1,2'-bipyrroles (MBPs) are a distinctive class of marine organic compounds. They are naturally produced, they have a unique carbon structure, they are highly halogenated, and they bioaccumulate in upper trophic levels. MBPs share many characteristics with persistent organic pollutants (POPs), and may prove to be useful natural analogues for these anthropogenic compounds. Further, their unique structure suggests that their biosynthetic organism(s) may have new genes to add to current knowledge of biosynthetic chemistry. The objectives of this dissertation were to further clarify the environmental distribution of MBPs, to examine whether MBPs biomagnify, and to investigate possible origins of these compounds through their stable nitrogen isotopic signatures. Results from these investigations have shown that over 40 highly brominated MBP congeners are present in marine mammals, fish, and squid from the Northwestern Atlantic Ocean. The most abundant MBPs do appear to biomagnify through the food web to reach the concentrations observed in marine mammals. This additional evidence affords greater confidence in the use of MBPs as natural analogues for POPs. However, differences in the environmental chemistry of MBPs and anthropogenic compounds are also evident, and may be due to these compounds' different origins, or to the capacity of degradative enzymes to act upon them.",
    "advisors": ["Christopher M. Reddy"],
    "text": "Halogenated 1'-methyl-1,2'-bipyrroles (MBPs) in the Norwestern Atlantic Halogenated 1'-methyl-1,2'-bipyrroles (MBPs) are a distinctive class of marine organic compounds. They are naturally produced, they have a unique carbon structure, they are highly halogenated, and they bioaccumulate in upper trophic levels. MBPs share many characteristics with persistent organic pollutants (POPs), and may prove to be useful natural analogues for these anthropogenic compounds. Further, their unique structure suggests that their biosynthetic organism(s) may have new genes to add to current knowledge of biosynthetic chemistry. The objectives of this dissertation were to further clarify the environmental distribution of MBPs, to examine whether MBPs biomagnify, and to investigate possible origins of these compounds through their stable nitrogen isotopic signatures. Results from these investigations have shown that over 40 highly brominated MBP congeners are present in marine mammals, fish, and squid from the Northwestern Atlantic Ocean. The most abundant MBPs do appear to biomagnify through the food web to reach the concentrations observed in marine mammals. This additional evidence affords greater confidence in the use of MBPs as natural analogues for POPs. However, differences in the environmental chemistry of MBPs and anthropogenic compounds are also evident, and may be due to these compounds' different origins, or to the capacity of degradative enzymes to act upon them."
}, {
    "id": "oai:dspace.mit.edu:1721.1/55059",
    "title": "Borehole electroseismic phenomena : field measurements and theory",
    "abstract": "A Stoneley wave propagating in a borehole generates a flow of pore fluid in permeable zones intersected by the borehole. In turn, this flow of pore fluid induces a streaming electrical field. This thesis is an experimental and theoretical investigation of the electrical fields induced by Stoneley waves. The main emphasis of this thesis is to understand the electroseismic phenomena that are observed in the field. In the first experiment described in this thesis, we measured Stoneley-wave-induced electrical fields in a borehole drilled through fractured igneous rocks. Analysis of field data confirms that the electrical fields that we measured were induced by fluid flow in fractures. The normalized amplitude of these electrical fields correlated with the fracture density log. In the second experiment, we measured Stoneley-wave-induced electrical fields in several boreholes in vuggy dolomite. In dolomite, the normalized amplitude of the Stoneley-wave-induced electrical field correlates with the porosity of the formation around the borehole. Further, the Stoneley-wave-induced electrical fields have anomalously high amplitudes at an isolated fracture that intersected two boreholes. To explain the experimental results, we developed a theoretical model for the Stoneley-wave-induced electrical fields. According to the model, the normalized amplitude of the Stoneley-wave-induced electrical field is proportional to the porosity and inversely proportional to the pore space tortuosity of a formation around a borehole. Moreover, the amplitude-versus-frequency behavior of this electrical field depends on the permeability of the formation. To further test the theory's prediction, we measured electrical potentials induced by the borehole Stoneley wave in the frequency range from 100Hz to 4kHz. The normalized amplitudes of the Stoneley-wave-induced electrical potentials measured in the field were consistent with the amplitudes predicted by the theory. Also, the amplitude- versus-frequency dependence of the electroseismic signals recorded at the depth of the large fracture roughly followed the trend predicted by the theory. However, the general amplitude-versus-frequency dependence of the electroseismic signals recorded in the field is more complicated than that predicted by the theory. The main contributions of this thesis are: 1. This thesis develops a borehole electroseismic measurement technique and demonstrates that it works in the field. This technique can be used to make electroseismic logging measurements. 2. This thesis investigates an electrical field induced by a borehole Stoneley wave. This electroseismic phenomenon is explained, measured in the field and modeled theoretically. 3. This thesis derives from field data a parameter that describes local electroseismic coupling in a formation around a borehole. This parameter, the normalized amplitude of the Stoneley-wave-induced electrical field, is defined as the ratio of an electrical field amplitude to a pressure amplitude in the Stoneley wave at a certain depth. This thesis demonstrates that the normalized amplitude of the Stoneley-wave- induced electrical field can be used to identify permeable fractures in situ. 4. This thesis uses field electroseismic measurements to quantitatively characterize rock formations around a borehole. Using the theoretical model developed in this thesis, a porosity log for fractured granite is derived from electroseismic field data.",
    "advisors": ["M. Nafi Toksz"],
    "text": "Borehole electroseismic phenomena : field measurements and theory A Stoneley wave propagating in a borehole generates a flow of pore fluid in permeable zones intersected by the borehole. In turn, this flow of pore fluid induces a streaming electrical field. This thesis is an experimental and theoretical investigation of the electrical fields induced by Stoneley waves. The main emphasis of this thesis is to understand the electroseismic phenomena that are observed in the field. In the first experiment described in this thesis, we measured Stoneley-wave-induced electrical fields in a borehole drilled through fractured igneous rocks. Analysis of field data confirms that the electrical fields that we measured were induced by fluid flow in fractures. The normalized amplitude of these electrical fields correlated with the fracture density log. In the second experiment, we measured Stoneley-wave-induced electrical fields in several boreholes in vuggy dolomite. In dolomite, the normalized amplitude of the Stoneley-wave-induced electrical field correlates with the porosity of the formation around the borehole. Further, the Stoneley-wave-induced electrical fields have anomalously high amplitudes at an isolated fracture that intersected two boreholes. To explain the experimental results, we developed a theoretical model for the Stoneley-wave-induced electrical fields. According to the model, the normalized amplitude of the Stoneley-wave-induced electrical field is proportional to the porosity and inversely proportional to the pore space tortuosity of a formation around a borehole. Moreover, the amplitude-versus-frequency behavior of this electrical field depends on the permeability of the formation. To further test the theory's prediction, we measured electrical potentials induced by the borehole Stoneley wave in the frequency range from 100Hz to 4kHz. The normalized amplitudes of the Stoneley-wave-induced electrical potentials measured in the field were consistent with the amplitudes predicted by the theory. Also, the amplitude- versus-frequency dependence of the electroseismic signals recorded at the depth of the large fracture roughly followed the trend predicted by the theory. However, the general amplitude-versus-frequency dependence of the electroseismic signals recorded in the field is more complicated than that predicted by the theory. The main contributions of this thesis are: 1. This thesis develops a borehole electroseismic measurement technique and demonstrates that it works in the field. This technique can be used to make electroseismic logging measurements. 2. This thesis investigates an electrical field induced by a borehole Stoneley wave. This electroseismic phenomenon is explained, measured in the field and modeled theoretically. 3. This thesis derives from field data a parameter that describes local electroseismic coupling in a formation around a borehole. This parameter, the normalized amplitude of the Stoneley-wave-induced electrical field, is defined as the ratio of an electrical field amplitude to a pressure amplitude in the Stoneley wave at a certain depth. This thesis demonstrates that the normalized amplitude of the Stoneley-wave- induced electrical field can be used to identify permeable fractures in situ. 4. This thesis uses field electroseismic measurements to quantitatively characterize rock formations around a borehole. Using the theoretical model developed in this thesis, a porosity log for fractured granite is derived from electroseismic field data."
}, {
    "id": "oai:dspace.mit.edu:1721.1/45765",
    "title": "Reservoir monitoring using induced seismicity at a petroleum field in Oman",
    "abstract": "This thesis presents methods of analysis and results from a petroleum reservoir monitoring application using induced seismicity data. The dissertation work is comprised of four major studies, each focusing on a different aspect of induced seismicity. First, methods and issues in location of induced seismicity are discussed, and strategies are proposed for determining accurate hypocenters of induced events. The importance of velocity model and how it impacts the depth determination of reservoir-induced events are demonstrated with real field data. A location method that is better-suited than other existing methods for this application is proposed. The accuracy and efficiency of this proposed location method is demonstrated with field data application.Secondly, event locations and interpretations are presented for about 1300 induced events recorded by a near-surface network over a seven year period at a petroleum field in Oman. The event hypocenters delineate two major northeast-southwest striking faults in the field, which are consistent with fault maps derived from reflection seismic interpretations. Spatiotemporal analysis of induced event locations reveals ongoing large scale fault reactivation in the field, and also suggests compaction of the gas reservoir as the major cause and water injection in the oil reservoir as the secondary cause for inducing seismicity in the field.Thirdly, event locations and interpretations are presented for a different set of induced seismicity data recorded by a deep-borehole network over an 11-month period at the same field. About 5400 events are located and analyzed, and detailed mapping of faults and fractures using the event hypocenters are presented.",
    "advisors": ["M. Nafi Toksz"],
    "text": "Reservoir monitoring using induced seismicity at a petroleum field in Oman This thesis presents methods of analysis and results from a petroleum reservoir monitoring application using induced seismicity data. The dissertation work is comprised of four major studies, each focusing on a different aspect of induced seismicity. First, methods and issues in location of induced seismicity are discussed, and strategies are proposed for determining accurate hypocenters of induced events. The importance of velocity model and how it impacts the depth determination of reservoir-induced events are demonstrated with real field data. A location method that is better-suited than other existing methods for this application is proposed. The accuracy and efficiency of this proposed location method is demonstrated with field data application.Secondly, event locations and interpretations are presented for about 1300 induced events recorded by a near-surface network over a seven year period at a petroleum field in Oman. The event hypocenters delineate two major northeast-southwest striking faults in the field, which are consistent with fault maps derived from reflection seismic interpretations. Spatiotemporal analysis of induced event locations reveals ongoing large scale fault reactivation in the field, and also suggests compaction of the gas reservoir as the major cause and water injection in the oil reservoir as the secondary cause for inducing seismicity in the field.Thirdly, event locations and interpretations are presented for a different set of induced seismicity data recorded by a deep-borehole network over an 11-month period at the same field. About 5400 events are located and analyzed, and detailed mapping of faults and fractures using the event hypocenters are presented."
}, {
    "id": "oai:dspace.mit.edu:1721.1/87510",
    "title": "Arc magmatism at different crustal levels, North Cascades, WA",
    "abstract": "The mechanisms of magma ascent and emplacement inferred from study of intrusive complexes have long been the subject of intense debate. Current models favor incremental construction, but much of this work has been focused on a single crustal level. However, the study of magmatism throughout the crust is critical for understanding how magma ascends through and intrudes surrounding crustal material. I present new field, geochronologic and geochemical data from three intrusive complexes emplaced at a range of crustal depths in the Cretaceous North Cascades magmatic arc. Integration of geological mapping and high-precision U-Pb TIMS geochronology allows me to demonstrate variable styles of intrusion in different complexes: Assembly of the Black Peak intrusive complex occurred via a series of small (<1 km 3) magmatic increments from ca. 91.7 Ma to 86.8 Ma. My data indicate each of these increments were emplaced and solidified without major assimilation of country rock. The Seven- Fingered Jack intrusive complex, emplaced around ~20-25 km, preserves a similar record of intrusion between ca. 91.8 Ma and 90.5 Ma. Significant compositional variability and antecrystic zircons suggest that the Seven-Fingered Jack represents the remnants of mid-crustal magmatic conduit. Geochronology from the deep-crustal (~25-30 km) Tenpeak intrusive complex, intruded between ca. 92.2 Ma and 89.5 Ma, suggests that plutons comprising the complex were assembled rapidly (<300 ka). These intrusive complexes represent different parts of an arc system, including deep- and shallow-crustal intrusions and a magmatic conduit. I propose a model where increments of magma migrated through the crust in magmatic conduits that were active multiple times. These conduits focused rising magma and served as a crustal filter. Geochronologic and isotopic variability between the Tenpeak and Black Peak intrusive complexes are likely a result of this filtering process and the vastly different conditions in the deep and shallow crust.",
    "advisors": ["Samuel A. Bowring"],
    "text": "Arc magmatism at different crustal levels, North Cascades, WA The mechanisms of magma ascent and emplacement inferred from study of intrusive complexes have long been the subject of intense debate. Current models favor incremental construction, but much of this work has been focused on a single crustal level. However, the study of magmatism throughout the crust is critical for understanding how magma ascends through and intrudes surrounding crustal material. I present new field, geochronologic and geochemical data from three intrusive complexes emplaced at a range of crustal depths in the Cretaceous North Cascades magmatic arc. Integration of geological mapping and high-precision U-Pb TIMS geochronology allows me to demonstrate variable styles of intrusion in different complexes: Assembly of the Black Peak intrusive complex occurred via a series of small (<1 km 3) magmatic increments from ca. 91.7 Ma to 86.8 Ma. My data indicate each of these increments were emplaced and solidified without major assimilation of country rock. The Seven- Fingered Jack intrusive complex, emplaced around ~20-25 km, preserves a similar record of intrusion between ca. 91.8 Ma and 90.5 Ma. Significant compositional variability and antecrystic zircons suggest that the Seven-Fingered Jack represents the remnants of mid-crustal magmatic conduit. Geochronology from the deep-crustal (~25-30 km) Tenpeak intrusive complex, intruded between ca. 92.2 Ma and 89.5 Ma, suggests that plutons comprising the complex were assembled rapidly (<300 ka). These intrusive complexes represent different parts of an arc system, including deep- and shallow-crustal intrusions and a magmatic conduit. I propose a model where increments of magma migrated through the crust in magmatic conduits that were active multiple times. These conduits focused rising magma and served as a crustal filter. Geochronologic and isotopic variability between the Tenpeak and Black Peak intrusive complexes are likely a result of this filtering process and the vastly different conditions in the deep and shallow crust."
}, {
    "id": "oai:dspace.mit.edu:1721.1/58445",
    "title": "Investigations of cloud altering effects of atmospheric aerosols using a new mixed Eulerian-Lagrangian aerosol model",
    "abstract": "Industry, urban development, and other anthropogenic influences have substantially altered the composition and size-distribution of atmospheric aerosol particles over the last century. This, in turn, has altered cloud albedo, lifetime, and patterns which together are thought to exert a negative radiative forcing on the climate; these are the indirect effects of atmospheric aerosols. The specifics of the process by which aerosol particles seed cloud particles are complex and highly uncertain. The goal of this thesis is to refine understanding of the role of various aerosol types in determining cloud properties. We approach this goal by constructing a new highly detailed aerosol-cloud process model that is designed to simulate condensation upon complex aerosol populations. We use this model to investigate the microphysics of aerosol-cloud interactions, specifically considering the role of cloud dynamics and of the ubiquitous mixed soot / sulfate aerosols. We describe the Mixed Eulerian-Lagrangian Aerosol Model (MELAM). This new computer model of aerosol microphysics is specifically tailored to simulate condensation and activation as accurately as possible. It specifically calculates aerosol thermodynamics, condensation, coagulation, gas and aqueous phase chemistry, and dissolution. The model is able to consider inorganic aerosols and aerosols with both inorganics and insoluble cores; the specific chemical system to be considered is specified by the user in text input files. Aerosol particles may be represented using \"sectional distributions\" or using a \"representative sample\" distribution which tracks individual particles.",
    "advisors": ["Ronald G. Prinn"],
    "text": "Investigations of cloud altering effects of atmospheric aerosols using a new mixed Eulerian-Lagrangian aerosol model Industry, urban development, and other anthropogenic influences have substantially altered the composition and size-distribution of atmospheric aerosol particles over the last century. This, in turn, has altered cloud albedo, lifetime, and patterns which together are thought to exert a negative radiative forcing on the climate; these are the indirect effects of atmospheric aerosols. The specifics of the process by which aerosol particles seed cloud particles are complex and highly uncertain. The goal of this thesis is to refine understanding of the role of various aerosol types in determining cloud properties. We approach this goal by constructing a new highly detailed aerosol-cloud process model that is designed to simulate condensation upon complex aerosol populations. We use this model to investigate the microphysics of aerosol-cloud interactions, specifically considering the role of cloud dynamics and of the ubiquitous mixed soot / sulfate aerosols. We describe the Mixed Eulerian-Lagrangian Aerosol Model (MELAM). This new computer model of aerosol microphysics is specifically tailored to simulate condensation and activation as accurately as possible. It specifically calculates aerosol thermodynamics, condensation, coagulation, gas and aqueous phase chemistry, and dissolution. The model is able to consider inorganic aerosols and aerosols with both inorganics and insoluble cores; the specific chemical system to be considered is specified by the user in text input files. Aerosol particles may be represented using \"sectional distributions\" or using a \"representative sample\" distribution which tracks individual particles."
}, {
    "id": "oai:dspace.mit.edu:1721.1/59738",
    "title": "Biogeochemical proxies for environmental and biotic conditions at the Permian-Triassic boundary",
    "abstract": "The extinction at the Permian-Triassic boundary marked one of the most profound events of the Phanerozoic Eon. Although numerous hypotheses have been proposed, the trigger mechanism continues to be debated. This thesis intends to examine the impact of oceanic conditions on the extinction event by analyzing hydrocarbon biomarkers. Hydrocarbon biomarkers are chemical fossils in sedimentary rocks that serve as proxies to measure the conditions that prevailed during deposition. In this thesis, biomarkers for redox conditions, depositional environment, microbial community and potential age-related biomarkers have been measured and are reported from four sections that span the Permian-Triassic boundary. The first section, from the Peace River Basin in modern-day western Canada, was deposited on the eastern margin of the Panthalassic Ocean and samples conditions in this global water body. The second section is from Kap Stosch, Greenland, and was deposited on the southern margin of an epicontinental sea situated in the northwest of the supercontinent Pangaea. The Great Bank of Guizhou, China is the third section studied, and it is a carbonate platform deposited on the southern edge of one of the smaller continental blocks that formed the eastern margin of the Paleo-Tethys Ocean. The fourth section, for the Permian-Triassic boundary is from Meishan, China, the type section for this boundary, and was deposited on the western margin of another one of the continental blocks at the edge of the Paleo-Tethys Ocean. The biomarker evidence from these sections was measured in ratios, absolute abundances and for 613C isotopic values. This evidence points to global marine conditions dominated by bacterial inputs in which photic zone euxinia was prevalent for extended time periods. Additional findings from compound-specific isotope data suggest that at isolated intervals, the chemocine may have extended even closer to the surface. The timing of these intervals implies that ocean conditions may have affected the extinction itself.",
    "advisors": ["Roger E. Summons"],
    "text": "Biogeochemical proxies for environmental and biotic conditions at the Permian-Triassic boundary The extinction at the Permian-Triassic boundary marked one of the most profound events of the Phanerozoic Eon. Although numerous hypotheses have been proposed, the trigger mechanism continues to be debated. This thesis intends to examine the impact of oceanic conditions on the extinction event by analyzing hydrocarbon biomarkers. Hydrocarbon biomarkers are chemical fossils in sedimentary rocks that serve as proxies to measure the conditions that prevailed during deposition. In this thesis, biomarkers for redox conditions, depositional environment, microbial community and potential age-related biomarkers have been measured and are reported from four sections that span the Permian-Triassic boundary. The first section, from the Peace River Basin in modern-day western Canada, was deposited on the eastern margin of the Panthalassic Ocean and samples conditions in this global water body. The second section is from Kap Stosch, Greenland, and was deposited on the southern margin of an epicontinental sea situated in the northwest of the supercontinent Pangaea. The Great Bank of Guizhou, China is the third section studied, and it is a carbonate platform deposited on the southern edge of one of the smaller continental blocks that formed the eastern margin of the Paleo-Tethys Ocean. The fourth section, for the Permian-Triassic boundary is from Meishan, China, the type section for this boundary, and was deposited on the western margin of another one of the continental blocks at the edge of the Paleo-Tethys Ocean. The biomarker evidence from these sections was measured in ratios, absolute abundances and for 613C isotopic values. This evidence points to global marine conditions dominated by bacterial inputs in which photic zone euxinia was prevalent for extended time periods. Additional findings from compound-specific isotope data suggest that at isolated intervals, the chemocine may have extended even closer to the surface. The timing of these intervals implies that ocean conditions may have affected the extinction itself."
}, {
    "id": "oai:dspace.mit.edu:1721.1/58363",
    "title": "The vertical structure of stratospheric planetary waves and its variability : theory and observations",
    "abstract": "Observations of the vertical structure of stratospheric planetary waves reveal a large variety of structures, and a variability both on seasonal and on daily time scales. The extent to which linear wave theory explains these structures and their time evolution at a given time or season is not well known. The sensitivity of linear wave models to details of the basic state and model damping, both of which are not determined from observations in great accuracy, makes it hard to determine why the observations deviate from modeled waves in any given case. In addition, the ability of the observations to resolve the vertical structure of planetary waves is not obvious, given the low vertical resolution of satellite retrievals. The goal of this thesis is to understand the sources of observed variability of vertical wave structure, in particular, to determine whether linear wave theory can explain this variability, and whether the observations are capable of resolving it. We start by testing the ability of satellite retrievals to resolve the vertical structure of the waves. We calculate the radiances that a virtual satellite sitting at the top of our model atmosphere would see, and invert them to obtain retrieved temperature fields. The comparison to the model temperatures suggests that the retrievals are able to resolve their general features quite well, with a few exceptions. Above 1.5mb there is little observed information in the retrievals, and errors start growing above 5 mb. Also, small scale features are not resolvable, but most waves have large enough vertical wavelengths to be resolved. We also identify dynamic situations in the real atmosphere which are more prone to retrieval errors. These are mostly relevant to summer or to the breakup of the polar vortex, when the existence of critical surfaces may cause the waves to have sharp features. The next part consists of understanding the relation between vertical wave structure and the wave propagation characteristics of the basic state in a series of linear wave models, both steady state and time dependent. We study the normal modes on a one dimensional (vertical) troposphere-stratosphere system, using a framework of wave geometry, which allows us to generalize the results to many basic states. A large variety of vertical wave structures is found, similar to observed. This variety is due to the existence of stratospheric turning points. We extend these results to basic states that vary in latitude and height in a nonseparable way. The main problem is how to separate the wave propagation into the vertical and meridional directions. Our approach is diagnostic, where we calculate meridional and vertical wavenumbers from the steady state wave solution to a given basic state, and use them as a diagnostic of the basic state wave propagation characteristics. In particular, we are able to determine the location of turning surfaces for meridional and vertical propagation. By applying this wavenumber diagnostic to many model runs we show that the existence of a stratospheric waveguide renders the problem qualitatively one dimensional by determining the meridional wavenumber, regardless of the characteristics of the tropospheric forcing. In particular, the effects of damping and turning surfaces on the vertical structure are qualitatively as in the vertical propagation problem. In a complementary study, we regard the waves as consisting of many wave activity packets that propagate from the troposphere through the stratosphere, until they dissipate. A technique that follows a wave packet on its journey through the stratosphere, while keeping track of variations in wave activity that are due to refraction of the waves, is introduced and applied to the model runs. This allows us to separate between the contributions to the wave activity budget of damping, refraction, and time variations in the source of wave activity. Also, we can estimate the time scale for vertical propagation through the stratosphere of specific wave events. Finally, we use our diagnostics to study observed wave episodes. We show that the differences in vertical wave structure between middle and late winter episodes in the southern hemisphere can be explained as a linear response to the seasonal evolution of the basic state wave propagation characteristics. We also show that the occasional daily time scale variations of vertical wave structure within a given wave episode are qualitatively a linear response to time variations of the basic state wave propagation characteristics. Since the basic state variations are wave driven, the relevant theory is quasi-linear. Estimates of wave propagation time scales, obtained using our wave activity diagnostic, are also consistent with the theory. We take this is a qualitative assessment of the applicability of quasi-linear wave propagation theory on daily time scales, as well as an assessment of the observations of the waves and the basic state. The latter is not obvious since most of the relevant variations in the basic state occur above 5mb, where observations are less accurate.",
    "advisors": ["Richard S. Lindzen"],
    "text": "The vertical structure of stratospheric planetary waves and its variability : theory and observations Observations of the vertical structure of stratospheric planetary waves reveal a large variety of structures, and a variability both on seasonal and on daily time scales. The extent to which linear wave theory explains these structures and their time evolution at a given time or season is not well known. The sensitivity of linear wave models to details of the basic state and model damping, both of which are not determined from observations in great accuracy, makes it hard to determine why the observations deviate from modeled waves in any given case. In addition, the ability of the observations to resolve the vertical structure of planetary waves is not obvious, given the low vertical resolution of satellite retrievals. The goal of this thesis is to understand the sources of observed variability of vertical wave structure, in particular, to determine whether linear wave theory can explain this variability, and whether the observations are capable of resolving it. We start by testing the ability of satellite retrievals to resolve the vertical structure of the waves. We calculate the radiances that a virtual satellite sitting at the top of our model atmosphere would see, and invert them to obtain retrieved temperature fields. The comparison to the model temperatures suggests that the retrievals are able to resolve their general features quite well, with a few exceptions. Above 1.5mb there is little observed information in the retrievals, and errors start growing above 5 mb. Also, small scale features are not resolvable, but most waves have large enough vertical wavelengths to be resolved. We also identify dynamic situations in the real atmosphere which are more prone to retrieval errors. These are mostly relevant to summer or to the breakup of the polar vortex, when the existence of critical surfaces may cause the waves to have sharp features. The next part consists of understanding the relation between vertical wave structure and the wave propagation characteristics of the basic state in a series of linear wave models, both steady state and time dependent. We study the normal modes on a one dimensional (vertical) troposphere-stratosphere system, using a framework of wave geometry, which allows us to generalize the results to many basic states. A large variety of vertical wave structures is found, similar to observed. This variety is due to the existence of stratospheric turning points. We extend these results to basic states that vary in latitude and height in a nonseparable way. The main problem is how to separate the wave propagation into the vertical and meridional directions. Our approach is diagnostic, where we calculate meridional and vertical wavenumbers from the steady state wave solution to a given basic state, and use them as a diagnostic of the basic state wave propagation characteristics. In particular, we are able to determine the location of turning surfaces for meridional and vertical propagation. By applying this wavenumber diagnostic to many model runs we show that the existence of a stratospheric waveguide renders the problem qualitatively one dimensional by determining the meridional wavenumber, regardless of the characteristics of the tropospheric forcing. In particular, the effects of damping and turning surfaces on the vertical structure are qualitatively as in the vertical propagation problem. In a complementary study, we regard the waves as consisting of many wave activity packets that propagate from the troposphere through the stratosphere, until they dissipate. A technique that follows a wave packet on its journey through the stratosphere, while keeping track of variations in wave activity that are due to refraction of the waves, is introduced and applied to the model runs. This allows us to separate between the contributions to the wave activity budget of damping, refraction, and time variations in the source of wave activity. Also, we can estimate the time scale for vertical propagation through the stratosphere of specific wave events. Finally, we use our diagnostics to study observed wave episodes. We show that the differences in vertical wave structure between middle and late winter episodes in the southern hemisphere can be explained as a linear response to the seasonal evolution of the basic state wave propagation characteristics. We also show that the occasional daily time scale variations of vertical wave structure within a given wave episode are qualitatively a linear response to time variations of the basic state wave propagation characteristics. Since the basic state variations are wave driven, the relevant theory is quasi-linear. Estimates of wave propagation time scales, obtained using our wave activity diagnostic, are also consistent with the theory. We take this is a qualitative assessment of the applicability of quasi-linear wave propagation theory on daily time scales, as well as an assessment of the observations of the waves and the basic state. The latter is not obvious since most of the relevant variations in the basic state occur above 5mb, where observations are less accurate."
}, {
    "id": "oai:dspace.mit.edu:1721.1/8233",
    "title": "Extensional evolution of the central East Greenland Caledonides",
    "abstract": "This thesis addresses the complexity of both syn- and post-orogenic extension in the overriding plate during Caledonian continental collision through field and laboratory investigations in the central East Greenland Caledonides. During the course of this work, attempts were made to answer some of the outstanding regional and local questions in East Greenland geology. Structural, U-Pb and Ar/Ar geochronologic, petrographic and thermobarometric data were combined to constrain and reconstruct a portion of the tectonic history of this orogen. Most extension was accommodated along a system of orogen-parallel, N-S striking normal faults known as the Fjord Region Detachment (FRD) system. The FRD system comprises two temporally distinct, but overlapping, splays just south of 73 N. The lowermost splay is called the Hogedal detachment was active from ca. 417 to 380 Ma, and was active for a second time as recently as ca. 357 Ma. The uppermost splay is the Tindern detachment. This fault was active from ca. 425-423 Ma, exhuming material at rates as fast as 6.5 mm/year. Continued extension in the hanging-wall of this fault accounts for additional denudation at much slower rates over a 25 my time-period. In-between activity on these faults, there is evidence to suggest that middle-crustal thickening continued to occur. Thus, the East Greenland Caledonides preserve evidence for crustal thickening (minimum -16 km) and orogen parallel shear, followed by rapid upper-middle crustal thinning (-13 km), followed by coeval middle-crustal thickening (unknown amount) and upper-crustal thinning (5 km), and ending with crustal collapse (-16 km thinning).",
    "advisors": ["Kip V. Hodges"],
    "text": "Extensional evolution of the central East Greenland Caledonides This thesis addresses the complexity of both syn- and post-orogenic extension in the overriding plate during Caledonian continental collision through field and laboratory investigations in the central East Greenland Caledonides. During the course of this work, attempts were made to answer some of the outstanding regional and local questions in East Greenland geology. Structural, U-Pb and Ar/Ar geochronologic, petrographic and thermobarometric data were combined to constrain and reconstruct a portion of the tectonic history of this orogen. Most extension was accommodated along a system of orogen-parallel, N-S striking normal faults known as the Fjord Region Detachment (FRD) system. The FRD system comprises two temporally distinct, but overlapping, splays just south of 73 N. The lowermost splay is called the Hogedal detachment was active from ca. 417 to 380 Ma, and was active for a second time as recently as ca. 357 Ma. The uppermost splay is the Tindern detachment. This fault was active from ca. 425-423 Ma, exhuming material at rates as fast as 6.5 mm/year. Continued extension in the hanging-wall of this fault accounts for additional denudation at much slower rates over a 25 my time-period. In-between activity on these faults, there is evidence to suggest that middle-crustal thickening continued to occur. Thus, the East Greenland Caledonides preserve evidence for crustal thickening (minimum -16 km) and orogen parallel shear, followed by rapid upper-middle crustal thinning (-13 km), followed by coeval middle-crustal thickening (unknown amount) and upper-crustal thinning (5 km), and ending with crustal collapse (-16 km thinning)."
}, {
    "id": "oai:dspace.mit.edu:1721.1/42920",
    "title": "Organic geochemical biosignatures in alkaline Hydrothermal ecosystems",
    "abstract": "The 13C content of microbial products are controlled by many factors, including the 13C content of the growth substrate, growth rate, the flux of carbon through various parts of the biochemical network, and the isotopic fractionation imposed by the enzymes of that network. We analyzed the 13C content of products of the methanogen Methanosarcina barkeri and found that fractionation varied strongly with substrate availability.These results inform our analysis of methanogen lipids from carbonates of the Lost City Hydrothermal Field. This ultramafic ecosystem produces methane highly enriched in 13C relative to most biotic methane. We find that the 13C enrichment in methanogen lipids is even stronger -- demonstrating that the Methanosarcinales in active vents are methane producers, and that they are likely carbon-limited. Archaea in other parts of the vent field at Lost City are methanotrophs. The application of lipid biomarkers helps unravel the multiple biological and abiotic sources of methane at Lost City.Closer examination of lipids from Lost City shows that most are ether-type glycolipids. The dominance of glycolipids over phospholipids may be a phosphorus-conservation strategy in waters that are likely phosphorus-poor. Ether core lipids are similar to those produced by sulfate-reducing bacteria in environments where methane is oxidized anaerobically. Insoluble residues in Lost City carbonates contain proteinaceous organic material and have end-member d15N values near 0 0/00, suggesting active nitrogen fixation is occurring.Biomass and lipids from Yellowstone hot springs also showed surprising enrichments in 13C. The common factor is high pH; unusual 13C enrichment may be common in alkaline hydrothermal systems.Organisms in terrestrial and marine alkaline hydrothermal systems produced organic carbon with d13C outside of the usual biological range.",
    "advisors": ["Roger E. Summons"],
    "text": "Organic geochemical biosignatures in alkaline Hydrothermal ecosystems The 13C content of microbial products are controlled by many factors, including the 13C content of the growth substrate, growth rate, the flux of carbon through various parts of the biochemical network, and the isotopic fractionation imposed by the enzymes of that network. We analyzed the 13C content of products of the methanogen Methanosarcina barkeri and found that fractionation varied strongly with substrate availability.These results inform our analysis of methanogen lipids from carbonates of the Lost City Hydrothermal Field. This ultramafic ecosystem produces methane highly enriched in 13C relative to most biotic methane. We find that the 13C enrichment in methanogen lipids is even stronger -- demonstrating that the Methanosarcinales in active vents are methane producers, and that they are likely carbon-limited. Archaea in other parts of the vent field at Lost City are methanotrophs. The application of lipid biomarkers helps unravel the multiple biological and abiotic sources of methane at Lost City.Closer examination of lipids from Lost City shows that most are ether-type glycolipids. The dominance of glycolipids over phospholipids may be a phosphorus-conservation strategy in waters that are likely phosphorus-poor. Ether core lipids are similar to those produced by sulfate-reducing bacteria in environments where methane is oxidized anaerobically. Insoluble residues in Lost City carbonates contain proteinaceous organic material and have end-member d15N values near 0 0/00, suggesting active nitrogen fixation is occurring.Biomass and lipids from Yellowstone hot springs also showed surprising enrichments in 13C. The common factor is high pH; unusual 13C enrichment may be common in alkaline hydrothermal systems.Organisms in terrestrial and marine alkaline hydrothermal systems produced organic carbon with d13C outside of the usual biological range."
}, {
    "id": "oai:dspace.mit.edu:1721.1/104593",
    "title": "Hydrous melt generation in the Earth's mantle",
    "abstract": "This thesis focuses on quantifying the role of HO in the generation and modification of mantle melts at shallow pressures in subduction zones. In the first and second chapters, two experimental studies are presented that investigate direct mantle melting and then subsequent reaction of deeper mantle melts with overlying, cooler depleted mantle. In Chapter 1, the melting behavior of an olivine + orthopyroxene +/- spinel - bearing fertile mantle composition is investigated as a function of variable pressure and water content. The experimental results are used to calibrate a model that can predict the pressure and temperature or the temperature and HO content of last equilibration for mantle melts that were in equilibrium with olivine orthopyroxene +/- spinel. In Chapter 2, reaction experiments were conducted to explore the role of melt - rock reaction in the shallow part of the mantle wedge. Results demonstrate the importance of both the temperature of the overlying mantle and the amount of infiltrating melt on the mantle lithology that remains after reaction. Reaction coefficients are calculated to quantify the experimental results. In Chapter 3, HO solubility was experimentally determined at upper mantle pressures. The 1.0 GPa result is the first HO solubility determination in basalt at any pressure above 0.6 GPa. The final chapter is a modeling study that shows how and when to correct for low pressure fractional crystallization to get lavas back to equilibrium with the mantle. Terms are calibrated (in part, on experiments presented in Chapter 1) that add HO to spinel lherzolite multiple saturation point models as well as to thermometers and barometers. The HO correction provides the quantitative and qualitative basis for making low pressure fractional crystallization corrections to near-primitive hydrous lavas. All chapters contribute to the understanding of subduction zone magmatism, with a particular emphasis on processes in the shallowest region of the mantle wedge.",
    "advisors": ["Timothy L. Grove"],
    "text": "Hydrous melt generation in the Earth's mantle This thesis focuses on quantifying the role of HO in the generation and modification of mantle melts at shallow pressures in subduction zones. In the first and second chapters, two experimental studies are presented that investigate direct mantle melting and then subsequent reaction of deeper mantle melts with overlying, cooler depleted mantle. In Chapter 1, the melting behavior of an olivine + orthopyroxene +/- spinel - bearing fertile mantle composition is investigated as a function of variable pressure and water content. The experimental results are used to calibrate a model that can predict the pressure and temperature or the temperature and HO content of last equilibration for mantle melts that were in equilibrium with olivine orthopyroxene +/- spinel. In Chapter 2, reaction experiments were conducted to explore the role of melt - rock reaction in the shallow part of the mantle wedge. Results demonstrate the importance of both the temperature of the overlying mantle and the amount of infiltrating melt on the mantle lithology that remains after reaction. Reaction coefficients are calculated to quantify the experimental results. In Chapter 3, HO solubility was experimentally determined at upper mantle pressures. The 1.0 GPa result is the first HO solubility determination in basalt at any pressure above 0.6 GPa. The final chapter is a modeling study that shows how and when to correct for low pressure fractional crystallization to get lavas back to equilibrium with the mantle. Terms are calibrated (in part, on experiments presented in Chapter 1) that add HO to spinel lherzolite multiple saturation point models as well as to thermometers and barometers. The HO correction provides the quantitative and qualitative basis for making low pressure fractional crystallization corrections to near-primitive hydrous lavas. All chapters contribute to the understanding of subduction zone magmatism, with a particular emphasis on processes in the shallowest region of the mantle wedge."
}, {
    "id": "oai:dspace.mit.edu:1721.1/113793",
    "title": "Understanding the chemistry of atmospheric particles using single particle mass spectrometry",
    "abstract": "This thesis explores ways in which single particle mass spectrometry can be extended, whether through hardware improvements, or through the use of advanced data processing techniques to provide new kinds of aerosol chemistry measurements. Most of this work has been carried out using the Particle Analysis by Laser Mass Spectrometry (PALMS) instrument, an aircraft deployable mass spectrometer that uses intense (~10 9 Wcm -2 ) UV laser pulses to vaporize and ionize single particles and measures their mass spectra using a time-of-flight mass spectrometer. Near-term and long-term hardware improvements as well as advanced data analysis techniques are explored in order to extract new chemical information from the thus obtained single particle mass spectra. Hardware improvements to PALMS are explored, such as the use of a high-powered femtosecond laser to obtain single particle mass spectra and a new high resolution compact mass analyzer. Also, a new commercial mass spectrometer LAAPToF is characterized and compared to PALMS. In addition to hardware improvements, novel data analysis techniques for analysis of single particle mass spectra were developed as a part of this work. In particular, a new method to identify biologically-derived particles is presented and used to derive vertical profiles of bioaerosol from near-surface to the upper troposphere.",
    "advisors": ["Daniel J. Cziczo"],
    "text": "Understanding the chemistry of atmospheric particles using single particle mass spectrometry This thesis explores ways in which single particle mass spectrometry can be extended, whether through hardware improvements, or through the use of advanced data processing techniques to provide new kinds of aerosol chemistry measurements. Most of this work has been carried out using the Particle Analysis by Laser Mass Spectrometry (PALMS) instrument, an aircraft deployable mass spectrometer that uses intense (~10 9 Wcm -2 ) UV laser pulses to vaporize and ionize single particles and measures their mass spectra using a time-of-flight mass spectrometer. Near-term and long-term hardware improvements as well as advanced data analysis techniques are explored in order to extract new chemical information from the thus obtained single particle mass spectra. Hardware improvements to PALMS are explored, such as the use of a high-powered femtosecond laser to obtain single particle mass spectra and a new high resolution compact mass analyzer. Also, a new commercial mass spectrometer LAAPToF is characterized and compared to PALMS. In addition to hardware improvements, novel data analysis techniques for analysis of single particle mass spectra were developed as a part of this work. In particular, a new method to identify biologically-derived particles is presented and used to derive vertical profiles of bioaerosol from near-surface to the upper troposphere."
}, {
    "id": "oai:dspace.mit.edu:1721.1/69206",
    "title": "Analysis and interpretation of tidal currents in the coastal boundary layer",
    "abstract": "Concern with the impact of human activities on the coastal region of the world's oceans has elicited interest in the so-called \"coastal boundary layer\"-that band of water adjacent to the coast where ocean currents adjust to the presence of a boundary. Within this zone, roughly 10 km wide, several physical processes appear to be important. One of these, the tides, is of particular interest because their deterministic nature allows unusually thorough analysis from short time series, and because they tend to obscure the other processes. The Coastal Boundary Layer Transect (COBOLT) experiment was conducted within 12 km of the south shore of Long Island, New York to elucidate the characteristics of the coastal boundary layer in the Middle Atlantic Bight. Analysis of data from this experiment shows that 35% of the kinetic energy of currents averaged over the 30 m depth are due to the semidiurnal and diurnal tides. The tidal ellipses, show considerable vertical structure. Near-surface tidal ellipses rotate in the clockwise direction for semidiurnal and diurnal tides, while near-bottom ellipses rotate in the counterclockwise direction for the semidiurnal tide. The angle between the major axis of the ellipse and the local coastline decreases downward for semidiurnal and increases downward for diurnal tides. The major axis of the tidal ellipse formed from the depth averaged semidiurnal currents is not parallel to the local shoreline  but is oriented at an angle of -15 degrees. This orientation \"tilt\" is a consequence of the onshore flux of energy which is computed to be about 800 watts/m. A constant eddy viscosity model with a slippery bottom boundary condition reproduces the main features observed in the vertical structure of both semidiurnal and diurnal tidal ellipses. Another model employing long, rotational, gravity waves (Sverdrup waves) and an absorbing coastline explains the ellipse orientations and onshore energy flux as a consequence of energy dissipation in shallow water. Finally, an analytical model with realistic topography suggests that tidal dissipation may occur very close (2-3 km) to the shore. Internal tidal oscillations primarily occur at diurnal frequencies in the COBOLT data. Analysis suggests that this energy may be Doppler-shifted to higher frequencies by the mean currents of the coastal region. These motions are trapped to the shore and are almost exclusively first baroclinic mode internal waves.",
    "advisors": ["Gabriel T. Csanady"],
    "text": "Analysis and interpretation of tidal currents in the coastal boundary layer Concern with the impact of human activities on the coastal region of the world's oceans has elicited interest in the so-called \"coastal boundary layer\"-that band of water adjacent to the coast where ocean currents adjust to the presence of a boundary. Within this zone, roughly 10 km wide, several physical processes appear to be important. One of these, the tides, is of particular interest because their deterministic nature allows unusually thorough analysis from short time series, and because they tend to obscure the other processes. The Coastal Boundary Layer Transect (COBOLT) experiment was conducted within 12 km of the south shore of Long Island, New York to elucidate the characteristics of the coastal boundary layer in the Middle Atlantic Bight. Analysis of data from this experiment shows that 35% of the kinetic energy of currents averaged over the 30 m depth are due to the semidiurnal and diurnal tides. The tidal ellipses, show considerable vertical structure. Near-surface tidal ellipses rotate in the clockwise direction for semidiurnal and diurnal tides, while near-bottom ellipses rotate in the counterclockwise direction for the semidiurnal tide. The angle between the major axis of the ellipse and the local coastline decreases downward for semidiurnal and increases downward for diurnal tides. The major axis of the tidal ellipse formed from the depth averaged semidiurnal currents is not parallel to the local shoreline  but is oriented at an angle of -15 degrees. This orientation \"tilt\" is a consequence of the onshore flux of energy which is computed to be about 800 watts/m. A constant eddy viscosity model with a slippery bottom boundary condition reproduces the main features observed in the vertical structure of both semidiurnal and diurnal tidal ellipses. Another model employing long, rotational, gravity waves (Sverdrup waves) and an absorbing coastline explains the ellipse orientations and onshore energy flux as a consequence of energy dissipation in shallow water. Finally, an analytical model with realistic topography suggests that tidal dissipation may occur very close (2-3 km) to the shore. Internal tidal oscillations primarily occur at diurnal frequencies in the COBOLT data. Analysis suggests that this energy may be Doppler-shifted to higher frequencies by the mean currents of the coastal region. These motions are trapped to the shore and are almost exclusively first baroclinic mode internal waves."
}, {
    "id": "oai:dspace.mit.edu:1721.1/113795",
    "title": "Mapping exoplanet clouds and albedo from phase curves and spectra",
    "abstract": "This thesis uses planetary albedo models to investigate variations in visible wavelength phase curves of exoplanets. We improve upon existing exoplanet giant planet albedo models and incorporate exoplanet general circulation models to analyze the composition and occurrence of clouds on tidally locked exoplanets. We confirm that non-uniform cloud coverage on the dayside of tidally locked exoplanets will affect the magnitude and location of the maximum of the phase curve. We then apply the models to the exoplanet Kepler-7b and consider the effect of varying cloud species, sedimentation efficiency, particle size, and cloud altitude. In the context of Kepler Space Telescope observations, we show that the cloud compositions and spatial distributions can be constrained. We also investigate exoplanet HD189733b, modeling its clouds, albedo and phase curves. We create 3D maps of cloud formation and analyze how cloud composition, spatial distribution, and temperature dependence affects albedo spectra for HD189733b. We use the modeled cloud patterns of spatially-varying composition and temperature to determine the observable albedo spectra and phase curves for HD189733b by fitting to the observations of Berdyugina et al. (2011), Evans el al. (2013), and Wiktorowicz et al. (2015). We show that these integrated albedo and general circulation models enable us to model non-uniform reflectivity due to exoplanet clouds, and to better interpret observations.",
    "advisors": ["Kerri Cahoy"],
    "text": "Mapping exoplanet clouds and albedo from phase curves and spectra This thesis uses planetary albedo models to investigate variations in visible wavelength phase curves of exoplanets. We improve upon existing exoplanet giant planet albedo models and incorporate exoplanet general circulation models to analyze the composition and occurrence of clouds on tidally locked exoplanets. We confirm that non-uniform cloud coverage on the dayside of tidally locked exoplanets will affect the magnitude and location of the maximum of the phase curve. We then apply the models to the exoplanet Kepler-7b and consider the effect of varying cloud species, sedimentation efficiency, particle size, and cloud altitude. In the context of Kepler Space Telescope observations, we show that the cloud compositions and spatial distributions can be constrained. We also investigate exoplanet HD189733b, modeling its clouds, albedo and phase curves. We create 3D maps of cloud formation and analyze how cloud composition, spatial distribution, and temperature dependence affects albedo spectra for HD189733b. We use the modeled cloud patterns of spatially-varying composition and temperature to determine the observable albedo spectra and phase curves for HD189733b by fitting to the observations of Berdyugina et al. (2011), Evans el al. (2013), and Wiktorowicz et al. (2015). We show that these integrated albedo and general circulation models enable us to model non-uniform reflectivity due to exoplanet clouds, and to better interpret observations."
}, {
    "id": "oai:dspace.mit.edu:1721.1/113796",
    "title": "The role of wavenumber one and two in the development of sudden stratospheric warmings",
    "abstract": "In this thesis, we investigate the effects of planetary waves one and two on the polar stratosphere during boreal winter. We use MERRA reanalysis data and the FMS shallow-water model to compare and contrast their propagation into the stratosphere, their interactions within the stratosphere, and their effects on the polar vortex. The results have implications for the predictability of sudden stratospheric warmings (SSWs), theories on the developments of vortex splits and the role of zonal winds in the tropics. In Chapter 2, we use correlations and regressions to demonstrate that the tropopause affects wavenumber one amplitudes more than wavenumber two. Thus, the statistical predictability of SSWs, based on synoptic events in the mid-troposphere (e.g. blockings), is limited. Composites of extreme heat fluxes reveal that they are likely caused by linear interference of the climatology and anomalies. The phases of anomalous planetary waves align with the climatology only during the largest heat fluxes. In Chapter 3, the effect of wave-wave interactions within the stratosphere is quantified by analyzing eddy energy budgets. The energy transfer from wavenumber one toward wavenumber two plays a key role in the vortex split in January 2013 and several other SSWs. This mechanism might explain the growth of wavenumber two in the stratosphere in nonresonant conditions. However, wave-wave interactions are small in averages over all splits since 1979 suggesting that different processes can lead to vortex splits and that the common SSW definitions do not capture the timing of planetary wave growth. In Chapter 4, we employ a shallow-water model to isolate the effects of wave one and two on the polar vortex over a large range of forcing amplitudes and vortex strengths. We are able to simulate SSW splits, which are unequivocally caused by wave-wave interactions. Furthermore, the initial response of the polar vortex depends strongly on the wavenumber of the forcing.",
    "advisors": ["R. Alan Plumb"],
    "text": "The role of wavenumber one and two in the development of sudden stratospheric warmings In this thesis, we investigate the effects of planetary waves one and two on the polar stratosphere during boreal winter. We use MERRA reanalysis data and the FMS shallow-water model to compare and contrast their propagation into the stratosphere, their interactions within the stratosphere, and their effects on the polar vortex. The results have implications for the predictability of sudden stratospheric warmings (SSWs), theories on the developments of vortex splits and the role of zonal winds in the tropics. In Chapter 2, we use correlations and regressions to demonstrate that the tropopause affects wavenumber one amplitudes more than wavenumber two. Thus, the statistical predictability of SSWs, based on synoptic events in the mid-troposphere (e.g. blockings), is limited. Composites of extreme heat fluxes reveal that they are likely caused by linear interference of the climatology and anomalies. The phases of anomalous planetary waves align with the climatology only during the largest heat fluxes. In Chapter 3, the effect of wave-wave interactions within the stratosphere is quantified by analyzing eddy energy budgets. The energy transfer from wavenumber one toward wavenumber two plays a key role in the vortex split in January 2013 and several other SSWs. This mechanism might explain the growth of wavenumber two in the stratosphere in nonresonant conditions. However, wave-wave interactions are small in averages over all splits since 1979 suggesting that different processes can lead to vortex splits and that the common SSW definitions do not capture the timing of planetary wave growth. In Chapter 4, we employ a shallow-water model to isolate the effects of wave one and two on the polar vortex over a large range of forcing amplitudes and vortex strengths. We are able to simulate SSW splits, which are unequivocally caused by wave-wave interactions. Furthermore, the initial response of the polar vortex depends strongly on the wavenumber of the forcing."
}, {
    "id": "oai:dspace.mit.edu:1721.1/111690",
    "title": "The geochemistry of methane isotopologues",
    "abstract": "This thesis documents the origin, distribution, and fate of methane and several of its isotopic forms on Earth. Using observational, experimental, and theoretical approaches, I illustrate how the relative abundances of CH, CH, CHD, and CHD record the formation, transport, and breakdown of methane in selected settings. Chapter 2 reports precise determinations of CHD, a \"clumped\" isotopologue of methane, in samples collected from various settings representing many of the major sources and reservoirs of methane on Earth. The results show that the information encoded by the abundance of CHD enables differentiation of methane generated by microbial, thermogenic, and abiogenic processes. A strong correlation between clumped- and hydrogen-isotope signatures in microbial methane is identified and quantitatively linked to the availability of H and the reversibility of microbially-mediated methanogenesis in the environment. Determination of CHD in combination with hydrogen-isotope ratios of methane and water provides a sensitive indicator of the extent of C-H bond equilibration, enables fingerprinting of methane-generating mechanisms, and in some cases, supplies direct constraints for locating the waters from which migrated gases were sourced. Chapter 3 applies this concept to constrain the origin of methane in hydrothermal fluids from sediment-poor vent fields hosted in mafic and ultramafic rocks on slow- and ultraslow-spreading mid-ocean ridges. The data support a hypogene model whereby methane forms abiotically within plutonic rocks of the oceanic crust at temperatures above ca. 300 C during respeciation of magmatic volatiles, and is subsequently extracted during active, convective hydrothermal circulation. Chapter 4 presents the results of culture experiments in which methane is oxidized in the presence of O by the bacterium Methylococcus capsulatus strain Bath. The results show that the clumped isotopologue abundances of partially-oxidized methane can be predicted from knowledge of C/C and D/H isotope fractionation factors alone.",
    "advisors": ["Shuhei Ono"],
    "text": "The geochemistry of methane isotopologues This thesis documents the origin, distribution, and fate of methane and several of its isotopic forms on Earth. Using observational, experimental, and theoretical approaches, I illustrate how the relative abundances of CH, CH, CHD, and CHD record the formation, transport, and breakdown of methane in selected settings. Chapter 2 reports precise determinations of CHD, a \"clumped\" isotopologue of methane, in samples collected from various settings representing many of the major sources and reservoirs of methane on Earth. The results show that the information encoded by the abundance of CHD enables differentiation of methane generated by microbial, thermogenic, and abiogenic processes. A strong correlation between clumped- and hydrogen-isotope signatures in microbial methane is identified and quantitatively linked to the availability of H and the reversibility of microbially-mediated methanogenesis in the environment. Determination of CHD in combination with hydrogen-isotope ratios of methane and water provides a sensitive indicator of the extent of C-H bond equilibration, enables fingerprinting of methane-generating mechanisms, and in some cases, supplies direct constraints for locating the waters from which migrated gases were sourced. Chapter 3 applies this concept to constrain the origin of methane in hydrothermal fluids from sediment-poor vent fields hosted in mafic and ultramafic rocks on slow- and ultraslow-spreading mid-ocean ridges. The data support a hypogene model whereby methane forms abiotically within plutonic rocks of the oceanic crust at temperatures above ca. 300 C during respeciation of magmatic volatiles, and is subsequently extracted during active, convective hydrothermal circulation. Chapter 4 presents the results of culture experiments in which methane is oxidized in the presence of O by the bacterium Methylococcus capsulatus strain Bath. The results show that the clumped isotopologue abundances of partially-oxidized methane can be predicted from knowledge of C/C and D/H isotope fractionation factors alone."
}, {
    "id": "oai:dspace.mit.edu:1721.1/58062",
    "title": "A laboratory study of localized boundary mixing in a rotating stratified fluid",
    "abstract": "Oceanic observations indicate that abyssal mixing is localized in regions of rough topography. How locally mixed fluid interacts with the ambient fluid is an open question. Laboratory experiments explore the interaction of mechanically induced boundary mixing and an interior body of linearly stratified rotating fluid. Turbulence is generated by a vertically oscillating horizontal bar, located at middepth along the tank wall. The turbulence forms a region of mixed fluid which quickly reaches a steady state height and collapses into the interior. The mixed layer thickness ... is independent of the Coriolis frequency f. N is the buoyancy frequency, co is the bar frequency, and the constant, Y=1 cm, is empirically determined by bar mechanics. In initial experiments, the bar is exposed on three sides. Mixed fluid intrudes directly into the interior as a radial front of uniform height, rather than as a boundary current. Mixed fluid volume grows linearly with time ... The circulation patterns suggest a model of unmixed fluid being laterally entrained with velocity, e Nhm, into the sides of a turbulent zone with height hm and width Lf ... where Lf is an equilibrium scale associated with rotational control of bar-generated turbulence. In accord with the model, outflux is constant, independent of stratification and restricted by rotation ... Later experiments investigate the role of lateral entrainment by confining the sides of the mixing bar between two walls, forming a channel open to the basin at one end. A small percentage of exported fluid enters a boundary current, but the bulk forms a cyclonic circulation in front of the bar. As the recirculation region expands to fill the channel, it restricts horizontal entrainment into the turbulent zone. The flux of mixed fluid decays with time.",
    "advisors": ["Karl R. Helfrich"],
    "text": "A laboratory study of localized boundary mixing in a rotating stratified fluid Oceanic observations indicate that abyssal mixing is localized in regions of rough topography. How locally mixed fluid interacts with the ambient fluid is an open question. Laboratory experiments explore the interaction of mechanically induced boundary mixing and an interior body of linearly stratified rotating fluid. Turbulence is generated by a vertically oscillating horizontal bar, located at middepth along the tank wall. The turbulence forms a region of mixed fluid which quickly reaches a steady state height and collapses into the interior. The mixed layer thickness ... is independent of the Coriolis frequency f. N is the buoyancy frequency, co is the bar frequency, and the constant, Y=1 cm, is empirically determined by bar mechanics. In initial experiments, the bar is exposed on three sides. Mixed fluid intrudes directly into the interior as a radial front of uniform height, rather than as a boundary current. Mixed fluid volume grows linearly with time ... The circulation patterns suggest a model of unmixed fluid being laterally entrained with velocity, e Nhm, into the sides of a turbulent zone with height hm and width Lf ... where Lf is an equilibrium scale associated with rotational control of bar-generated turbulence. In accord with the model, outflux is constant, independent of stratification and restricted by rotation ... Later experiments investigate the role of lateral entrainment by confining the sides of the mixing bar between two walls, forming a channel open to the basin at one end. A small percentage of exported fluid enters a boundary current, but the bulk forms a cyclonic circulation in front of the bar. As the recirculation region expands to fill the channel, it restricts horizontal entrainment into the turbulent zone. The flux of mixed fluid decays with time."
}, {
    "id": "oai:dspace.mit.edu:1721.1/58394",
    "title": "Teleseismic imaging of the Slave craton and implications for the assembly of cratonic lithosphere",
    "abstract": "In this dissertation, I investigate the assembly and evolution of the Archean cratonic lithosphere by using two complementary seismological methods to image the lithospheric structure of the Slave craton in Canada. First, I perform surface wave tomographic inversions to constrain the depth dependence of Rayleigh wave phase velocity, shear wave velocity, and azimuthal anisotropy of the Slave cratonic lithosphere. The tomographic images reveal high shear wave velocities associated with a particularly depleted, cold, and unperturbed Archean cratonic lithosphere. Furthermore, the inversions reveal distinct anisotropic domains in the crust, the lithospheric mantle, and the sub-lithospheric mantle. These results reflect the evolutionary history of the cratonic lithosphere. Secondly, I image seismic discontinuities in the lithosphere using receiver-function analysis of converted P-to-S waves. The resulting seismic profile shows a pronounced low velocity discontinuity at -100 km depth beneath the central Slave craton. This seismic discontinuity shows striking spatial correlation with both an electrical conductive anomaly derived from magnetotelluric sounding, as well as a petrologically-defined ultra-depleted layer. The synthesis of coincident seismic, electrical, and petrological evidence supports that this geophysical and petrological boundary represents a compositional interface marked by alteration minerals. I suggest that this mineralization resulted from relict metasomatism associated with an Archean subduction event, which played an important role in the assembly of the Slave craton. Finally, to improve the efficiency and automation of receiver function calculation and data preprocessing workflow, I develop an application of an array-conditioned deconvolution technique for effectively processing large amounts of seismic array data. I demonstrate that this technique is readily applicable to teleseismic array data. This technique is especially effective in turning noisy traces from earthquakes with smaller magnitudes into usable data.",
    "advisors": ["Stphane Rondenay"],
    "text": "Teleseismic imaging of the Slave craton and implications for the assembly of cratonic lithosphere In this dissertation, I investigate the assembly and evolution of the Archean cratonic lithosphere by using two complementary seismological methods to image the lithospheric structure of the Slave craton in Canada. First, I perform surface wave tomographic inversions to constrain the depth dependence of Rayleigh wave phase velocity, shear wave velocity, and azimuthal anisotropy of the Slave cratonic lithosphere. The tomographic images reveal high shear wave velocities associated with a particularly depleted, cold, and unperturbed Archean cratonic lithosphere. Furthermore, the inversions reveal distinct anisotropic domains in the crust, the lithospheric mantle, and the sub-lithospheric mantle. These results reflect the evolutionary history of the cratonic lithosphere. Secondly, I image seismic discontinuities in the lithosphere using receiver-function analysis of converted P-to-S waves. The resulting seismic profile shows a pronounced low velocity discontinuity at -100 km depth beneath the central Slave craton. This seismic discontinuity shows striking spatial correlation with both an electrical conductive anomaly derived from magnetotelluric sounding, as well as a petrologically-defined ultra-depleted layer. The synthesis of coincident seismic, electrical, and petrological evidence supports that this geophysical and petrological boundary represents a compositional interface marked by alteration minerals. I suggest that this mineralization resulted from relict metasomatism associated with an Archean subduction event, which played an important role in the assembly of the Slave craton. Finally, to improve the efficiency and automation of receiver function calculation and data preprocessing workflow, I develop an application of an array-conditioned deconvolution technique for effectively processing large amounts of seismic array data. I demonstrate that this technique is readily applicable to teleseismic array data. This technique is especially effective in turning noisy traces from earthquakes with smaller magnitudes into usable data."
}, {
    "id": "oai:dspace.mit.edu:1721.1/58533",
    "title": "Dynamics of the equatorial undercurrent and its determination",
    "abstract": "This study focuses on the zonal weakening, eastern termination and seasonal variations of the Atlantic equatorial undercurrent (EUC). The main and most original contribution of the dissertation is a detailed analysis of the Atlantic EUC simulated by Philander and Pacanowski's (1986)general circulation model (GCM), which provides a novel description of the dynamical regimes governing various regions of a nonlinear stratified undercurrent. Only in a narrow deep western region of the simulation does one find an approximately inertial regime corresponding to zonal acceleration. Elsewhere frictional processes cannot be ignored. The bulk of the mid-basin model EUC terminates in the overlying westward surface flow while only a small fraction (the deeper more inertial layers) terminates at the eastern coast. In agreement with observations, a robust feature of the GCM not present in simpler models is the apparent migration of the EUC core from above the thermocline in the west to below it in the east. In the GCM, this happens because the eastward flow is eroded more efficiently by vertical friction above the base of the thermocline than by lateral friction at greater depths. This mechanism is a plausible one for the observed EUC. A scale analysis using a depth scale which decreases with distance eastwards predicts the model zonal transition between western inertial and eastern inertio-frictional regimes. Historical and recent observations and simple models of the equatorial and coastal eastern undercurrents are reviewed, and a new analysis of current measurements in the eastern equatorial Atlantic is presented. Although the measurements are inadequate for definitive conclusions, they suggest that Lukas' (1981) claim of a spring surge of the Pacific EUC to the eastern coast and a seasonal branching of the EUC into a coastal southeastward undercurrent may also hold for the Atlantic Ocean. To improve the agreement between observed and modelled strength of the eastern undercurrent, it is suggested that the eddy coefficient of horizontal mixing should be reduced in future GCM simulations.",
    "advisors": ["Mark Cane, Philip Richardson"],
    "text": "Dynamics of the equatorial undercurrent and its determination This study focuses on the zonal weakening, eastern termination and seasonal variations of the Atlantic equatorial undercurrent (EUC). The main and most original contribution of the dissertation is a detailed analysis of the Atlantic EUC simulated by Philander and Pacanowski's (1986)general circulation model (GCM), which provides a novel description of the dynamical regimes governing various regions of a nonlinear stratified undercurrent. Only in a narrow deep western region of the simulation does one find an approximately inertial regime corresponding to zonal acceleration. Elsewhere frictional processes cannot be ignored. The bulk of the mid-basin model EUC terminates in the overlying westward surface flow while only a small fraction (the deeper more inertial layers) terminates at the eastern coast. In agreement with observations, a robust feature of the GCM not present in simpler models is the apparent migration of the EUC core from above the thermocline in the west to below it in the east. In the GCM, this happens because the eastward flow is eroded more efficiently by vertical friction above the base of the thermocline than by lateral friction at greater depths. This mechanism is a plausible one for the observed EUC. A scale analysis using a depth scale which decreases with distance eastwards predicts the model zonal transition between western inertial and eastern inertio-frictional regimes. Historical and recent observations and simple models of the equatorial and coastal eastern undercurrents are reviewed, and a new analysis of current measurements in the eastern equatorial Atlantic is presented. Although the measurements are inadequate for definitive conclusions, they suggest that Lukas' (1981) claim of a spring surge of the Pacific EUC to the eastern coast and a seasonal branching of the EUC into a coastal southeastward undercurrent may also hold for the Atlantic Ocean. To improve the agreement between observed and modelled strength of the eastern undercurrent, it is suggested that the eddy coefficient of horizontal mixing should be reduced in future GCM simulations."
}, {
    "id": "oai:dspace.mit.edu:1721.1/90680",
    "title": "Numerical modeling, suppression, and imaging of elastic wave scattering by near-surface heterogeneities",
    "abstract": "In arid environments, near-surface complexity and surface topography present major challenges to land seismic data acquisition and processing. These challenges can severely affect data quality and introduce uncertainty into reservoir imaging and characterization. The primary objectives of this thesis are to model and study the contribution of near-surface heterogeneities on seismic wavefield scattering for better understanding of land seismic data, develop an effective approach to filter out the scattered noise from the seismic records to enhance the signal-to-noise ratio, and to accurately image and locate the near-surface heterogeneities. The first part of this thesis is concerned with simulating the effects of seismic wave scattering from buried, shallow, subsurface heterogeneities through finite difference numerical forward modeling. The near-surface scattered wavefield is modeled by separating the incident (i.e., in the absence of scatterers) from the total wavefield by means of a perturbation method. Wave propagation is simulated for several earth models with different near-surface characteristics to isolate and quantify the influence of scattering on the quality of the seismic signal. We show that the scattered field is equivalent to the radiation field of an equivalent elastic source excited at the scatterer locations. Moreover, the scattered waves consist mostly of body waves scattered to surface waves and are, generally, as large as, or larger than, the reflections. These scattered waves often obscure weak primary reflections and can severely degrade the image quality. The results indicate that the scattered energy depends strongly on the properties of the shallow scatterers and increases with increasing impedance contrast, increasing sizes of the scatterers, decreasing depth of the scatterers, and increasing the attenuation factor of the background medium. Also, sources deployed at depth generate weak surface waves, whereas deep receivers record weak scattered surface waves. The analysis and quantified results help in the understanding of the scattering mechanisms and, therefore, can lead to developing new acquisition and processing techniques to reduce the scattered surface waves and enhance the quality of the seismic image. The second part of this thesis develops an approach based on spatially varying local-slope estimation, aiming at alleviating the effects of scattered surface waves and enhancing the quality of the seismic signal. Understanding the mechanism and behavior of the simulated scattered surface waves in the first part of this thesis form the basis for designing the filtering scheme. The algorithm is based on predicting the spatially varying slope of the noise, using steerable filters, and separating the signal and noise components by applying a directional non-linear filter oriented toward the noise direction to predict the noise and then subtract it from the data. The slope estimation step using steerable filters is very efficient, as it requires only a linear combination of a set of basis filters at fixed orientation to synthesize an image filtered at an arbitrary orientation. We apply our filtering approach to simulated data as well as to seismic data recorded in the field to suppress the scattered surface waves from reflected body-waves, and demonstrate its superiority over conventional f - k techniques in signal preservation and noise suppression. The third part of this thesis presents an approach for prestack elastic reverse time migration (RTM) to locate and image near-surface heterogeneities using the near-surface scattered waves (e.g., body to P, S, and surface waves). The approach back-projects the scattered waves until they are in phase with the incident waves at the scatterer locations. The P wave components (divergence of the wavefield) are derived from the spatial derivatives of the measured wavefields. Imaging the near-surface heterogeneities is important for planning seismic surveys or explaining nearsurface related anomalies in the data. The scattered body-to-surface waves travel horizontally along the free surface, and, therefore, they provide optimal illumination of the near-surface compared to scattered body-to-body waves. Additionally, the elastic RTM scheme preserves the relative amplitude because all wave propagation losses, including mode conversions, are properly taken into account. We demonstrate, using synthetic data, that elastic RTM of near-surface scattered waves constructs an accurate and reliable depth image of near-surface scatterers.",
    "advisors": ["M. Nafi Toksz"],
    "text": "Numerical modeling, suppression, and imaging of elastic wave scattering by near-surface heterogeneities In arid environments, near-surface complexity and surface topography present major challenges to land seismic data acquisition and processing. These challenges can severely affect data quality and introduce uncertainty into reservoir imaging and characterization. The primary objectives of this thesis are to model and study the contribution of near-surface heterogeneities on seismic wavefield scattering for better understanding of land seismic data, develop an effective approach to filter out the scattered noise from the seismic records to enhance the signal-to-noise ratio, and to accurately image and locate the near-surface heterogeneities. The first part of this thesis is concerned with simulating the effects of seismic wave scattering from buried, shallow, subsurface heterogeneities through finite difference numerical forward modeling. The near-surface scattered wavefield is modeled by separating the incident (i.e., in the absence of scatterers) from the total wavefield by means of a perturbation method. Wave propagation is simulated for several earth models with different near-surface characteristics to isolate and quantify the influence of scattering on the quality of the seismic signal. We show that the scattered field is equivalent to the radiation field of an equivalent elastic source excited at the scatterer locations. Moreover, the scattered waves consist mostly of body waves scattered to surface waves and are, generally, as large as, or larger than, the reflections. These scattered waves often obscure weak primary reflections and can severely degrade the image quality. The results indicate that the scattered energy depends strongly on the properties of the shallow scatterers and increases with increasing impedance contrast, increasing sizes of the scatterers, decreasing depth of the scatterers, and increasing the attenuation factor of the background medium. Also, sources deployed at depth generate weak surface waves, whereas deep receivers record weak scattered surface waves. The analysis and quantified results help in the understanding of the scattering mechanisms and, therefore, can lead to developing new acquisition and processing techniques to reduce the scattered surface waves and enhance the quality of the seismic image. The second part of this thesis develops an approach based on spatially varying local-slope estimation, aiming at alleviating the effects of scattered surface waves and enhancing the quality of the seismic signal. Understanding the mechanism and behavior of the simulated scattered surface waves in the first part of this thesis form the basis for designing the filtering scheme. The algorithm is based on predicting the spatially varying slope of the noise, using steerable filters, and separating the signal and noise components by applying a directional non-linear filter oriented toward the noise direction to predict the noise and then subtract it from the data. The slope estimation step using steerable filters is very efficient, as it requires only a linear combination of a set of basis filters at fixed orientation to synthesize an image filtered at an arbitrary orientation. We apply our filtering approach to simulated data as well as to seismic data recorded in the field to suppress the scattered surface waves from reflected body-waves, and demonstrate its superiority over conventional f - k techniques in signal preservation and noise suppression. The third part of this thesis presents an approach for prestack elastic reverse time migration (RTM) to locate and image near-surface heterogeneities using the near-surface scattered waves (e.g., body to P, S, and surface waves). The approach back-projects the scattered waves until they are in phase with the incident waves at the scatterer locations. The P wave components (divergence of the wavefield) are derived from the spatial derivatives of the measured wavefields. Imaging the near-surface heterogeneities is important for planning seismic surveys or explaining nearsurface related anomalies in the data. The scattered body-to-surface waves travel horizontally along the free surface, and, therefore, they provide optimal illumination of the near-surface compared to scattered body-to-body waves. Additionally, the elastic RTM scheme preserves the relative amplitude because all wave propagation losses, including mode conversions, are properly taken into account. We demonstrate, using synthetic data, that elastic RTM of near-surface scattered waves constructs an accurate and reliable depth image of near-surface scatterers."
}, {
    "id": "oai:dspace.mit.edu:1721.1/109055",
    "title": "Internal hydraulic jumps with upstream shear",
    "abstract": "Internal hydraulic jumps in flows with upstream shear are investigated numerically and theoretically. The role of upstream shear has not previously been thoroughly investigated, although it is important in many oceanographic flows such as exchange flows and stratified flow over topography. Several two-layer shock joining theories, characterized by their distribution of dissipation in the jump, are considered and extended to include upstream shear, entrainment, and topography. Theoretical results are also compared to 2D and some 3D numerical simulations of the full Navier-Stokes equations, which allow continuous velocity and density distributions. The solution space of idealized jumps with small upstream shear is identified using two-layer theories, which shows that upstream shear allows larger jumps to form and allows jumps for a larger range of parameters. Numerical simulations reveal several jump structures that can occur in these flows, including an undular bore, a fully turbulent jump, and a smooth front turbulent jump. At low shear, the 2D mixing efficiency is constant across simulations. As shear increases, the basic two-layer theories no longer provide solutions. Numerical simulations show that entrainment becomes significant as the shear increases, and adding entrainment and shape parameters to describe the continuous velocity profiles is required to accurately describe the simulations using two-layered theory. The entrainment depends on the upstream shear and can be predicted with a modified theory. However, use of the theory is limited due to its sensitivity to the value of the shape parameters. The 2D mixing efficiency also decreases significantly as shear increases. Finally, more realistic 2D and some 3D simulations including topography bridge the gap between the highly idealized simulations and the very realistic work of others. Simulations with topography show additional jump types, including a higher mode jump with a wedge of homogeneous, stagnant fluid similar to a structure seen in Knight Inlet. In all cases, numerical simulations are used to identify trends in the mixing and jumps structures that can occur in internal hydraulic jumps.",
    "advisors": ["Karl Helfrich"],
    "text": "Internal hydraulic jumps with upstream shear Internal hydraulic jumps in flows with upstream shear are investigated numerically and theoretically. The role of upstream shear has not previously been thoroughly investigated, although it is important in many oceanographic flows such as exchange flows and stratified flow over topography. Several two-layer shock joining theories, characterized by their distribution of dissipation in the jump, are considered and extended to include upstream shear, entrainment, and topography. Theoretical results are also compared to 2D and some 3D numerical simulations of the full Navier-Stokes equations, which allow continuous velocity and density distributions. The solution space of idealized jumps with small upstream shear is identified using two-layer theories, which shows that upstream shear allows larger jumps to form and allows jumps for a larger range of parameters. Numerical simulations reveal several jump structures that can occur in these flows, including an undular bore, a fully turbulent jump, and a smooth front turbulent jump. At low shear, the 2D mixing efficiency is constant across simulations. As shear increases, the basic two-layer theories no longer provide solutions. Numerical simulations show that entrainment becomes significant as the shear increases, and adding entrainment and shape parameters to describe the continuous velocity profiles is required to accurately describe the simulations using two-layered theory. The entrainment depends on the upstream shear and can be predicted with a modified theory. However, use of the theory is limited due to its sensitivity to the value of the shape parameters. The 2D mixing efficiency also decreases significantly as shear increases. Finally, more realistic 2D and some 3D simulations including topography bridge the gap between the highly idealized simulations and the very realistic work of others. Simulations with topography show additional jump types, including a higher mode jump with a wedge of homogeneous, stagnant fluid similar to a structure seen in Knight Inlet. In all cases, numerical simulations are used to identify trends in the mixing and jumps structures that can occur in internal hydraulic jumps."
}, {
    "id": "oai:dspace.mit.edu:1721.1/29052",
    "title": "Interactions between mantle plumes and mid-ocean ridges : constraints from geophysics, geochemistry, and geodynamical modeling",
    "abstract": "This thesis studies interactions between mid-ocean ridges and mantle plumes using geophysics, geochemistry, and geodynamical modeling. Chapter 1 investigates the effects of the Marion and Bouvet hotspots on the ultra-slow spreading, highly-segmented Southwest Indian Ridge (SWIR). Gravity data indicate that both Marion and Bouvet impart high-amplitude mantle Bouguer anomaly lows to the ridge axis, and suggest that long-offset transforms may diminish along-axis plume flow. Building upon this observation, Chapter 2 presents a series of 3D numerical models designed to quantify the sensitivity of along-axis plume-driven mantle flow to transform offset length, spreading rate, and mantle viscosity structure. The calculations illustrate that long-offset transforms in ultra-slow spreading environments may significantly curtail plume dispersion. Chapter 3 investigates helium isotope systematics along the western SWIR as well as near a global array of hotspots. The first part of this study reports uniformly low 3He/4He ratios of 6.3-7.3 R/Ra along the SWIR from 9-24E, compared to values of 8 +/- 1 Ra for normal mid-ocean ridge basalt. The favored explanation for these low values is addition of (U+Th) into the mantle source by crustal and/or lithospheric recycling. Although high He/4He values have been observed along the SWIR near Bouvet Island to the west, there is no evidence for elevated 3He/4He ratios along this section of the SWIR. The second part of Chapter 3 investigates the relationship between 3He/4He ratios and geophysical indicators of plume robustness for nine hotspots.",
    "advisors": ["Jian Lin"],
    "text": "Interactions between mantle plumes and mid-ocean ridges : constraints from geophysics, geochemistry, and geodynamical modeling This thesis studies interactions between mid-ocean ridges and mantle plumes using geophysics, geochemistry, and geodynamical modeling. Chapter 1 investigates the effects of the Marion and Bouvet hotspots on the ultra-slow spreading, highly-segmented Southwest Indian Ridge (SWIR). Gravity data indicate that both Marion and Bouvet impart high-amplitude mantle Bouguer anomaly lows to the ridge axis, and suggest that long-offset transforms may diminish along-axis plume flow. Building upon this observation, Chapter 2 presents a series of 3D numerical models designed to quantify the sensitivity of along-axis plume-driven mantle flow to transform offset length, spreading rate, and mantle viscosity structure. The calculations illustrate that long-offset transforms in ultra-slow spreading environments may significantly curtail plume dispersion. Chapter 3 investigates helium isotope systematics along the western SWIR as well as near a global array of hotspots. The first part of this study reports uniformly low 3He/4He ratios of 6.3-7.3 R/Ra along the SWIR from 9-24E, compared to values of 8 +/- 1 Ra for normal mid-ocean ridge basalt. The favored explanation for these low values is addition of (U+Th) into the mantle source by crustal and/or lithospheric recycling. Although high He/4He values have been observed along the SWIR near Bouvet Island to the west, there is no evidence for elevated 3He/4He ratios along this section of the SWIR. The second part of Chapter 3 investigates the relationship between 3He/4He ratios and geophysical indicators of plume robustness for nine hotspots."
}, {
    "id": "oai:dspace.mit.edu:1721.1/45769",
    "title": "The development of orogenic plateaus : Plateaus: case studies examining relationships between tectonics, crustal strength, surface deformation, and plateau morphology",
    "abstract": "This thesis addresses processes associated with the uplift, deformation, and erosion of orogenic plateaus. The timing and mechanisms of uplift of the Tibetan Plateau and the Altiplano are the subject of ongoing debate. Central issues include the strength of the lower crust and the role of lower crustal flow, the relative importance of continuous deformation versus block deformation, and the possibility of lithospheric delamination. The goal of this thesis is to further explore several of these issues using a combination of numerical modeling, field observations, and thermochronology. I investigate controls on the large-scale evolution of the Tibetan Plateau and the Altiplano using a new quasithree-dimensional viscous flow model that allows for both the development of a weak lower crust and lateral and temporal viscosity variations. Modeling motivated by the Tibetan Plateau shows that lateral variations in crustal strength can have a significant effect on surface velocities throughout the plateau, as well as on the location, shape, and slope of plateau margins and the overall plateau morphology. Model results suggest that crustal strength heterogeneities may be responsible for a number of seemingly unrelated aspects of Tibetan Plateau morphology and deformation. Modeling motivated by the Altiplano explores the relationship between subduction angle, the strength of the lower crust, crustal thickening, and surface shortening in the Central Andes. Model results illustrate that lower crustal flow above regions of steep-slab subduction can redistribute material along strike and can explain discrepancies between surface shortening and crustal thickness in the northern and southern Altiplano. I address the distribution of Middle Cenozoic deformation on the eastern margin of the Tibetan Plateau by using field observations and thermochronology to document an episode of extension and constrain its timing to the Oligocene. Finally, I examine the response of a major river system to flow over an abrupt plateau margin by using topographic data, cosmogenic nuclide dating, and numerical modeling to describe the incision history of the Colorado River into the southwestern Colorado Plateau.",
    "advisors": ["Leigh H. Royden"],
    "text": "The development of orogenic plateaus : Plateaus: case studies examining relationships between tectonics, crustal strength, surface deformation, and plateau morphology This thesis addresses processes associated with the uplift, deformation, and erosion of orogenic plateaus. The timing and mechanisms of uplift of the Tibetan Plateau and the Altiplano are the subject of ongoing debate. Central issues include the strength of the lower crust and the role of lower crustal flow, the relative importance of continuous deformation versus block deformation, and the possibility of lithospheric delamination. The goal of this thesis is to further explore several of these issues using a combination of numerical modeling, field observations, and thermochronology. I investigate controls on the large-scale evolution of the Tibetan Plateau and the Altiplano using a new quasithree-dimensional viscous flow model that allows for both the development of a weak lower crust and lateral and temporal viscosity variations. Modeling motivated by the Tibetan Plateau shows that lateral variations in crustal strength can have a significant effect on surface velocities throughout the plateau, as well as on the location, shape, and slope of plateau margins and the overall plateau morphology. Model results suggest that crustal strength heterogeneities may be responsible for a number of seemingly unrelated aspects of Tibetan Plateau morphology and deformation. Modeling motivated by the Altiplano explores the relationship between subduction angle, the strength of the lower crust, crustal thickening, and surface shortening in the Central Andes. Model results illustrate that lower crustal flow above regions of steep-slab subduction can redistribute material along strike and can explain discrepancies between surface shortening and crustal thickness in the northern and southern Altiplano. I address the distribution of Middle Cenozoic deformation on the eastern margin of the Tibetan Plateau by using field observations and thermochronology to document an episode of extension and constrain its timing to the Oligocene. Finally, I examine the response of a major river system to flow over an abrupt plateau margin by using topographic data, cosmogenic nuclide dating, and numerical modeling to describe the incision history of the Colorado River into the southwestern Colorado Plateau."
}, {
    "id": "oai:dspace.mit.edu:1721.1/107106",
    "title": "An uncertainty-focused approach to modeling the atmospheric chemistry of persistent organic pollutants",
    "abstract": "In this thesis, I study polycyclic aromatic hydrocarbons (PAHs) and perfluorocarboxylic acids (PFCAs). PAHs are by-products of burning and therefore have important anthropogenic sources in the combustion of fuels, biomass, etc. PFCAs and their atmospheric precursors are used in making firefighting foams, non-stick coatings, and other surfactant applications. I quantitatively examine the relative importance of uncertainty in emissions and physicochemical properties (including reaction rate constants) to Northern Hemisphere (NH) and Arctic PAH concentrations. NH average concentrations are more sensitive to uncertainty in the atmospheric lifetime than to emissions rate. The largest uncertainty reductions would come from precise experimental determination of PHE, PYR and BaP rate constants for the reaction with OH. I calculate long-chain PFCA formation theoretical maximum yields for the degradation of precursor species at a representative sample of atmospheric conditions from a three dimensional chemical transport model, finding that atmospheric conditions farther from pollution sources have both higher capacities to form long chain PFCAs and higher uncertainties in those capacities. I present results from newly developed simulations of atmospheric PFCA formation and fate using the chemical transport model GEOS-Chem, simulating the degradation of fluorotelomer precursors, as well as deposition and transport of the precursors, intermediates and end-products of the PFCA formation chemistry. I compare the model results to remote deposition measurements and find that it reproduces Arctic deposition of PFOA effectively. Given the most recent precursor emission inventory, the atmospheric indirect source of PFOA and PFNA is 10-45 t/yr globally and 0.2-0.7 t/yr to the Arctic.",
    "advisors": ["Noelle Eckley Selin"],
    "text": "An uncertainty-focused approach to modeling the atmospheric chemistry of persistent organic pollutants In this thesis, I study polycyclic aromatic hydrocarbons (PAHs) and perfluorocarboxylic acids (PFCAs). PAHs are by-products of burning and therefore have important anthropogenic sources in the combustion of fuels, biomass, etc. PFCAs and their atmospheric precursors are used in making firefighting foams, non-stick coatings, and other surfactant applications. I quantitatively examine the relative importance of uncertainty in emissions and physicochemical properties (including reaction rate constants) to Northern Hemisphere (NH) and Arctic PAH concentrations. NH average concentrations are more sensitive to uncertainty in the atmospheric lifetime than to emissions rate. The largest uncertainty reductions would come from precise experimental determination of PHE, PYR and BaP rate constants for the reaction with OH. I calculate long-chain PFCA formation theoretical maximum yields for the degradation of precursor species at a representative sample of atmospheric conditions from a three dimensional chemical transport model, finding that atmospheric conditions farther from pollution sources have both higher capacities to form long chain PFCAs and higher uncertainties in those capacities. I present results from newly developed simulations of atmospheric PFCA formation and fate using the chemical transport model GEOS-Chem, simulating the degradation of fluorotelomer precursors, as well as deposition and transport of the precursors, intermediates and end-products of the PFCA formation chemistry. I compare the model results to remote deposition measurements and find that it reproduces Arctic deposition of PFOA effectively. Given the most recent precursor emission inventory, the atmospheric indirect source of PFOA and PFNA is 10-45 t/yr globally and 0.2-0.7 t/yr to the Arctic."
}, {
    "id": "oai:dspace.mit.edu:1721.1/91832",
    "title": "Modelling signal interactions with application to financial time series",
    "abstract": "In this thesis, we concern ourselves with the problem of reasoning over a set of objects evolving over time that are coupled through interaction structures that are themselves changing over time. We focus on inferring time-varying interaction structures among a set of objects from sequences of noisy time series observations with the caveat that the number of interaction structures is not known a priori. Furthermore, we aim to develop an inference procedure that operates online, meaning that it is capable of incorporating observations as they arrive. We develop an online nonparametric inference algorithm called Online Nonparametric Switching Temporal Interaction Model inference (ONSTIM). ONSTIM is an extension of the work of Dzunic and Fisher [1], who employ a linear Gaussian model with time-varying transition dynamics as the generative graphical model for observed time series. Like Dzunic and Fisher, we employ sampling approaches to perform inference. Instead of presupposing a fixed number of interaction structures, however, we allow for proposal of new interaction structures sampled from a prior distribution as new observations are incorporated into our inference. We then demonstrate the viability of ONSTIM on synthetic and financial datasets. Synthetic datasets are sampled from a generative model, and financial datasets are constructed from the price data of various US stocks and ETFs.",
    "advisors": ["John W. Fisher III"],
    "text": "Modelling signal interactions with application to financial time series In this thesis, we concern ourselves with the problem of reasoning over a set of objects evolving over time that are coupled through interaction structures that are themselves changing over time. We focus on inferring time-varying interaction structures among a set of objects from sequences of noisy time series observations with the caveat that the number of interaction structures is not known a priori. Furthermore, we aim to develop an inference procedure that operates online, meaning that it is capable of incorporating observations as they arrive. We develop an online nonparametric inference algorithm called Online Nonparametric Switching Temporal Interaction Model inference (ONSTIM). ONSTIM is an extension of the work of Dzunic and Fisher [1], who employ a linear Gaussian model with time-varying transition dynamics as the generative graphical model for observed time series. Like Dzunic and Fisher, we employ sampling approaches to perform inference. Instead of presupposing a fixed number of interaction structures, however, we allow for proposal of new interaction structures sampled from a prior distribution as new observations are incorporated into our inference. We then demonstrate the viability of ONSTIM on synthetic and financial datasets. Synthetic datasets are sampled from a generative model, and financial datasets are constructed from the price data of various US stocks and ETFs."
}, {
    "id": "oai:dspace.mit.edu:1721.1/9080",
    "title": "Attack development for intrusion detector evaluation",
    "abstract": "An important goal of the 1999 DARPA Intrusion Detection Evaluation was to promote the development of intrusion detection systems that can detect new attacks. This thesis describes UNIX attacks developed for the 1999 DARPA Evaluation. Some attacks were new in 1999 and others were stealthy versions of 1998 User-to-Root attacks designed to evade network-based intrusion detection systems. In addition, new and old attacks were fragmented at the packet level to evade network-based intrusion detection systems. Results demonstrated that new and stealthy attacks were not detected well. New attacks that were never seen before were not detected by any network-based systems. Stealthy attacks, modified to be difficult to detect by network intrusion detection systems, were detected less accurately than clear versions. The best network-based system detected 42% of clear attacks and only 11% of stealthy attacks at 10 false alarms per day. A few attacks and background sessions modified with packet modifications eluded network intrusion detection systems causing them to generate false negatives and false positives due to improper TCP/IP reassembly.",
    "advisors": ["Richard Lippmann"],
    "text": "Attack development for intrusion detector evaluation An important goal of the 1999 DARPA Intrusion Detection Evaluation was to promote the development of intrusion detection systems that can detect new attacks. This thesis describes UNIX attacks developed for the 1999 DARPA Evaluation. Some attacks were new in 1999 and others were stealthy versions of 1998 User-to-Root attacks designed to evade network-based intrusion detection systems. In addition, new and old attacks were fragmented at the packet level to evade network-based intrusion detection systems. Results demonstrated that new and stealthy attacks were not detected well. New attacks that were never seen before were not detected by any network-based systems. Stealthy attacks, modified to be difficult to detect by network intrusion detection systems, were detected less accurately than clear versions. The best network-based system detected 42% of clear attacks and only 11% of stealthy attacks at 10 false alarms per day. A few attacks and background sessions modified with packet modifications eluded network intrusion detection systems causing them to generate false negatives and false positives due to improper TCP/IP reassembly."
}, {
    "id": "oai:dspace.mit.edu:1721.1/34099",
    "title": "A VLSI systolic array processor for complex singular value decomposition",
    "abstract": "undefined The singular value decomposition is one example of a variety of more complex routines that are finding use in modern high performance signal processing systems. In the interest of achieving the maximum possible performance, a systolic array processor for computing the singular value decomposition of an arbitrary complex matrix was designed using a silicon compiler system. This system allows for ease of design by specification of the processor architecture in a high level language, utilizing parts from a variety of cell libraries, while still benefiting from the power of custom VLSI. The level of abstraction provided by this system allowed more complex functional units to be built up from existing simple library parts. A novel fast interpolation cell for computation of square roots and inverse square roots was designed, allowing for a new algebraic approach to the singular value decomposition problem. The processors connect together in a systolic array to maximize computational efficiency while minimizing overhead due to high communication requirements. ",
    "advisors": ["Srinivas Devadas", "Steven R. Broadstone"],
    "text": "A VLSI systolic array processor for complex singular value decomposition undefined The singular value decomposition is one example of a variety of more complex routines that are finding use in modern high performance signal processing systems. In the interest of achieving the maximum possible performance, a systolic array processor for computing the singular value decomposition of an arbitrary complex matrix was designed using a silicon compiler system. This system allows for ease of design by specification of the processor architecture in a high level language, utilizing parts from a variety of cell libraries, while still benefiting from the power of custom VLSI. The level of abstraction provided by this system allowed more complex functional units to be built up from existing simple library parts. A novel fast interpolation cell for computation of square roots and inverse square roots was designed, allowing for a new algebraic approach to the singular value decomposition problem. The processors connect together in a systolic array to maximize computational efficiency while minimizing overhead due to high communication requirements. "
}, {
    "id": "oai:dspace.mit.edu:1721.1/9082",
    "title": "Recovery of 3D articulated motion from 2D correspondences",
    "abstract": "Recovering the 3D motion of the human body is an important problem in computer vision. Applications that would benefit from 3D motion include physical therapy, computer user interfaces, and 3D animation. Unfortunately, recovering 3D position from one 2D camera is an inherently ill-posed problem. This thesis focuses on recovery of 3D motion of an articulated model using 2D correspondences from an existing 2D tracker. A number of constraints are used to aid in reconstruction: (i) kinematic constraints from a 3D kinematic model, (ii) joint angle limits, (iii) dynamic smoothing, and (iv) key frames. These methods are used successfully to recover 3D motion from video sequences. Also presented is a method for recovering 3D motion from motion capture data, as well as a method for recovering kinematic model connectivity from 2D tracks.",
    "advisors": ["W. Eric L. Grimson"],
    "text": "Recovery of 3D articulated motion from 2D correspondences Recovering the 3D motion of the human body is an important problem in computer vision. Applications that would benefit from 3D motion include physical therapy, computer user interfaces, and 3D animation. Unfortunately, recovering 3D position from one 2D camera is an inherently ill-posed problem. This thesis focuses on recovery of 3D motion of an articulated model using 2D correspondences from an existing 2D tracker. A number of constraints are used to aid in reconstruction: (i) kinematic constraints from a 3D kinematic model, (ii) joint angle limits, (iii) dynamic smoothing, and (iv) key frames. These methods are used successfully to recover 3D motion from video sequences. Also presented is a method for recovering 3D motion from motion capture data, as well as a method for recovering kinematic model connectivity from 2D tracks."
}, {
    "id": "oai:dspace.mit.edu:1721.1/41030",
    "title": "Stepped-frequency pulse train waveforms for improved radar range resolution",
    "abstract": "The traditional approach of improving radar range resolution using a linear frequency modulated chirp signal requires the full width of the frequency spectrum, which is not feasible in the UHF band due to interference or frequency allocation for other purposes. In this study a linear frequency modulated chirp signal is approximated using two stepped-frequency pulse train waveforms, a continuous wave pulse train and a linear frequency modulated pulse train. The continuous wave pulse train consists of a series of single frequency pulses, each at a different frequency. It is found to be susceptible to corruption due to target motion. The linear frequency modulated pulse train consists of linear frequency modulation within pulses, each at a different center frequency. Simulations are used to demonstrate that both approaches approximate a linear frequency modulated chirp signal, and performance is degraded when there is a gap in the frequency band or if there is phase distortion due to target motion. However, it is shown that a linear frequency modulated pulse train with frequency overlaps between pulses can be used to reduce or eliminate phase distortions resulting from target motion provided the target is moving with constant velocity. The validity of the technique is demonstrated by non-coherently processing radar data from an internal moving target simulator and data from actual planes to resolve targets from their reflected image in order to estimate target height.",
    "advisors": ["Herbert M. Aumann", "James K. Roberge"],
    "text": "Stepped-frequency pulse train waveforms for improved radar range resolution The traditional approach of improving radar range resolution using a linear frequency modulated chirp signal requires the full width of the frequency spectrum, which is not feasible in the UHF band due to interference or frequency allocation for other purposes. In this study a linear frequency modulated chirp signal is approximated using two stepped-frequency pulse train waveforms, a continuous wave pulse train and a linear frequency modulated pulse train. The continuous wave pulse train consists of a series of single frequency pulses, each at a different frequency. It is found to be susceptible to corruption due to target motion. The linear frequency modulated pulse train consists of linear frequency modulation within pulses, each at a different center frequency. Simulations are used to demonstrate that both approaches approximate a linear frequency modulated chirp signal, and performance is degraded when there is a gap in the frequency band or if there is phase distortion due to target motion. However, it is shown that a linear frequency modulated pulse train with frequency overlaps between pulses can be used to reduce or eliminate phase distortions resulting from target motion provided the target is moving with constant velocity. The validity of the technique is demonstrated by non-coherently processing radar data from an internal moving target simulator and data from actual planes to resolve targets from their reflected image in order to estimate target height."
}, {
    "id": "oai:dspace.mit.edu:1721.1/91815",
    "title": "PhysioMiner : a scalable cloud based framework for physiological waveform mining",
    "abstract": "This work presents PhysioMiner, a large scale machine learning and analytics framework for physiological waveform mining. It is a scalable and flexible solution for researchers and practitioners to build predictive models from physiological time series data. It allows users to specify arbitrary features and conditions to train the model, computing everything in parallel in the cloud. PhysioMiner is tested on a large dataset of electrocardiography (ECG) from 6000 patients in the MIMIC database. Signals are cleaned and processed, and features are extracted per period. A total of 1.2 billion heart beats were processed and 26 billion features were extracted resulting in half a terabyte database. These features were aggregated for windows corresponding to patient events. These aggregated features were fed into DELPHI, a multi algorithm multi parameter cloud based system to build a predictive model. An area under the curve of 0.693 was achieved for an acute hypotensive event prediction from the ECG waveform alone. The results demonstrate the scalability and flexibility of PhysioMiner on real world data. PhysioMiner will be an important tool for researchers to spend less time building systems, and more time building predictive models.",
    "advisors": ["Kalyan Veeramachaneni", "Una-May O'Reilly"],
    "text": "PhysioMiner : a scalable cloud based framework for physiological waveform mining This work presents PhysioMiner, a large scale machine learning and analytics framework for physiological waveform mining. It is a scalable and flexible solution for researchers and practitioners to build predictive models from physiological time series data. It allows users to specify arbitrary features and conditions to train the model, computing everything in parallel in the cloud. PhysioMiner is tested on a large dataset of electrocardiography (ECG) from 6000 patients in the MIMIC database. Signals are cleaned and processed, and features are extracted per period. A total of 1.2 billion heart beats were processed and 26 billion features were extracted resulting in half a terabyte database. These features were aggregated for windows corresponding to patient events. These aggregated features were fed into DELPHI, a multi algorithm multi parameter cloud based system to build a predictive model. An area under the curve of 0.693 was achieved for an acute hypotensive event prediction from the ECG waveform alone. The results demonstrate the scalability and flexibility of PhysioMiner on real world data. PhysioMiner will be an important tool for researchers to spend less time building systems, and more time building predictive models."
}, {
    "id": "oai:dspace.mit.edu:1721.1/36678",
    "title": "Store Buffers : implementing single cycle store instructions in write-through, write-back and set associative caches",
    "abstract": "This thesis proposes a new mechanism, called Store Buffers, for implementing single cycle store instructions in a pipelined processor. Single cycle store instructions are difficult to implement because in most cases the tag check must be performed before the data can be written into the data cache. Store buffers allow a store instruction to read the cache tag as it. passes through the pipe while keeping the store instruction data buffered in a backup register until the data cache is free. This strategy guarantees single cycle store execution without increasing the hit access time or degrading the performance of the data cache for simple direct-mapped caches, as well as for more complex set associative and write-back caches. As larger caches are incorporated on-chip, the speed of store instructions becomes an increasingly important part of the overall performance. The first part of the thesis describes the design and implementation of store buffers in write through, write-back, direct-mapped and set associative caches. The second part describes the implementation and simulation of store buffers in a 6-stage pipeline with a direct mapped write-through pipelined cache. The performance of this method is compared to other cache write techniques. Preliminary results show that store buffers perform better than other store strategies under high IO latencies and cache thrashing. With as few as three buffers, they significantly reduce the number of cycles per instruction.",
    "advisors": ["Anant Agarwal"],
    "text": "Store Buffers : implementing single cycle store instructions in write-through, write-back and set associative caches This thesis proposes a new mechanism, called Store Buffers, for implementing single cycle store instructions in a pipelined processor. Single cycle store instructions are difficult to implement because in most cases the tag check must be performed before the data can be written into the data cache. Store buffers allow a store instruction to read the cache tag as it. passes through the pipe while keeping the store instruction data buffered in a backup register until the data cache is free. This strategy guarantees single cycle store execution without increasing the hit access time or degrading the performance of the data cache for simple direct-mapped caches, as well as for more complex set associative and write-back caches. As larger caches are incorporated on-chip, the speed of store instructions becomes an increasingly important part of the overall performance. The first part of the thesis describes the design and implementation of store buffers in write through, write-back, direct-mapped and set associative caches. The second part describes the implementation and simulation of store buffers in a 6-stage pipeline with a direct mapped write-through pipelined cache. The performance of this method is compared to other cache write techniques. Preliminary results show that store buffers perform better than other store strategies under high IO latencies and cache thrashing. With as few as three buffers, they significantly reduce the number of cycles per instruction."
}, {
    "id": "oai:dspace.mit.edu:1721.1/47720",
    "title": "A compact windowing system for the Curl environment",
    "abstract": "Curl is a programming language for creating web content. It is capable of running under a Linux operating system using X windows for a graphics interface. There are applications of Curl which do not need such a large and complex graphics system as Xwindows. The compact windowing system eliminates much of the unnecessary functionality of X windows while implementing the necessary components. The system makes use of the video hardware using svgalib for Linux. The system also make use of the Freetype TrueType font library to use TrueType fonts for text rendering. This eliminates one incompatibility between the Linux and MS Windows versions of Curl.",
    "advisors": ["Steve Ward"],
    "text": "A compact windowing system for the Curl environment Curl is a programming language for creating web content. It is capable of running under a Linux operating system using X windows for a graphics interface. There are applications of Curl which do not need such a large and complex graphics system as Xwindows. The compact windowing system eliminates much of the unnecessary functionality of X windows while implementing the necessary components. The system makes use of the video hardware using svgalib for Linux. The system also make use of the Freetype TrueType font library to use TrueType fonts for text rendering. This eliminates one incompatibility between the Linux and MS Windows versions of Curl."
}, {
    "id": "oai:dspace.mit.edu:1721.1/62393",
    "title": "Complete VLSI implementation of improved low complexity chase Reed-Solomon decoders",
    "abstract": "This thesis presents a complete VLSI design of improved low complexity chase (LCC) decoders for Reed-Solomon (RS) codes. This is the first attempt in published research that implements LCC decoders at the circuit level. Based on the joint algorithm research with University of Hawaii, we propose several new techniques for complexity reduction in LCC decoders and apply them in the VLSI design for RS [255, 239,17] (LCC255) and RS [31, 25, 7] (LCC31) codes. The major algorithm improvement is that the interpolation is performed over a subset of test vectors to avoid redundant decoding. Also the factorization formula is reshaped to avoid large computation complexity overlooked in previous research. To maintain the effectiveness of algorithm improvements, we find it necessary to adopt the systematic message encoding, instead of the evaluation-map encoding used in the previous work on interpolation decoders. The LCC255 and LCC31 decoders are both implemented in 90nm CMOS process with the areas of 1.01mm 2 and 0.255mm 2 respectively. Simulations show that with 1.2V supply voltage they can achieve the energy efficiencies of 67pJ/bit and 34pJ/bit at the maximum throughputs of 2.5Gbps and 1.3Gbps respectively. The proposed algorithm changes, combined with optimized macro- and micro-architectures, result in a 70% complexity reduction (measured with gate count). This new LCC design also achieves 17x better energy-efficiency than a standard Chase decoder (projected from the most recent reported Reed Solomon decoder implementation) for equivalent area, latency and throughput. The comparison of the two decoders links the significantly higher decoding energy cost to the better decoding performance. We quantitatively compute the cost of the decoding gain as the adjusted area of LCC255 being 7.5 times more than LCC31.",
    "advisors": ["Vladimir M. Stojanovi"],
    "text": "Complete VLSI implementation of improved low complexity chase Reed-Solomon decoders This thesis presents a complete VLSI design of improved low complexity chase (LCC) decoders for Reed-Solomon (RS) codes. This is the first attempt in published research that implements LCC decoders at the circuit level. Based on the joint algorithm research with University of Hawaii, we propose several new techniques for complexity reduction in LCC decoders and apply them in the VLSI design for RS [255, 239,17] (LCC255) and RS [31, 25, 7] (LCC31) codes. The major algorithm improvement is that the interpolation is performed over a subset of test vectors to avoid redundant decoding. Also the factorization formula is reshaped to avoid large computation complexity overlooked in previous research. To maintain the effectiveness of algorithm improvements, we find it necessary to adopt the systematic message encoding, instead of the evaluation-map encoding used in the previous work on interpolation decoders. The LCC255 and LCC31 decoders are both implemented in 90nm CMOS process with the areas of 1.01mm 2 and 0.255mm 2 respectively. Simulations show that with 1.2V supply voltage they can achieve the energy efficiencies of 67pJ/bit and 34pJ/bit at the maximum throughputs of 2.5Gbps and 1.3Gbps respectively. The proposed algorithm changes, combined with optimized macro- and micro-architectures, result in a 70% complexity reduction (measured with gate count). This new LCC design also achieves 17x better energy-efficiency than a standard Chase decoder (projected from the most recent reported Reed Solomon decoder implementation) for equivalent area, latency and throughput. The comparison of the two decoders links the significantly higher decoding energy cost to the better decoding performance. We quantitatively compute the cost of the decoding gain as the adjusted area of LCC255 being 7.5 times more than LCC31."
}, {
    "id": "oai:dspace.mit.edu:1721.1/71275",
    "title": "Structured decomposition of adaptive applications",
    "abstract": "We describe an approach to automate certain high-level implementation decisions in a pervasive application, allowing them to be postponed until run time. Our system enables a model in which an application programmer can specify the behavior of an adaptive application as a set of open-ended decision points. We formalize decision points as Goals, each of which may be satisfied by a set of scripts called Techniques. The set of Techniques vying to satisfy any Goal is additive and may be extended at runtime without needing to modify or remove any existing Techniques. Our system provides a framework in which Techniques may compete and interoperate at runtime in order to maintain an adaptive application. Technique development may be distributed and incremental, providing a path for the decentralized evolution of applications. Benchmarks show that our system imposes reasonable overhead during application startup and adaptation.",
    "advisors": ["Steve Ward"],
    "text": "Structured decomposition of adaptive applications We describe an approach to automate certain high-level implementation decisions in a pervasive application, allowing them to be postponed until run time. Our system enables a model in which an application programmer can specify the behavior of an adaptive application as a set of open-ended decision points. We formalize decision points as Goals, each of which may be satisfied by a set of scripts called Techniques. The set of Techniques vying to satisfy any Goal is additive and may be extended at runtime without needing to modify or remove any existing Techniques. Our system provides a framework in which Techniques may compete and interoperate at runtime in order to maintain an adaptive application. Technique development may be distributed and incremental, providing a path for the decentralized evolution of applications. Benchmarks show that our system imposes reasonable overhead during application startup and adaptation."
}, {
    "id": "oai:dspace.mit.edu:1721.1/38692",
    "title": "Development of the recess mounting with monolithic metallization optoelectronic integrated circuit technology for optical clock distribution applications",
    "abstract": "Recess mounting with monolithic metallization, or RM3 integration, is used to integrate Ino.47Ga0.53As/InP based lattice-matched high quantum efficiency p-i-n photodetectors on silicon chips to build high performance optoelectronic integrated circuits [1]. In RM3 integration, partially processed heterostructure devices are placed in recesses formed in the dielectric layers covering the surface of an integrated circuit chip, the surface is planarized, and monolithic processing is continued to transform the heterostructures into optoelectronic devices monolithically integrated with the underlying electronic circuitry. Two different RM3 techniques have been investigated, Aligned Pillar Bonding (APB) and OptoPill Assembly (OPA). APB integrates lattice mismatched materials using aligned, selective area wafer bonding at reduced temperature (under 3500C), which protects the electronic chips from the adverse effects of high temperatures, and reduces the thermal expansion mismatch concerns. In the OPA technique, optoelectronic heterostructures are processed into circular pills of 8 gm height and 45 gm diameter, the pills are released from the substrate, and collected through a process that involves decanting.",
    "advisors": ["Clifton G. Fonstad, Jr"],
    "text": "Development of the recess mounting with monolithic metallization optoelectronic integrated circuit technology for optical clock distribution applications Recess mounting with monolithic metallization, or RM3 integration, is used to integrate Ino.47Ga0.53As/InP based lattice-matched high quantum efficiency p-i-n photodetectors on silicon chips to build high performance optoelectronic integrated circuits [1]. In RM3 integration, partially processed heterostructure devices are placed in recesses formed in the dielectric layers covering the surface of an integrated circuit chip, the surface is planarized, and monolithic processing is continued to transform the heterostructures into optoelectronic devices monolithically integrated with the underlying electronic circuitry. Two different RM3 techniques have been investigated, Aligned Pillar Bonding (APB) and OptoPill Assembly (OPA). APB integrates lattice mismatched materials using aligned, selective area wafer bonding at reduced temperature (under 3500C), which protects the electronic chips from the adverse effects of high temperatures, and reduces the thermal expansion mismatch concerns. In the OPA technique, optoelectronic heterostructures are processed into circular pills of 8 gm height and 45 gm diameter, the pills are released from the substrate, and collected through a process that involves decanting."
}, {
    "id": "oai:dspace.mit.edu:1721.1/115729",
    "title": "Exploration vs. exploitation in coupon personalization",
    "abstract": "Personalized offers aim to maximize profit by taking into account customer preferences inferred from past purchase behavior. For large retailers with extensive product offerings, learning customer preferences can be challenging due to relatively short purchase histories of most customers. To alleviate the dearth of data, we propose exploiting similarities among products and among customers to reduce problem dimensions. We also propose that retailers use personalized offers not only to maximize expected profit, but to actively learn their customers' preferences. An offer that does not maximize expected profit given current information may still provide valuable insights about customer preferences. This information enables more profitable coupon allocation and higher profits in the long run. In this thesis we 1) derive approximate inference algorithms to learn customer preferences from purchase data in real time, 2) formulate the retailers' offer allocation problem as a multi armed bandit and explore solution strategies.",
    "advisors": ["Devavrat Shah"],
    "text": "Exploration vs. exploitation in coupon personalization Personalized offers aim to maximize profit by taking into account customer preferences inferred from past purchase behavior. For large retailers with extensive product offerings, learning customer preferences can be challenging due to relatively short purchase histories of most customers. To alleviate the dearth of data, we propose exploiting similarities among products and among customers to reduce problem dimensions. We also propose that retailers use personalized offers not only to maximize expected profit, but to actively learn their customers' preferences. An offer that does not maximize expected profit given current information may still provide valuable insights about customer preferences. This information enables more profitable coupon allocation and higher profits in the long run. In this thesis we 1) derive approximate inference algorithms to learn customer preferences from purchase data in real time, 2) formulate the retailers' offer allocation problem as a multi armed bandit and explore solution strategies."
}, {
    "id": "oai:dspace.mit.edu:1721.1/75454",
    "title": "Design and fabrication of a MEMS-array pressure sensor system for passive underwater navigation inspired by the lateral line",
    "abstract": "An object within a fluid flow generates local pressure variations that are unique and characteristic to the object's shape and size. For example, a three-dimensional object or a wall-like obstacle obstructs flow and creates sharp pressure gradients nearby. Similarly, unsteady flow contains vortical patterns with associated unique pressure signatures. Detection of obstacles, as well as identification of unsteady flow features, is required for autonomous undersea vehicle (AUV) navigation. An array of passive underwater pressure sensors, with their ability to An object within a fluid flow generates local pressure variations that are unique and characteristic to the object's shape and size. For example, a three-dimensional object or a wall-like obstacle obstructs flow and creates sharp pressure gradients nearby. Similarly, unsteady flow contains vortical patterns with associated unique pressure signatures. Detection of obstacles, as well as identification of unsteady flow features, is required for autonomous undersea vehicle (AUV) navigation. An array of passive underwater pressure sensors, with their ability to \"touch at a distance\" with minimal power consumption, would be able to resolve the pressure signatures of obstacles in the near field and the wake of objects in the intermediate field. As an additional benefit, with proper design, pressure sensors can also be used to sample acoustic signals as well. Fish already have a biological version of such a pressure sensor system, namely the lateral line organ, a spatially-distributed set of sensors over a fish's body that allows the fish to monitor its hydrodynamic environment, influenced by the external disturbances. Through its ability to resolve the pressure signature of objects, the fish obtains \"hydrodynamic pictures\". Inspired by the fish lateral line, this thesis describes the development of a high-density array of microelectromechanical systems (MEMS) pressure sensors built in KOH-etched silicon and HF-etched Pyrex wafers. A novel strain-gauge resistor design is discussed, and standard CMOS/MEMS fabrication techniques were used to build sensors based on the strain-gauge resistors and thin silicon diphragms. Measurements of the diaphragm deflection and strain-gauge resistance changes in response to changes in applied external pressure confirm that the devices can be reliably calibrated for use as pressure sensors to enable passive navigation by AUVs. A set of sensors with millimeter-scale spacing, 2.1 to 2.5 [mu]V/Pa sensitivity, sub-pascal pressure resolution, and -2000 Pa to 2000 Pa pressure range has been demonstrated. Finally, an integrated circuit for array processing and signal amplification and to be fabricated with the pressure sensors is proposed.",
    "advisors": ["Jeffrey H. Lang"],
    "text": "Design and fabrication of a MEMS-array pressure sensor system for passive underwater navigation inspired by the lateral line An object within a fluid flow generates local pressure variations that are unique and characteristic to the object's shape and size. For example, a three-dimensional object or a wall-like obstacle obstructs flow and creates sharp pressure gradients nearby. Similarly, unsteady flow contains vortical patterns with associated unique pressure signatures. Detection of obstacles, as well as identification of unsteady flow features, is required for autonomous undersea vehicle (AUV) navigation. An array of passive underwater pressure sensors, with their ability to An object within a fluid flow generates local pressure variations that are unique and characteristic to the object's shape and size. For example, a three-dimensional object or a wall-like obstacle obstructs flow and creates sharp pressure gradients nearby. Similarly, unsteady flow contains vortical patterns with associated unique pressure signatures. Detection of obstacles, as well as identification of unsteady flow features, is required for autonomous undersea vehicle (AUV) navigation. An array of passive underwater pressure sensors, with their ability to \"touch at a distance\" with minimal power consumption, would be able to resolve the pressure signatures of obstacles in the near field and the wake of objects in the intermediate field. As an additional benefit, with proper design, pressure sensors can also be used to sample acoustic signals as well. Fish already have a biological version of such a pressure sensor system, namely the lateral line organ, a spatially-distributed set of sensors over a fish's body that allows the fish to monitor its hydrodynamic environment, influenced by the external disturbances. Through its ability to resolve the pressure signature of objects, the fish obtains \"hydrodynamic pictures\". Inspired by the fish lateral line, this thesis describes the development of a high-density array of microelectromechanical systems (MEMS) pressure sensors built in KOH-etched silicon and HF-etched Pyrex wafers. A novel strain-gauge resistor design is discussed, and standard CMOS/MEMS fabrication techniques were used to build sensors based on the strain-gauge resistors and thin silicon diphragms. Measurements of the diaphragm deflection and strain-gauge resistance changes in response to changes in applied external pressure confirm that the devices can be reliably calibrated for use as pressure sensors to enable passive navigation by AUVs. A set of sensors with millimeter-scale spacing, 2.1 to 2.5 [mu]V/Pa sensitivity, sub-pascal pressure resolution, and -2000 Pa to 2000 Pa pressure range has been demonstrated. Finally, an integrated circuit for array processing and signal amplification and to be fabricated with the pressure sensors is proposed."
}, {
    "id": "oai:dspace.mit.edu:1721.1/84718",
    "title": "Dynamic application of problem solving strategies : dependency-based flow control",
    "abstract": "While humans may solve problems by applying any one of a number of different problem solving strategies, computerized problem solving is typically brittle, limited in the number of available strategies and ways of combining them to solve a problem. In this thesis, I present a method to flexibly select and combine problem solving strategies by using a constraint-propagation network, informed by higher-order knowledge about goals and what is known, to selectively control the activity of underlying problem solvers. Knowledge within each problem solver as well as the constraint-propagation network are represented as a network of explicit propositions, each described with respect to five interrelated axes of concrete and abstract knowledge about each proposition. Knowledge within each axis is supported by a set of dependencies that allow for both the adjustment of belief based on modifying supports for solutions and the production of justifications of that belief. I show that this method may be used to solve a variety of real-world problems and provide meaningful justifications for solutions to these problems, including decision-making based on numerical evaluation of risk and the evaluation of whether or not a document may be legally sent to a recipient in accordance with a policy controlling its dissemination.",
    "advisors": ["Gerald Jay Sussman"],
    "text": "Dynamic application of problem solving strategies : dependency-based flow control While humans may solve problems by applying any one of a number of different problem solving strategies, computerized problem solving is typically brittle, limited in the number of available strategies and ways of combining them to solve a problem. In this thesis, I present a method to flexibly select and combine problem solving strategies by using a constraint-propagation network, informed by higher-order knowledge about goals and what is known, to selectively control the activity of underlying problem solvers. Knowledge within each problem solver as well as the constraint-propagation network are represented as a network of explicit propositions, each described with respect to five interrelated axes of concrete and abstract knowledge about each proposition. Knowledge within each axis is supported by a set of dependencies that allow for both the adjustment of belief based on modifying supports for solutions and the production of justifications of that belief. I show that this method may be used to solve a variety of real-world problems and provide meaningful justifications for solutions to these problems, including decision-making based on numerical evaluation of risk and the evaluation of whether or not a document may be legally sent to a recipient in accordance with a policy controlling its dissemination."
}, {
    "id": "oai:dspace.mit.edu:1721.1/93839",
    "title": "User-controlled privacy for personal mobile data",
    "abstract": "Smartphones collect a wide range of sensor data, ranging from the basic, such as location, accelerometer, and Bluetooth, to the more advanced, such as heart rate. Mobile apps on the Android and iOS platforms provide users with \"all-or-nothing\" controls during installation to get permission for data collection and use. Users have to either agree to have the app collect and use all the requested data or not use the app at all. This is slowly changing with the iOS framework, which now allows users to turn off location sharing with specific apps even after installation. MIT Living Lab platform is a mobile app development platform that uses openPDS to provide MIT users with personal data stores but currently lacks user controls for privacy. This thesis presents PrivacyMate, a suite of tools for MIT Living Labs that provide user-controllable privacy mechanisms for mobile apps. PrivacyMate aims to enable users to maintain better control over their mobile personal data. It extends the model of iOS and allows users to select or deselect various types of data (more than just location information) for collection and use by apps. Users can also provide temporal and spatial specifications to indicate a context in which they are comfortable sharing their data with certain apps. We incorporate the privacy mechanisms offered by PrivacyMate into two mobile apps built on the MIT Living Lab platform: ScheduleME and MIT-FIT. ScheduleME enables users to schedule meetings without disclosing either their locations or points of interest. MIT-FIT enables users to track personal and aggregate high-activity regions and times, as well as view personalized fitness-related event recommendations. The MIT Living Lab team is planning to eventually deploy PrivacyMate and MIT-FIT to the entire MIT community.",
    "advisors": ["Lalana Kagal"],
    "text": "User-controlled privacy for personal mobile data Smartphones collect a wide range of sensor data, ranging from the basic, such as location, accelerometer, and Bluetooth, to the more advanced, such as heart rate. Mobile apps on the Android and iOS platforms provide users with \"all-or-nothing\" controls during installation to get permission for data collection and use. Users have to either agree to have the app collect and use all the requested data or not use the app at all. This is slowly changing with the iOS framework, which now allows users to turn off location sharing with specific apps even after installation. MIT Living Lab platform is a mobile app development platform that uses openPDS to provide MIT users with personal data stores but currently lacks user controls for privacy. This thesis presents PrivacyMate, a suite of tools for MIT Living Labs that provide user-controllable privacy mechanisms for mobile apps. PrivacyMate aims to enable users to maintain better control over their mobile personal data. It extends the model of iOS and allows users to select or deselect various types of data (more than just location information) for collection and use by apps. Users can also provide temporal and spatial specifications to indicate a context in which they are comfortable sharing their data with certain apps. We incorporate the privacy mechanisms offered by PrivacyMate into two mobile apps built on the MIT Living Lab platform: ScheduleME and MIT-FIT. ScheduleME enables users to schedule meetings without disclosing either their locations or points of interest. MIT-FIT enables users to track personal and aggregate high-activity regions and times, as well as view personalized fitness-related event recommendations. The MIT Living Lab team is planning to eventually deploy PrivacyMate and MIT-FIT to the entire MIT community."
}, {
    "id": "oai:dspace.mit.edu:1721.1/71512",
    "title": "Defeating eavesdropping with quantum illumination",
    "abstract": "Quantum illumination is a paradigm for using entanglement to gain a performance advantage-in comparison with classical-state systems of the same optical power-over lossy, noisy channels that destroy entanglement. Previous work has shown how it can be used to defeat passive eavesdropping on a two-way Alice-to-Bob-to-Alice communication protocol, in which the eavesdropper, Eve, merely listens to Alice and Bob's transmissions. This thesis extends that work in several ways. First, it derives a lower bound on information advantage that Alice enjoys over Eve in the passive eavesdropping scenario. Next, it explores the performance of alternative practical receivers for Alice, as well as various high-order modulation formats for the passive eavesdropping case. Finally, this thesis extends previous analysis to consider how Alice and Bob can minimize their vulnerability to Eve's doing active eavesdropping, i.e., when she injects her own light into the channel.",
    "advisors": ["Jeffrey H. Shapiro"],
    "text": "Defeating eavesdropping with quantum illumination Quantum illumination is a paradigm for using entanglement to gain a performance advantage-in comparison with classical-state systems of the same optical power-over lossy, noisy channels that destroy entanglement. Previous work has shown how it can be used to defeat passive eavesdropping on a two-way Alice-to-Bob-to-Alice communication protocol, in which the eavesdropper, Eve, merely listens to Alice and Bob's transmissions. This thesis extends that work in several ways. First, it derives a lower bound on information advantage that Alice enjoys over Eve in the passive eavesdropping scenario. Next, it explores the performance of alternative practical receivers for Alice, as well as various high-order modulation formats for the passive eavesdropping case. Finally, this thesis extends previous analysis to consider how Alice and Bob can minimize their vulnerability to Eve's doing active eavesdropping, i.e., when she injects her own light into the channel."
}, {
    "id": "oai:dspace.mit.edu:1721.1/69772",
    "title": "Efficient silicon micro-reactors for thermophotovoltaic applications",
    "abstract": "Thermophotovoltaic (TPV) systems passively generate electricity from the combustion of fuel. Although TPV conversion systems have advantages, they suffer from low efficiency. This thesis investigates different ways to increase the efficiency of TPV systems. In particular the thesis details micro-fabrication of silicon micro-reactors, and twodimensional tungsten photonic crystals (2D W PhC) for high-temperature applications such as selective thermal emitters for TPV energy conversion. Interference lithography and reactive ion etching are used to produce large-area single-crystal tungsten 2D PhC's. The fabricated PhC consists of an array of cylindrical cavities with 800nm diameter, 1.2 pm depth, and 1.2 pm period. Extensive characterization and calibration of all micro-fabrication steps for both micro-reactors and 2D PhC's are presented. Experimentally-obtained thermal emission spectra of the 2D PhC structures match well with numerical predictions.",
    "advisors": ["Leslie Kolodziejski"],
    "text": "Efficient silicon micro-reactors for thermophotovoltaic applications Thermophotovoltaic (TPV) systems passively generate electricity from the combustion of fuel. Although TPV conversion systems have advantages, they suffer from low efficiency. This thesis investigates different ways to increase the efficiency of TPV systems. In particular the thesis details micro-fabrication of silicon micro-reactors, and twodimensional tungsten photonic crystals (2D W PhC) for high-temperature applications such as selective thermal emitters for TPV energy conversion. Interference lithography and reactive ion etching are used to produce large-area single-crystal tungsten 2D PhC's. The fabricated PhC consists of an array of cylindrical cavities with 800nm diameter, 1.2 pm depth, and 1.2 pm period. Extensive characterization and calibration of all micro-fabrication steps for both micro-reactors and 2D PhC's are presented. Experimentally-obtained thermal emission spectra of the 2D PhC structures match well with numerical predictions."
}, {
    "id": "oai:dspace.mit.edu:1721.1/118078",
    "title": "Alternating conditional expectation (ACE) applied to classification and recommendation problems",
    "abstract": "In this thesis, a geometric framework for describing relevant information in a collection of data is applied for the general problems of selecting informative features (dimension reduction) from high dimensional data. The framework can be used in an unsupervised manner, extracting universal features that can be used later for general classification of data. This framework is derived by applying local approximations on the space of probability distributions and a small perturbation approach. With this approach, different information theoretic results can be interpreted as linear algebra optimizations based on the norms of vectors in a linear space, which are in general, easier to carry out. Fundamentally, using known procedures such as Singular Value Decomposition (SVD) and Principal Component Analysis (PCA), dimension reduction for maximizing power can be achieved in a straight forward manner. Using the geometric framework, we relate calculation of SVD of a particular matrix related to a probabilistic channel to the application of Alternating Conditional Expectation (ACE) in the problem of optimal regression. The key takeaway of this method is that such problems can be studied in the space of distributions of the data and not the space of outcomes. This geometric framework allows to give an operational meaning to information metrics in the context of data analysis and feature selection. Additionally, it provides a method to obtain universal classification functions without knowledge of the important feature of the problem. This framework is the applied to the problem of data classification and analysis with satisfactory results.",
    "advisors": ["Lizhong Zheng"],
    "text": "Alternating conditional expectation (ACE) applied to classification and recommendation problems In this thesis, a geometric framework for describing relevant information in a collection of data is applied for the general problems of selecting informative features (dimension reduction) from high dimensional data. The framework can be used in an unsupervised manner, extracting universal features that can be used later for general classification of data. This framework is derived by applying local approximations on the space of probability distributions and a small perturbation approach. With this approach, different information theoretic results can be interpreted as linear algebra optimizations based on the norms of vectors in a linear space, which are in general, easier to carry out. Fundamentally, using known procedures such as Singular Value Decomposition (SVD) and Principal Component Analysis (PCA), dimension reduction for maximizing power can be achieved in a straight forward manner. Using the geometric framework, we relate calculation of SVD of a particular matrix related to a probabilistic channel to the application of Alternating Conditional Expectation (ACE) in the problem of optimal regression. The key takeaway of this method is that such problems can be studied in the space of distributions of the data and not the space of outcomes. This geometric framework allows to give an operational meaning to information metrics in the context of data analysis and feature selection. Additionally, it provides a method to obtain universal classification functions without knowledge of the important feature of the problem. This framework is the applied to the problem of data classification and analysis with satisfactory results."
}, {
    "id": "oai:dspace.mit.edu:1721.1/117814",
    "title": "Systemic risk in the interbank lending market",
    "abstract": "Our goal is to understand the functioning of the interbank lending market in times of market stress. Working towards this goal, we conduct theoretical analysis and simulation to study the effects of network structure and shock scenarios on systemic risk in the market. We consider shocks of various sizes at both global and local scales. In terms of risk measures, we study relative systemic loss and the default rate, separating the latter quantity into fundamental default and contagion. Our simulations suggest that all systemic risk measures are similar on the well-studied directed Erds-Rnyi model and the more complex fitness model if we match the mean density and the mean edge weight of these two models. We show through both derivations and simulations that the network size has little effect on systemic risk when the network is sufficiently large. Moreover, as the mean degree grows, the different default rates considered all increase, while relative systemic loss decreases. Furthermore, simulations suggest that local shocks tend to cause more harm than global shocks of the same total size. We also derive upper and lower bounds on a bank's probability of default, only using its neighbors' information. For implementation, we build a method for real-time, automatic, interpretable assessment of financial systemic risk, which only requires temporal snapshots of observable data. Our algorithm takes in partial data, inferring a random graph model, and then generates empirical distributions for risk measures. The first part relies on inferring a fitness model that is compatible with observed information. For the second part, we use simulations to obtain empirical distributions for systemic risk that arises from interbank clearing. We test our method on synthetic data and apply it to the federal funds market using empirical data. Our method is fast enough to be incorporated into algorithms that produce intraday time trajectories of risk prediction. The data requirement is practical for investors as well as regulators, policy-makers, and financial institutions.",
    "advisors": ["John N. Tsitsiklis", "Munther A. Dahleh"],
    "text": "Systemic risk in the interbank lending market Our goal is to understand the functioning of the interbank lending market in times of market stress. Working towards this goal, we conduct theoretical analysis and simulation to study the effects of network structure and shock scenarios on systemic risk in the market. We consider shocks of various sizes at both global and local scales. In terms of risk measures, we study relative systemic loss and the default rate, separating the latter quantity into fundamental default and contagion. Our simulations suggest that all systemic risk measures are similar on the well-studied directed Erds-Rnyi model and the more complex fitness model if we match the mean density and the mean edge weight of these two models. We show through both derivations and simulations that the network size has little effect on systemic risk when the network is sufficiently large. Moreover, as the mean degree grows, the different default rates considered all increase, while relative systemic loss decreases. Furthermore, simulations suggest that local shocks tend to cause more harm than global shocks of the same total size. We also derive upper and lower bounds on a bank's probability of default, only using its neighbors' information. For implementation, we build a method for real-time, automatic, interpretable assessment of financial systemic risk, which only requires temporal snapshots of observable data. Our algorithm takes in partial data, inferring a random graph model, and then generates empirical distributions for risk measures. The first part relies on inferring a fitness model that is compatible with observed information. For the second part, we use simulations to obtain empirical distributions for systemic risk that arises from interbank clearing. We test our method on synthetic data and apply it to the federal funds market using empirical data. Our method is fast enough to be incorporated into algorithms that produce intraday time trajectories of risk prediction. The data requirement is practical for investors as well as regulators, policy-makers, and financial institutions."
}, {
    "id": "oai:dspace.mit.edu:1721.1/38556",
    "title": "Optical frequency domain imaging of human retina and choroid",
    "abstract": "Optical coherence tomography (OCT) has emerged as a practical noninvasive technology for imaging the microstructure of the human eye in vivo. Using optical interferometry to spatially-resolve backreflections from within tissue, this high-resolution technique provides cross-sectional images of the anterior and posterior eye segments that had previously only been possible with histology. Current commercially-available OCT systems suffer limitations in speed and sensitivity, preventing them from effective screening of the retina and having a larger impact on the clinical environment. While other technological advances have addressed this problem, they are inadequate for imaging the choroid, which can be useful for evaluating choroidal disorders as well as early stages of retinal diseases. The objective of this thesis was to develop a new ophthalmic imaging method, termed optical frequency domain imaging (OFDI), to overcome these limitations. Preliminary imaging of the posterior segment of human eyes in vivo was performed to evaluate the utility of this instrument for comprehensive ophthalmic examination.",
    "advisors": ["Seok-Hyun Yun", "Brett E. Bouma"],
    "text": "Optical frequency domain imaging of human retina and choroid Optical coherence tomography (OCT) has emerged as a practical noninvasive technology for imaging the microstructure of the human eye in vivo. Using optical interferometry to spatially-resolve backreflections from within tissue, this high-resolution technique provides cross-sectional images of the anterior and posterior eye segments that had previously only been possible with histology. Current commercially-available OCT systems suffer limitations in speed and sensitivity, preventing them from effective screening of the retina and having a larger impact on the clinical environment. While other technological advances have addressed this problem, they are inadequate for imaging the choroid, which can be useful for evaluating choroidal disorders as well as early stages of retinal diseases. The objective of this thesis was to develop a new ophthalmic imaging method, termed optical frequency domain imaging (OFDI), to overcome these limitations. Preliminary imaging of the posterior segment of human eyes in vivo was performed to evaluate the utility of this instrument for comprehensive ophthalmic examination."
}, {
    "id": "oai:dspace.mit.edu:1721.1/53145",
    "title": "File system unification using LatticeFS",
    "abstract": "LatticeFS is a namespace unification system designed to merge multiple source file systems into a single working file system. atticeFS can be used to merge multiple software package directories, work with multiple file systems as if they are one, and share a single storage medium among multiple machines. On a high level, LatticeFS takes as input an arbitrary number of file system paths, and mounts a new virtual drive that will appear to the user as a union of the input file systems. Of course, attempting to combine multiple file systems will inevitably be met with conflicts. Situations in which multiple input file systems contain files/directories with the same name will be common in large systems; which file/directory should the user be exposed to in this case? Previous work such as UnionFS solved the problem by giving each input file system a strict priority value, and when a conflict occurred, the file/directory with the highest priority was the one shown to the user. In LatticeFS, we have introduced a plug-in system in which different strategies for resolving conflicts can be easily swapped in and out; additionally, handlers for special file types can also be \"plugged\" into the system. This paper describes and evaluates all aspects of LatticeFS in detail.",
    "advisors": ["Stephen A. Ward"],
    "text": "File system unification using LatticeFS LatticeFS is a namespace unification system designed to merge multiple source file systems into a single working file system. atticeFS can be used to merge multiple software package directories, work with multiple file systems as if they are one, and share a single storage medium among multiple machines. On a high level, LatticeFS takes as input an arbitrary number of file system paths, and mounts a new virtual drive that will appear to the user as a union of the input file systems. Of course, attempting to combine multiple file systems will inevitably be met with conflicts. Situations in which multiple input file systems contain files/directories with the same name will be common in large systems; which file/directory should the user be exposed to in this case? Previous work such as UnionFS solved the problem by giving each input file system a strict priority value, and when a conflict occurred, the file/directory with the highest priority was the one shown to the user. In LatticeFS, we have introduced a plug-in system in which different strategies for resolving conflicts can be easily swapped in and out; additionally, handlers for special file types can also be \"plugged\" into the system. This paper describes and evaluates all aspects of LatticeFS in detail."
}, {
    "id": "oai:dspace.mit.edu:1721.1/34360",
    "title": "An implementation of a 5.25 GHz transceiver for high data rate wireless applications",
    "abstract": "The desire for transmission of high data rate information across wireless channels has grown immensely over the past decade. Wireless devices available today including mobile phones, wireless local area networks (WLANs) and Bluetooth radios have realized a wide variety of applications at data rates ranging from 10s of kbit/s to 10s of Mbit/s. Mobile telephone design strives for large transmit distances, Bluetooth technology enables communication between two close range devices, and wireless LAN strives to achieve a high data rate wireless link within an office or home environment. This link is traditionally implemented through a central access point that communicates with one or more workstations. Due to the large number of applications demanding high speed wireless links, the aspiration for even higher data rates is prevalent.",
    "advisors": ["Charles G. Sodini"],
    "text": "An implementation of a 5.25 GHz transceiver for high data rate wireless applications The desire for transmission of high data rate information across wireless channels has grown immensely over the past decade. Wireless devices available today including mobile phones, wireless local area networks (WLANs) and Bluetooth radios have realized a wide variety of applications at data rates ranging from 10s of kbit/s to 10s of Mbit/s. Mobile telephone design strives for large transmit distances, Bluetooth technology enables communication between two close range devices, and wireless LAN strives to achieve a high data rate wireless link within an office or home environment. This link is traditionally implemented through a central access point that communicates with one or more workstations. Due to the large number of applications demanding high speed wireless links, the aspiration for even higher data rates is prevalent."
}, {
    "id": "oai:dspace.mit.edu:1721.1/36782",
    "title": "Dielectric Resonator Antennas : theory and design",
    "abstract": "Theoretical models for the analysis of Dielectric Resonator Antenna (DRA) are developed. There are no exact solutions to many of the problems in analytical form, therefore a strong focus on the physical interpretation of the numerical results is presented alongside theoretical models. I have used the physical interpretation of the numerical results to lay down some important design rules. A few new inventions associated with the DRA are also included. These are the elliptical DRA, the DRA with a rectangular slot, the adjustable reactance feed, the triangular DRA and the dual band DRA-patch antenna.",
    "advisors": ["Ali Tassoudji", "Jin Au Kong"],
    "text": "Dielectric Resonator Antennas : theory and design Theoretical models for the analysis of Dielectric Resonator Antenna (DRA) are developed. There are no exact solutions to many of the problems in analytical form, therefore a strong focus on the physical interpretation of the numerical results is presented alongside theoretical models. I have used the physical interpretation of the numerical results to lay down some important design rules. A few new inventions associated with the DRA are also included. These are the elliptical DRA, the DRA with a rectangular slot, the adjustable reactance feed, the triangular DRA and the dual band DRA-patch antenna."
}, {
    "id": "oai:dspace.mit.edu:1721.1/101580",
    "title": "Optimization of electron optics in a resonator cavity using Nelder-Mead simplex search for the quantum electron microscope",
    "abstract": "The Quantum Electron Microscope (QEM) is a proposed imaging modality that aims to reduce or eliminate the effects of radiation on living cells compared to traditional electron microscopy techniques. In recent years, an interaction free measurement scheme was proposed by Putnam and Yanik [1], and an implementation of this idea is being developed by an international collaboration. The current implementation foresees an electron cavity, which can be installed into a regular scanning electron microscope, to allow multiple passes of two electron wavefunctions over the specimen. In order to implement this idea, multiple different electron optical designs were proposed. Extensive simulation work is required to test and validate these designs. This work outlines the simulation work done for QEM, and proposes a general framework for optimizing electron trajectory simulations using Nelder-Mead search. It also provides a library of MATLAB wrapper functions and optimization methods to be used with the Integrated Lorentz-2E software.",
    "advisors": ["Mehmet Fatih Yanik"],
    "text": "Optimization of electron optics in a resonator cavity using Nelder-Mead simplex search for the quantum electron microscope The Quantum Electron Microscope (QEM) is a proposed imaging modality that aims to reduce or eliminate the effects of radiation on living cells compared to traditional electron microscopy techniques. In recent years, an interaction free measurement scheme was proposed by Putnam and Yanik [1], and an implementation of this idea is being developed by an international collaboration. The current implementation foresees an electron cavity, which can be installed into a regular scanning electron microscope, to allow multiple passes of two electron wavefunctions over the specimen. In order to implement this idea, multiple different electron optical designs were proposed. Extensive simulation work is required to test and validate these designs. This work outlines the simulation work done for QEM, and proposes a general framework for optimizing electron trajectory simulations using Nelder-Mead search. It also provides a library of MATLAB wrapper functions and optimization methods to be used with the Integrated Lorentz-2E software."
}, {
    "id": "oai:dspace.mit.edu:1721.1/29695",
    "title": "Biologically-plausible six-legged running : control and simulation",
    "abstract": "This thesis presents a controller which produces a stable, dynamic 1.4 meter per second run in a simulated twelve degree of freedom six-legged robot. The algorithm is relatively simple; it consists of only a few hand-tuned feedback loops and is defined by a total of 13 parameters. The control utilizes no vestibular-type inputs to actively control orientation. Evidence from perturbation, robustness, motion analysis, and parameter sensitivity tests indicate a high degree of stability in the simulated gait. The control approach generates a run with an aerial phase, utilizes force information to signal aerial phase leg retraction, has a forward running velocity determined by a single parameter, and couples stance and swing legs using angular momentum information. Both the hypotheses behind the control and the resulting gait are argued to be plausible models of biological locomotion.",
    "advisors": ["Hugh M. Herr"],
    "text": "Biologically-plausible six-legged running : control and simulation This thesis presents a controller which produces a stable, dynamic 1.4 meter per second run in a simulated twelve degree of freedom six-legged robot. The algorithm is relatively simple; it consists of only a few hand-tuned feedback loops and is defined by a total of 13 parameters. The control utilizes no vestibular-type inputs to actively control orientation. Evidence from perturbation, robustness, motion analysis, and parameter sensitivity tests indicate a high degree of stability in the simulated gait. The control approach generates a run with an aerial phase, utilizes force information to signal aerial phase leg retraction, has a forward running velocity determined by a single parameter, and couples stance and swing legs using angular momentum information. Both the hypotheses behind the control and the resulting gait are argued to be plausible models of biological locomotion."
}, {
    "id": "oai:dspace.mit.edu:1721.1/30099",
    "title": "Double-gated field emission arrays",
    "abstract": "There is a need for massively parallel, individually addressed and focused electron sources for applications such as flat panel displays, mass storage and multi-beam electron beam lithography. This project fabricates and characterizes double-gated field emission devices with high aspect ratio. One of the gates extracts the electrons while the second gate focuses the electrons into small spots. High aspect ratio silicon field emitters were defined by reactive ion etching of silicon followed by multiple depositions of polycrystalline oxide insulators and silicon gates. The layers were defined by a combination of lithography, chemical mechanical polishing and micromachining. We obtained devices with gate and focus apertures of 0.4[mu]m and 1.2[mu]m diameter. The anode current has very little dependence on the focus voltage and the ratio of the focus field factor to the gate field factor F / G is 0.015. Scanning electron micrographs of the devices, numerical simulation and spot size measurements on a phosphor screen confirmed these results. An e-beam resist, PMMA, was successfully exposed using the FEA device as an electron source.",
    "advisors": ["Akintunde Ibitayo (Tayo) Akinwande"],
    "text": "Double-gated field emission arrays There is a need for massively parallel, individually addressed and focused electron sources for applications such as flat panel displays, mass storage and multi-beam electron beam lithography. This project fabricates and characterizes double-gated field emission devices with high aspect ratio. One of the gates extracts the electrons while the second gate focuses the electrons into small spots. High aspect ratio silicon field emitters were defined by reactive ion etching of silicon followed by multiple depositions of polycrystalline oxide insulators and silicon gates. The layers were defined by a combination of lithography, chemical mechanical polishing and micromachining. We obtained devices with gate and focus apertures of 0.4[mu]m and 1.2[mu]m diameter. The anode current has very little dependence on the focus voltage and the ratio of the focus field factor to the gate field factor F / G is 0.015. Scanning electron micrographs of the devices, numerical simulation and spot size measurements on a phosphor screen confirmed these results. An e-beam resist, PMMA, was successfully exposed using the FEA device as an electron source."
}, {
    "id": "oai:dspace.mit.edu:1721.1/17966",
    "title": "Delivering real-time holographic video content with off-the-shelf PC hardware",
    "abstract": "We present a PC based system to simultaneously compute real-time holographic video content and to serve as a framebuffer to drive a holographic video display. Our system uses only 3 PCs each equipped with an nVidia Quadro FX 3000G video card. It replaces a SGI Onyx and the custom built Cheops Image Processing System that previously served as the platform driving the MIT second-generation Holovideo display. With a prototype content generation implementation, we compute holographic stereograms and update the display at a rate of roughly 2 frames per second.",
    "advisors": ["V. Michael Bove, Jr"],
    "text": "Delivering real-time holographic video content with off-the-shelf PC hardware We present a PC based system to simultaneously compute real-time holographic video content and to serve as a framebuffer to drive a holographic video display. Our system uses only 3 PCs each equipped with an nVidia Quadro FX 3000G video card. It replaces a SGI Onyx and the custom built Cheops Image Processing System that previously served as the platform driving the MIT second-generation Holovideo display. With a prototype content generation implementation, we compute holographic stereograms and update the display at a rate of roughly 2 frames per second."
}, {
    "id": "oai:dspace.mit.edu:1721.1/108959",
    "title": "Optimal sizing of solar and battery assets in decentralized micro-grids with demand-side management",
    "abstract": "Solar-based community micro-grids and individual home systems have been recognized as key enablers of electricity provision to the over one billion people living without energy access to-date. Despite significant cost reductions in solar panels, these options can still be cost-prohibitive mainly due over-sizing of generation assets corresponding with a lack of ability to actively manage electricity demand. The main contribution shared is the methodology and optimization approach of least-cost combinations of generation asset sizes, in solar panels and batteries, subject to meeting reliability constraints; these results are based on a techno-economic modeling approach constructed for assessing decentralized micro-grids with demand-side management capabilities. The software model constructed is implemented to represent the technical characteristics of a low-voltage, direct current network architecture and computational capabilities of a power management device. The main use-case of the model presented is based on serving representative, aggregated, household-level load profiles combined with simulated power output from solar photovoltaic modules and the kinetic operating constraints of lead-acid batteries at hourly timesteps over year-long simulations. The state-space for solutions is based on available solar module and battery capacities from distributors in Jharkhand, India. Additional work presented also extends to real-time operation of such isolated micro-grids with requisite local computation. First, for load disaggregation and forecasting purposes, clustering algorithms and statistical learning techniques are applied on quantitative results from inferred load profiles based on data logged from off-grid solar home systems. Second, results from an optimization approach to accurately parametrize a lead-acid battery model for potential usage in real-time field implementation are also shared. Economic results, sensitivity analyses around key technical and financial input assumptions, and comparisons in cost reductions due to the optimization of solar and battery assets for decentralized micro-grids with demand-side management capabilities are subsequently presented. The work concludes with insights and policy implications on establishing differentiated willingness-to-pay, tiers of service, and dynamic price-setting in advanced micro-grids.",
    "advisors": ["Rajeev Ram", "Munther Dahleh"],
    "text": "Optimal sizing of solar and battery assets in decentralized micro-grids with demand-side management Solar-based community micro-grids and individual home systems have been recognized as key enablers of electricity provision to the over one billion people living without energy access to-date. Despite significant cost reductions in solar panels, these options can still be cost-prohibitive mainly due over-sizing of generation assets corresponding with a lack of ability to actively manage electricity demand. The main contribution shared is the methodology and optimization approach of least-cost combinations of generation asset sizes, in solar panels and batteries, subject to meeting reliability constraints; these results are based on a techno-economic modeling approach constructed for assessing decentralized micro-grids with demand-side management capabilities. The software model constructed is implemented to represent the technical characteristics of a low-voltage, direct current network architecture and computational capabilities of a power management device. The main use-case of the model presented is based on serving representative, aggregated, household-level load profiles combined with simulated power output from solar photovoltaic modules and the kinetic operating constraints of lead-acid batteries at hourly timesteps over year-long simulations. The state-space for solutions is based on available solar module and battery capacities from distributors in Jharkhand, India. Additional work presented also extends to real-time operation of such isolated micro-grids with requisite local computation. First, for load disaggregation and forecasting purposes, clustering algorithms and statistical learning techniques are applied on quantitative results from inferred load profiles based on data logged from off-grid solar home systems. Second, results from an optimization approach to accurately parametrize a lead-acid battery model for potential usage in real-time field implementation are also shared. Economic results, sensitivity analyses around key technical and financial input assumptions, and comparisons in cost reductions due to the optimization of solar and battery assets for decentralized micro-grids with demand-side management capabilities are subsequently presented. The work concludes with insights and policy implications on establishing differentiated willingness-to-pay, tiers of service, and dynamic price-setting in advanced micro-grids."
}, {
    "id": "oai:dspace.mit.edu:1721.1/112821",
    "title": "Tenant-level network performance isolation in Flowtune",
    "abstract": "Performance isolation is a major concern for multi-tenant datacenters. Many service level agreements include a specification on the allotment of resources. For tenants, these resource guarantees are critical to the availability and efficiency of their services. While CPU, disk, and memory isolation are well-understood, network performance isolation is less straightforward. In this thesis, I investigate methods for enforcing bandwidth fairness guarantees for logical networks in a datacenter and implement network performance isolation in Flowtune. Flowtune is a datacenter network architecture which introduces a centralized arbiter to enforce congestion control at the flowlet level. Flowtune achieves rapid convergence to a desired allocation of network resources in addition to reducing tail latencies in various settings. However, Flowtune currently does not provide tenant-level network performance isolation.",
    "advisors": ["Jonathan Perry", "Hari Balakrishnan"],
    "text": "Tenant-level network performance isolation in Flowtune Performance isolation is a major concern for multi-tenant datacenters. Many service level agreements include a specification on the allotment of resources. For tenants, these resource guarantees are critical to the availability and efficiency of their services. While CPU, disk, and memory isolation are well-understood, network performance isolation is less straightforward. In this thesis, I investigate methods for enforcing bandwidth fairness guarantees for logical networks in a datacenter and implement network performance isolation in Flowtune. Flowtune is a datacenter network architecture which introduces a centralized arbiter to enforce congestion control at the flowlet level. Flowtune achieves rapid convergence to a desired allocation of network resources in addition to reducing tail latencies in various settings. However, Flowtune currently does not provide tenant-level network performance isolation."
}, {
    "id": "oai:dspace.mit.edu:1721.1/37066",
    "title": "Algorithms for simulating human pre-mRNA splicing decisions",
    "abstract": "In this thesis, I developed a program, ExonScan, to simulate constitutive human pre-mRNA splicing. ExonScan includes several models for splicing components, including splice sites, exonic splicing enhancers, exonic splicing silencers, and intronic splicing enhancers. I used ExonScan to test various aspects of human splicing, including correlation of splicing signal strength with tissue expression levels, the effectiveness of experimentally determined exonic splicing silencers, and splice site identification.",
    "advisors": ["Christopher B. Burge"],
    "text": "Algorithms for simulating human pre-mRNA splicing decisions In this thesis, I developed a program, ExonScan, to simulate constitutive human pre-mRNA splicing. ExonScan includes several models for splicing components, including splice sites, exonic splicing enhancers, exonic splicing silencers, and intronic splicing enhancers. I used ExonScan to test various aspects of human splicing, including correlation of splicing signal strength with tissue expression levels, the effectiveness of experimentally determined exonic splicing silencers, and splice site identification."
}, {
    "id": "oai:dspace.mit.edu:1721.1/85805",
    "title": "A stacked full-bridge microinverter topology for photovoltaic applications",
    "abstract": "Previous work has been done to develop a microinverter for solar photovoltaic applications consisting of a high-frequency series resonant inverter and transformer section connected to a a cycloconverter that modulates the resonant current into a single-phase 240 VRMS utility line. This thesis presents a new stacked full-bridge topology that improves upon the previous high-frequency inverter section. By utilizing new operating modes to reduce the reliance on frequency control and allowing for the use of lower blocking voltage transistors, the operating frequency range of the HF inverter is reduced and efficiency is increased, especially at low output powers and lower portions of the line cycle. The design of an experimental prototype to test the stacked full-bridge HF inverter topology is presented along with test results that demonstrate the success of the topology. Future improvements to increase performance are also suggested.",
    "advisors": ["David J. Perreault"],
    "text": "A stacked full-bridge microinverter topology for photovoltaic applications Previous work has been done to develop a microinverter for solar photovoltaic applications consisting of a high-frequency series resonant inverter and transformer section connected to a a cycloconverter that modulates the resonant current into a single-phase 240 VRMS utility line. This thesis presents a new stacked full-bridge topology that improves upon the previous high-frequency inverter section. By utilizing new operating modes to reduce the reliance on frequency control and allowing for the use of lower blocking voltage transistors, the operating frequency range of the HF inverter is reduced and efficiency is increased, especially at low output powers and lower portions of the line cycle. The design of an experimental prototype to test the stacked full-bridge HF inverter topology is presented along with test results that demonstrate the success of the topology. Future improvements to increase performance are also suggested."
}, {
    "id": "oai:dspace.mit.edu:1721.1/85510",
    "title": "Teaching computer science principles using StarLogoTNG",
    "abstract": "This thesis outlines the development of a 3-module set of lesson plans implemented using StarLogoTNG. The purpose of these lesson plans are to serve as a vehicle for teaching and reinforcing specific learning objectives of the CollegeBoard's Advanced Placement Computer Science Principles course, which has 7 main themes. Each lesson plan has as its focus a subset of learning objectives from one of the themes of Creativity, Data, or Internet, while simultaneously incorporating additional learning goals from the themes of Abstraction, Programming, Algorithms, and Impact. These interactive lesson plans go beyond the use of StarLogoTNG to complete specific tasks by integrating meaningful class discussions and occasional peer instruction and peer review activities. Such activities become catalysts for students to develop a deeper understanding of the course materials. By connecting learning goals from different themes of the course and packaging them in cohesive lesson plans that utilize methods of teaching for understanding, this thesis aims to provide a useful and effective set of a materials for the instruction of computer science principles.",
    "advisors": ["Eric Klopfer"],
    "text": "Teaching computer science principles using StarLogoTNG This thesis outlines the development of a 3-module set of lesson plans implemented using StarLogoTNG. The purpose of these lesson plans are to serve as a vehicle for teaching and reinforcing specific learning objectives of the CollegeBoard's Advanced Placement Computer Science Principles course, which has 7 main themes. Each lesson plan has as its focus a subset of learning objectives from one of the themes of Creativity, Data, or Internet, while simultaneously incorporating additional learning goals from the themes of Abstraction, Programming, Algorithms, and Impact. These interactive lesson plans go beyond the use of StarLogoTNG to complete specific tasks by integrating meaningful class discussions and occasional peer instruction and peer review activities. Such activities become catalysts for students to develop a deeper understanding of the course materials. By connecting learning goals from different themes of the course and packaging them in cohesive lesson plans that utilize methods of teaching for understanding, this thesis aims to provide a useful and effective set of a materials for the instruction of computer science principles."
}, {
    "id": "oai:dspace.mit.edu:1721.1/77016",
    "title": "Frequency domain model-based intracranial pressure estimation",
    "abstract": "Elevation of intracranial pressure (ICP), the pressure of the fluid surrounding the brain, can require urgent medical attention. Current methods for determining ICP are invasive, require neurosurgical expertise, and can lead to infection. ICP measurement is therefore limited to the sickest patients, though many others could potentially benefit from availability of this vital sign. We present a frequency-domain approach to ICP estimation using a simple lumped, linear time-invariant model of cerebrovascular dynamics. Preliminary results from 28 records of patients with severe traumatic brain injury are presented and discussed. Suggestions for future work to improve the estimation algorithm are proposed.",
    "advisors": ["George C. Verghese", "Faisal M. Kashif"],
    "text": "Frequency domain model-based intracranial pressure estimation Elevation of intracranial pressure (ICP), the pressure of the fluid surrounding the brain, can require urgent medical attention. Current methods for determining ICP are invasive, require neurosurgical expertise, and can lead to infection. ICP measurement is therefore limited to the sickest patients, though many others could potentially benefit from availability of this vital sign. We present a frequency-domain approach to ICP estimation using a simple lumped, linear time-invariant model of cerebrovascular dynamics. Preliminary results from 28 records of patients with severe traumatic brain injury are presented and discussed. Suggestions for future work to improve the estimation algorithm are proposed."
}, {
    "id": "oai:dspace.mit.edu:1721.1/82391",
    "title": "Model-based compressive sensing with Earth Mover's Distance constraints",
    "abstract": "In compressive sensing, we want to recover ... from linear measurements of the form ... describes the measurement process. Standard results in compressive sensing show that it is possible to exactly recover the signal x from only m ... measurements for certain types of matrices. Model-based compressive sensing reduces the number of measurements even further by limiting the supports of x to a subset of the ... possible supports. Such a family of supports is called a structured sparsity model. In this thesis, we introduce a structured sparsity model for two-dimensional signals that have similar support in neighboring columns. We quantify the change in support between neighboring columns with the Earth Mover's Distance (EMD), which measures both how many elements of the support change and how far the supported elements move. We prove that for a reasonable limit on the EMD between adjacent columns, we can recover signals in our model from only ... measurements, where w is the width of the signal. This is an asymptotic improvement over the ... bound in standard compressive sensing. While developing the algorithmic tools for our proposed structured sparsity model, we also extend the model-based compressed sensing framework. In order to use a structured sparsity model in compressive sensing, we need a model projection algorithm that, given an arbitrary signal x, returns the best approximation in the model. We relax this constraint and develop a variant of IHT, an existing sparse recovery algorithm, that works with approximate model projection algorithms.",
    "advisors": ["Piotr Indyk"],
    "text": "Model-based compressive sensing with Earth Mover's Distance constraints In compressive sensing, we want to recover ... from linear measurements of the form ... describes the measurement process. Standard results in compressive sensing show that it is possible to exactly recover the signal x from only m ... measurements for certain types of matrices. Model-based compressive sensing reduces the number of measurements even further by limiting the supports of x to a subset of the ... possible supports. Such a family of supports is called a structured sparsity model. In this thesis, we introduce a structured sparsity model for two-dimensional signals that have similar support in neighboring columns. We quantify the change in support between neighboring columns with the Earth Mover's Distance (EMD), which measures both how many elements of the support change and how far the supported elements move. We prove that for a reasonable limit on the EMD between adjacent columns, we can recover signals in our model from only ... measurements, where w is the width of the signal. This is an asymptotic improvement over the ... bound in standard compressive sensing. While developing the algorithmic tools for our proposed structured sparsity model, we also extend the model-based compressed sensing framework. In order to use a structured sparsity model in compressive sensing, we need a model projection algorithm that, given an arbitrary signal x, returns the best approximation in the model. We relax this constraint and develop a variant of IHT, an existing sparse recovery algorithm, that works with approximate model projection algorithms."
}, {
    "id": "oai:dspace.mit.edu:1721.1/18007",
    "title": "Graph-based privacy preference expression for the semantic web",
    "abstract": "The Web is changing. Originally a medium for human-readable documents, the next generation Semantic Web is opening up toolkit that provides vast opportunities for sharing information that is encoded in a machine-readable form. The opportunities for sharing data also are opportunities for encroaching on people's privacy. While some technologies already exist for expressing privacy preferences, they do not integrate closely with the Semantic Web. In addition, previous approaches to privacy expression do not easily expand to data shared over multiple hops, with different privacy preferences at each hop. The Private Information Management Agent is an attempt to mitigate these concerns.",
    "advisors": ["Daniel J. Weitzner", "Ronald L. Rivest"],
    "text": "Graph-based privacy preference expression for the semantic web The Web is changing. Originally a medium for human-readable documents, the next generation Semantic Web is opening up toolkit that provides vast opportunities for sharing information that is encoded in a machine-readable form. The opportunities for sharing data also are opportunities for encroaching on people's privacy. While some technologies already exist for expressing privacy preferences, they do not integrate closely with the Semantic Web. In addition, previous approaches to privacy expression do not easily expand to data shared over multiple hops, with different privacy preferences at each hop. The Private Information Management Agent is an attempt to mitigate these concerns."
}, {
    "id": "oai:dspace.mit.edu:1721.1/37099",
    "title": "A Web application to improve emotional awareness in high-functioning autistics",
    "abstract": "The web application built here is based on the idea of presenting scenarios to users, using text, and having the users choose likely emotions that match the scenarios. Taken for granted by most neurotypical people, high-functioning autistics are often lacking in this area of social-skill development. This idea of emotion to scenario matching is accomplished using a series of different games that take different approaches to exercise these skills. The application relies on the two main Artificial Intelligence (AI) approaches. The first AI approach is classical, relying on computer-based algorithms developed by others to judge text and put out the correct affect or emotion. The other part of the AI relies on users of the system contributing via regular usage or explicit correction to train the system in a type of feedback loop.",
    "advisors": ["Henry Lieberman"],
    "text": "A Web application to improve emotional awareness in high-functioning autistics The web application built here is based on the idea of presenting scenarios to users, using text, and having the users choose likely emotions that match the scenarios. Taken for granted by most neurotypical people, high-functioning autistics are often lacking in this area of social-skill development. This idea of emotion to scenario matching is accomplished using a series of different games that take different approaches to exercise these skills. The application relies on the two main Artificial Intelligence (AI) approaches. The first AI approach is classical, relying on computer-based algorithms developed by others to judge text and put out the correct affect or emotion. The other part of the AI relies on users of the system contributing via regular usage or explicit correction to train the system in a type of feedback loop."
}, {
    "id": "oai:dspace.mit.edu:1721.1/33201",
    "title": "Optimal control of controllable switched systems",
    "abstract": "Many of the existing techniques for controlling switched systems either require the solution to a complex optimization problem or significant sacrifices to either stability or performance to offer practical controllers. In [13], it is shown that stabilizing, practical controllers with meaningful performance guarantees can be constructed for a specific class of hybrid systems by parameterizing the controller actions by a finite set. We extend this approach to the control of controllable switched systems by constraining the switching portion of the control input and fixing the feedback controller for each subsystem. We show that, under reasonable assumptions, the resulting system is guaranteed to converge to the target while providing meaningful performance. We apply our approach to the direct-injection stratified charge (DISC) engine and compare the results to that of a model predictive controller designed for the same application.",
    "advisors": ["Munther A. Dahleh"],
    "text": "Optimal control of controllable switched systems Many of the existing techniques for controlling switched systems either require the solution to a complex optimization problem or significant sacrifices to either stability or performance to offer practical controllers. In [13], it is shown that stabilizing, practical controllers with meaningful performance guarantees can be constructed for a specific class of hybrid systems by parameterizing the controller actions by a finite set. We extend this approach to the control of controllable switched systems by constraining the switching portion of the control input and fixing the feedback controller for each subsystem. We show that, under reasonable assumptions, the resulting system is guaranteed to converge to the target while providing meaningful performance. We apply our approach to the direct-injection stratified charge (DISC) engine and compare the results to that of a model predictive controller designed for the same application."
}, {
    "id": "oai:dspace.mit.edu:1721.1/16825",
    "title": "A framework for multi-modal input in a pervasive computing environment",
    "abstract": "In this thesis, we propose a framework that uses multiple-domains and multi-modal techniques to disambiguate a variety of natural human input modes. This system is based on the input needs of pervasive computing users. The work extends the Galaxy architecture developed by the Spoken Language Systems group at MIT. Just as speech recognition disambiguates an input wave form by using a grammar to find the best matching phrase, we use the same mechanism to disambiguate other input forms, T9 in particular. A skeleton version of the framework was implemented to show this framework is possible and to explore some of the issues that might arise. The system currently works for both T9 and Speech modes. The framework also includes potential for any other type of input for which a recognizer can be built such as graffiti input.",
    "advisors": ["Larry Rudolph"],
    "text": "A framework for multi-modal input in a pervasive computing environment In this thesis, we propose a framework that uses multiple-domains and multi-modal techniques to disambiguate a variety of natural human input modes. This system is based on the input needs of pervasive computing users. The work extends the Galaxy architecture developed by the Spoken Language Systems group at MIT. Just as speech recognition disambiguates an input wave form by using a grammar to find the best matching phrase, we use the same mechanism to disambiguate other input forms, T9 in particular. A skeleton version of the framework was implemented to show this framework is possible and to explore some of the issues that might arise. The system currently works for both T9 and Speech modes. The framework also includes potential for any other type of input for which a recognizer can be built such as graffiti input."
}, {
    "id": "oai:dspace.mit.edu:1721.1/34370",
    "title": "Active pixel sensors for X-ray astronomy",
    "abstract": "An active pixel sensor array, APS-1, has been fabricated for the purpose of scientific x-ray detection. This thesis presents the results of testing the device. Alternate design architectures are explored. Recommendations are made for a next-generation sensor. CCDs have been the dominant x-ray sensor in astronomy for over ten years. Limitations inherent to CCDs are starting to become important. Active pixel sensors (APS) provide an alternate architecture that may solve these problems. APS-1 is a first-generation sensor designed by Lincoln Laboratory's Advanced Silicon Technology Group. APS-1 is fabricated in a fully depleted silicon-on-insulator (FDSOI) technology. FDSOI is especially well-suited to produce a scientific x-ray imager. The device includes sixteen different pixel variations to determine the processing parameters that can produce the best imager. Dark current, noise, and responsivity of the various pixel designs was measured using an electronics system adapted from a CCD test system. X-rays were detected at room temperature. Ordinary active pixels have high noise levels ( 70 electrons). Many pixel designs capable of lower noise have been presented in the literature. Active reset, pixel-level CDS, and CTIA pixel designs are discussed in detail and simulated. A second-generation sensor from Lincoln Laboratory, using pixel-level CDS, is discussed. This device, APS-2, will be available for testing in 2006. APS-2 simulation results are presented. It is expected to have an input-referred noise of less than five electrons, near the performance of modern CCDs.",
    "advisors": ["Mark W. Bautz", "Kent H. Lundberg"],
    "text": "Active pixel sensors for X-ray astronomy An active pixel sensor array, APS-1, has been fabricated for the purpose of scientific x-ray detection. This thesis presents the results of testing the device. Alternate design architectures are explored. Recommendations are made for a next-generation sensor. CCDs have been the dominant x-ray sensor in astronomy for over ten years. Limitations inherent to CCDs are starting to become important. Active pixel sensors (APS) provide an alternate architecture that may solve these problems. APS-1 is a first-generation sensor designed by Lincoln Laboratory's Advanced Silicon Technology Group. APS-1 is fabricated in a fully depleted silicon-on-insulator (FDSOI) technology. FDSOI is especially well-suited to produce a scientific x-ray imager. The device includes sixteen different pixel variations to determine the processing parameters that can produce the best imager. Dark current, noise, and responsivity of the various pixel designs was measured using an electronics system adapted from a CCD test system. X-rays were detected at room temperature. Ordinary active pixels have high noise levels ( 70 electrons). Many pixel designs capable of lower noise have been presented in the literature. Active reset, pixel-level CDS, and CTIA pixel designs are discussed in detail and simulated. A second-generation sensor from Lincoln Laboratory, using pixel-level CDS, is discussed. This device, APS-2, will be available for testing in 2006. APS-2 simulation results are presented. It is expected to have an input-referred noise of less than five electrons, near the performance of modern CCDs."
}, {
    "id": "oai:dspace.mit.edu:1721.1/110868",
    "title": "Integrated LIDAR with optical phased arrays in silicon photonics",
    "abstract": "Light detection and ranging (LIDAR) has become an ubiquitous ranging technology. LIDAR systems are integral to almost all autonomous vehicles and robotics. Most LIDAR systems today use discrete free-space optical components and utilize a mechanical apparatus for beam steering. Apart from the relative high cost of the system, this mechanical apparatus limits the scan rate of the LIDAR system while increasing both size and complexity. This leads to concerns about long-term reliability, especially in harsh environments. In this thesis, the design and experimental results of an integrated chip-scale frequency-modulated continuous-wave LIDAR system are presented. This system has the capability of measuring both distance and velocity simultaneously with a 20mm resolution and a 2m range. Its functionality is then extended by utilizing optical phased arrays as a transmitter and receiver for solid-state beam steering. The phased array utilized has a grouped cascaded phase shifter architecture and is shown to have a steering range of 46x36. This is the first integrated coherent LIDAR system based on optical phased arrays. In order to have a viable LIDAR system with optical phased arrays, high beam powers and large aperture sizes are needed. A silicon nitride distribution network is used to enable high on-chip power because of the low material nonlinearities. An ultra-high main beam power of 520mW is reported. A phased array is demonstrated with an ultra-large aperture size of 4x4mm2, achieving a record-small and near diffraction limited spot size of 0.021x0.021 with a side lobe suppression of 10 dB. This is the largest optical phased array to date by an order of magnitude and shows the scalability of optical phased arrays. Finally, an optical phased array at a visible wavelength of 635nm is shown with an aperture size of 0.5x0.5mm2 and a spot size of 0.064x0.074. This demonstration moves large-scale integrated photonics into the visible spectrum and has potential applications in bathymetric LIDAR.",
    "advisors": ["Michael R. Watts"],
    "text": "Integrated LIDAR with optical phased arrays in silicon photonics Light detection and ranging (LIDAR) has become an ubiquitous ranging technology. LIDAR systems are integral to almost all autonomous vehicles and robotics. Most LIDAR systems today use discrete free-space optical components and utilize a mechanical apparatus for beam steering. Apart from the relative high cost of the system, this mechanical apparatus limits the scan rate of the LIDAR system while increasing both size and complexity. This leads to concerns about long-term reliability, especially in harsh environments. In this thesis, the design and experimental results of an integrated chip-scale frequency-modulated continuous-wave LIDAR system are presented. This system has the capability of measuring both distance and velocity simultaneously with a 20mm resolution and a 2m range. Its functionality is then extended by utilizing optical phased arrays as a transmitter and receiver for solid-state beam steering. The phased array utilized has a grouped cascaded phase shifter architecture and is shown to have a steering range of 46x36. This is the first integrated coherent LIDAR system based on optical phased arrays. In order to have a viable LIDAR system with optical phased arrays, high beam powers and large aperture sizes are needed. A silicon nitride distribution network is used to enable high on-chip power because of the low material nonlinearities. An ultra-high main beam power of 520mW is reported. A phased array is demonstrated with an ultra-large aperture size of 4x4mm2, achieving a record-small and near diffraction limited spot size of 0.021x0.021 with a side lobe suppression of 10 dB. This is the largest optical phased array to date by an order of magnitude and shows the scalability of optical phased arrays. Finally, an optical phased array at a visible wavelength of 635nm is shown with an aperture size of 0.5x0.5mm2 and a spot size of 0.064x0.074. This demonstration moves large-scale integrated photonics into the visible spectrum and has potential applications in bathymetric LIDAR."
}, {
    "id": "oai:dspace.mit.edu:1721.1/92973",
    "title": "Power monitoring in integrated circuits",
    "abstract": "Power monitoring is needed in most electrical systems, and is crucial for ensuring reliability in everything from industrial and telecom applications, to automotive and consumer electronics. Power monitoring of integrated circuits (ICs) is also essential, as today ICs exist in most electrical and electronic systems, in a vast range of applications. Many ICs, including power ICs, have functional blocks across the chip that are used for different purposes. Measuring circuit block currents in both analog and digital ICs is important in a wide range of applications, including power management as well as IC testing and fault detection and analysis. For example, the presence of different kinds of faults in IC circuit blocks during IC fabrication causes the currents flowing through these circuit blocks to change from the expected values. There has been general interest in monitoring currents through different circuit blocks in an attempt to identify the location and type of the faults. Previous works on non intrusive load monitoring as well as on power-line communications (PLCs) provide motivation for the work presented here. The techniques are extended and used to develop a new method for power monitoring in ICs. Most solutions to the challenge of measuring currents in different circuit blocks of the IC involve adding circuitry that is both costly and power consuming. In this work, a new method is proposed to enable individual measurement of current consumed in each circuit block within an IC while adding negligible area and power overhead. This method works by encoding the individual current signatures in the main supply current of the IC, which can then be sensed and sampled off-chip, and then disaggregated through signal processing. A demonstration of this power monitoring scheme is given on a modular discrete platform that is implemented based on the UC3842 current-mode controller IC, which can also be used for educational purposes.",
    "advisors": ["Steven B. Leeb", "Al-Thaddeus Avestruz"],
    "text": "Power monitoring in integrated circuits Power monitoring is needed in most electrical systems, and is crucial for ensuring reliability in everything from industrial and telecom applications, to automotive and consumer electronics. Power monitoring of integrated circuits (ICs) is also essential, as today ICs exist in most electrical and electronic systems, in a vast range of applications. Many ICs, including power ICs, have functional blocks across the chip that are used for different purposes. Measuring circuit block currents in both analog and digital ICs is important in a wide range of applications, including power management as well as IC testing and fault detection and analysis. For example, the presence of different kinds of faults in IC circuit blocks during IC fabrication causes the currents flowing through these circuit blocks to change from the expected values. There has been general interest in monitoring currents through different circuit blocks in an attempt to identify the location and type of the faults. Previous works on non intrusive load monitoring as well as on power-line communications (PLCs) provide motivation for the work presented here. The techniques are extended and used to develop a new method for power monitoring in ICs. Most solutions to the challenge of measuring currents in different circuit blocks of the IC involve adding circuitry that is both costly and power consuming. In this work, a new method is proposed to enable individual measurement of current consumed in each circuit block within an IC while adding negligible area and power overhead. This method works by encoding the individual current signatures in the main supply current of the IC, which can then be sensed and sampled off-chip, and then disaggregated through signal processing. A demonstration of this power monitoring scheme is given on a modular discrete platform that is implemented based on the UC3842 current-mode controller IC, which can also be used for educational purposes."
}, {
    "id": "oai:dspace.mit.edu:1721.1/37100",
    "title": "Identifying and modeling unwanted traffic on the Internet",
    "abstract": "Accurate models of Internet traffic are important for successful testing of devices that provide network security. However, with the growth of the Internet. it has become increasingly difficult to develop and maintain accurate traffic models. While much internet traffic is legitimate, productive communications between users and services, a significant portion of Internet traffic is the result of unwanted messages sent to IP addresses without regard as to whether there is an active host at that address. In an effort to analyze unwanted traffic, tools were developed that generate statistics and plots on captured unwanted traffic to unused IP addresses. These tools were used on a four-day period of traffic received on an inactive IPv4 class A network address space. Each class B subnet in this address space received an average of 7 million packets corresponding to 21 packets per second. Analyses were performed on a range of class B and C subnets with the intent of discovering the types of variability that are characteristic of unwanted traffic. Traffic volume over time, number of scans, destinations ports, and traffic sources varied substantially across class B and C subnets.",
    "advisors": ["Richard Lippmann"],
    "text": "Identifying and modeling unwanted traffic on the Internet Accurate models of Internet traffic are important for successful testing of devices that provide network security. However, with the growth of the Internet. it has become increasingly difficult to develop and maintain accurate traffic models. While much internet traffic is legitimate, productive communications between users and services, a significant portion of Internet traffic is the result of unwanted messages sent to IP addresses without regard as to whether there is an active host at that address. In an effort to analyze unwanted traffic, tools were developed that generate statistics and plots on captured unwanted traffic to unused IP addresses. These tools were used on a four-day period of traffic received on an inactive IPv4 class A network address space. Each class B subnet in this address space received an average of 7 million packets corresponding to 21 packets per second. Analyses were performed on a range of class B and C subnets with the intent of discovering the types of variability that are characteristic of unwanted traffic. Traffic volume over time, number of scans, destinations ports, and traffic sources varied substantially across class B and C subnets."
}, {
    "id": "oai:dspace.mit.edu:1721.1/42120",
    "title": "Design and editing 2.5-dimensional terrain in StarLogo TNG",
    "abstract": "StarLogo TNG is \"The Next Generation\" in block-based decentralized programming for modeling and simulation software. Its aim is to make computer programming more appealing for students in middle school and high school. Part of the draw of StarLogo TNG is its 3-D rendered world called Spaceland where \"agents\" live on a terrain made of a grid of \"patches\". This thesis evaluates and outlines the redesign of Spaceland and its associated terrain editor based on user-task analysis, and discusses the design of new data structures to support the desired features.",
    "advisors": ["Eric Klopfer"],
    "text": "Design and editing 2.5-dimensional terrain in StarLogo TNG StarLogo TNG is \"The Next Generation\" in block-based decentralized programming for modeling and simulation software. Its aim is to make computer programming more appealing for students in middle school and high school. Part of the draw of StarLogo TNG is its 3-D rendered world called Spaceland where \"agents\" live on a terrain made of a grid of \"patches\". This thesis evaluates and outlines the redesign of Spaceland and its associated terrain editor based on user-task analysis, and discusses the design of new data structures to support the desired features."
}, {
    "id": "oai:dspace.mit.edu:1721.1/53142",
    "title": "Enabling diagnostics in user interfaces for CAD applications",
    "abstract": "Computer aided design (CAD) applications such as Autodesk Civil 3D allow the user to specify design constraints for a number of common geometries. These applications typically prompt the user for the required constraints and then attempt to find a feasible solution. When there is no feasible solution, however, there is little or no explanation given to the user. Furthermore, given the number of degrees of freedom, it is unreasonable to expect the user to be able to analyze the solution space of the problem in order to correct his input. In this thesis I describe an extension to the geometric solvers in Civil 3D that will enable new user interfaces to assist the user in correcting his input. Furthermore I present several example user interfaces that demonstrate these new capabilities.",
    "advisors": ["Daniel Philbrick", "Srini Devadas"],
    "text": "Enabling diagnostics in user interfaces for CAD applications Computer aided design (CAD) applications such as Autodesk Civil 3D allow the user to specify design constraints for a number of common geometries. These applications typically prompt the user for the required constraints and then attempt to find a feasible solution. When there is no feasible solution, however, there is little or no explanation given to the user. Furthermore, given the number of degrees of freedom, it is unreasonable to expect the user to be able to analyze the solution space of the problem in order to correct his input. In this thesis I describe an extension to the geometric solvers in Civil 3D that will enable new user interfaces to assist the user in correcting his input. Furthermore I present several example user interfaces that demonstrate these new capabilities."
}, {
    "id": "oai:dspace.mit.edu:1721.1/41656",
    "title": "Implementation of H.264 Decoder in Bluespec SystemVerilog",
    "abstract": "In this thesis, I present a implementation of a H.264 decoder designed in Bluespec SystemVerilog, a high level hardware description language. This design is intended to serve both as a more understandable reference code, as well as a starting point for efficient hardware implementations. I illustrate this by modifying this initial design to meet a performance requirement of 720p at 60 frames per second.",
    "advisors": ["Arvind"],
    "text": "Implementation of H.264 Decoder in Bluespec SystemVerilog In this thesis, I present a implementation of a H.264 decoder designed in Bluespec SystemVerilog, a high level hardware description language. This design is intended to serve both as a more understandable reference code, as well as a starting point for efficient hardware implementations. I illustrate this by modifying this initial design to meet a performance requirement of 720p at 60 frames per second."
}, {
    "id": "oai:dspace.mit.edu:1721.1/120368",
    "title": "Dielectric reliability in GaN metal-insulator-semiconductor high electron mobility transistors",
    "abstract": "GaN Metal Insulator Semiconductor High Electron Mobility Transistors (GaN MIS-HEMTs) show excellent promise as high voltage power transistors that can operate efficiently at high temperatures and frequencies. However, current GaN technology faces several obstacles, one of which is Time-Dependent Dielectric Breakdown (TDDB) of the gate dielectric. Under prolonged electrical stress, the gate dielectric suffers a catastrophic breakdown that renders the transistor useless. Understanding the physics behind gate dielectric breakdown and accurately estimating the average time to failure of the dielectric are of critical importance. TDDB is conventionally studied under DC conditions. However, as actual device operation in power circuits involves rapid switching between on and off states, it is important to determine if estimations done from DC stress results are accurate. Due to the rich dynamics of the GaN MIS-HEMT system such as electron trapping and carrier accumulation at the dielectric/AlGaN interface, unaccounted physics might be introduced under AC stress that may cause error in DC estimation. To this end, we characterize TDDB behavior of GaN MIS-HEMTs at both DC stress conditions and more accurate AC stress conditions. We find that TDDB behavior is improved for AC stress compared to DC stress conditions at high stress frequencies. At 100 kHz, the average dielectric breakdown time is twice the average dielectric breakdown time under DC stress conditions. Furthermore, the impact of tensile mechanical stress on TDDB under DC stress is investigated. This is an important concern because of the piezoelectric nature of GaN and the substantial lattice mismatch between Si, GaN and AlGaN that results in high mechanical strain in the active portion of the device. If mechanical stress significantly impacts TDDB, designers will have to work with further constraints to ensure minimal stress across the dielectric. To address this, we have carried out measurements of TDDB under [epsilon] = 0.29% tensile strain. We find that TDDB in both the On-state and Off-state stress conditions are unaffected by this mechanical stress. Through measurements done in this thesis, we gather further insight towards understanding the physics behind TDDB. Through AC stress we find that the dynamics of the GaN MIS-HEMTs prolong dielectric breakdown times. Through mechanical stress we find that modulation of the 2-Dimensional Electron Gas and dielectric bond straining have minimal impact on TDDB.",
    "advisors": ["Jess A. del Alamo"],
    "text": "Dielectric reliability in GaN metal-insulator-semiconductor high electron mobility transistors GaN Metal Insulator Semiconductor High Electron Mobility Transistors (GaN MIS-HEMTs) show excellent promise as high voltage power transistors that can operate efficiently at high temperatures and frequencies. However, current GaN technology faces several obstacles, one of which is Time-Dependent Dielectric Breakdown (TDDB) of the gate dielectric. Under prolonged electrical stress, the gate dielectric suffers a catastrophic breakdown that renders the transistor useless. Understanding the physics behind gate dielectric breakdown and accurately estimating the average time to failure of the dielectric are of critical importance. TDDB is conventionally studied under DC conditions. However, as actual device operation in power circuits involves rapid switching between on and off states, it is important to determine if estimations done from DC stress results are accurate. Due to the rich dynamics of the GaN MIS-HEMT system such as electron trapping and carrier accumulation at the dielectric/AlGaN interface, unaccounted physics might be introduced under AC stress that may cause error in DC estimation. To this end, we characterize TDDB behavior of GaN MIS-HEMTs at both DC stress conditions and more accurate AC stress conditions. We find that TDDB behavior is improved for AC stress compared to DC stress conditions at high stress frequencies. At 100 kHz, the average dielectric breakdown time is twice the average dielectric breakdown time under DC stress conditions. Furthermore, the impact of tensile mechanical stress on TDDB under DC stress is investigated. This is an important concern because of the piezoelectric nature of GaN and the substantial lattice mismatch between Si, GaN and AlGaN that results in high mechanical strain in the active portion of the device. If mechanical stress significantly impacts TDDB, designers will have to work with further constraints to ensure minimal stress across the dielectric. To address this, we have carried out measurements of TDDB under [epsilon] = 0.29% tensile strain. We find that TDDB in both the On-state and Off-state stress conditions are unaffected by this mechanical stress. Through measurements done in this thesis, we gather further insight towards understanding the physics behind TDDB. Through AC stress we find that the dynamics of the GaN MIS-HEMTs prolong dielectric breakdown times. Through mechanical stress we find that modulation of the 2-Dimensional Electron Gas and dielectric bond straining have minimal impact on TDDB."
}, {
    "id": "oai:dspace.mit.edu:1721.1/33381",
    "title": "Graphical real-time simulation tool for passive UHF RFID environments",
    "abstract": "In this thesis, I present the design and implementation of a real-time simulation tool, RFID Vis, that is used to simulate a UHF RFID environment. The simulation tool simulates environments containing to pallets of cases as is common in parts of the supply chain. The simulation tool consists of two parts, a graphical front end which interfaces with the user as well as displays the electromagnetic power present in a given volume of space in an intuitive manner and an electromagnetics simulation engine which takes care of all the electromagnetic calculations and approximations. The simulation tool is written in C++ using Microsoft DirectX 9.0 to interface with the graphics hardware. RFID Vis enables users to quickly simulate a real world operating scenario providing insights and building intuition.",
    "advisors": ["Daniel W. Engels"],
    "text": "Graphical real-time simulation tool for passive UHF RFID environments In this thesis, I present the design and implementation of a real-time simulation tool, RFID Vis, that is used to simulate a UHF RFID environment. The simulation tool simulates environments containing to pallets of cases as is common in parts of the supply chain. The simulation tool consists of two parts, a graphical front end which interfaces with the user as well as displays the electromagnetic power present in a given volume of space in an intuitive manner and an electromagnetics simulation engine which takes care of all the electromagnetic calculations and approximations. The simulation tool is written in C++ using Microsoft DirectX 9.0 to interface with the graphics hardware. RFID Vis enables users to quickly simulate a real world operating scenario providing insights and building intuition."
}, {
    "id": "oai:dspace.mit.edu:1721.1/34148",
    "title": "A device user interface for the guided ablative therapy of cardiac arrhythmias",
    "abstract": "Radio Frequency Ablation (RFA) of cardiac arrhythmias involves the guidance of an ablation catheter to the site of the arrhythmia and the administration of a high-intensity radio-frequency current to the tissue. The current technique used to locate the arrhythmic site suffers from a number of drawbacks. Ablation is a trial-and-error procedure and may require many hours, during which the arrhythmia is ongoing. Patients with hemodynamically unstable VT are therefore excluded, as are those with more complex arrhythmias, accounting for an estimated 90% of patients. Furthermore, the technique is only successful in 71% to 76% of the cases to which it is applied. A new algorithm was recently identified that allows the non-invasive and rapid detection of the origin of an arrhythmia from body-surface ECG signals, making the RFA procedure accessible to many patients hitherto excluded. Software implementing this algorithm, and providing a multi-layer graphical user-interface to operate in conjunction with an RFA device, has been designed and implemented. If used in tandem with commercially available ECG and ablation catheter devices, this software will allow cardiologists to deliver ablating currents much more precisely and more quickly than is currently possible, and reach a far wider group of patients.",
    "advisors": ["Richard J. Cohen"],
    "text": "A device user interface for the guided ablative therapy of cardiac arrhythmias Radio Frequency Ablation (RFA) of cardiac arrhythmias involves the guidance of an ablation catheter to the site of the arrhythmia and the administration of a high-intensity radio-frequency current to the tissue. The current technique used to locate the arrhythmic site suffers from a number of drawbacks. Ablation is a trial-and-error procedure and may require many hours, during which the arrhythmia is ongoing. Patients with hemodynamically unstable VT are therefore excluded, as are those with more complex arrhythmias, accounting for an estimated 90% of patients. Furthermore, the technique is only successful in 71% to 76% of the cases to which it is applied. A new algorithm was recently identified that allows the non-invasive and rapid detection of the origin of an arrhythmia from body-surface ECG signals, making the RFA procedure accessible to many patients hitherto excluded. Software implementing this algorithm, and providing a multi-layer graphical user-interface to operate in conjunction with an RFA device, has been designed and implemented. If used in tandem with commercially available ECG and ablation catheter devices, this software will allow cardiologists to deliver ablating currents much more precisely and more quickly than is currently possible, and reach a far wider group of patients."
}, {
    "id": "oai:dspace.mit.edu:1721.1/34677",
    "title": "Optical studies of super-collimation in photonic crystals",
    "abstract": "Recent developments in material science and engineering have made possible the fabrication of photonic crystals for optical wavelengths. These periodic structures of alternating high-to-low index of refraction materials allow the observation of peculiar effects, in particular, the propagation of optical beams without spatial spreading. This effect, called super-collimation (also known as self-collimation), allows diffraction-free propagation of micron-sized beams over centimeter-scale distances. This linear effect is a natural result of the unique dispersive properties of photonic crystals. In this thesis, these dispersive properties are studied in a two-dimensional photonic crystal slab. Both qualitative and quantitative descriptions are presented. The beam propagation method was used to simulate the evolution of a Gaussian beam inside such structures. The wavelength dependence of the super-collimation effect was studied, and it was observed that the optimum wavelength for this device was around 1500 nm. A precise contact-mode near-field optical microscopy technique was used to obtain high-resolution images of the beam profile at different positions along the photonic crystal, and showed that a 2 [micro]m beam width was conserved over 3 mm. In addition, high-resolution confocal measurements confirmed the size of the beam after 5 mm of propagation.",
    "advisors": ["Erich P. Ippen"],
    "text": "Optical studies of super-collimation in photonic crystals Recent developments in material science and engineering have made possible the fabrication of photonic crystals for optical wavelengths. These periodic structures of alternating high-to-low index of refraction materials allow the observation of peculiar effects, in particular, the propagation of optical beams without spatial spreading. This effect, called super-collimation (also known as self-collimation), allows diffraction-free propagation of micron-sized beams over centimeter-scale distances. This linear effect is a natural result of the unique dispersive properties of photonic crystals. In this thesis, these dispersive properties are studied in a two-dimensional photonic crystal slab. Both qualitative and quantitative descriptions are presented. The beam propagation method was used to simulate the evolution of a Gaussian beam inside such structures. The wavelength dependence of the super-collimation effect was studied, and it was observed that the optimum wavelength for this device was around 1500 nm. A precise contact-mode near-field optical microscopy technique was used to obtain high-resolution images of the beam profile at different positions along the photonic crystal, and showed that a 2 [micro]m beam width was conserved over 3 mm. In addition, high-resolution confocal measurements confirmed the size of the beam after 5 mm of propagation."
}, {
    "id": "oai:dspace.mit.edu:1721.1/8570",
    "title": "Adaptive delivery of real-time streaming video",
    "abstract": "While there is an increasing demand for streaming video applications on the Internet, various network characteristics make the deployment of these applications more challenging than traditional Internet applications like email and the Web. The applications that transmit data over the Internet must cope with the time-varying bandwidth and delay characteristics of the Internet and must be resilient to packet loss. This thesis examines these challenges and presents a system design and implementation that ameliorates some of the important problems with video streaming over the Internet. Video sequences are typically compressed in a format such as MPEG-4 to achieve bandwidth efficiency. Video compression exploits redundancy between frames to achieve higher compression. However, packet loss can be detrimental to compressed video with interdependent frames because errors potentially propagate across many frames. While the need for low latency prevents the retransmission of all lost data, we leverage the characteristics of MPEG-4 to selectively retransmit only the most important data in order to limit the propagation of errors. We quantify the effects of packet loss on the quality of MPEG-4 video, develop an analytical model to explain these effects, and present an RTP-compatible protocol-which we call SR-RTP--to adaptively deliver higher quality video in the face of packet loss. The Internet's variable bandwidth and delay make it difficult to achieve high utilization, Tcp friendliness, and a high-quality constant playout rate; a video streaming system should adapt to these changing conditions and tailor the quality of the transmitted bitstream to available bandwidth. Traditional congestion avoidance schemes such as TCP's additive-increase/multiplicative/decrease (AIMD) cause large oscillations in transmission rates that degrade the perceptual quality of the video stream. To combat bandwidth variation, we design a scheme for performing quality adaptation of layered video for a general family of congestion control algorithms called binomial congestion control and show that a combination of smooth congestion control and clever receiver-buffered quality adaptation can reduce oscillations, increase interactivity, and deliver higher quality video for a given amount of buffering. We have integrated this selective reliability and quality adaptation into a publicly available software library. Using this system as a testbed, we show that the use of selective reliability can greatly increase the quality of received video, and that the use of binomial congestion control and receiver quality adaptation allow for increased user interactivity and better video quality.",
    "advisors": ["Hari Balakrishnan"],
    "text": "Adaptive delivery of real-time streaming video While there is an increasing demand for streaming video applications on the Internet, various network characteristics make the deployment of these applications more challenging than traditional Internet applications like email and the Web. The applications that transmit data over the Internet must cope with the time-varying bandwidth and delay characteristics of the Internet and must be resilient to packet loss. This thesis examines these challenges and presents a system design and implementation that ameliorates some of the important problems with video streaming over the Internet. Video sequences are typically compressed in a format such as MPEG-4 to achieve bandwidth efficiency. Video compression exploits redundancy between frames to achieve higher compression. However, packet loss can be detrimental to compressed video with interdependent frames because errors potentially propagate across many frames. While the need for low latency prevents the retransmission of all lost data, we leverage the characteristics of MPEG-4 to selectively retransmit only the most important data in order to limit the propagation of errors. We quantify the effects of packet loss on the quality of MPEG-4 video, develop an analytical model to explain these effects, and present an RTP-compatible protocol-which we call SR-RTP--to adaptively deliver higher quality video in the face of packet loss. The Internet's variable bandwidth and delay make it difficult to achieve high utilization, Tcp friendliness, and a high-quality constant playout rate; a video streaming system should adapt to these changing conditions and tailor the quality of the transmitted bitstream to available bandwidth. Traditional congestion avoidance schemes such as TCP's additive-increase/multiplicative/decrease (AIMD) cause large oscillations in transmission rates that degrade the perceptual quality of the video stream. To combat bandwidth variation, we design a scheme for performing quality adaptation of layered video for a general family of congestion control algorithms called binomial congestion control and show that a combination of smooth congestion control and clever receiver-buffered quality adaptation can reduce oscillations, increase interactivity, and deliver higher quality video for a given amount of buffering. We have integrated this selective reliability and quality adaptation into a publicly available software library. Using this system as a testbed, we show that the use of selective reliability can greatly increase the quality of received video, and that the use of binomial congestion control and receiver quality adaptation allow for increased user interactivity and better video quality."
}, {
    "id": "oai:dspace.mit.edu:1721.1/105961",
    "title": "An adaptive partitioning scheme for ad-hoc and time-varying database analytics",
    "abstract": "Data partitioning significantly improves query performance in distributed database systems. A large number of techniques have been proposed to efficiently partition a dataset, often focusing on finding the best partitioning for a particular query workload. However, many modern analytic applications involve ad-hoc or exploratory analysis where users do not have a representative query workload. Furthermore, workloads change over time as businesses evolve or as analysts gain better understanding of their data. Static workload-based data partitioning techniques are therefore not suitable for such settings. In this thesis, we present Amoeba, an adaptive distributed storage system for data skipping. It does not require an upfront query workload and adapts the data partitioning according to the queries posed by users over time. We present the data structures, partitioning algorithms, and an efficient implementation on top of Apache Spark and HDFS. Our experimental results show that the Amoeba storage system provides improved query performance for ad-hoc workloads, adapts to changes in the query workloads, and converges to a steady state in case of recurring workloads. On a real world workload, Amoeba reduces the total workload runtime by 1.8x compared to Spark with data partitioned and 3.4x compared to unmodified Spark.",
    "advisors": ["Samuel Madden"],
    "text": "An adaptive partitioning scheme for ad-hoc and time-varying database analytics Data partitioning significantly improves query performance in distributed database systems. A large number of techniques have been proposed to efficiently partition a dataset, often focusing on finding the best partitioning for a particular query workload. However, many modern analytic applications involve ad-hoc or exploratory analysis where users do not have a representative query workload. Furthermore, workloads change over time as businesses evolve or as analysts gain better understanding of their data. Static workload-based data partitioning techniques are therefore not suitable for such settings. In this thesis, we present Amoeba, an adaptive distributed storage system for data skipping. It does not require an upfront query workload and adapts the data partitioning according to the queries posed by users over time. We present the data structures, partitioning algorithms, and an efficient implementation on top of Apache Spark and HDFS. Our experimental results show that the Amoeba storage system provides improved query performance for ad-hoc workloads, adapts to changes in the query workloads, and converges to a steady state in case of recurring workloads. On a real world workload, Amoeba reduces the total workload runtime by 1.8x compared to Spark with data partitioned and 3.4x compared to unmodified Spark."
}, {
    "id": "oai:dspace.mit.edu:1721.1/26710",
    "title": "Cooperative routing in wireless networks",
    "abstract": "In this thesis, we study the problem of energy efficiency and reliability in wireless ad-hoc networks. First, we introduce the idea of wireless cooperation advantage. We formulate the problem of finding the minimum energy cooperative route for a wireless network under idealized channel and receiver models. Fundamental to the understanding of the routing problem is the understanding of the optimal power allocation for a single message transmission between two sets of nodes. We present the solution to this problem, and use that as the basis for solving the minimum energy cooperative routing problem. We analytically obtain the energy savings in regular line and regular grid networks. We propose heuristics for selecting the cooperative route in random networks and give simulation results confirming significant energy savings achieved through cooperation. In the second part, we study the problem of route reliability in a multi-hop network. We look at the reliability issue at the link level and extend those result to a wireless network setting. In the network setting, we first define and analyze the reliability for a fixed route and then propose algorithms for finding the optimal route between a source-destination pair of nodes. The relationship between the route reliability and consumed power is studied. The idea of route diversity is introduced as a way to improve the reliability by taking advantage of the broadcast property, the independence of fading state between different pairs of nodes, and space diversity created by multiple intermediate relay nodes along the route. We give analytical results on improvements due to route diversity in some simple network topologies.",
    "advisors": ["Eytan Modiano, Lizhong Zheng", "Jinane Abounadi"],
    "text": "Cooperative routing in wireless networks In this thesis, we study the problem of energy efficiency and reliability in wireless ad-hoc networks. First, we introduce the idea of wireless cooperation advantage. We formulate the problem of finding the minimum energy cooperative route for a wireless network under idealized channel and receiver models. Fundamental to the understanding of the routing problem is the understanding of the optimal power allocation for a single message transmission between two sets of nodes. We present the solution to this problem, and use that as the basis for solving the minimum energy cooperative routing problem. We analytically obtain the energy savings in regular line and regular grid networks. We propose heuristics for selecting the cooperative route in random networks and give simulation results confirming significant energy savings achieved through cooperation. In the second part, we study the problem of route reliability in a multi-hop network. We look at the reliability issue at the link level and extend those result to a wireless network setting. In the network setting, we first define and analyze the reliability for a fixed route and then propose algorithms for finding the optimal route between a source-destination pair of nodes. The relationship between the route reliability and consumed power is studied. The idea of route diversity is introduced as a way to improve the reliability by taking advantage of the broadcast property, the independence of fading state between different pairs of nodes, and space diversity created by multiple intermediate relay nodes along the route. We give analytical results on improvements due to route diversity in some simple network topologies."
}, {
    "id": "oai:dspace.mit.edu:1721.1/100670",
    "title": "Graph analytics on relational databases",
    "abstract": "Graph analytics has become increasing popular in the recent years. Conventionally, data is stored in relational databases that have been refined over decades, resulting in highly optimized data processing engines. However, the awkwardness of expressing iterative queries in SQL makes the relational query-processing model inadequate for graph analytics, leading to many alternative solutions. Our research explores the possibility of combining a more natural query model with relational databases for graph analytics. In particular, we bring together a graph-natural vertex-centric query interface to highly optimized column-oriented relational databases, thus providing the efficiency of relational engines and ease-of-use of new graph systems. Throughout the thesis, we used stochastic gradient descent, a loss-minimization algorithm applied in many machine learning and graph analytics queries, as the example iterative algorithm. We implemented two different approaches for emulating a vertex-centric interface on a leading column-oriented database, Vertica: disk-based and main-memory based. The disk-based solution stores data for each iteration in relational tables and allows for interleaving SQL queries with graph algorithms. The main-memory approach stores data in memory, allowing faster updates. We applied optimizations to both implementations, which included refining logical and physical query plans, applying algorithm-level improvements and performing system-specific optimizations. The experiments and results show that the two implementations provide reasonable performance in comparison with popular graph processing systems. We present a detailed cost analysis of the two implementations and study the effect of each individual optimization on the query performance.",
    "advisors": ["Samuel Madden"],
    "text": "Graph analytics on relational databases Graph analytics has become increasing popular in the recent years. Conventionally, data is stored in relational databases that have been refined over decades, resulting in highly optimized data processing engines. However, the awkwardness of expressing iterative queries in SQL makes the relational query-processing model inadequate for graph analytics, leading to many alternative solutions. Our research explores the possibility of combining a more natural query model with relational databases for graph analytics. In particular, we bring together a graph-natural vertex-centric query interface to highly optimized column-oriented relational databases, thus providing the efficiency of relational engines and ease-of-use of new graph systems. Throughout the thesis, we used stochastic gradient descent, a loss-minimization algorithm applied in many machine learning and graph analytics queries, as the example iterative algorithm. We implemented two different approaches for emulating a vertex-centric interface on a leading column-oriented database, Vertica: disk-based and main-memory based. The disk-based solution stores data for each iteration in relational tables and allows for interleaving SQL queries with graph algorithms. The main-memory approach stores data in memory, allowing faster updates. We applied optimizations to both implementations, which included refining logical and physical query plans, applying algorithm-level improvements and performing system-specific optimizations. The experiments and results show that the two implementations provide reasonable performance in comparison with popular graph processing systems. We present a detailed cost analysis of the two implementations and study the effect of each individual optimization on the query performance."
}, {
    "id": "oai:dspace.mit.edu:1721.1/66454",
    "title": "Development of gallium nitride power transistors",
    "abstract": "GaN-based high-voltage transistors have outstanding properties for the development of ultra-high efficiency and compact power electronics. This thesis describes a new process technology for the fabrication of GaN power devices optimized for their use in efficient power distribution systems in computer micro-processors. An existing process flow was used to fabricate the baseline single-finger transistors and additional process steps were developed and optimized to fabricate multi-finger devices with total gate widths up to 12mm. These transistors offer the current and on-resistance levels required by future GaN-based power converters. Transistors with various gate widths were fabricated and characterized by DC and capacitancevoltage measurements to study how the main transistor metrics scale with gate width.",
    "advisors": ["Toms Palacios"],
    "text": "Development of gallium nitride power transistors GaN-based high-voltage transistors have outstanding properties for the development of ultra-high efficiency and compact power electronics. This thesis describes a new process technology for the fabrication of GaN power devices optimized for their use in efficient power distribution systems in computer micro-processors. An existing process flow was used to fabricate the baseline single-finger transistors and additional process steps were developed and optimized to fabricate multi-finger devices with total gate widths up to 12mm. These transistors offer the current and on-resistance levels required by future GaN-based power converters. Transistors with various gate widths were fabricated and characterized by DC and capacitancevoltage measurements to study how the main transistor metrics scale with gate width."
}, {
    "id": "oai:dspace.mit.edu:1721.1/28449",
    "title": "VISTA : a visualization tool for computer architects",
    "abstract": "As computer architectures continue to grow in complexity, software developers and hardware engineers cope with the increasing complexity by developing proprietary applications, simulations and tool sets to understand the behavior of these complex systems. Although the field of information visualization is leading to powerful applications in many areas, information visualization applications for computer architecture development are either tightly coupled with a specific architecture or target a wide range of computer system data. This thesis introduces the Visualization Tool for Computer Architects (VISTA) Environment. The VISTA Environment is an extensible and modular information visualization environment for hardware engineers, software developers and educators to visualize data from a variety of computer architecture simulations at different levels of abstraction. The VISTA Environment leverages common attributes in simulation data, computer architecture visualizations, and computer architecture development methods to create a powerful information visualization environment to aid in designing, understanding and communicating complex computer architectures.",
    "advisors": ["Krste Asanovi"],
    "text": "VISTA : a visualization tool for computer architects As computer architectures continue to grow in complexity, software developers and hardware engineers cope with the increasing complexity by developing proprietary applications, simulations and tool sets to understand the behavior of these complex systems. Although the field of information visualization is leading to powerful applications in many areas, information visualization applications for computer architecture development are either tightly coupled with a specific architecture or target a wide range of computer system data. This thesis introduces the Visualization Tool for Computer Architects (VISTA) Environment. The VISTA Environment is an extensible and modular information visualization environment for hardware engineers, software developers and educators to visualize data from a variety of computer architecture simulations at different levels of abstraction. The VISTA Environment leverages common attributes in simulation data, computer architecture visualizations, and computer architecture development methods to create a powerful information visualization environment to aid in designing, understanding and communicating complex computer architectures."
}, {
    "id": "oai:dspace.mit.edu:1721.1/33331",
    "title": "On symbolic analysis of cryptographic protocols",
    "abstract": "The universally composable symbolic analysis (UCSA) framework layers Dolev-Yao style symbolic analysis on top of the universally composable (UC) secure framework to construct computationally sound proofs of cryptographic protocol security. The original proposal of the UCSA framework by Canetti and Herzog (2004) focused on protocols that only use public key encryption to achieve 2-party mutual authentication or key exchange. This thesis expands the framework to include protocols that use digital signatures as well. In the process of expanding the framework, we identify a flaw in the framework's use of UC ideal functionality FKE. We also identify issues that arise when combining FKE with the current formulation of ideal signature functionality FSI,. Motivated by these discoveries, we redefine the FPKE and FsIG functionalities appropriately.",
    "advisors": ["Ronald L. Rivest", "Ran Canetti"],
    "text": "On symbolic analysis of cryptographic protocols The universally composable symbolic analysis (UCSA) framework layers Dolev-Yao style symbolic analysis on top of the universally composable (UC) secure framework to construct computationally sound proofs of cryptographic protocol security. The original proposal of the UCSA framework by Canetti and Herzog (2004) focused on protocols that only use public key encryption to achieve 2-party mutual authentication or key exchange. This thesis expands the framework to include protocols that use digital signatures as well. In the process of expanding the framework, we identify a flaw in the framework's use of UC ideal functionality FKE. We also identify issues that arise when combining FKE with the current formulation of ideal signature functionality FSI,. Motivated by these discoveries, we redefine the FPKE and FsIG functionalities appropriately."
}, {
    "id": "oai:dspace.mit.edu:1721.1/62448",
    "title": "High fidelity pattern transfer in InP photonic device fabrication",
    "abstract": "The photonic industry is driven by the information ages demand for higher bandwidth. To meet the future demands of 10 Tbit networks, photonic integrated circuits (PIC) are required. Device performance is affected by everything from component coupling to electrical connectivity of the active components. However the most fundamental and often challenging aspect of photonic device fabrication is dimensional control. At 1550 nm, line width tolerance range between 1 pm to 0.05 pm.[1] Although these tolerances are easily achieved using lithography technology such as electron beam lithography (EBL) or 193 nm projection, neither are viable optical options for InP production.[2] The purpose of this thesis is to develop a fabrication process for InP Faraday rotators using standard, high throughput lithographic and etching techniques. The Faraday rotator is a 1.4 Jim InP-InGaAsP-InP waveguide with a line width tolerance of  0.07 pm.",
    "advisors": ["Rajeev R. Ram"],
    "text": "High fidelity pattern transfer in InP photonic device fabrication The photonic industry is driven by the information ages demand for higher bandwidth. To meet the future demands of 10 Tbit networks, photonic integrated circuits (PIC) are required. Device performance is affected by everything from component coupling to electrical connectivity of the active components. However the most fundamental and often challenging aspect of photonic device fabrication is dimensional control. At 1550 nm, line width tolerance range between 1 pm to 0.05 pm.[1] Although these tolerances are easily achieved using lithography technology such as electron beam lithography (EBL) or 193 nm projection, neither are viable optical options for InP production.[2] The purpose of this thesis is to develop a fabrication process for InP Faraday rotators using standard, high throughput lithographic and etching techniques. The Faraday rotator is a 1.4 Jim InP-InGaAsP-InP waveguide with a line width tolerance of  0.07 pm."
}, {
    "id": "oai:dspace.mit.edu:1721.1/76997",
    "title": "Can silhouette execution mitigate VM boot storms?",
    "abstract": "Server virtualization enables data centers to run many VMs on individual hosts - this reduces costs, simplifies administration and facilitates management. Improvement in hardware and virtualization technology, coupled with the use of virtualization for desktop machines with modest steady-state resource utilization, is expected to allow individual hosts to run thousands of VMs at the same time. Such high VM densities per host would allow data centers to reap unprecedented cost-savings in the future. Unfortunately, unusually high CPU and memory pressure generated when many VMs boot up concurrently can cripple hosts that can otherwise run many VMs. Over provisioning hardware to avoid prohibitively high boot latencies that result from these - often daily - boot storms is clearly expensive. The aim of this thesis is to investigate whether a hypervisor could theoretically exploit the overlap in the instruction streams of concurrently booting VMs to reduce CPU pressure in boot storms. This idea, which we name silhouette execution, would allow hypervisors to use the CPU in a scalable way, much like transparent page sharing allows a hypervisor to use its limited memory in a scalable fashion. To evaluate silhouette execution, we studied user-space instruction streams from a few Linux services using dynamic instrumentation. We statistically profiled the extent of nondeterminism in program execution, and compiled the reasons behind any execution differences. Though there is significant overlap in the user-mode instruction streams of Linux services, our simple simulations show that silhouette execution would increase CPU pressure by 13% for 100 VMs and 6% for 1000 VMs. To remedy this, we present a few strategies for reducing synthetic differences in execution in user-space programs. Our simulations show that silhouette execution can reduce CPU pressure on a host by a factor of 8x for 100 VMs and a factor of 19x for 1000 VMs once these strategies are used. We believe that the insights provided in this thesis on controlling execution differences in concurrently booting VMs via dynamic instrumentation are a prelude to a successful future implementation of silhouette execution.",
    "advisors": ["Saman P. Amarasinghe"],
    "text": "Can silhouette execution mitigate VM boot storms? Server virtualization enables data centers to run many VMs on individual hosts - this reduces costs, simplifies administration and facilitates management. Improvement in hardware and virtualization technology, coupled with the use of virtualization for desktop machines with modest steady-state resource utilization, is expected to allow individual hosts to run thousands of VMs at the same time. Such high VM densities per host would allow data centers to reap unprecedented cost-savings in the future. Unfortunately, unusually high CPU and memory pressure generated when many VMs boot up concurrently can cripple hosts that can otherwise run many VMs. Over provisioning hardware to avoid prohibitively high boot latencies that result from these - often daily - boot storms is clearly expensive. The aim of this thesis is to investigate whether a hypervisor could theoretically exploit the overlap in the instruction streams of concurrently booting VMs to reduce CPU pressure in boot storms. This idea, which we name silhouette execution, would allow hypervisors to use the CPU in a scalable way, much like transparent page sharing allows a hypervisor to use its limited memory in a scalable fashion. To evaluate silhouette execution, we studied user-space instruction streams from a few Linux services using dynamic instrumentation. We statistically profiled the extent of nondeterminism in program execution, and compiled the reasons behind any execution differences. Though there is significant overlap in the user-mode instruction streams of Linux services, our simple simulations show that silhouette execution would increase CPU pressure by 13% for 100 VMs and 6% for 1000 VMs. To remedy this, we present a few strategies for reducing synthetic differences in execution in user-space programs. Our simulations show that silhouette execution can reduce CPU pressure on a host by a factor of 8x for 100 VMs and a factor of 19x for 1000 VMs once these strategies are used. We believe that the insights provided in this thesis on controlling execution differences in concurrently booting VMs via dynamic instrumentation are a prelude to a successful future implementation of silhouette execution."
}, {
    "id": "oai:dspace.mit.edu:1721.1/78535",
    "title": "Flying between obstacles with an autonomous knife-edge maneuver",
    "abstract": "We develop an aircraft and control system that is capable of repeatedly performing a high speed (7m/s or 16 MPH) \"knife-edge\" maneuver through a gap that is smaller than the aircraft's wingspan. The maneuver consists of flying towards a gap, rolling to a significant angle, accurately navigating between the obstacles, and rolling back to horizontal. The speed and roll-rate required demand a control system capable of highly precise, repeatable maneuvers. We address the necessary control theory, path planning, and hardware requirements for such a maneuver, and give a proposal for a new system that may improve upon the existing techniques.",
    "advisors": ["Russ Tedrake"],
    "text": "Flying between obstacles with an autonomous knife-edge maneuver We develop an aircraft and control system that is capable of repeatedly performing a high speed (7m/s or 16 MPH) \"knife-edge\" maneuver through a gap that is smaller than the aircraft's wingspan. The maneuver consists of flying towards a gap, rolling to a significant angle, accurately navigating between the obstacles, and rolling back to horizontal. The speed and roll-rate required demand a control system capable of highly precise, repeatable maneuvers. We address the necessary control theory, path planning, and hardware requirements for such a maneuver, and give a proposal for a new system that may improve upon the existing techniques."
}, {
    "id": "oai:dspace.mit.edu:1721.1/28427",
    "title": "The StreamIt development tool : a programming environment for StreamIt",
    "abstract": "StreamIt [28] is a high-level programming language intended for the development of large-scale and high-performance streaming applications that are characterized by the processing of data streams by modular structures. The StreamIt Development Tool (SDT) [25] is designed to aid the coding and simultaneous code- and graph-based debugging and visualizing of programs written in StreamIt. The goal is to provide a graphical programming environment that simply and intuitively conveys the hierarchical and structured nature of the StreamIt language by visually interpreting the dynamic behavior and graph representation of a StreamIt application. Consequently, the SDT provides utilities for program creation and code editing, compilation and launch support, breakpoints and code stepping, general debugging infrastructure, help support, stream graph examination and navigation, and stream data display, modification, and tracking. A user study evaluating the SDT uncovered several problems and areas of improvement that need to be addressed before this tool can approach its goals. Assessment of the SDT's efficacy in its current state is inconclusive--the SDT demonstrates both the ability to improve and hinder a user's debugging ability. Facilitating effective coding and debugging techniques and developing for scalability are critical elements in improving the SDT's effectiveness.",
    "advisors": ["Saman Amarasinghe"],
    "text": "The StreamIt development tool : a programming environment for StreamIt StreamIt [28] is a high-level programming language intended for the development of large-scale and high-performance streaming applications that are characterized by the processing of data streams by modular structures. The StreamIt Development Tool (SDT) [25] is designed to aid the coding and simultaneous code- and graph-based debugging and visualizing of programs written in StreamIt. The goal is to provide a graphical programming environment that simply and intuitively conveys the hierarchical and structured nature of the StreamIt language by visually interpreting the dynamic behavior and graph representation of a StreamIt application. Consequently, the SDT provides utilities for program creation and code editing, compilation and launch support, breakpoints and code stepping, general debugging infrastructure, help support, stream graph examination and navigation, and stream data display, modification, and tracking. A user study evaluating the SDT uncovered several problems and areas of improvement that need to be addressed before this tool can approach its goals. Assessment of the SDT's efficacy in its current state is inconclusive--the SDT demonstrates both the ability to improve and hinder a user's debugging ability. Facilitating effective coding and debugging techniques and developing for scalability are critical elements in improving the SDT's effectiveness."
}, {
    "id": "oai:dspace.mit.edu:1721.1/66313",
    "title": "Evolutionary algorithms for compiler-enabled program autotuning",
    "abstract": "PetaBricks [4, 21, 7, 3, 5] is an implicitly parallel programming language which, through the process of autotuning, can automatically optimize programs for fast QoS-aware execution on any hardware. In this thesis we develop and evaluate two PetaBricks autotuners: INCREA and SiblingRivalry. INCREA, based on a novel bottom-up evolutionary algorithm, optimizes programs offline at compile time. SiblingRivalry improves on INCREA by optimizing online during a program's execution, dynamically adapting to changes in hardware and the operating system. Continuous adaptation is achieved through racing, where half of available resources are devoted to always-on learning. We evaluate INCREA and SiblingRivalry on a large number of real-world benchmarks, and show that our autotuners can significantly speed up PetaBricks programs with respect to many non-tuned and mis-tuned baselines. Our results indicate the need for a continuous learning loop that can optimize efficiently by exploiting online knowledge of a program's performance. The results leave open the question of how to solve the online optimization problem on all cores, i.e. without racing.",
    "advisors": ["Una-May O'Reilly"],
    "text": "Evolutionary algorithms for compiler-enabled program autotuning PetaBricks [4, 21, 7, 3, 5] is an implicitly parallel programming language which, through the process of autotuning, can automatically optimize programs for fast QoS-aware execution on any hardware. In this thesis we develop and evaluate two PetaBricks autotuners: INCREA and SiblingRivalry. INCREA, based on a novel bottom-up evolutionary algorithm, optimizes programs offline at compile time. SiblingRivalry improves on INCREA by optimizing online during a program's execution, dynamically adapting to changes in hardware and the operating system. Continuous adaptation is achieved through racing, where half of available resources are devoted to always-on learning. We evaluate INCREA and SiblingRivalry on a large number of real-world benchmarks, and show that our autotuners can significantly speed up PetaBricks programs with respect to many non-tuned and mis-tuned baselines. Our results indicate the need for a continuous learning loop that can optimize efficiently by exploiting online knowledge of a program's performance. The results leave open the question of how to solve the online optimization problem on all cores, i.e. without racing."
}, {
    "id": "oai:dspace.mit.edu:1721.1/17978",
    "title": "Vision based robot navigation",
    "abstract": "In this thesis we propose a vision-based robot navigation system that constructs a high level topological representation of the world. A robot using this system learns to recognize rooms and spaces by building a hidden Markov model of the environment. Motion planning is performed by doing bidirectional heuristic search with a discrete set of actions that account for the robot's nonholonomic constraints. The intent of this project is to create a system that allows a robot to be able to explore and to navigate in a wide variety of environments in a way that facilitates goal-oriented tasks.",
    "advisors": ["Leslie P. Kaelbling"],
    "text": "Vision based robot navigation In this thesis we propose a vision-based robot navigation system that constructs a high level topological representation of the world. A robot using this system learns to recognize rooms and spaces by building a hidden Markov model of the environment. Motion planning is performed by doing bidirectional heuristic search with a discrete set of actions that account for the robot's nonholonomic constraints. The intent of this project is to create a system that allows a robot to be able to explore and to navigate in a wide variety of environments in a way that facilitates goal-oriented tasks."
}, {
    "id": "oai:dspace.mit.edu:1721.1/33761",
    "title": "Imitation learning of whole-body grasps",
    "abstract": "Humans often learn to manipulate objects by observing other people. In much the same way, robots can use imitation learning to pick up useful skills. A system is demonstrated here for using imitation learning to teach a robot to grasp objects using both hand and whole-body grasps, which use the arms and torso as well as hands. Demonstration grasp trajectories are created by teleoperating a simulated robot to pick up simulated objects, and stored as sequences of keyframes in which contacts with the object are gained or lost. When presented with a new object, the system compares it against the objects in a stored database to pick a demonstrated grasp used on a similar object. Both objects are modeled as a combination of primitives-boxes, cylinders, and spheres-and the primitives for each object are grouped into 'functional groups' that geometrically match parts of the new object with similar parts of the demonstration object. These functional groups are then used to map contact points from the demonstration object to the new object, and the resulting adapted keyframes are adjusted and checked for feasibility. Finally, a trajectory is found that moves among the keyframes in the adapted grasp sequence, and the full trajectory is tested for feasibility by executing it in the simulation. The system successfully uses this method to pick up 92 out of 100 randomly generated test objects in simulation.",
    "advisors": ["Tomsz Lozano-Prez"],
    "text": "Imitation learning of whole-body grasps Humans often learn to manipulate objects by observing other people. In much the same way, robots can use imitation learning to pick up useful skills. A system is demonstrated here for using imitation learning to teach a robot to grasp objects using both hand and whole-body grasps, which use the arms and torso as well as hands. Demonstration grasp trajectories are created by teleoperating a simulated robot to pick up simulated objects, and stored as sequences of keyframes in which contacts with the object are gained or lost. When presented with a new object, the system compares it against the objects in a stored database to pick a demonstrated grasp used on a similar object. Both objects are modeled as a combination of primitives-boxes, cylinders, and spheres-and the primitives for each object are grouped into 'functional groups' that geometrically match parts of the new object with similar parts of the demonstration object. These functional groups are then used to map contact points from the demonstration object to the new object, and the resulting adapted keyframes are adjusted and checked for feasibility. Finally, a trajectory is found that moves among the keyframes in the adapted grasp sequence, and the full trajectory is tested for feasibility by executing it in the simulation. The system successfully uses this method to pick up 92 out of 100 randomly generated test objects in simulation."
}, {
    "id": "oai:dspace.mit.edu:1721.1/32095",
    "title": "Predicate dispatching in the Common Lisp Object",
    "abstract": "I have added support for predicate dispatching, a powerful generalization of other dispatching mechanisms, to the Common Lisp Object System (CLOS). To demonstrate its utility, I used predicate dispatching to enhance Weyl, a computer algebra system which doubles as a CLOS library. My result is Dispatching-Enhanced Weyl (DEW), a computer algebra system that I have demonstrated to be well suited for both users and programmers.",
    "advisors": ["Howard E. Shrobe"],
    "text": "Predicate dispatching in the Common Lisp Object I have added support for predicate dispatching, a powerful generalization of other dispatching mechanisms, to the Common Lisp Object System (CLOS). To demonstrate its utility, I used predicate dispatching to enhance Weyl, a computer algebra system which doubles as a CLOS library. My result is Dispatching-Enhanced Weyl (DEW), a computer algebra system that I have demonstrated to be well suited for both users and programmers."
}, {
    "id": "oai:dspace.mit.edu:1721.1/112843",
    "title": "Crafting certified elliptic curve cryptography implementations in Coq",
    "abstract": "Elliptic curve cryptography has become a de-facto standard for protecting the privacy and integrity of internet communications. To minimize the operational cost and enable near-universal adoption, increasingly sophisticated implementation techniques have been developed. While the complete specification of an elliptic curve cryptosystem (in terms of middle school mathematics) fits on the back of a napkin, the fast implementations span thousands of lines of low-level code and are only intelligible to a small group of experts. However, the complexity of the code makes it prone to bugs, which have rendered well-designed security systems completely ineffective. I describe a principled approach for writing crypto code simultaneously with machine-checkable functional correctness proofs that compose into an end-to-end certificate tying highly optimized C code to the simplest specification used for verification so far. Despite using template-based synthesis for creating low-level code, this workflow offers good control over performance: I was able to match the fastest C implementation of X25519 to within 1% of arithmetic instructions per inner loop and 7% of overall execution time. While the development method itself relies heavily on a proof assistant such as Coq and most techniques are explained through code snippets, every Coq feature is introduced and motivated when it is first used to accommodate a non-Coq-savvy reader.",
    "advisors": ["Adam Chlipala"],
    "text": "Crafting certified elliptic curve cryptography implementations in Coq Elliptic curve cryptography has become a de-facto standard for protecting the privacy and integrity of internet communications. To minimize the operational cost and enable near-universal adoption, increasingly sophisticated implementation techniques have been developed. While the complete specification of an elliptic curve cryptosystem (in terms of middle school mathematics) fits on the back of a napkin, the fast implementations span thousands of lines of low-level code and are only intelligible to a small group of experts. However, the complexity of the code makes it prone to bugs, which have rendered well-designed security systems completely ineffective. I describe a principled approach for writing crypto code simultaneously with machine-checkable functional correctness proofs that compose into an end-to-end certificate tying highly optimized C code to the simplest specification used for verification so far. Despite using template-based synthesis for creating low-level code, this workflow offers good control over performance: I was able to match the fastest C implementation of X25519 to within 1% of arithmetic instructions per inner loop and 7% of overall execution time. While the development method itself relies heavily on a proof assistant such as Coq and most techniques are explained through code snippets, every Coq feature is introduced and motivated when it is first used to accommodate a non-Coq-savvy reader."
}, {
    "id": "oai:dspace.mit.edu:1721.1/28456",
    "title": "Versioning for the Haystack system",
    "abstract": "In this thesis, the design and implementation of a repository for electronic information is presented. While designed with the Haystack system in mind, the repository can easily be used by anyone familiar with RDF. This repository performs the basic task of storing the information that the user collects, in addition to automatically performing several other tasks in an effort to make retrieving the information simple and efficient. The additional tasks involve the storing of when and why information enters the repository. The hope is that this additional data will help the user when searching their repository. A person's sense of time and data dependence is a strong organizing princple that can help them locate a file simply because they remember when or why they archived it, or perhaps when they used it last. This repository allows a user to narrow the search of their information space in regards to time and dependence.",
    "advisors": ["David Karger", "Lynn Stein"],
    "text": "Versioning for the Haystack system In this thesis, the design and implementation of a repository for electronic information is presented. While designed with the Haystack system in mind, the repository can easily be used by anyone familiar with RDF. This repository performs the basic task of storing the information that the user collects, in addition to automatically performing several other tasks in an effort to make retrieving the information simple and efficient. The additional tasks involve the storing of when and why information enters the repository. The hope is that this additional data will help the user when searching their repository. A person's sense of time and data dependence is a strong organizing princple that can help them locate a file simply because they remember when or why they archived it, or perhaps when they used it last. This repository allows a user to narrow the search of their information space in regards to time and dependence."
}, {
    "id": "oai:dspace.mit.edu:1721.1/61160",
    "title": "ALA ASIC : a standard cell library for Asynchronous Logic Automata",
    "abstract": "This thesis demonstrates a hardware library with related tools and designs for Asynchronous Logic Automata (ALA) gates in a generic 90nm process development kit that allows a direct one-to-one mapping from software to hardware. Included are basic design tools to enable writing ALA software, the necessary hardware designs for implementation, and simulation techniques for quickly verifying correctness and performance. This thesis also documents many of the hazards and opportunities for improving them including helpful variations to the ALA model, design tool needs, better simulation models, and hardware improvements. To embody software you could compile a hardware description language to an FPGA or synthesize it all the way to transistors. Alternatively, you could use your favorite high level language and run it on a standard processor. However, the widening gap between traditional models of computation and the reality of the underlying hardware has led to massive costs for design and fabrication as well as numerous issues for scalability and portability. Unlike any of these other approaches, ALA aligns computational and physical descriptions making it possible to use a direct one-to-one mapping to convert an ALA program to a circuit or other physical artifact that executes that program. No unpredictable fitters or compilers are needed and no extra expertise is needed for specific technologies. Similar to Mead-Conway design rules ALA designs trade flexibility for portability and ease of design. Unlike Mead- Conway design rules, ALA designs do not require any further verification-the design rule primitives are logical operations suitable for use in analysis at the algorithmic level. ALA separates many of the scaling issues that plague integrated circuit design by cleanly separating algorithm design from hardware engineering-improving design verification, tape-out costs (by reusing masks), yield, portability, and the ability to break designs across multiple chips. ALA designs are not limited to integrated circuits and could just as easily be implemented in microfluidics, magnetic logic, or a lattice of molecular logic gates. Although each of these technologies would require implementing a basic set of gates and tiling rules, hardware (or equivalently software) can be developed using the same deterministic noiseless digital abstraction using the same design in many different technologies.",
    "advisors": ["Neil Gershenfeld"],
    "text": "ALA ASIC : a standard cell library for Asynchronous Logic Automata This thesis demonstrates a hardware library with related tools and designs for Asynchronous Logic Automata (ALA) gates in a generic 90nm process development kit that allows a direct one-to-one mapping from software to hardware. Included are basic design tools to enable writing ALA software, the necessary hardware designs for implementation, and simulation techniques for quickly verifying correctness and performance. This thesis also documents many of the hazards and opportunities for improving them including helpful variations to the ALA model, design tool needs, better simulation models, and hardware improvements. To embody software you could compile a hardware description language to an FPGA or synthesize it all the way to transistors. Alternatively, you could use your favorite high level language and run it on a standard processor. However, the widening gap between traditional models of computation and the reality of the underlying hardware has led to massive costs for design and fabrication as well as numerous issues for scalability and portability. Unlike any of these other approaches, ALA aligns computational and physical descriptions making it possible to use a direct one-to-one mapping to convert an ALA program to a circuit or other physical artifact that executes that program. No unpredictable fitters or compilers are needed and no extra expertise is needed for specific technologies. Similar to Mead-Conway design rules ALA designs trade flexibility for portability and ease of design. Unlike Mead- Conway design rules, ALA designs do not require any further verification-the design rule primitives are logical operations suitable for use in analysis at the algorithmic level. ALA separates many of the scaling issues that plague integrated circuit design by cleanly separating algorithm design from hardware engineering-improving design verification, tape-out costs (by reusing masks), yield, portability, and the ability to break designs across multiple chips. ALA designs are not limited to integrated circuits and could just as easily be implemented in microfluidics, magnetic logic, or a lattice of molecular logic gates. Although each of these technologies would require implementing a basic set of gates and tiling rules, hardware (or equivalently software) can be developed using the same deterministic noiseless digital abstraction using the same design in many different technologies."
}, {
    "id": "oai:dspace.mit.edu:1721.1/53174",
    "title": "Improving search quality of the Google search appliance",
    "abstract": "In this thesis, we describe various experiments on the ranking function of the Google Search Appliance to improve search quality. An evolutionary computation framework is implemented and applied to optimize various parameter settings of the ranking function. We evaluate the importance of IDF in the ranking function and achieve small improvements in performance. We also examine many ways to combining the query-independent and query-dependent scores. Lastly, we perform various experiments with signals based on the positions of the query terms in the document.",
    "advisors": ["David Elworthy", "Regina Barzilay"],
    "text": "Improving search quality of the Google search appliance In this thesis, we describe various experiments on the ranking function of the Google Search Appliance to improve search quality. An evolutionary computation framework is implemented and applied to optimize various parameter settings of the ranking function. We evaluate the importance of IDF in the ranking function and achieve small improvements in performance. We also examine many ways to combining the query-independent and query-dependent scores. Lastly, we perform various experiments with signals based on the positions of the query terms in the document."
}, {
    "id": "oai:dspace.mit.edu:1721.1/33390",
    "title": "EClerk office assistant",
    "abstract": "For decades, people have continued to collect an inordinate amount of paper documents containing important information that should be easily accessible. This paper clutter inhibits indexing this information and easily searching through it. This thesis presents the code architecture and user interface design of the Electronic Clerk, a proof-of-concept electronic office assistant. The Electronic Clerk (EClerk) is a device to assist in reducing paper clutter in the office environment. The device takes paper and speech as input, performs data binding between input streams in order to attach metadata to each document, and structures the data using the Resource Description Framework (RDF) standard. The hardware structure of EClerk consists of a dedicated computer, video camera, scanner, touchscreen, and microphone for capturing input. The software structure consists of the Galaxy speech recognition system, the Haystack information client for retrieval and modification of the collected data, optical character recognition, and a graphical user interface that provides continuous feedback to the user. Primary design principles for this device include providing continuous user feedback and robustness to imperfect input in order to provide a truly usable system.",
    "advisors": ["Seth Teller"],
    "text": "EClerk office assistant For decades, people have continued to collect an inordinate amount of paper documents containing important information that should be easily accessible. This paper clutter inhibits indexing this information and easily searching through it. This thesis presents the code architecture and user interface design of the Electronic Clerk, a proof-of-concept electronic office assistant. The Electronic Clerk (EClerk) is a device to assist in reducing paper clutter in the office environment. The device takes paper and speech as input, performs data binding between input streams in order to attach metadata to each document, and structures the data using the Resource Description Framework (RDF) standard. The hardware structure of EClerk consists of a dedicated computer, video camera, scanner, touchscreen, and microphone for capturing input. The software structure consists of the Galaxy speech recognition system, the Haystack information client for retrieval and modification of the collected data, optical character recognition, and a graphical user interface that provides continuous feedback to the user. Primary design principles for this device include providing continuous user feedback and robustness to imperfect input in order to provide a truly usable system."
}, {
    "id": "oai:dspace.mit.edu:1721.1/41639",
    "title": "Compiling array computations for the Fresh Breeze Parallel Processor",
    "abstract": "Fresh Breeze is a highly parallel architecture currently under development, which strives to provide high performance scientific computing with simple programmability. The architecture provides for multithreaded determinate execution with a write-once shared memory system. In particular, Fresh Breeze data structures must be constructed from directed acyclic graphs of immutable fixed-size chunks of memory, rather than laid out in a mutable linear memory. While this model is well suited for executing functional programs, the goal of this thesis is to see if conventional programs can be efficiently compiled for this novel memory system and parallelization model, focusing specifically on array-based linear algebra computations. We compile a subset of Java, targeting the Fresh Breeze instruction set. The compiler, using a static data-flow graph intermediate representation, performs analysis and transformations which reduce communication with the shared memory and identify opportunities for parallelization.",
    "advisors": ["Jack Dennis"],
    "text": "Compiling array computations for the Fresh Breeze Parallel Processor Fresh Breeze is a highly parallel architecture currently under development, which strives to provide high performance scientific computing with simple programmability. The architecture provides for multithreaded determinate execution with a write-once shared memory system. In particular, Fresh Breeze data structures must be constructed from directed acyclic graphs of immutable fixed-size chunks of memory, rather than laid out in a mutable linear memory. While this model is well suited for executing functional programs, the goal of this thesis is to see if conventional programs can be efficiently compiled for this novel memory system and parallelization model, focusing specifically on array-based linear algebra computations. We compile a subset of Java, targeting the Fresh Breeze instruction set. The compiler, using a static data-flow graph intermediate representation, performs analysis and transformations which reduce communication with the shared memory and identify opportunities for parallelization."
}, {
    "id": "oai:dspace.mit.edu:1721.1/113295",
    "title": "Obstacle detection and tracking in an urban environment Using 3D LiDAR and a Mobileye 560",
    "abstract": "In order to navigate in an urban environment, a vehicle must be able to reliably detect and track dynamic obstacles such as vehicles, pedestrians, bicycles, and motorcycles. This paper presents a sensor fusion algorithm which combines tracking information from a Mobileye 560 and a Velodyne HDL-64E. The Velodyne tracking module first extracts obstacles by removing the ground plane points and then segmenting the remaining points using Euclidean Cluster Extraction. The Velodyne tracking module then uses the Kuhn-Munkres algorithm to associate Velodyne obstacles of the same type between time steps. The sensor fusion module associates and tracks obstacles from both the Velodyne and Mobileye tracking modules. It is able to reliably associate the same Velodyne and Mobileye obstacle between frames, although the Velodyne tracking module only provides robust tracking in simple scenes such as bridges.",
    "advisors": ["Sertac Karaman"],
    "text": "Obstacle detection and tracking in an urban environment Using 3D LiDAR and a Mobileye 560 In order to navigate in an urban environment, a vehicle must be able to reliably detect and track dynamic obstacles such as vehicles, pedestrians, bicycles, and motorcycles. This paper presents a sensor fusion algorithm which combines tracking information from a Mobileye 560 and a Velodyne HDL-64E. The Velodyne tracking module first extracts obstacles by removing the ground plane points and then segmenting the remaining points using Euclidean Cluster Extraction. The Velodyne tracking module then uses the Kuhn-Munkres algorithm to associate Velodyne obstacles of the same type between time steps. The sensor fusion module associates and tracks obstacles from both the Velodyne and Mobileye tracking modules. It is able to reliably associate the same Velodyne and Mobileye obstacle between frames, although the Velodyne tracking module only provides robust tracking in simple scenes such as bridges."
}, {
    "id": "oai:dspace.mit.edu:1721.1/120421",
    "title": "Probabilistic latent variable modeling for predicting future well-being and assessing behavioral influences on mood, stress and health",
    "abstract": "In recent years, there has been a shift in the psychological research literature from an emphasis on dysfunction to a focus on well-being and positive mental health. As a result, enhancing well-being in individuals has become a viable approach to improving health, in addition to treating disorders when present. Also, the availability of rich multi-modal datasets and advances in machine learning methods have spurred an increase in research studies assessing well-being objectively. However, most of these studies tend to primarily focus on using data to estimate or detect the current state of well-being as opposed to the prediction of well-being. In addition, these studies investigate how stand-alone health behaviors and not a combination of health behaviors influence well-being. Furthermore, these studies do not provide data-backed insights and recommendations to individuals seeking to improve their well-being. In this dissertation, we use a real-world dataset from a population of college students and interpretable machine learning methods to (1) predict future mood, stress and health, (2) uncover how combinations of health behaviors work together to influence well-being, and (3) understand how to make evidence-based recommendations to individuals looking to improve their well-being. The use of these methods contributes to the development of objective techniques that can help individuals monitor their wellbeing. In addition, insights from this study contribute to knowledge advancement on how combinations of daily human behaviors can affect well-being.",
    "advisors": ["Rosalind W. Picard"],
    "text": "Probabilistic latent variable modeling for predicting future well-being and assessing behavioral influences on mood, stress and health In recent years, there has been a shift in the psychological research literature from an emphasis on dysfunction to a focus on well-being and positive mental health. As a result, enhancing well-being in individuals has become a viable approach to improving health, in addition to treating disorders when present. Also, the availability of rich multi-modal datasets and advances in machine learning methods have spurred an increase in research studies assessing well-being objectively. However, most of these studies tend to primarily focus on using data to estimate or detect the current state of well-being as opposed to the prediction of well-being. In addition, these studies investigate how stand-alone health behaviors and not a combination of health behaviors influence well-being. Furthermore, these studies do not provide data-backed insights and recommendations to individuals seeking to improve their well-being. In this dissertation, we use a real-world dataset from a population of college students and interpretable machine learning methods to (1) predict future mood, stress and health, (2) uncover how combinations of health behaviors work together to influence well-being, and (3) understand how to make evidence-based recommendations to individuals looking to improve their well-being. The use of these methods contributes to the development of objective techniques that can help individuals monitor their wellbeing. In addition, insights from this study contribute to knowledge advancement on how combinations of daily human behaviors can affect well-being."
}, {
    "id": "oai:dspace.mit.edu:1721.1/55085",
    "title": "Analysis and transfer of photographic viewpoint and appearance",
    "abstract": "To make a compelling photograph, photographers need to carefully choose the subject and composition of a picture, to select the right lens and viewpoint, and to make great efforts with lighting and post-processing to arrange the tones and contrast. Unfortunately, such painstaking work and advanced skill is out of reach for casual photographers. In addition, for professional photographers, it is important to improve workflow efficiency. The goal of our work is to allow users to achieve a faithful viewpoint for rephotography and a particular appearance with ease and speed. To this end, we analyze and transfer properties of a model photo to a new photo. In particular, we transfer the viewpoint of a reference photo to enable rephotography. In addition, we transfer photographic appearance from a model photo to a new input photo. In this thesis,we present two contributions that transfer photographic view and look using model photographs and one contribution that magnifies existing defocus given a single photo. First, we address the challenge of viewpoint matching for rephotography. Our interactive, computer-vision-based technique helps users match the viewpoint of a reference photograph at capture time. Next, we focus on the tonal aspects of photographic look using post-processing. Users just need to provide a pair of photos, an input and a model, and our technique automatically transfers the look from the model to the input. Finally, we magnify defocus given a single image. We analyze the existing defocus in the input image and increase the amount of defocus present in out-of focus regions.",
    "advisors": ["Frdo Durand"],
    "text": "Analysis and transfer of photographic viewpoint and appearance To make a compelling photograph, photographers need to carefully choose the subject and composition of a picture, to select the right lens and viewpoint, and to make great efforts with lighting and post-processing to arrange the tones and contrast. Unfortunately, such painstaking work and advanced skill is out of reach for casual photographers. In addition, for professional photographers, it is important to improve workflow efficiency. The goal of our work is to allow users to achieve a faithful viewpoint for rephotography and a particular appearance with ease and speed. To this end, we analyze and transfer properties of a model photo to a new photo. In particular, we transfer the viewpoint of a reference photo to enable rephotography. In addition, we transfer photographic appearance from a model photo to a new input photo. In this thesis,we present two contributions that transfer photographic view and look using model photographs and one contribution that magnifies existing defocus given a single photo. First, we address the challenge of viewpoint matching for rephotography. Our interactive, computer-vision-based technique helps users match the viewpoint of a reference photograph at capture time. Next, we focus on the tonal aspects of photographic look using post-processing. Users just need to provide a pair of photos, an input and a model, and our technique automatically transfers the look from the model to the input. Finally, we magnify defocus given a single image. We analyze the existing defocus in the input image and increase the amount of defocus present in out-of focus regions."
}, {
    "id": "oai:dspace.mit.edu:1721.1/44709",
    "title": "Development of multimodal spectroscopy for the detection of vulnerable atherosclerotic plaques",
    "abstract": "The combination of reflectance, fluorescence, and Raman spectroscopy - which is termed multimodal spectroscopy (MMS) - provides complementary and depth-sensitive information about tissue composition. As such, MMS can provide biochemical and morphological information useful in detecting vulnerable atherosclerotic plaques, that is, plaques most prone to rupture and causing sudden death. Early detection of these vulnerable plaques is critical to reducing patient mortality associated with cardiovascular disease. In developing MMS into a clinical diagnostic modality, several scientific and engineering directions are explored in this work: the physical motivation for MMS, the framework of quantitative extraction of spectral parameters, the spectral probes that enable the efficient collection of data, a clinical instrument able to provide real-time diagnosis, and, finally, a clinical implementation of the entire methodology. The motivation for MMS is shown through a pilot in vitro study using carotid artery specimens, which shows the promise for MMS to detect features of vulnerable plaque. Having established the motivation, the next step describes the mathematical tools used to extract quantitative spectral parameters and, moreover, to assess the uncertainty and confidence of the spectral information. In order to implement MMS, the development of an efficient, specialized MMS probe for data acquisition and a compact and practical clinical MMS instrument are described. Lastly, in vivo and ex vivo results from a relatively large clinical study of vulnerable plaque in humans show excellent agreement between MMS and histopathology. Specifically, MMS is shown to have the ability to detect a thin fibrous cap, necrotic core or superficial foam cells, and thrombus.",
    "advisors": ["Michael S. Feld"],
    "text": "Development of multimodal spectroscopy for the detection of vulnerable atherosclerotic plaques The combination of reflectance, fluorescence, and Raman spectroscopy - which is termed multimodal spectroscopy (MMS) - provides complementary and depth-sensitive information about tissue composition. As such, MMS can provide biochemical and morphological information useful in detecting vulnerable atherosclerotic plaques, that is, plaques most prone to rupture and causing sudden death. Early detection of these vulnerable plaques is critical to reducing patient mortality associated with cardiovascular disease. In developing MMS into a clinical diagnostic modality, several scientific and engineering directions are explored in this work: the physical motivation for MMS, the framework of quantitative extraction of spectral parameters, the spectral probes that enable the efficient collection of data, a clinical instrument able to provide real-time diagnosis, and, finally, a clinical implementation of the entire methodology. The motivation for MMS is shown through a pilot in vitro study using carotid artery specimens, which shows the promise for MMS to detect features of vulnerable plaque. Having established the motivation, the next step describes the mathematical tools used to extract quantitative spectral parameters and, moreover, to assess the uncertainty and confidence of the spectral information. In order to implement MMS, the development of an efficient, specialized MMS probe for data acquisition and a compact and practical clinical MMS instrument are described. Lastly, in vivo and ex vivo results from a relatively large clinical study of vulnerable plaque in humans show excellent agreement between MMS and histopathology. Specifically, MMS is shown to have the ability to detect a thin fibrous cap, necrotic core or superficial foam cells, and thrombus."
}, {
    "id": "oai:dspace.mit.edu:1721.1/39732",
    "title": "Verification of d-wave pairing symmetry by microwave intermodulation distortion measurements in yttrium barium copper oxide",
    "abstract": "We report measurements of the temperature and power dependence of the microwave frequency intermodulation distortion (IMD) in high quality pulsed laser deposition (PLD) Yttrium Barium Copper Oxide (YBCO) on LaAlO3 substrate. A low-temperature (T < 30 K) increase in IMD is the observation of an upturn of the nonlinear coefficient of the quadratic field dependence of the penetration depth. This IMD upturn is limited by the nonlinear Meissner effect that has been predicted for d-wave high-T, superconductors. Various amounts of IMD increase are observed for different films with impurity (Ni, Zn and Ca) doping and other defects. The demonstration of the IMD upturn and the nonlinear Meissner effect were possible because the IMD measurement is an extremely sensitive method to detect the penetration depth change at even less than 0.01 nm. IMDs from various samples tend to merge at a single universal value at 0 K regardless of disorder, defects, and impurities due to the node singularity at 0 K. There is a similar converging trend in IMD towards the transition temperature T, due to the quasiparticle thermal excitation and depletion of superelectrons. It is most likely that IMD has both intrinsic and extrinsic contributions.",
    "advisors": ["Daniel E. Oates", "Terry P. Orlando"],
    "text": "Verification of d-wave pairing symmetry by microwave intermodulation distortion measurements in yttrium barium copper oxide We report measurements of the temperature and power dependence of the microwave frequency intermodulation distortion (IMD) in high quality pulsed laser deposition (PLD) Yttrium Barium Copper Oxide (YBCO) on LaAlO3 substrate. A low-temperature (T < 30 K) increase in IMD is the observation of an upturn of the nonlinear coefficient of the quadratic field dependence of the penetration depth. This IMD upturn is limited by the nonlinear Meissner effect that has been predicted for d-wave high-T, superconductors. Various amounts of IMD increase are observed for different films with impurity (Ni, Zn and Ca) doping and other defects. The demonstration of the IMD upturn and the nonlinear Meissner effect were possible because the IMD measurement is an extremely sensitive method to detect the penetration depth change at even less than 0.01 nm. IMDs from various samples tend to merge at a single universal value at 0 K regardless of disorder, defects, and impurities due to the node singularity at 0 K. There is a similar converging trend in IMD towards the transition temperature T, due to the quasiparticle thermal excitation and depletion of superelectrons. It is most likely that IMD has both intrinsic and extrinsic contributions."
}, {
    "id": "oai:dspace.mit.edu:1721.1/17963",
    "title": "Diffractive optics for maskless lithography and imaging",
    "abstract": "Semiconductor industry has primarily been driven by the capability of lithography to pattern smaller and smaller features. However due to increasing mask costs and complexity, and increasing tool costs, the state-of-the-art technology in lithography is accessible only to a select few. Zone-plate array lithography (ZPAL) is a novel method of maskless lithography that aims to alleviate some of these issues while offering a solution that can be extended to the limits of nanolithography. In ZPAL, an array of diffractive lenses is used to form an array of spots on the substrate. Each spot is modulated independently by means of spatial-light modulators. This essentially creates a \"parallel laserwriter\". In addition, this lithography system can be converted into a parallel-confocal microscope, which enables fast, high-resolution imaging. This thesis addresses the performance of diffractive lenses, particularly high-numerical aperture zone plates for lithography and imaging using a combination of experimental and theoretical studies. A novel proximity-effect correction algorithm that was implemented effectively in a ZPAL system is also described. Variations to another diffractive lens known as the photon sieve are proposed. The first ever lithography results performed using these new elements are presented in this thesis.",
    "advisors": ["Henry I. Smith"],
    "text": "Diffractive optics for maskless lithography and imaging Semiconductor industry has primarily been driven by the capability of lithography to pattern smaller and smaller features. However due to increasing mask costs and complexity, and increasing tool costs, the state-of-the-art technology in lithography is accessible only to a select few. Zone-plate array lithography (ZPAL) is a novel method of maskless lithography that aims to alleviate some of these issues while offering a solution that can be extended to the limits of nanolithography. In ZPAL, an array of diffractive lenses is used to form an array of spots on the substrate. Each spot is modulated independently by means of spatial-light modulators. This essentially creates a \"parallel laserwriter\". In addition, this lithography system can be converted into a parallel-confocal microscope, which enables fast, high-resolution imaging. This thesis addresses the performance of diffractive lenses, particularly high-numerical aperture zone plates for lithography and imaging using a combination of experimental and theoretical studies. A novel proximity-effect correction algorithm that was implemented effectively in a ZPAL system is also described. Variations to another diffractive lens known as the photon sieve are proposed. The first ever lithography results performed using these new elements are presented in this thesis."
}, {
    "id": "oai:dspace.mit.edu:1721.1/107289",
    "title": "Tradeoffs of the use of SiGe buffer layers in tandem GaAsP/Si solar cells",
    "abstract": "III-V multi-junction solar cells currently have the highest reported theoretical and experimental energy conversion efficiency but their cost, mainly attributed to the use of expensive substrates, limits their widespread use for terrestrial applications. Successful integration of III--V's on a Si substrate to enable a III-V/Si tandem cell can lower the cost of energy by combining the high-efficiency of the III--V materials with the low-cost and abundance of the Si substrate. A maximum theoretical efficiency of 44.8% from a tandem cell on Si can be achieved by using a GaAsP (Eg=1.7 eV) as the top cell. Out of several possible integration routes, the use of a linearly graded SiGe buffer as interfacial layer between the two cells potentially yields the highest quality for the epitaxial GaAsP layer, an essential requirement for realization of high-efficiency solar cells. In this thesis, the impact of the SiGe buffer layer on the optical and electrical characteristics of the bottom Si cell of a GaAsP/Si tandem solar cell was assessed via experimental work. The growth of a SiGe buffer layer was shown to increase the threading dislocation density and as a result the leakage current of the bottom Si cell by about 10x. In addition, the low-bandgap SiGe absorbs more than 80% of the light that is intended for the Si sub-cell, reducing the short-circuit current of the Si cell from 33 mA/cm to only 6 mA/cm. By using a step-cell design, in which the SiGe was partially etched to allow more light to reach the bottom cell, the current was increased to 20 mA/cm. To quantify the merits of the studied approach as well as evaluate other approaches, we have carried out a theoretical study of absorbed irradiance in a Si single-junction cell, a bonded GaAsP/Si tandem cell, a GaAsP/SiGe/Si tandem cell as well as the step-cell design. The GaAsP/Si bonded tandem cell showed 24% relative improvement in light absorption over a single-junction Si cell. The addition of a SiGe graded buffer was shown to reduce the total absorption by 25%, bringing the efficiency of GaAsP/SiGe/Si tandem cell under that of the Si single-junction cell. The step-cell design, even though successful in increasing light absorption, was not found effective in achieving a higher absorbed power density than that of the Si cell. These results suggest that any future work on integrating GaAsP cells on Si towards a high-performance tandem cell should be focused on using a higher-bandgap material as a graded buffer or using a wafer bonding technique.",
    "advisors": ["Judy L. Hoyt", "Jess A. del Alamo"],
    "text": "Tradeoffs of the use of SiGe buffer layers in tandem GaAsP/Si solar cells III-V multi-junction solar cells currently have the highest reported theoretical and experimental energy conversion efficiency but their cost, mainly attributed to the use of expensive substrates, limits their widespread use for terrestrial applications. Successful integration of III--V's on a Si substrate to enable a III-V/Si tandem cell can lower the cost of energy by combining the high-efficiency of the III--V materials with the low-cost and abundance of the Si substrate. A maximum theoretical efficiency of 44.8% from a tandem cell on Si can be achieved by using a GaAsP (Eg=1.7 eV) as the top cell. Out of several possible integration routes, the use of a linearly graded SiGe buffer as interfacial layer between the two cells potentially yields the highest quality for the epitaxial GaAsP layer, an essential requirement for realization of high-efficiency solar cells. In this thesis, the impact of the SiGe buffer layer on the optical and electrical characteristics of the bottom Si cell of a GaAsP/Si tandem solar cell was assessed via experimental work. The growth of a SiGe buffer layer was shown to increase the threading dislocation density and as a result the leakage current of the bottom Si cell by about 10x. In addition, the low-bandgap SiGe absorbs more than 80% of the light that is intended for the Si sub-cell, reducing the short-circuit current of the Si cell from 33 mA/cm to only 6 mA/cm. By using a step-cell design, in which the SiGe was partially etched to allow more light to reach the bottom cell, the current was increased to 20 mA/cm. To quantify the merits of the studied approach as well as evaluate other approaches, we have carried out a theoretical study of absorbed irradiance in a Si single-junction cell, a bonded GaAsP/Si tandem cell, a GaAsP/SiGe/Si tandem cell as well as the step-cell design. The GaAsP/Si bonded tandem cell showed 24% relative improvement in light absorption over a single-junction Si cell. The addition of a SiGe graded buffer was shown to reduce the total absorption by 25%, bringing the efficiency of GaAsP/SiGe/Si tandem cell under that of the Si single-junction cell. The step-cell design, even though successful in increasing light absorption, was not found effective in achieving a higher absorbed power density than that of the Si cell. These results suggest that any future work on integrating GaAsP cells on Si towards a high-performance tandem cell should be focused on using a higher-bandgap material as a graded buffer or using a wafer bonding technique."
}, {
    "id": "oai:dspace.mit.edu:1721.1/42056",
    "title": "Learning by learning to communicate",
    "abstract": "Human intelligence is a product of cooperation among many different specialists. Much of this cooperation must be learned, but we do not yet have a mechanism that explains how this might happen for the \"high-level\" agile cooperation that permeates our daily lives. I propose that the various specialists learn to cooperate by learning to communicate, basing this proposal on the phenomenon of communication bootstrapping, in which shared experiences form a basis for agreement on a system of signals. In this dissertation, I lay out a roadmap for investigating this hypothesis, identifying problems that must be overcome in order to understand the capabilities of communication bootstrapping and in order to test whether it is exploited by human intelligence. I then demonstrate progress along the course of investigation laid out in my roadmap: * I establish a measure of developmental cost that allows me to eliminate many possible designs * I develop a method of engineering devices for use in models of intelligence, including characterizing their behavior under a wide variety of conditions and compensating for their misbehavior using failure simplification. * I develop mechanisms that reliably produce communication bootstrapping such that it can be used to connect specialists in an engineered system. * I construct a demonstration system including a simulated world and pair of observers that learn world dynamics via communication bootstrapping.",
    "advisors": ["Gerald Jay Sussman"],
    "text": "Learning by learning to communicate Human intelligence is a product of cooperation among many different specialists. Much of this cooperation must be learned, but we do not yet have a mechanism that explains how this might happen for the \"high-level\" agile cooperation that permeates our daily lives. I propose that the various specialists learn to cooperate by learning to communicate, basing this proposal on the phenomenon of communication bootstrapping, in which shared experiences form a basis for agreement on a system of signals. In this dissertation, I lay out a roadmap for investigating this hypothesis, identifying problems that must be overcome in order to understand the capabilities of communication bootstrapping and in order to test whether it is exploited by human intelligence. I then demonstrate progress along the course of investigation laid out in my roadmap: * I establish a measure of developmental cost that allows me to eliminate many possible designs * I develop a method of engineering devices for use in models of intelligence, including characterizing their behavior under a wide variety of conditions and compensating for their misbehavior using failure simplification. * I develop mechanisms that reliably produce communication bootstrapping such that it can be used to connect specialists in an engineered system. * I construct a demonstration system including a simulated world and pair of observers that learn world dynamics via communication bootstrapping."
}, {
    "id": "oai:dspace.mit.edu:1721.1/97800",
    "title": "Micro-optic elements for a compact opto-electronic integrated neural coprocessor",
    "abstract": "The research done for this thesis was aimed at developing the optical elements needed for the Compact Opto-electronic Integrated Neural coprocessor (COIN coprocessor) project. The COIN coprocessor is an implementation of a feed forward neural network using free-space optical interconnects to communicate between neurons. Prior work on this project had assumed these interconnects would be formed using Holographic Optical Elements (HOEs), so early work for this thesis was directed along these lines. Important limits to the use of HOEs in the COIN system were identified and evaluated. In particular, the problem of changing wavelength between the hologram recording and readout steps was examined and it was shown that there is no general solution to this problem when the hologram to be recorded is constructed with more than two plane waves interfering with each other. Two experimental techniques, the holographic bead lens and holographic liftoff, were developed as partial workarounds to the identified limitations. As an alternative to HOEs, an optical element based on the concept of the Fresnel Zone Plate was developed and experimentally tested. The zone plate based elements offer an easily scalable method for fabricating the COIN optical interconnects using standard lithographic processes and appear to be the best choice for the COIN coprocessor project at this time. In addition to the development of the optical elements for the COIN coprocessor, this thesis also looks at the impact of optical element efficiency on the power consumption of the COIN coprocessor. Finally, a model of the COIN network based on the current COIN design was used to compare the performance and cost of the COIN system with competing implementations of neural networks, with the conclusion that at this time the proposed COIN coprocessor system is still a competitive option for neural network implementations.",
    "advisors": ["Cardinal Warde"],
    "text": "Micro-optic elements for a compact opto-electronic integrated neural coprocessor The research done for this thesis was aimed at developing the optical elements needed for the Compact Opto-electronic Integrated Neural coprocessor (COIN coprocessor) project. The COIN coprocessor is an implementation of a feed forward neural network using free-space optical interconnects to communicate between neurons. Prior work on this project had assumed these interconnects would be formed using Holographic Optical Elements (HOEs), so early work for this thesis was directed along these lines. Important limits to the use of HOEs in the COIN system were identified and evaluated. In particular, the problem of changing wavelength between the hologram recording and readout steps was examined and it was shown that there is no general solution to this problem when the hologram to be recorded is constructed with more than two plane waves interfering with each other. Two experimental techniques, the holographic bead lens and holographic liftoff, were developed as partial workarounds to the identified limitations. As an alternative to HOEs, an optical element based on the concept of the Fresnel Zone Plate was developed and experimentally tested. The zone plate based elements offer an easily scalable method for fabricating the COIN optical interconnects using standard lithographic processes and appear to be the best choice for the COIN coprocessor project at this time. In addition to the development of the optical elements for the COIN coprocessor, this thesis also looks at the impact of optical element efficiency on the power consumption of the COIN coprocessor. Finally, a model of the COIN network based on the current COIN design was used to compare the performance and cost of the COIN system with competing implementations of neural networks, with the conclusion that at this time the proposed COIN coprocessor system is still a competitive option for neural network implementations."
}, {
    "id": "oai:dspace.mit.edu:1721.1/44411",
    "title": "Implementation and modeling of a scheduled Optical Flow Switching (OFS) network",
    "abstract": "In this thesis we present analysis of Optical Flow Switching (OFS), an architectural approach for enabling all-optical user to user connections for transmission of Internet traffic. We first describe a demonstration of OFS on the ONRAMP test environment which is a MAN optical network implemented in hardware in the Boston geographic area. This demonstration shows the viability of OFS in an actual implementation, with good performance results and an assessment over OFS overheads. Then, we use stochastic models to quantify the behavior of an OFS network. Strong quantitative evidence leads us to draw the conclusion that scheduling is a necessary component of any architectural approach to implementing OFS in a Metro Area network (MAN).",
    "advisors": ["Vincent Chan"],
    "text": "Implementation and modeling of a scheduled Optical Flow Switching (OFS) network In this thesis we present analysis of Optical Flow Switching (OFS), an architectural approach for enabling all-optical user to user connections for transmission of Internet traffic. We first describe a demonstration of OFS on the ONRAMP test environment which is a MAN optical network implemented in hardware in the Boston geographic area. This demonstration shows the viability of OFS in an actual implementation, with good performance results and an assessment over OFS overheads. Then, we use stochastic models to quantify the behavior of an OFS network. Strong quantitative evidence leads us to draw the conclusion that scheduling is a necessary component of any architectural approach to implementing OFS in a Metro Area network (MAN)."
}, {
    "id": "oai:dspace.mit.edu:1721.1/78547",
    "title": "A theoretical analysis of interstitial hydrogen : pressure-composition-temperature, chemical potential, enthalpy and entropy",
    "abstract": "We provide a first principles analysis of the physics and thermodynamics of interstitial hydrogen in metal. By utilizing recent advances in Density Functional Theory (DFT) to get state energies of the metal-hydrogen system, we are able to model the absorption process fairly accurately. A connection to experiment is made via Pressure-Composition-Temperature (PCT) isotherms, and thermodynamic molar quantities. In the model, we understand the excess entropy of absorbed hydrogen in terms of the change in its accessible microstates. A connection is also made between the entropy and electronic states of interstitial hydrogen. However, our model indicates that this connection is too small to account for experimental results. Therefore, a conclusion is made that the entropy of absorbed hydrogen is mostly (non-ideal) configurational in nature. To model the latter in a manner consistent with experiment, we have explored a new model that posits a weak binding between clusters of hydrogen atoms at neighboring sites. We have developed a formulation and fitted the results to experimental data. We find a least squares fitting of the model to the entropy and enthalpy results in model parameters which seem physically reasonable. The resulting model appears to provide a natural physical explanation for the dependence of the excess entropy on loading.",
    "advisors": ["Peter L. Hagelstein"],
    "text": "A theoretical analysis of interstitial hydrogen : pressure-composition-temperature, chemical potential, enthalpy and entropy We provide a first principles analysis of the physics and thermodynamics of interstitial hydrogen in metal. By utilizing recent advances in Density Functional Theory (DFT) to get state energies of the metal-hydrogen system, we are able to model the absorption process fairly accurately. A connection to experiment is made via Pressure-Composition-Temperature (PCT) isotherms, and thermodynamic molar quantities. In the model, we understand the excess entropy of absorbed hydrogen in terms of the change in its accessible microstates. A connection is also made between the entropy and electronic states of interstitial hydrogen. However, our model indicates that this connection is too small to account for experimental results. Therefore, a conclusion is made that the entropy of absorbed hydrogen is mostly (non-ideal) configurational in nature. To model the latter in a manner consistent with experiment, we have explored a new model that posits a weak binding between clusters of hydrogen atoms at neighboring sites. We have developed a formulation and fitted the results to experimental data. We find a least squares fitting of the model to the entropy and enthalpy results in model parameters which seem physically reasonable. The resulting model appears to provide a natural physical explanation for the dependence of the excess entropy on loading."
}, {
    "id": "oai:dspace.mit.edu:1721.1/93058",
    "title": "Structural and functional imaging of the human and small animal eyes using ultrahigh speed Fourier domain optical coherence tomography",
    "abstract": "Optical coherence tomography (OCT) is a non-invasive optical imaging technique that allows the three-dimensional structure of biological tissue to be visualized with micrometer resolution. In ophthalmology OCT has the unique advantage that it provides cross-sectional images of the retina and choroid noninvasively and in vivo, which have led OCT to be a clinical standard for the diagnosis of a variety of retinal diseases. Although current commercial Fourier domain OCT systems have high imaging speeds of 20-100kHz A-scan rates, these imaging speeds are not sufficient for more advanced structural and functional imaging techniques. Current state-of-the-art spectral domain and swept source OCT provide ultrahigh imaging speeds of >200kHz A-scan rates. These speeds enable functional imaging of retinal blood flow, OCT angiography of the retinal and choroidal microvasculature, and wide field volumetric structural imaging of the retina and choroid. In this thesis, advances in structural and functional ophthalmic imaging techniques for the human and small animal eyes are investigated using ultrahigh speed Fourier domain OCT. The following topics are discussed: (1) a method for numerically extracting and compensating dispersion mismatch in ultrahigh resolution spectral domain OCT, (2) ultrahigh speed spectral domain imaging in the small animal eye for measuring total retinal blood flow, (3) development of ultrahigh speed phase stable swept source OCT system for human retinal imaging, (4) OCT angiography of the choriocapillaris in the human eye, (5) clinical applications of OCT angiography in retinal diseases, including diabetic retinopathy and age-related macular degeneration, (6) small animal anesthesia protocol for functional hemodynamic imaging, and (7) imaging of neurovascular coupling in small animals using ultrahigh speed OCT.",
    "advisors": ["James G. Fujimoto"],
    "text": "Structural and functional imaging of the human and small animal eyes using ultrahigh speed Fourier domain optical coherence tomography Optical coherence tomography (OCT) is a non-invasive optical imaging technique that allows the three-dimensional structure of biological tissue to be visualized with micrometer resolution. In ophthalmology OCT has the unique advantage that it provides cross-sectional images of the retina and choroid noninvasively and in vivo, which have led OCT to be a clinical standard for the diagnosis of a variety of retinal diseases. Although current commercial Fourier domain OCT systems have high imaging speeds of 20-100kHz A-scan rates, these imaging speeds are not sufficient for more advanced structural and functional imaging techniques. Current state-of-the-art spectral domain and swept source OCT provide ultrahigh imaging speeds of >200kHz A-scan rates. These speeds enable functional imaging of retinal blood flow, OCT angiography of the retinal and choroidal microvasculature, and wide field volumetric structural imaging of the retina and choroid. In this thesis, advances in structural and functional ophthalmic imaging techniques for the human and small animal eyes are investigated using ultrahigh speed Fourier domain OCT. The following topics are discussed: (1) a method for numerically extracting and compensating dispersion mismatch in ultrahigh resolution spectral domain OCT, (2) ultrahigh speed spectral domain imaging in the small animal eye for measuring total retinal blood flow, (3) development of ultrahigh speed phase stable swept source OCT system for human retinal imaging, (4) OCT angiography of the choriocapillaris in the human eye, (5) clinical applications of OCT angiography in retinal diseases, including diabetic retinopathy and age-related macular degeneration, (6) small animal anesthesia protocol for functional hemodynamic imaging, and (7) imaging of neurovascular coupling in small animals using ultrahigh speed OCT."
}, {
    "id": "oai:dspace.mit.edu:1721.1/108849",
    "title": "Integrated optical quantum manipulation and measurement of trapped ions",
    "abstract": "Individual atomic ions confined in designed electromagnetic potentials and manipulated via lasers are strong candidates as physical bases for quantum information processing (QIP). This is in large part due to their long coherence times, in distinguishability, and strong Coulomb interactions. Much work in recent years has utilized these properties to implement increasingly precise quantum operations essential for QIP, as well as to conduct increasingly sophisticated experiments on few-ion systems. Many questions remain however regarding how to implement the significant classical apparatus required to control and measure many ions (and indeed any physical qubit under study) in a scalable way that furthermore does not compromise qubit quality. This work draws on techniques in integrated optics to address this question. Planar-fabricated waveguides and gratings integrated with planar ion traps are demonstrated to allow optical addressing of individual 88Sr+ions 50 [mu]m above the chip surface with distraction-limited focused beams, with advantages in stability and scalability. Motivated by the requirement for low crosstalk in qubit addressing, we show also that intuitively designed devices can generate precisely tailored intensity profiles at the ion locations, with distraction-limited side lobe intensities characterized to the 5x10-6 level in relative intensity up to 25 [mu]m from the focus. Such devices can be implemented alongside complex systems in complementary metal-oxide-semiconductor (CMOS) processes. We show in addition that the multiple patternable metal layers present in CMOS processes can be used to create complex planar ion traps with performance comparable to simple single-layer traps, and that CMOS silicon avalanche photodiodes may be employed for scalable quantum state readout. Finally we show initial results on integrated electro-optic modulators for visible light. These results open possibilities for experiments with trapped ions in the short term, and indicate routes to achieving large-scale systems of thousands or more ions in the future. Though ion qubits may seem isolated from scalable solid-state technologies, it appears this apparent isolation may uniquely allow a cooperation with complex planar-fabricated optical and electronic systems without introducing additional decoherence.",
    "advisors": ["Rajeev J. Ram"],
    "text": "Integrated optical quantum manipulation and measurement of trapped ions Individual atomic ions confined in designed electromagnetic potentials and manipulated via lasers are strong candidates as physical bases for quantum information processing (QIP). This is in large part due to their long coherence times, in distinguishability, and strong Coulomb interactions. Much work in recent years has utilized these properties to implement increasingly precise quantum operations essential for QIP, as well as to conduct increasingly sophisticated experiments on few-ion systems. Many questions remain however regarding how to implement the significant classical apparatus required to control and measure many ions (and indeed any physical qubit under study) in a scalable way that furthermore does not compromise qubit quality. This work draws on techniques in integrated optics to address this question. Planar-fabricated waveguides and gratings integrated with planar ion traps are demonstrated to allow optical addressing of individual 88Sr+ions 50 [mu]m above the chip surface with distraction-limited focused beams, with advantages in stability and scalability. Motivated by the requirement for low crosstalk in qubit addressing, we show also that intuitively designed devices can generate precisely tailored intensity profiles at the ion locations, with distraction-limited side lobe intensities characterized to the 5x10-6 level in relative intensity up to 25 [mu]m from the focus. Such devices can be implemented alongside complex systems in complementary metal-oxide-semiconductor (CMOS) processes. We show in addition that the multiple patternable metal layers present in CMOS processes can be used to create complex planar ion traps with performance comparable to simple single-layer traps, and that CMOS silicon avalanche photodiodes may be employed for scalable quantum state readout. Finally we show initial results on integrated electro-optic modulators for visible light. These results open possibilities for experiments with trapped ions in the short term, and indicate routes to achieving large-scale systems of thousands or more ions in the future. Though ion qubits may seem isolated from scalable solid-state technologies, it appears this apparent isolation may uniquely allow a cooperation with complex planar-fabricated optical and electronic systems without introducing additional decoherence."
}, {
    "id": "oai:dspace.mit.edu:1721.1/111878",
    "title": "Sensing and timekeeping using a light-trapping diamond waveguide",
    "abstract": "Solid-state quantum systems have emerged as promising sensing platforms. In particular, the spin properties of nitrogen vacancy (NV) color centers in diamond make them outstanding sensors of magnetic fields, electric fields, and temperature under ambient conditions. This thesis focuses on spin-based sensing using multimode diamond waveguide structures to efficiently use large ensembles of NV centers (> 10). Temperature-stabilized precision magnetometry, thermometry, and electrometry are discussed. In addition, the precision characterization of the NV ground state structure under a transverse magnetic field and the use of NV-diamond for spin-based clocks are reported.",
    "advisors": ["Dirk Englund"],
    "text": "Sensing and timekeeping using a light-trapping diamond waveguide Solid-state quantum systems have emerged as promising sensing platforms. In particular, the spin properties of nitrogen vacancy (NV) color centers in diamond make them outstanding sensors of magnetic fields, electric fields, and temperature under ambient conditions. This thesis focuses on spin-based sensing using multimode diamond waveguide structures to efficiently use large ensembles of NV centers (> 10). Temperature-stabilized precision magnetometry, thermometry, and electrometry are discussed. In addition, the precision characterization of the NV ground state structure under a transverse magnetic field and the use of NV-diamond for spin-based clocks are reported."
}, {
    "id": "oai:dspace.mit.edu:1721.1/8012",
    "title": "Minimum description complexity",
    "abstract": "The classical problem of model selection among parametric model sets is considered. The goal is to choose a model set which best represents observed data. The critical task is the choice of a criterion for model set comparison. Pioneer information theoretic based approaches to this problem are Akaike information criterion (AIC) and different forms of minimum description length (MDL). The prior assumption in these methods is that the unknown true model is a member of all the competing sets. We introduce a new method of model selection: minimum description complexity (MDC). The approach is motivated by the Kullback-Leibler information distance. The method suggests choosing the model set for which the model set relative entropy is minimum. We provide a probabilistic method of MDC estimation for a class of parametric model sets. In this calculation the key factor is our prior assumption: unlike the existing methods, no assumption of the true model being a member of the competing model sets is needed. The main strength of the MDC calculation is in its method of extracting information from the observed data.",
    "advisors": ["Munther A. Dahleh"],
    "text": "Minimum description complexity The classical problem of model selection among parametric model sets is considered. The goal is to choose a model set which best represents observed data. The critical task is the choice of a criterion for model set comparison. Pioneer information theoretic based approaches to this problem are Akaike information criterion (AIC) and different forms of minimum description length (MDL). The prior assumption in these methods is that the unknown true model is a member of all the competing sets. We introduce a new method of model selection: minimum description complexity (MDC). The approach is motivated by the Kullback-Leibler information distance. The method suggests choosing the model set for which the model set relative entropy is minimum. We provide a probabilistic method of MDC estimation for a class of parametric model sets. In this calculation the key factor is our prior assumption: unlike the existing methods, no assumption of the true model being a member of the competing model sets is needed. The main strength of the MDC calculation is in its method of extracting information from the observed data."
}, {
    "id": "oai:dspace.mit.edu:1721.1/28709",
    "title": "Information theoretic aspects of the control and the mode estimation of stochastic systems",
    "abstract": "(cont.) parallel with a communication paradigm and deriving an analysis of performance. In our approach, the switching system is viewed as an encoder of the mode, which is interpreted as the message, while a probing signal establishes a random code. Using a distortion function, we define an uncertainty ball where the estimates are guaranteed to lie with probability arbitrarily close to 1. The radius of the uncertainty ball is directly related to the entropy rate of the switching process.",
    "advisors": ["Munther A. Daleh"],
    "text": "Information theoretic aspects of the control and the mode estimation of stochastic systems (cont.) parallel with a communication paradigm and deriving an analysis of performance. In our approach, the switching system is viewed as an encoder of the mode, which is interpreted as the message, while a probing signal establishes a random code. Using a distortion function, we define an uncertainty ball where the estimates are guaranteed to lie with probability arbitrarily close to 1. The radius of the uncertainty ball is directly related to the entropy rate of the switching process."
}, {
    "id": "oai:dspace.mit.edu:1721.1/117843",
    "title": "A proximal atomic coordination algorithm for distributed optimization in distribution grids",
    "abstract": "The control and regulation of power grids has historically relied upon large-scale scheduleable generation and relatively stable load demand profiles. With the advent of extensive local renewable energy generation technologies as well as the incorporation of load responsive demand response (DR) methodologies, it has become imperative that new distributed control strategies are developed to better regulate the increasingly volatile nature of modern generation and load profiles. In this thesis, we introduce a distributed control strategy called Proximal Atomic Coordination (PAC) to solve for optimal control strategies in distributed power grids, a problem called Optimal Power Flow (OPF). Using a convex relaxed variant of OPF, we show that PAC exhibits sub-linear convergence to the optimal ergodic cost, and linear convergence to the OPF solution. We demonstrate our results on various power grid topologies with large levels of renewable energy penetration and DR, and show that PAC converges to optimal control profiles in these scenarios. We further show that in certain regimes PAC outperforms the standard distributed 2-Block ADMM algorithm, and we discuss the benefits of using PAC over 2-Block ADMM and other standard distributed solvers.",
    "advisors": ["Anuradha Annaswamy"],
    "text": "A proximal atomic coordination algorithm for distributed optimization in distribution grids The control and regulation of power grids has historically relied upon large-scale scheduleable generation and relatively stable load demand profiles. With the advent of extensive local renewable energy generation technologies as well as the incorporation of load responsive demand response (DR) methodologies, it has become imperative that new distributed control strategies are developed to better regulate the increasingly volatile nature of modern generation and load profiles. In this thesis, we introduce a distributed control strategy called Proximal Atomic Coordination (PAC) to solve for optimal control strategies in distributed power grids, a problem called Optimal Power Flow (OPF). Using a convex relaxed variant of OPF, we show that PAC exhibits sub-linear convergence to the optimal ergodic cost, and linear convergence to the OPF solution. We demonstrate our results on various power grid topologies with large levels of renewable energy penetration and DR, and show that PAC converges to optimal control profiles in these scenarios. We further show that in certain regimes PAC outperforms the standard distributed 2-Block ADMM algorithm, and we discuss the benefits of using PAC over 2-Block ADMM and other standard distributed solvers."
}, {
    "id": "oai:dspace.mit.edu:1721.1/33935",
    "title": "Hybrid organic/quantum dot thin film structures and devices",
    "abstract": "Organic light emitting diodes have undergone rapid advancement over the course of the past decade. Similarly, quantum dot synthesis has progressed to the point that room temperature highly efficient photoluminescence can be realized. It is the purpose of this work to utilize the beneficial properties of these two material sets in a robust light emitting device. New deposition techniques are necessary to the realization of this goal, enabling QD organic hybrids to be created in a quick and reliable manner compatible with known device fabrication methods. With these techniques, quantum dot light emitting devices are fabricated, measured, and analyzed. The devices are of high efficiency and color saturation, and provide us with a test bed for understanding the interactions between inorganic QDs and organic thin films.",
    "advisors": ["Vladimir Bulovi", "Terry Orlando"],
    "text": "Hybrid organic/quantum dot thin film structures and devices Organic light emitting diodes have undergone rapid advancement over the course of the past decade. Similarly, quantum dot synthesis has progressed to the point that room temperature highly efficient photoluminescence can be realized. It is the purpose of this work to utilize the beneficial properties of these two material sets in a robust light emitting device. New deposition techniques are necessary to the realization of this goal, enabling QD organic hybrids to be created in a quick and reliable manner compatible with known device fabrication methods. With these techniques, quantum dot light emitting devices are fabricated, measured, and analyzed. The devices are of high efficiency and color saturation, and provide us with a test bed for understanding the interactions between inorganic QDs and organic thin films."
}, {
    "id": "oai:dspace.mit.edu:1721.1/8178",
    "title": "Synthetic aperture microscopy",
    "abstract": "In the late 1800's, Ernst Abbe, research director of the Carl Zeiss Optical Works, wrote down the rules for a lens to form a sharp image. Advances in communications theory, signal processing, and computers have allowed us.finally to break those rules. Our \"Synthetic Aperture Microscope\" floods a large region with a richly complex, finely structured pattern of light-the interference pattern of a ring of n coherent sources. A target within the volume of the interference fluoresces (or scatters or transmits) an amount of lights that reveals correspondences with this \"probing illumination.\" Modulating'fthe phases and amplitudes of the n beams with carefully chosen modulation signals causes the probe illumination to step through a predetermined or measured family of patterns. A sensor records the target's response in a time-sequence. This time-sequence contains each of order n2 complex Fourier coefficients of the target. Each of these coefficients is encrypted by a unique spread-spectrum key embedded in the amplitude and phase modulation signals. Signal processing picks out these coefficients to reconstruct an image of the target. Low resolution conventional imaging maps an array of \"targets\" (actually portions of a larger target) to a CCD array, thus allowing this sensing process to be done in parallel over a large region. The end result is to boost the resolution of a conventional imager by hundreds to thousands of sub-pixels per physical pixel. Both theoretical and experimental work on the engineering to make the concept practical are reported.",
    "advisors": ["Thomas F. Knight, Jr.", "Dennis M. Freeman"],
    "text": "Synthetic aperture microscopy In the late 1800's, Ernst Abbe, research director of the Carl Zeiss Optical Works, wrote down the rules for a lens to form a sharp image. Advances in communications theory, signal processing, and computers have allowed us.finally to break those rules. Our \"Synthetic Aperture Microscope\" floods a large region with a richly complex, finely structured pattern of light-the interference pattern of a ring of n coherent sources. A target within the volume of the interference fluoresces (or scatters or transmits) an amount of lights that reveals correspondences with this \"probing illumination.\" Modulating'fthe phases and amplitudes of the n beams with carefully chosen modulation signals causes the probe illumination to step through a predetermined or measured family of patterns. A sensor records the target's response in a time-sequence. This time-sequence contains each of order n2 complex Fourier coefficients of the target. Each of these coefficients is encrypted by a unique spread-spectrum key embedded in the amplitude and phase modulation signals. Signal processing picks out these coefficients to reconstruct an image of the target. Low resolution conventional imaging maps an array of \"targets\" (actually portions of a larger target) to a CCD array, thus allowing this sensing process to be done in parallel over a large region. The end result is to boost the resolution of a conventional imager by hundreds to thousands of sub-pixels per physical pixel. Both theoretical and experimental work on the engineering to make the concept practical are reported."
}, {
    "id": "oai:dspace.mit.edu:1721.1/87449",
    "title": "Procedural authoring of solid models",
    "abstract": "This thesis investigates the creation, representation, and manipulation of volumetric geometry suitable for computer graphics applications. In order to capture and reproduce the appearance and behavior of many objects, it is necessary to model the internal structures and materials, and how they change over time. However, producing real-world effects with standard surface modeling techniques can be extremely challenging. My key contribution is a concise procedural approach for authoring layered, solid models. Using a simple scripting language, a complete volumetric representation of an object, including its internal structure, can be created from one or more input surfaces, such as scanned polygonal meshes, CAD models or implicit surfaces. Furthermore, the resulting model can be easily modified using sculpting and simulation tools, such as the Finite Element Method or particle systems, which are embedded as operators in the language. Simulation is treated as a modeling tool rather than merely a device for animation, which provides a novel level of abstraction for interacting with simulation environments. I present an implementation of the language using a flexible tetrahedral representation, which I chose because of its advantages for simulation tasks. The language and implementation are demonstrated on a variety of complex examples that were inspired by real-world objects.",
    "advisors": ["Julie Dorsey", "Leonard McMillan"],
    "text": "Procedural authoring of solid models This thesis investigates the creation, representation, and manipulation of volumetric geometry suitable for computer graphics applications. In order to capture and reproduce the appearance and behavior of many objects, it is necessary to model the internal structures and materials, and how they change over time. However, producing real-world effects with standard surface modeling techniques can be extremely challenging. My key contribution is a concise procedural approach for authoring layered, solid models. Using a simple scripting language, a complete volumetric representation of an object, including its internal structure, can be created from one or more input surfaces, such as scanned polygonal meshes, CAD models or implicit surfaces. Furthermore, the resulting model can be easily modified using sculpting and simulation tools, such as the Finite Element Method or particle systems, which are embedded as operators in the language. Simulation is treated as a modeling tool rather than merely a device for animation, which provides a novel level of abstraction for interacting with simulation environments. I present an implementation of the language using a flexible tetrahedral representation, which I chose because of its advantages for simulation tasks. The language and implementation are demonstrated on a variety of complex examples that were inspired by real-world objects."
}, {
    "id": "oai:dspace.mit.edu:1721.1/93833",
    "title": "Accountable systems : enabling appropriate use of information on the Web",
    "abstract": ".The Web is plagued by problems of privacy and piracy. In each instance, outdated laws combined with current technology provides little reassurance to information providers, and may have damaging side effects. To meet this challenge, we have designed, built, and tested and present a new architecture for information exchange on the Internet called HTTPA (Hyper Text Transfer Protocol with Accountability). In this 'Accountable' architecture, information use is tracked from its creation through its modification, repurposing and republishing with the help of the 'Provenance Tracking Network', a decentralized network of peers that together record the rules governing resources on the Web, coupled with how these resources are shared and used. We found that the accountable systems framework provides an attractive compromise where the rights and abilities of parties to control access and use is balanced against the burden of restrictions imposed for two prototype applications; one dealing with privacy in healthcare, and the other with rights in photo sharing. Healthcare patients given the ability to be notified of use of their medical records judged that they had sufficient privacy protection, while doctors obtained easier access to the records. Providers of photos could be assured their images were not being misused, without the many drawbacks that digital rights management (DRM) systems impose on those consuming the material. In a similar vein in which the growth of e-commerce Web sites led to the massive adoption of HTTPS, we envision that over time HTTPA will be accepted by the larger Web community to meet the concerns of privacy and copyright violations on the Web.",
    "advisors": ["Tim Berners-Lee", "Lalana Kagal"],
    "text": "Accountable systems : enabling appropriate use of information on the Web .The Web is plagued by problems of privacy and piracy. In each instance, outdated laws combined with current technology provides little reassurance to information providers, and may have damaging side effects. To meet this challenge, we have designed, built, and tested and present a new architecture for information exchange on the Internet called HTTPA (Hyper Text Transfer Protocol with Accountability). In this 'Accountable' architecture, information use is tracked from its creation through its modification, repurposing and republishing with the help of the 'Provenance Tracking Network', a decentralized network of peers that together record the rules governing resources on the Web, coupled with how these resources are shared and used. We found that the accountable systems framework provides an attractive compromise where the rights and abilities of parties to control access and use is balanced against the burden of restrictions imposed for two prototype applications; one dealing with privacy in healthcare, and the other with rights in photo sharing. Healthcare patients given the ability to be notified of use of their medical records judged that they had sufficient privacy protection, while doctors obtained easier access to the records. Providers of photos could be assured their images were not being misused, without the many drawbacks that digital rights management (DRM) systems impose on those consuming the material. In a similar vein in which the growth of e-commerce Web sites led to the massive adoption of HTTPS, we envision that over time HTTPA will be accepted by the larger Web community to meet the concerns of privacy and copyright violations on the Web."
}, {
    "id": "oai:dspace.mit.edu:1721.1/117840",
    "title": "Computation by block copolymer self-assembly",
    "abstract": "Unconventional computation is a paradigm of computation that uses novel information tokens from natural systems to perform information processing. Using the complexity of physical systems, unconventional computing systems can efficiently solve problems that are difficult to solve classically. In this thesis, we use block copolymer self-assembly, a well-studied phenomenon in polymer science, to develop a new approach to computing by applying directed self-assembly to implement Ising-model-based computing systems in materials. In the first part of the thesis, we investigate directed self-assembly of block copolymer thin films within templates of different polygonal shapes. We define a two-state system based on the two degenerate alignment orientations of the ladder-shaped block copolymer structures formed inside square confinements, and study properties of the two-state system. In the second part of the thesis, we demonstrate an Ising lattice setup for directed self-assembly of block copolymers defined on two-dimensional arrays of posts. We develop an Ising-model-based simulation method that can perform block copolymer pattern prediction and template design. Finally, we design simple Boolean logic gates as a proof-of-concept demonstration of computation.",
    "advisors": ["Karl K. Berggren"],
    "text": "Computation by block copolymer self-assembly Unconventional computation is a paradigm of computation that uses novel information tokens from natural systems to perform information processing. Using the complexity of physical systems, unconventional computing systems can efficiently solve problems that are difficult to solve classically. In this thesis, we use block copolymer self-assembly, a well-studied phenomenon in polymer science, to develop a new approach to computing by applying directed self-assembly to implement Ising-model-based computing systems in materials. In the first part of the thesis, we investigate directed self-assembly of block copolymer thin films within templates of different polygonal shapes. We define a two-state system based on the two degenerate alignment orientations of the ladder-shaped block copolymer structures formed inside square confinements, and study properties of the two-state system. In the second part of the thesis, we demonstrate an Ising lattice setup for directed self-assembly of block copolymers defined on two-dimensional arrays of posts. We develop an Ising-model-based simulation method that can perform block copolymer pattern prediction and template design. Finally, we design simple Boolean logic gates as a proof-of-concept demonstration of computation."
}, {
    "id": "oai:dspace.mit.edu:1721.1/69773",
    "title": "Real-time brain-machine interface architectures : neural decoding from plan to movement",
    "abstract": "Brain-machine interfaces (BMI) aim to enable motor function in individuals with neurological injury or disease, by recording the neural activity, mapping or 'decoding' it into a motor command, and then controlling a device such as a computer interface or robotic arm. BMI research has largely focused on the problem of restoring the original motor function. The goal therefore has been to achieve a performance close to that of the healthy individual. There have been compelling proof of concept demonstrations of the utility of such BMIs in the past decade. However, performance of these systems needs to be significantly improved before they become clinically viable. Moreover, while developing high-performance BMIs with the goal of matching the original motor function is indeed valuable, a compelling goal is that of designing BMIs that can surpass original motor function. In this thesis, we first develop a novel real-time BMI for restoration of natural motor function. We then introduce a BMI architecture aimed at enhancing original motor function. We implement both our designs in rhesus monkeys. To facilitate the restoration of lost motor function, BMIs have focused on either estimating the continuous movement trajectory or target intent. However, natural movement often incorporates both. Moreover, both target and trajectory information are encoded in the motor cortical areas. These suggest that BMIs should be designed to combine these principal aspects of movement. We develop a novel two-stage BMI to decode jointly the target and trajectory of a reaching movement. First, we decode the intended target from neural spiking activity before movement initiation. Second, we combine the decoded target with the spiking activity during movement to estimate the trajectory. To do so, we use an optimal feedback-control design that aims to emulate the sensorimotor processing underlying actual motor control and directly processes the spiking activity using point process modeling in real time. We show that the two-stage BMI performs more accurately than either stage alone. Correct target prediction can compensate for inaccurate trajectory estimation and vice versa. This BMI also performs significantly better than linear regression approaches demonstrating the advantage of a design that more closely mimics the sensorimotor system.",
    "advisors": ["Gregory W. Wornell", "Emery N. Brown"],
    "text": "Real-time brain-machine interface architectures : neural decoding from plan to movement Brain-machine interfaces (BMI) aim to enable motor function in individuals with neurological injury or disease, by recording the neural activity, mapping or 'decoding' it into a motor command, and then controlling a device such as a computer interface or robotic arm. BMI research has largely focused on the problem of restoring the original motor function. The goal therefore has been to achieve a performance close to that of the healthy individual. There have been compelling proof of concept demonstrations of the utility of such BMIs in the past decade. However, performance of these systems needs to be significantly improved before they become clinically viable. Moreover, while developing high-performance BMIs with the goal of matching the original motor function is indeed valuable, a compelling goal is that of designing BMIs that can surpass original motor function. In this thesis, we first develop a novel real-time BMI for restoration of natural motor function. We then introduce a BMI architecture aimed at enhancing original motor function. We implement both our designs in rhesus monkeys. To facilitate the restoration of lost motor function, BMIs have focused on either estimating the continuous movement trajectory or target intent. However, natural movement often incorporates both. Moreover, both target and trajectory information are encoded in the motor cortical areas. These suggest that BMIs should be designed to combine these principal aspects of movement. We develop a novel two-stage BMI to decode jointly the target and trajectory of a reaching movement. First, we decode the intended target from neural spiking activity before movement initiation. Second, we combine the decoded target with the spiking activity during movement to estimate the trajectory. To do so, we use an optimal feedback-control design that aims to emulate the sensorimotor processing underlying actual motor control and directly processes the spiking activity using point process modeling in real time. We show that the two-stage BMI performs more accurately than either stage alone. Correct target prediction can compensate for inaccurate trajectory estimation and vice versa. This BMI also performs significantly better than linear regression approaches demonstrating the advantage of a design that more closely mimics the sensorimotor system."
}, {
    "id": "oai:dspace.mit.edu:1721.1/60099",
    "title": "Ultrastructure and nanomechanical properties of aggrecan from native cartilage and engineered tissue",
    "abstract": "Electrostatic interactions associated with aggrecan, one of the major components of the cartilage extracellular matrix, are responsible for ~50% of the equilibrium compressive elastic modulus of the tissue. The bottle-brush-shaped aggrecan consists of a core protein to which ~100 sulfated glycosaminoglycan (sGAG) chains are attached. Loss of sGAG is one early events in the pathogenesis of osteoarthritis and the resulting degradation of cartilage is irreversible due to its limited capacity for self-repair. Tissue engineering is one of the techniques which holds great potential for cartilage repair. In order to achieve successful repair, a clear understanding of native and engineered cartilage aggrecan is essential. With atomic force microscopy and high resolution force microscopy, the structure of aggrecan single molecules and the nanomechanical properties of an end-grafted aggrecan monolayer were quantified. Adult human aggrecan showed significantly shorter GAG chains and core proteins as well as lower molecular stiffness compared to that of newborn aggrecan. After enzymatic digestion of chondroitin sulfate (CS) GAGs, keratan sulfate GAG chains were visualized near the N-terminal domain of a less extended core protein. Direct visualization of aggrecan aggregates confirmed the structure of the constituent hyaluronic acid, aggrecan G1 domain, and link protein. Increased flexibility of the core protein was found near the G1 domain, which may facilitate the aggregate self-assembly process. Aggregated and non-aggregated aggrecan both showed remarked flexibility (i.e., decreased extension ratio) when the aggrecan areal density increased. These findings on intra- and inter-molecular structure provide insights into the structure-property relationships of aggrecan in vivo. Aggrecan produced by animal-matched bone marrow stromal cells (BMSCs) and chondrocytes seeded in peptide hydrogel were evaluated for their age-associated structure and nanomechanical properties. Independent of age, BMSCs produced longer core proteins and GAG chains than the chondrocytes, suggesting that the BMSC-produced aggrecan was characteristic of that from young cartilage. Comparison of the adult BMSC-produced aggrecan with adult cartilage-extracted aggrecan revealed that adult BMSC-aggrecan has a phenotype characteristic of young growth cartilage: primarily full-length aggrecan core, longer GAG chains and a higher content of chondroitin-4-sulfate in the CS-GAG chains, the latter identified via fluorescence assisted carbohydrate electrophoresis. The nanomechanical stiffness of BMSC-aggrecan was demonstrably greater than that of cartilage-aggrecan at the same total sGAG (fixed charge) density. These results support the use of adult BMSCs for cell-based cartilage repair.",
    "advisors": ["Alan J. Grodzinsky", "Christine Ortiz"],
    "text": "Ultrastructure and nanomechanical properties of aggrecan from native cartilage and engineered tissue Electrostatic interactions associated with aggrecan, one of the major components of the cartilage extracellular matrix, are responsible for ~50% of the equilibrium compressive elastic modulus of the tissue. The bottle-brush-shaped aggrecan consists of a core protein to which ~100 sulfated glycosaminoglycan (sGAG) chains are attached. Loss of sGAG is one early events in the pathogenesis of osteoarthritis and the resulting degradation of cartilage is irreversible due to its limited capacity for self-repair. Tissue engineering is one of the techniques which holds great potential for cartilage repair. In order to achieve successful repair, a clear understanding of native and engineered cartilage aggrecan is essential. With atomic force microscopy and high resolution force microscopy, the structure of aggrecan single molecules and the nanomechanical properties of an end-grafted aggrecan monolayer were quantified. Adult human aggrecan showed significantly shorter GAG chains and core proteins as well as lower molecular stiffness compared to that of newborn aggrecan. After enzymatic digestion of chondroitin sulfate (CS) GAGs, keratan sulfate GAG chains were visualized near the N-terminal domain of a less extended core protein. Direct visualization of aggrecan aggregates confirmed the structure of the constituent hyaluronic acid, aggrecan G1 domain, and link protein. Increased flexibility of the core protein was found near the G1 domain, which may facilitate the aggregate self-assembly process. Aggregated and non-aggregated aggrecan both showed remarked flexibility (i.e., decreased extension ratio) when the aggrecan areal density increased. These findings on intra- and inter-molecular structure provide insights into the structure-property relationships of aggrecan in vivo. Aggrecan produced by animal-matched bone marrow stromal cells (BMSCs) and chondrocytes seeded in peptide hydrogel were evaluated for their age-associated structure and nanomechanical properties. Independent of age, BMSCs produced longer core proteins and GAG chains than the chondrocytes, suggesting that the BMSC-produced aggrecan was characteristic of that from young cartilage. Comparison of the adult BMSC-produced aggrecan with adult cartilage-extracted aggrecan revealed that adult BMSC-aggrecan has a phenotype characteristic of young growth cartilage: primarily full-length aggrecan core, longer GAG chains and a higher content of chondroitin-4-sulfate in the CS-GAG chains, the latter identified via fluorescence assisted carbohydrate electrophoresis. The nanomechanical stiffness of BMSC-aggrecan was demonstrably greater than that of cartilage-aggrecan at the same total sGAG (fixed charge) density. These results support the use of adult BMSCs for cell-based cartilage repair."
}, {
    "id": "oai:dspace.mit.edu:1721.1/38201",
    "title": "An electromechanical valve drive incorporating a nonlinear mechanical transformer",
    "abstract": "In traditional internal combustion engines, a camshaft acts on the valve stems to open and close the valves. Valve timing is fixed relative to piston position. On the other hand, if a valve is flexibly controlled by a variable valve actuation (VVA) system, we can achieve significant improvements in fuel efficiency, engine performance, and emissions. One of the most advanced variable valve actuation systems is the VVA operated by an electromechanical actuator without a camshaft, the so-called bi-positional electromechanical valve drive (EMVD). Existing EMVDs characteristically use a spring to provide the required mechanical power for operating a valve. The use of a spring provides many benefits to the design of the system, but it also results in difficult design challenges. The large holding force against the spring at the ends of the stroke suggests the use of a normal-force electromagnetic actuator, which, from a servomechanical point of view, is considerably inferior to a shear-force actuator. Furthermore, the large holding force generates a large jerk at the beginning and the end of a stroke and makes it difficult to achieve soft valve landing. An innovative electromechanical valve drive (EMVD) design is proposed, which incorporates a nonlinear mechanical transformer and a shear-force actuator. This allows not only fast but also smooth valve motion, almost zero seating velocity, zero holding power, and improved control with acceptable electric power. This proposed concept is modeled, analyzed, simulated, designed, and implemented. Experimental results show the beneficial features of the promising proposed concept.",
    "advisors": ["John G. Kassakian"],
    "text": "An electromechanical valve drive incorporating a nonlinear mechanical transformer In traditional internal combustion engines, a camshaft acts on the valve stems to open and close the valves. Valve timing is fixed relative to piston position. On the other hand, if a valve is flexibly controlled by a variable valve actuation (VVA) system, we can achieve significant improvements in fuel efficiency, engine performance, and emissions. One of the most advanced variable valve actuation systems is the VVA operated by an electromechanical actuator without a camshaft, the so-called bi-positional electromechanical valve drive (EMVD). Existing EMVDs characteristically use a spring to provide the required mechanical power for operating a valve. The use of a spring provides many benefits to the design of the system, but it also results in difficult design challenges. The large holding force against the spring at the ends of the stroke suggests the use of a normal-force electromagnetic actuator, which, from a servomechanical point of view, is considerably inferior to a shear-force actuator. Furthermore, the large holding force generates a large jerk at the beginning and the end of a stroke and makes it difficult to achieve soft valve landing. An innovative electromechanical valve drive (EMVD) design is proposed, which incorporates a nonlinear mechanical transformer and a shear-force actuator. This allows not only fast but also smooth valve motion, almost zero seating velocity, zero holding power, and improved control with acceptable electric power. This proposed concept is modeled, analyzed, simulated, designed, and implemented. Experimental results show the beneficial features of the promising proposed concept."
}, {
    "id": "oai:dspace.mit.edu:1721.1/8699",
    "title": "Principled computational methods for the validation discovery of genetic regulatory networks",
    "abstract": "As molecular biology continues to evolve in the direction of high-throughput collection of data, it has become increasingly necessary to develop computational methods for analyzing observed data that are at once both sophisticated enough to capture essential features of biological phenomena and at the same time approachable in terms of their application. We demonstrate how graphical models, and Bayesian networks in particular, can be used to model genetic regulatory networks. These methods are well-suited to this problem owing to their ability to model more than pair-wise relationships between variables, their ability to guard against over-fitting, and their robustness in the face of noisy data. Moreover, Bayesian network models can be scored in a principled manner in the presence of both genomic expression and location data. We develop methods for extending Bayesian network semantics to include edge annotations that allow us to model statistical dependencies between biological factors with greater refinement. We derive principled methods for scoring these annotated Bayesian networks. Using these models in the presence of genomic expression data requires suitable methods for the normalization and discretization of this data.",
    "advisors": ["David K. Gifford"],
    "text": "Principled computational methods for the validation discovery of genetic regulatory networks As molecular biology continues to evolve in the direction of high-throughput collection of data, it has become increasingly necessary to develop computational methods for analyzing observed data that are at once both sophisticated enough to capture essential features of biological phenomena and at the same time approachable in terms of their application. We demonstrate how graphical models, and Bayesian networks in particular, can be used to model genetic regulatory networks. These methods are well-suited to this problem owing to their ability to model more than pair-wise relationships between variables, their ability to guard against over-fitting, and their robustness in the face of noisy data. Moreover, Bayesian network models can be scored in a principled manner in the presence of both genomic expression and location data. We develop methods for extending Bayesian network semantics to include edge annotations that allow us to model statistical dependencies between biological factors with greater refinement. We derive principled methods for scoring these annotated Bayesian networks. Using these models in the presence of genomic expression data requires suitable methods for the normalization and discretization of this data."
}, {
    "id": "oai:dspace.mit.edu:1721.1/36185",
    "title": "Improving aggregate user utilities and providing fairness in multi-rate wireless LANs",
    "abstract": "A distributed medium access control (MAC) protocol is responsible for allocating the shared spectrum efficiently and fairly among competing devices using a wireless local area network. Unfortunately, existing MAC protocols, including 802.11's DCF, achieve neither efficiency nor fairness under many realistic conditions. In this dissertation, we show that both bit and frame-based fairness,the most widely used notions, lead to drastically reduced aggregate throughput and increased average delay in typical environments, in which competing nodes transmit at different data transmission rates. We demonstrate the advantages of time-based fairness, in which each competing node receives an equal share of the wireless channel occupancy time. Through analysis, experiments on a Linux test bed, and simulation, we demonstrate that time-based fairness can lead to significant improvements in aggregate throughput and average delay. Through a game theoretic analysis and simulation, we also show that existing MAC protocols encourage non-cooperative nodes to employ globally inefficient transmission strategies that lead to low aggregate throughput. We show that providing long-term time share guarantees among competing nodes leads rational nodes to employ efficient transmission strategies at equilibriums.",
    "advisors": ["John Guttag"],
    "text": "Improving aggregate user utilities and providing fairness in multi-rate wireless LANs A distributed medium access control (MAC) protocol is responsible for allocating the shared spectrum efficiently and fairly among competing devices using a wireless local area network. Unfortunately, existing MAC protocols, including 802.11's DCF, achieve neither efficiency nor fairness under many realistic conditions. In this dissertation, we show that both bit and frame-based fairness,the most widely used notions, lead to drastically reduced aggregate throughput and increased average delay in typical environments, in which competing nodes transmit at different data transmission rates. We demonstrate the advantages of time-based fairness, in which each competing node receives an equal share of the wireless channel occupancy time. Through analysis, experiments on a Linux test bed, and simulation, we demonstrate that time-based fairness can lead to significant improvements in aggregate throughput and average delay. Through a game theoretic analysis and simulation, we also show that existing MAC protocols encourage non-cooperative nodes to employ globally inefficient transmission strategies that lead to low aggregate throughput. We show that providing long-term time share guarantees among competing nodes leads rational nodes to employ efficient transmission strategies at equilibriums."
}, {
    "id": "oai:dspace.mit.edu:1721.1/35596",
    "title": "Circuit-aware system design techniques for wireless communication",
    "abstract": "When designing wireless communication systems, many hardware details are hidden from the algorithm designer, especially with analog hardware. While it is difficult for a designer to understand all aspects of a complex system, some knowledge of circuit constraints can improve system performance by relaxing design constraints. The specifications of a circuit design are generally not equally difficult to meet, allowing excess margin in one area to be used to relax more difficult design constraints. We first propose an uplink/downlink architecture for a network with a multiple antenna central server. This design takes advantage of the central server to allow the nodes to achieve multiplexing gain by forming virtual arrays without coordination, or diversity gain to decrease SNR requirements. Computation and memory are offloaded from the nodes to the server, allowing less complex, inexpensive nodes to be used. We can further use this SNR margin to reduce circuit area and power consumption, sacrificing system capacity for circuit optimization. Besides the more common transmit power reduction, large passive analog components can be removed to reduce chip area, and bias currents lowered to save power at the expense of noise figure. Given the inevitable crosstalk coupling of circuits, we determine the minimum required crosstalk isolation in terms of circuit gain and signal range.",
    "advisors": ["Gregory W. Wornell"],
    "text": "Circuit-aware system design techniques for wireless communication When designing wireless communication systems, many hardware details are hidden from the algorithm designer, especially with analog hardware. While it is difficult for a designer to understand all aspects of a complex system, some knowledge of circuit constraints can improve system performance by relaxing design constraints. The specifications of a circuit design are generally not equally difficult to meet, allowing excess margin in one area to be used to relax more difficult design constraints. We first propose an uplink/downlink architecture for a network with a multiple antenna central server. This design takes advantage of the central server to allow the nodes to achieve multiplexing gain by forming virtual arrays without coordination, or diversity gain to decrease SNR requirements. Computation and memory are offloaded from the nodes to the server, allowing less complex, inexpensive nodes to be used. We can further use this SNR margin to reduce circuit area and power consumption, sacrificing system capacity for circuit optimization. Besides the more common transmit power reduction, large passive analog components can be removed to reduce chip area, and bias currents lowered to save power at the expense of noise figure. Given the inevitable crosstalk coupling of circuits, we determine the minimum required crosstalk isolation in terms of circuit gain and signal range."
}, {
    "id": "oai:dspace.mit.edu:1721.1/30155",
    "title": "Quantum wells on indium gallium arsenic compositionally graded buffers realized by molecular beam epitaxy",
    "abstract": "For a long time, there has been a desire to extend the emission wavelength of GaAs-based quantum well lasers, with the aim of eventually replacing InP with GaAs as the substrate of choice for communication applications. Using dilute nitride GaInAsN QWs or InAs quantum dots, emission wavelengths have successfully been extended to 1.3 m, but significant difficulties have been met going beyond 1.3 m. In this thesis, we present an alternative approach, namely, the molecular beam epitaxy (MBE) growth of quantum wells on top of indium gallium arsenic compositionally graded buffers, with the indium composition in the buffers linearly graded from 0% to 15% or 20%. We observed that one can obtain strong quantum emission on top of such graded buffers only under a very restricted range of growth conditions, detailed in this thesis, which are not compatible with the subsequent growth of the aluminum-containing barriers necessary for carrier confinement. Furthermore, upon proper ex-situ annealing, it was able to obtain QW emission as strong as, sometimes even stronger than, that from QWs pseudomorphically grown on GaAs.However, when even slight tensile or compressive strain was added to the QWs, severe degradation occurred, which was likely related with the amount of surface roughness induced by the crosshatches developed during and after the growth of the graded buffers. Temperature dependent photoluminescence was employed as a tool to investigate the relationship between the ex-situ annealing, strain and quantum well photoluminescence. It was found that there was a significant PL decay mechanism between 50K to about 250K for the aluminum containing unannealed quantum well samples. For the unstrained ones, this mechanism could be removed effectively by annealing. However, strain in quantum well was observed to retard this removal. The same observations were made in both the pseudomorphically and metamorphically grown samples, but the metamorphic ones seemed to suffer more from the retardation.Finally, the theoretical modeling of the photoluminescence temperature dependence was reformulated such that physical processes or band diagram features could be related to the measurement results. Only under restricted circumstances, our formulation was found to be identical to the existing, commonly used, description of the photoluminescence temperature dependence.",
    "advisors": ["Clifton G. Fonstad, Jr"],
    "text": "Quantum wells on indium gallium arsenic compositionally graded buffers realized by molecular beam epitaxy For a long time, there has been a desire to extend the emission wavelength of GaAs-based quantum well lasers, with the aim of eventually replacing InP with GaAs as the substrate of choice for communication applications. Using dilute nitride GaInAsN QWs or InAs quantum dots, emission wavelengths have successfully been extended to 1.3 m, but significant difficulties have been met going beyond 1.3 m. In this thesis, we present an alternative approach, namely, the molecular beam epitaxy (MBE) growth of quantum wells on top of indium gallium arsenic compositionally graded buffers, with the indium composition in the buffers linearly graded from 0% to 15% or 20%. We observed that one can obtain strong quantum emission on top of such graded buffers only under a very restricted range of growth conditions, detailed in this thesis, which are not compatible with the subsequent growth of the aluminum-containing barriers necessary for carrier confinement. Furthermore, upon proper ex-situ annealing, it was able to obtain QW emission as strong as, sometimes even stronger than, that from QWs pseudomorphically grown on GaAs.However, when even slight tensile or compressive strain was added to the QWs, severe degradation occurred, which was likely related with the amount of surface roughness induced by the crosshatches developed during and after the growth of the graded buffers. Temperature dependent photoluminescence was employed as a tool to investigate the relationship between the ex-situ annealing, strain and quantum well photoluminescence. It was found that there was a significant PL decay mechanism between 50K to about 250K for the aluminum containing unannealed quantum well samples. For the unstrained ones, this mechanism could be removed effectively by annealing. However, strain in quantum well was observed to retard this removal. The same observations were made in both the pseudomorphically and metamorphically grown samples, but the metamorphic ones seemed to suffer more from the retardation.Finally, the theoretical modeling of the photoluminescence temperature dependence was reformulated such that physical processes or band diagram features could be related to the measurement results. Only under restricted circumstances, our formulation was found to be identical to the existing, commonly used, description of the photoluminescence temperature dependence."
}, {
    "id": "oai:dspace.mit.edu:1721.1/37913",
    "title": "Games, puzzles, and computation",
    "abstract": "There is a fundamental connection between the notions of game and of computation. At its most basic level, this is implied by any game complexity result, but the connection is deeper than this. One example is the concept of alternating nondeterminism, which is intimately connected with two-player games. In the first half of this thesis, I develop the idea of game as computation to a greater degree than has been done previously. I present a general family of games, called Constraint Logic, which is both mathematically simple and ideally suited for reductions to many actual board games. A deterministic version of Constraint Logic corresponds to a novel kind of logic circuit which is monotone and reversible. At the other end of the spectrum, I show that a multiplayer version of Constraint Logic is undecidable. That there are undecidable games using finite physical resources is philosophically important, and raises issues related to the Church-Turing thesis. In the second half of this thesis, I apply the Constraint Logic formalism to many actual games and puzzles, providing new hardness proofs. These applications include sliding-block puzzles, sliding-coin puzzles, plank puzzles, hinged polygon dissections, Amazons, Kohane, Cross Purposes, Tip over, and others.",
    "advisors": ["Erik D. Demaine", "Gerald J. Sussman"],
    "text": "Games, puzzles, and computation There is a fundamental connection between the notions of game and of computation. At its most basic level, this is implied by any game complexity result, but the connection is deeper than this. One example is the concept of alternating nondeterminism, which is intimately connected with two-player games. In the first half of this thesis, I develop the idea of game as computation to a greater degree than has been done previously. I present a general family of games, called Constraint Logic, which is both mathematically simple and ideally suited for reductions to many actual board games. A deterministic version of Constraint Logic corresponds to a novel kind of logic circuit which is monotone and reversible. At the other end of the spectrum, I show that a multiplayer version of Constraint Logic is undecidable. That there are undecidable games using finite physical resources is philosophically important, and raises issues related to the Church-Turing thesis. In the second half of this thesis, I apply the Constraint Logic formalism to many actual games and puzzles, providing new hardness proofs. These applications include sliding-block puzzles, sliding-coin puzzles, plank puzzles, hinged polygon dissections, Amazons, Kohane, Cross Purposes, Tip over, and others."
}, {
    "id": "oai:dspace.mit.edu:1721.1/101565",
    "title": "Rethinking the application-database interface",
    "abstract": "Applications that interact with database management systems (DBMSs) are ubiquitous in our daily lives. Such database applications are usually hosted on an application server and perform many small accesses over the network to a DBMS hosted on a database server to retrieve data for processing. For decades, the database and programming systems research communities have worked on optimizing such applications from different perspectives: database researchers have built highly efficient DBMSs, and programming systems researchers have developed specialized compilers and runtime systems for hosting applications. However, there has been relative little work that examines the interface between these two software layers to improve application performance. In this thesis, I show how making use of application semantics and optimizing across these layers of the software stack can help us improve the performance of database applications. In particular, I describe three projects that optimize database applications by looking at both the programming system and the DBMS in tandem. By carefully revisiting the interface between the DBMS and the application, and by applying a mix of declarative database optimization and modern program analysis and synthesis techniques, we show that multiple orders of magnitude speedups are possible in real-world applications. I conclude by highlighting future work in the area, and propose a vision towards automatically generating application-specific data stores.",
    "text": "Rethinking the application-database interface Applications that interact with database management systems (DBMSs) are ubiquitous in our daily lives. Such database applications are usually hosted on an application server and perform many small accesses over the network to a DBMS hosted on a database server to retrieve data for processing. For decades, the database and programming systems research communities have worked on optimizing such applications from different perspectives: database researchers have built highly efficient DBMSs, and programming systems researchers have developed specialized compilers and runtime systems for hosting applications. However, there has been relative little work that examines the interface between these two software layers to improve application performance. In this thesis, I show how making use of application semantics and optimizing across these layers of the software stack can help us improve the performance of database applications. In particular, I describe three projects that optimize database applications by looking at both the programming system and the DBMS in tandem. By carefully revisiting the interface between the DBMS and the application, and by applying a mix of declarative database optimization and modern program analysis and synthesis techniques, we show that multiple orders of magnitude speedups are possible in real-world applications. I conclude by highlighting future work in the area, and propose a vision towards automatically generating application-specific data stores."
}, {
    "id": "oai:dspace.mit.edu:1721.1/117838",
    "title": "Architecture design for highly flexible and energy-efficient deep neural network accelerators",
    "abstract": "Deep neural networks (DNNs) are the backbone of modern artificial intelligence (AI). However, due to their high computational complexity and diverse shapes and sizes, dedicated accelerators that can achieve high performance and energy efficiency across a wide range of DNNs are critical for enabling AI in real-world applications. To address this, we present Eyeriss, a co-design of software and hardware architecture for DNN processing that is optimized for performance, energy efficiency and flexibility. Eyeriss features a novel Row-Stationary (RS) dataflow to minimize data movement when processing a DNN, which is the bottleneck of both performance and energy efficiency. The RS dataflow supports highly-parallel processing while fully exploiting data reuse in a multi-level memory hierarchy to optimize for the overall system energy efficiency given any DNN shape and size. It achieves 1.4x to 2.5x higher energy efficiency than other existing dataflows. To support the RS dataflow, we present two versions of the Eyeriss architecture. Eyeriss v1 targets large DNNs that have plenty of data reuse. It features a flexible mapping strategy for high performance and a multicast on-chip network (NoC) for high data reuse, and further exploits data sparsity to reduce processing element (PE) power by 45% and off-chip bandwidth by up to 1.9x. Fabricated in a 65nm CMOS, Eyeriss v1 consumes 278 mW at 34.7 fps for the CONV layers of AlexNet, which is 10x more efficient than a mobile GPU. Eyeriss v2 addresses support for the emerging compact DNNs that introduce higher variation in data reuse. It features a RS+ dataflow that improves PE utilization, and a flexible and scalable NoC that adapts to the bandwidth requirement while also exploiting available data reuse. Together, they provide over 10x higher throughput than Eyeriss v1 at 256 PEs. Eyeriss v2 also exploits sparsity and SIMD for an additional 6x increase in throughput.",
    "advisors": ["Vivienne Sze", "Joel Emer"],
    "text": "Architecture design for highly flexible and energy-efficient deep neural network accelerators Deep neural networks (DNNs) are the backbone of modern artificial intelligence (AI). However, due to their high computational complexity and diverse shapes and sizes, dedicated accelerators that can achieve high performance and energy efficiency across a wide range of DNNs are critical for enabling AI in real-world applications. To address this, we present Eyeriss, a co-design of software and hardware architecture for DNN processing that is optimized for performance, energy efficiency and flexibility. Eyeriss features a novel Row-Stationary (RS) dataflow to minimize data movement when processing a DNN, which is the bottleneck of both performance and energy efficiency. The RS dataflow supports highly-parallel processing while fully exploiting data reuse in a multi-level memory hierarchy to optimize for the overall system energy efficiency given any DNN shape and size. It achieves 1.4x to 2.5x higher energy efficiency than other existing dataflows. To support the RS dataflow, we present two versions of the Eyeriss architecture. Eyeriss v1 targets large DNNs that have plenty of data reuse. It features a flexible mapping strategy for high performance and a multicast on-chip network (NoC) for high data reuse, and further exploits data sparsity to reduce processing element (PE) power by 45% and off-chip bandwidth by up to 1.9x. Fabricated in a 65nm CMOS, Eyeriss v1 consumes 278 mW at 34.7 fps for the CONV layers of AlexNet, which is 10x more efficient than a mobile GPU. Eyeriss v2 addresses support for the emerging compact DNNs that introduce higher variation in data reuse. It features a RS+ dataflow that improves PE utilization, and a flexible and scalable NoC that adapts to the bandwidth requirement while also exploiting available data reuse. Together, they provide over 10x higher throughput than Eyeriss v1 at 256 PEs. Eyeriss v2 also exploits sparsity and SIMD for an additional 6x increase in throughput."
}, {
    "id": "oai:dspace.mit.edu:1721.1/62383",
    "title": "Using rigging and transfer to animate 3D characters",
    "abstract": "Transferring a mesh or skeletal animation onto a new mesh currently requires significant manual effort. For skeletal animations, this involves rigging the character, by specifying how the skeleton is positioned relative to the character and how posing the skeleton drives the character's shape. Currently, artists typically manually position the skeleton joints and paint skinning weights onto the character to associate points on the character surface with bones. For this problem, we present a fully automatic rigging algorithm based on the geometry of the target mesh. Given a generic skeleton, the method computes both joint placement and the character surface attachment automatically. For mesh animations, current techniques are limited to transferring the motion literally using a correspondence between the characters' surfaces. Instead, I propose an example-based method that can transfer motion between far more different characters and that gives the user more control over how to adapt the motion to the new character.",
    "advisors": ["Jovan Popovi"],
    "text": "Using rigging and transfer to animate 3D characters Transferring a mesh or skeletal animation onto a new mesh currently requires significant manual effort. For skeletal animations, this involves rigging the character, by specifying how the skeleton is positioned relative to the character and how posing the skeleton drives the character's shape. Currently, artists typically manually position the skeleton joints and paint skinning weights onto the character to associate points on the character surface with bones. For this problem, we present a fully automatic rigging algorithm based on the geometry of the target mesh. Given a generic skeleton, the method computes both joint placement and the character surface attachment automatically. For mesh animations, current techniques are limited to transferring the motion literally using a correspondence between the characters' surfaces. Instead, I propose an example-based method that can transfer motion between far more different characters and that gives the user more control over how to adapt the motion to the new character."
}, {
    "id": "oai:dspace.mit.edu:1721.1/37918",
    "title": "Thermophotovoltaics : shaping the flow of thermal radiation",
    "abstract": "This thesis explores the modeling, design, and optimization of photonic crystals as spectral control components for high-performance thermophotovoltaic (TPV) power conversion. In particular, we focus on the use of one-dimensional and two dimensional photonic crystals as optical filters and selective thermal emitters for thermophotovoltaic and micro-thermophotovoltaic (micro-TPV)) applications. In addition, we explore fundamental limitations of photonic crystal thermal emitters and provide new insights into the limiting power transfer mechanisms that are relevant for TPV, micro-TPV, lighting and sensor applications. Ideal thermodynamic models that capture dominant power transfer mechanism for TPV and micro-TPV case, are developed and used for the design, optimization and system performance estimation of TPV systems with photonic-crystals. Furthermore, we propose for the first time two new classes of narrow-band thermal emitters that use the resonant cavity effect. The first type of narrow-band thermal emitters rely on vertical-cavity to enhance the thermal emission of highly reflective materials (e.g metals). This class of emitters was named the vertical cavity enhanced resonant thermal emitter (VERTE).",
    "advisors": ["John G. Kassakian"],
    "text": "Thermophotovoltaics : shaping the flow of thermal radiation This thesis explores the modeling, design, and optimization of photonic crystals as spectral control components for high-performance thermophotovoltaic (TPV) power conversion. In particular, we focus on the use of one-dimensional and two dimensional photonic crystals as optical filters and selective thermal emitters for thermophotovoltaic and micro-thermophotovoltaic (micro-TPV)) applications. In addition, we explore fundamental limitations of photonic crystal thermal emitters and provide new insights into the limiting power transfer mechanisms that are relevant for TPV, micro-TPV, lighting and sensor applications. Ideal thermodynamic models that capture dominant power transfer mechanism for TPV and micro-TPV case, are developed and used for the design, optimization and system performance estimation of TPV systems with photonic-crystals. Furthermore, we propose for the first time two new classes of narrow-band thermal emitters that use the resonant cavity effect. The first type of narrow-band thermal emitters rely on vertical-cavity to enhance the thermal emission of highly reflective materials (e.g metals). This class of emitters was named the vertical cavity enhanced resonant thermal emitter (VERTE)."
}, {
    "id": "oai:dspace.mit.edu:1721.1/40497",
    "title": "Channel-adapted quantum error correction",
    "abstract": "Quantum error correction (QEC) is an essential concept for any quantum information processing device. Typically, QEC is designed with minimal assumptions about the noise process; this generic assumption exacts a high cost in efficiency and performance. We examine QEC methods that are adapted to the physical noise model. In physical systems, errors are not likely to be arbitrary; rather we will have reasonable models for the structure of quantum decoherence. We may choose quantum error correcting codes and recovery operations that specifically target the most likely errors. This can increase QEC performance and also reduce the required overhead. We present a convex optimization method to determine the optimal (in terms of average entanglement fidelity) recovery operation for a given channel, encoding, and information source. This is solvable via a semidefinite program (SDP). We derive an analytic solution to the optimal recovery for the case of stabilizer codes, the completely mixed input source, and channels characterized by Pauli group errors. We present computational algorithms to generate near-optimal recovery operations structured to begin with a projective syndrome measurement.",
    "advisors": ["Peter W. Shor", "Moe Z. Win"],
    "text": "Channel-adapted quantum error correction Quantum error correction (QEC) is an essential concept for any quantum information processing device. Typically, QEC is designed with minimal assumptions about the noise process; this generic assumption exacts a high cost in efficiency and performance. We examine QEC methods that are adapted to the physical noise model. In physical systems, errors are not likely to be arbitrary; rather we will have reasonable models for the structure of quantum decoherence. We may choose quantum error correcting codes and recovery operations that specifically target the most likely errors. This can increase QEC performance and also reduce the required overhead. We present a convex optimization method to determine the optimal (in terms of average entanglement fidelity) recovery operation for a given channel, encoding, and information source. This is solvable via a semidefinite program (SDP). We derive an analytic solution to the optimal recovery for the case of stabilizer codes, the completely mixed input source, and channels characterized by Pauli group errors. We present computational algorithms to generate near-optimal recovery operations structured to begin with a projective syndrome measurement."
}, {
    "id": "oai:dspace.mit.edu:1721.1/8698",
    "title": "Exploring scatterer anisotrophy in synthetic aperture radar via sub-aperture analysis",
    "abstract": "Scattering from man-made objects in SAR imagery exhibits aspect and frequency dependencies which are not always well modeled by standard SAR imaging techniques based on the ideal point scattering model. This is particularly the case for highresolution wide-band and wide-aperture data where model deviations are even more pronounced. If ignored, these deviations will reduce recognition performance due to the model mismatch, but when appropriately accounted for, these deviations from the ideal point scattering model can be exploited as attributes to better distinguish scatterers and their respective targets. With this in mind, this thesis develops an efficient modeling framework based on a sub-aperture pyramid to utilize scatterer anisotropy for the purpose of target classification. Two approaches are presented to exploit scatterer anisotropy using the sub-aperture pyramid. The first is a nonparametric classifier that learns the azimuthal dependencies within an image and makes a classification decision based on the learned dependencies. The second approach is a parametric attribution of the observed anisotropy characterizing the azimuthal location and concentration of the scattering response. Working from the sub-aperture scattering model, we develop a hypothesis test to characterize anisotropy. We start with an isolated scatterer model which produces a test with an intuitive interpretation. We then address the problem of robustness to interfering scatterers by extending the model to account for neighboring scatterers which corrupt the anisotropy attribution.",
    "advisors": ["Alan S. Willsky"],
    "text": "Exploring scatterer anisotrophy in synthetic aperture radar via sub-aperture analysis Scattering from man-made objects in SAR imagery exhibits aspect and frequency dependencies which are not always well modeled by standard SAR imaging techniques based on the ideal point scattering model. This is particularly the case for highresolution wide-band and wide-aperture data where model deviations are even more pronounced. If ignored, these deviations will reduce recognition performance due to the model mismatch, but when appropriately accounted for, these deviations from the ideal point scattering model can be exploited as attributes to better distinguish scatterers and their respective targets. With this in mind, this thesis develops an efficient modeling framework based on a sub-aperture pyramid to utilize scatterer anisotropy for the purpose of target classification. Two approaches are presented to exploit scatterer anisotropy using the sub-aperture pyramid. The first is a nonparametric classifier that learns the azimuthal dependencies within an image and makes a classification decision based on the learned dependencies. The second approach is a parametric attribution of the observed anisotropy characterizing the azimuthal location and concentration of the scattering response. Working from the sub-aperture scattering model, we develop a hypothesis test to characterize anisotropy. We start with an isolated scatterer model which produces a test with an intuitive interpretation. We then address the problem of robustness to interfering scatterers by extending the model to account for neighboring scatterers which corrupt the anisotropy attribution."
}, {
    "id": "oai:dspace.mit.edu:1721.1/66004",
    "title": "Integration of photonic and passive microfluidic devices into lab-on-chip with femtosecond laser materials processing",
    "abstract": "Femtosecond laser materials processing is a powerful method for the integration of high resolution, 3D structures into Lab-On-Chip (LOC) systems. One major application of femtosecond laser materials processing is waveguide fabrication in glass via index modification. We demonstrate the ability to fabricate couplers and Mach-Zehnder Interferometers (MZI) with good repeatability and flexibility. An in-depth characterization of the spectral characteristics of symmetric directional couplers and MZI is presented. The spectral data from a series of unbalanced MZI is used to characterize changes in the waveguide propagation constant. Towards integrated sensing, we demonstrate the application of femtosecond laser waveguide fabrication to the integration of a MZI into a homemade and commercial LOC for label-free optical detection. The MZI has a unique tilted 3D geometry with one arm crossing a microfluidic channel and enables spatially resolved sensing of changes in the refractive index of the content inside the channel with a limit of detection as low as 1x10 4 RIU. Another major technique in femtosecond laser materials processing is femtosecond two-photon polymerization (TPP). TPP is used to integrate 3D porous filters into a commercial LOC and testing of the filter shows virtually 100% efficient separation of 3 tm polystyrene spheres from a liquid solution. The direct write and maskless nature of femtosecond materials processing makes it a powerful method to integrate 3D devices into LOC without altering existing elements or changing the microfluidic channel fabrication.",
    "advisors": ["James G. Fujimoto"],
    "text": "Integration of photonic and passive microfluidic devices into lab-on-chip with femtosecond laser materials processing Femtosecond laser materials processing is a powerful method for the integration of high resolution, 3D structures into Lab-On-Chip (LOC) systems. One major application of femtosecond laser materials processing is waveguide fabrication in glass via index modification. We demonstrate the ability to fabricate couplers and Mach-Zehnder Interferometers (MZI) with good repeatability and flexibility. An in-depth characterization of the spectral characteristics of symmetric directional couplers and MZI is presented. The spectral data from a series of unbalanced MZI is used to characterize changes in the waveguide propagation constant. Towards integrated sensing, we demonstrate the application of femtosecond laser waveguide fabrication to the integration of a MZI into a homemade and commercial LOC for label-free optical detection. The MZI has a unique tilted 3D geometry with one arm crossing a microfluidic channel and enables spatially resolved sensing of changes in the refractive index of the content inside the channel with a limit of detection as low as 1x10 4 RIU. Another major technique in femtosecond laser materials processing is femtosecond two-photon polymerization (TPP). TPP is used to integrate 3D porous filters into a commercial LOC and testing of the filter shows virtually 100% efficient separation of 3 tm polystyrene spheres from a liquid solution. The direct write and maskless nature of femtosecond materials processing makes it a powerful method to integrate 3D devices into LOC without altering existing elements or changing the microfluidic channel fabrication."
}, {
    "id": "oai:dspace.mit.edu:1721.1/105955",
    "title": "Scalable algorithms for semi-automatic segmentation of electron microscopy images of the brain tissue",
    "abstract": "I present a set of fast and scalable algorithms for segmenting very large 3D images of brain tissue. Currently, light and electron microscopy can now produce terascale 3D images within hours. Extracting the information about the shapes and connectivity of the neurons require fast and accurate image segmentation algorithms. Due to the sheer size of the problem, traditional approaches might be computationally infeasible. I focus on an segmentation pipeline that breaks up the segmentation problem into multiple stages, each of which can be improved independently. In the first step of the pipeline, convolutional neural networks are used to predict segment boundaries. Watershed transform is then used to obtain an over-segmentation, which is then reduced using agglomerative clustering algorithms. Finally, manual or computer-assisted proof reading is done by experts. In this thesis, I revisit the traditional approaches for training and applying convolutional neural networks, and propose: - A fast and scalable 3D convolutional network training algorithm suited for multi-core and many-core shared memory machines. The two main quantities of the algorithm are: (1) minimizing the required computation by using FFT-based convolution with memoization, and (2) parallelization approach that can utilize large number of CPUs while minimizing any required synchronization. - A high throughput inference algorithm that can utilize all available computational resources, CPUs and GPUs. I introduce a set of highly parallel algorithms for different layer types and architectures, and show how to combine them to achieve very high throughput. Additionally, I study the theoretical properties of the watershed transform of edge- weighed graphs and propose a liner-time algorithm. I propose a set of modification to the standard algorithm and a quasi-linear agglomerative clustering algorithm that can greatly reduce the over-segmentation produced by the standard watershed algorithm.",
    "advisors": ["H. Sebastian Seung, Frdo Durand", "Nir Shavit"],
    "text": "Scalable algorithms for semi-automatic segmentation of electron microscopy images of the brain tissue I present a set of fast and scalable algorithms for segmenting very large 3D images of brain tissue. Currently, light and electron microscopy can now produce terascale 3D images within hours. Extracting the information about the shapes and connectivity of the neurons require fast and accurate image segmentation algorithms. Due to the sheer size of the problem, traditional approaches might be computationally infeasible. I focus on an segmentation pipeline that breaks up the segmentation problem into multiple stages, each of which can be improved independently. In the first step of the pipeline, convolutional neural networks are used to predict segment boundaries. Watershed transform is then used to obtain an over-segmentation, which is then reduced using agglomerative clustering algorithms. Finally, manual or computer-assisted proof reading is done by experts. In this thesis, I revisit the traditional approaches for training and applying convolutional neural networks, and propose: - A fast and scalable 3D convolutional network training algorithm suited for multi-core and many-core shared memory machines. The two main quantities of the algorithm are: (1) minimizing the required computation by using FFT-based convolution with memoization, and (2) parallelization approach that can utilize large number of CPUs while minimizing any required synchronization. - A high throughput inference algorithm that can utilize all available computational resources, CPUs and GPUs. I introduce a set of highly parallel algorithms for different layer types and architectures, and show how to combine them to achieve very high throughput. Additionally, I study the theoretical properties of the watershed transform of edge- weighed graphs and propose a liner-time algorithm. I propose a set of modification to the standard algorithm and a quasi-linear agglomerative clustering algorithm that can greatly reduce the over-segmentation produced by the standard watershed algorithm."
}, {
    "id": "oai:dspace.mit.edu:1721.1/44903",
    "title": "Quantitative information-flow tracking for real systems",
    "abstract": "An information-flow security policy constrains a computer system's end-to-end use of information, even as it is transformed in computation. For instance, a policy would not just restrict what secret data could be revealed directly, but restrict any output that might allow inferences about the secret. Expressing such a policy quantitatively, in terms of a specific number of bits of information, is often an effective program independent way of distinguishing what scenarios should be allowed and disallowed. This thesis describes a family of new techniques for measuring how much information about a program's secret inputs is revealed by its public outputs on a particular execution, in order to check a quantitative policy on realistic systems. Our approach builds on dynamic tainting, tracking at runtime which bits might contain secret in formation, and also uses static control-flow regions to soundly account for implicit flows via branches and pointer operations. We introduce a new graph model that bounds information flow by the maximum flow between inputs and outputs in a flow network representation of an execution. The flow bounds obtained with maximum flow are much more precise than those based on tainting alone (which is equivalent to graph reachability). The bounds are a conservative estimate of channel capacity: the amount of information that could be transmitted by an adversary making an arbitrary choice of secret inputs. We describe an implementation named Flowcheck, built using the Valgrind framework for x86/Linux binaries, and use it to perform case studies on six real C, C++, and Objective C programs, three of which have more than 250,000 lines of code. We used the tool to check the confidentiality of a different kind of information appropriate to each program. Its results either verified that the information was appropriately kept secret on the examined executions, or revealed unacceptable leaks, in one case due to a previously unknown bug.",
    "advisors": ["Michael D. Ernst"],
    "text": "Quantitative information-flow tracking for real systems An information-flow security policy constrains a computer system's end-to-end use of information, even as it is transformed in computation. For instance, a policy would not just restrict what secret data could be revealed directly, but restrict any output that might allow inferences about the secret. Expressing such a policy quantitatively, in terms of a specific number of bits of information, is often an effective program independent way of distinguishing what scenarios should be allowed and disallowed. This thesis describes a family of new techniques for measuring how much information about a program's secret inputs is revealed by its public outputs on a particular execution, in order to check a quantitative policy on realistic systems. Our approach builds on dynamic tainting, tracking at runtime which bits might contain secret in formation, and also uses static control-flow regions to soundly account for implicit flows via branches and pointer operations. We introduce a new graph model that bounds information flow by the maximum flow between inputs and outputs in a flow network representation of an execution. The flow bounds obtained with maximum flow are much more precise than those based on tainting alone (which is equivalent to graph reachability). The bounds are a conservative estimate of channel capacity: the amount of information that could be transmitted by an adversary making an arbitrary choice of secret inputs. We describe an implementation named Flowcheck, built using the Valgrind framework for x86/Linux binaries, and use it to perform case studies on six real C, C++, and Objective C programs, three of which have more than 250,000 lines of code. We used the tool to check the confidentiality of a different kind of information appropriate to each program. Its results either verified that the information was appropriately kept secret on the examined executions, or revealed unacceptable leaks, in one case due to a previously unknown bug."
}, {
    "id": "oai:dspace.mit.edu:1721.1/8117",
    "title": "Circuit design and technological limitations of silicon RFICs for wireless applications",
    "abstract": "Semiconductor technologies have been a key to the growth in wireless communication over the past decade, bringing added convenience and accessibility through advantages in cost, size, and power dissipation. A better understanding of how an IC technology affects critical RF signal chain components will greatly aid the design of wireless systems and the development of process technologies for the increasingly complex applications that lie on the horizon. Many of the evolving applications will embody the concept of adaptive performance to extract the maximum capability from the RF link in terms of bandwidth, dynamic range, and power consumption-further engaging the interplay of circuits and devices is this design space and making it even more difficult to discern a clear guide upon which to base technology decisions. Rooted in these observations, this research focuses on two key themes: 1) devising methods of implementing RF circuits which allow the performance to be dynamically tuned to match real-time conditions in a power-efficient manner, and 2) refining approaches for thinking about the optimization of RF circuits at the device level. Working toward a 5.8 GHz receiver consistent with 1 GBit/s operation, signal path topologies and adjustable biasing circuits are developed for low-noise amplifiers (LNAs) and voltage-controlled oscillators (VCOs) to provide a facility by which power can be conserved when the demand for sensitivity is low. As an integral component in this effort, tools for exploring device level issues are illustrated with both circuit types, helping to identify physical limitations and design techniques through which they can be mitigated.",
    "advisors": ["Charles G. Sodini"],
    "text": "Circuit design and technological limitations of silicon RFICs for wireless applications Semiconductor technologies have been a key to the growth in wireless communication over the past decade, bringing added convenience and accessibility through advantages in cost, size, and power dissipation. A better understanding of how an IC technology affects critical RF signal chain components will greatly aid the design of wireless systems and the development of process technologies for the increasingly complex applications that lie on the horizon. Many of the evolving applications will embody the concept of adaptive performance to extract the maximum capability from the RF link in terms of bandwidth, dynamic range, and power consumption-further engaging the interplay of circuits and devices is this design space and making it even more difficult to discern a clear guide upon which to base technology decisions. Rooted in these observations, this research focuses on two key themes: 1) devising methods of implementing RF circuits which allow the performance to be dynamically tuned to match real-time conditions in a power-efficient manner, and 2) refining approaches for thinking about the optimization of RF circuits at the device level. Working toward a 5.8 GHz receiver consistent with 1 GBit/s operation, signal path topologies and adjustable biasing circuits are developed for low-noise amplifiers (LNAs) and voltage-controlled oscillators (VCOs) to provide a facility by which power can be conserved when the demand for sensitivity is low. As an integral component in this effort, tools for exploring device level issues are illustrated with both circuit types, helping to identify physical limitations and design techniques through which they can be mitigated."
}, {
    "id": "oai:dspace.mit.edu:1721.1/62424",
    "title": "Transforms for prediction residuals in video coding",
    "abstract": "Typically the same transform, the 2-D Discrete Cosine Transform (DCT), is used to compress both image intensities in image coding and prediction residuals in video coding. Major prediction residuals include the motion compensated prediction residual, the resolution enhancement residual in scalable video coding, and the intra prediction residual in intra-frame coding. The 2-D DCT is efficient at decorrelating images, but the spatial characteristics of prediction residuals can be significantly different from the spatial characteristics of images, and developing transforms that are adapted to the characteristics of prediction residuals can improve their compression efficiency. In this thesis, we explore the differences between the characteristics of images and prediction residuals by analyzing their local anisotropic characteristics and develop transforms adapted to the local anisotropic characteristics of some types of prediction residuals. The analysis shows that local regions in images have 2-D anisotropic characteristics and many regions in several types of prediction residuals have 1-D anisotropic characteristics. Based on this insight, we develop 1-D transforms for these residuals. We perform experiments to evaluate the potential gains achievable from using these transforms within the H.264 codec, and the experimental results indicate that these transforms can increase the compression efficiency of these residuals.",
    "advisors": ["Jae S. Lim"],
    "text": "Transforms for prediction residuals in video coding Typically the same transform, the 2-D Discrete Cosine Transform (DCT), is used to compress both image intensities in image coding and prediction residuals in video coding. Major prediction residuals include the motion compensated prediction residual, the resolution enhancement residual in scalable video coding, and the intra prediction residual in intra-frame coding. The 2-D DCT is efficient at decorrelating images, but the spatial characteristics of prediction residuals can be significantly different from the spatial characteristics of images, and developing transforms that are adapted to the characteristics of prediction residuals can improve their compression efficiency. In this thesis, we explore the differences between the characteristics of images and prediction residuals by analyzing their local anisotropic characteristics and develop transforms adapted to the local anisotropic characteristics of some types of prediction residuals. The analysis shows that local regions in images have 2-D anisotropic characteristics and many regions in several types of prediction residuals have 1-D anisotropic characteristics. Based on this insight, we develop 1-D transforms for these residuals. We perform experiments to evaluate the potential gains achievable from using these transforms within the H.264 codec, and the experimental results indicate that these transforms can increase the compression efficiency of these residuals."
}, {
    "id": "oai:dspace.mit.edu:1721.1/66469",
    "title": "Coincidence detection in the cochlear nucleus : implications for the coding of pitch",
    "abstract": "The spatio-temporal pattern in the auditory nerve (AN), i.e. the temporal pattern of AN fiber activity across the tonotopic axis, provides cues to important features in sounds such as pitch, loudness, and spatial location. These spatio-temporal cues may be extracted by central neurons in the cochlear nucleus (CN) that receive inputs from AN fibers innervating different cochlear regions and are sensitive to their relative timing. One possible mechanism for this extraction is cross-frequency coincidence detection (CD), in which a central neuron converts the degree of cross-frequency coincidence in the AN into a rate response by preferentially firing when its AN inputs across the tonotopic axis discharge in synchrony. We implemented a CD model receiving AN inputs from varying extents of the tonotopic axis, and compared responses of model CD cells with those of single units recorded in the CN of the anesthetized cat. We used Huffman stimuli, which have flat magnitude spectra and a single phase transition, to systematically manipulate the relative timing across AN fibers and to evaluate the sensitivity of model CD cells and CN units to the spatiotemporal pattern of AN discharges. Using a maximum likelihood approach, we found that certain unit types (primary-like-with-notch and some phase lockers) had responses consistent with cross-frequency CD cell. Some of these CN units provide input to neurons in a binaural circuit that process cues for sound localization and are sensitive to interaural level differences. A possible functional role of a cross-frequency CD mechanism in the CN is to increase the dynamic range of these binaural neurons. However, many other CN units had responses more consistent with AN fibers than with CD cells. We hypothesized that CN units resembling cross-frequency CD cells (as determined by their responses to Huffman stimuli) would convert spatio-temporal cues to pitch in the AN into rate cues that are robust with level. We found that, in response to harmonic complex tones, cross-frequency CD cells and some CN units (primary-like-with-notch and choppers) maintained robust rate cues at high levels compared to AN fibers, suggesting that at least some CN neurons extend the dynamic range of rate representations of pitch beyond that found in AN fibers. However, there was no obvious correlation between robust rate cues in individual CN units and similarity to cross-frequency CD cells as determined by responses to Huffman stimuli. It is likely that a model including more realistic inputs, membrane channels, and spiking mechanism, or other mechanisms such as lateral inhibition or spatial and temporal summation over spatially distributed inputs would provide insight into the neural mechanisms that give rise to the robust rate cues observed in some CN units.",
    "advisors": ["Bertrand Delgutte"],
    "text": "Coincidence detection in the cochlear nucleus : implications for the coding of pitch The spatio-temporal pattern in the auditory nerve (AN), i.e. the temporal pattern of AN fiber activity across the tonotopic axis, provides cues to important features in sounds such as pitch, loudness, and spatial location. These spatio-temporal cues may be extracted by central neurons in the cochlear nucleus (CN) that receive inputs from AN fibers innervating different cochlear regions and are sensitive to their relative timing. One possible mechanism for this extraction is cross-frequency coincidence detection (CD), in which a central neuron converts the degree of cross-frequency coincidence in the AN into a rate response by preferentially firing when its AN inputs across the tonotopic axis discharge in synchrony. We implemented a CD model receiving AN inputs from varying extents of the tonotopic axis, and compared responses of model CD cells with those of single units recorded in the CN of the anesthetized cat. We used Huffman stimuli, which have flat magnitude spectra and a single phase transition, to systematically manipulate the relative timing across AN fibers and to evaluate the sensitivity of model CD cells and CN units to the spatiotemporal pattern of AN discharges. Using a maximum likelihood approach, we found that certain unit types (primary-like-with-notch and some phase lockers) had responses consistent with cross-frequency CD cell. Some of these CN units provide input to neurons in a binaural circuit that process cues for sound localization and are sensitive to interaural level differences. A possible functional role of a cross-frequency CD mechanism in the CN is to increase the dynamic range of these binaural neurons. However, many other CN units had responses more consistent with AN fibers than with CD cells. We hypothesized that CN units resembling cross-frequency CD cells (as determined by their responses to Huffman stimuli) would convert spatio-temporal cues to pitch in the AN into rate cues that are robust with level. We found that, in response to harmonic complex tones, cross-frequency CD cells and some CN units (primary-like-with-notch and choppers) maintained robust rate cues at high levels compared to AN fibers, suggesting that at least some CN neurons extend the dynamic range of rate representations of pitch beyond that found in AN fibers. However, there was no obvious correlation between robust rate cues in individual CN units and similarity to cross-frequency CD cells as determined by responses to Huffman stimuli. It is likely that a model including more realistic inputs, membrane channels, and spiking mechanism, or other mechanisms such as lateral inhibition or spatial and temporal summation over spatially distributed inputs would provide insight into the neural mechanisms that give rise to the robust rate cues observed in some CN units."
}, {
    "id": "oai:dspace.mit.edu:1721.1/35286",
    "title": "Low-complexity approaches to distributed data dissemination",
    "abstract": "In this thesis we consider practical ways of disseminating information from multiple senders to multiple receivers in an optimal or provably close-to-optimal fashion. The basis for our discussion of optimal transmission of information is mostly information theoretic - but the methods that we apply to do so in a low-complexity fashion draw from a number of different engineering disciplines. The three canonical multiple-input, multiple-output problems we focus our attention upon are: * The Slepian-Wolf problem where multiple correlated sources must be distributedly compressed and recovered with a common receiver. * The discrete memoryless multiple access problem where multiple senders communicate across a common channel to a single receiver. * The deterministic broadcast channel problem where multiple messages are sent from a common sender to multiple receivers through a deterministic medium. Chapter 1 serves as an introduction and provides models, definitions, and a discussion of barriers between theory and practice for the three canonical data dissemination problems we will discuss. Here we also discuss how these three problems are all in different senses 'dual' to each other, and use this as a motivating force to attack them with unifying themes.",
    "advisors": ["Muriel Mdard"],
    "text": "Low-complexity approaches to distributed data dissemination In this thesis we consider practical ways of disseminating information from multiple senders to multiple receivers in an optimal or provably close-to-optimal fashion. The basis for our discussion of optimal transmission of information is mostly information theoretic - but the methods that we apply to do so in a low-complexity fashion draw from a number of different engineering disciplines. The three canonical multiple-input, multiple-output problems we focus our attention upon are: * The Slepian-Wolf problem where multiple correlated sources must be distributedly compressed and recovered with a common receiver. * The discrete memoryless multiple access problem where multiple senders communicate across a common channel to a single receiver. * The deterministic broadcast channel problem where multiple messages are sent from a common sender to multiple receivers through a deterministic medium. Chapter 1 serves as an introduction and provides models, definitions, and a discussion of barriers between theory and practice for the three canonical data dissemination problems we will discuss. Here we also discuss how these three problems are all in different senses 'dual' to each other, and use this as a motivating force to attack them with unifying themes."
}, {
    "id": "oai:dspace.mit.edu:1721.1/45880",
    "title": "The paradigm of partial erasures",
    "abstract": "This thesis is a study of erasures in cryptographic protocols. Erasing old data and keys is an important capability of honest parties in cryptographic protocols. It is useful in many settings, including proactive security in the presence of a mobile adversary, adaptive security in the presence of an adaptive adversary, forward security, and intrusion resilience. Some of these settings, such as achieving proactive security, is provably impossible without some form of erasures. Other settings, such as designing protocols that are secure against adaptive adversaries, are much simpler to achieve when erasures are allowed. Protocols for all these contexts typically assume the ability to perfectly erase information. Unfortunately, as amply demonstrated in the systems literature, perfect erasures are hard to implement in practice. We propose a model of imperfect or partial erasures where erasure instructions are only partially effective and leave almost all the data intact, thus giving the honest parties only a limited capability to dispose old data. Nonetheless, we show how to design protocols for all of the above settings (including proactive security, adaptive security, forward security, and intrusion resilience) for which this weak form of erasures suffices. We do not have to invent entirely new protocols, but rather show how to automatically modify protocols relying on perfect erasures into ones for which partial erasures suffices. Stated most generally, we provide a compiler that transforms any protocol relying on perfect erasures for security into one with the same functionality that remains secure even if the erasures are only partial. The key idea is a new redundant representation of secret data which can still be computed on, and yet is rendered useless when partially erased. We prove that any such compiler must incur a cost in additional storage, and that our compiler is near optimal in terms of its storage overhead. We also give computationally more efficient compilers for a number of special cases: (1) when all the computations on secrets can be done in constant parallel time (NC); (2) for a class of proactive secret sharing protocols where we leave the protocol intact except for changing the representation of the shares of the secret and the instructions that modify the shares (to correspondingly modify the new representation instead).",
    "advisors": ["Shafi Goldwasser", "Ran Canetti"],
    "text": "The paradigm of partial erasures This thesis is a study of erasures in cryptographic protocols. Erasing old data and keys is an important capability of honest parties in cryptographic protocols. It is useful in many settings, including proactive security in the presence of a mobile adversary, adaptive security in the presence of an adaptive adversary, forward security, and intrusion resilience. Some of these settings, such as achieving proactive security, is provably impossible without some form of erasures. Other settings, such as designing protocols that are secure against adaptive adversaries, are much simpler to achieve when erasures are allowed. Protocols for all these contexts typically assume the ability to perfectly erase information. Unfortunately, as amply demonstrated in the systems literature, perfect erasures are hard to implement in practice. We propose a model of imperfect or partial erasures where erasure instructions are only partially effective and leave almost all the data intact, thus giving the honest parties only a limited capability to dispose old data. Nonetheless, we show how to design protocols for all of the above settings (including proactive security, adaptive security, forward security, and intrusion resilience) for which this weak form of erasures suffices. We do not have to invent entirely new protocols, but rather show how to automatically modify protocols relying on perfect erasures into ones for which partial erasures suffices. Stated most generally, we provide a compiler that transforms any protocol relying on perfect erasures for security into one with the same functionality that remains secure even if the erasures are only partial. The key idea is a new redundant representation of secret data which can still be computed on, and yet is rendered useless when partially erased. We prove that any such compiler must incur a cost in additional storage, and that our compiler is near optimal in terms of its storage overhead. We also give computationally more efficient compilers for a number of special cases: (1) when all the computations on secrets can be done in constant parallel time (NC); (2) for a class of proactive secret sharing protocols where we leave the protocol intact except for changing the representation of the shares of the secret and the instructions that modify the shares (to correspondingly modify the new representation instead)."
}, {
    "id": "oai:dspace.mit.edu:1721.1/38690",
    "title": "On the synthesis of switched output feedback controllers for linear, time-invariant systems",
    "abstract": "The theory of switching systems has seen many advances in the past decade. Its beginnings were founded primarily due to the physical limitations in devices to implement control such as relays, but today there exists a strong interest in the development of switching systems where switching is introduced as a means of increasing performance. With the newer set of problems that arise from this viewpoint comes the need for many new tools for analysis and design. Analysis tools which include, for instance, the celebrated work on multiple Lyapunov functions are extensive. Tools for the design of switched systems also exist, but, in many cases, the method of designing stabilizing switching laws is often a separate process from the method which is used to determine the set of vector fields between which switching takes place. For instance, one typical method of designing switching controllers for linear, time-invariant (LTI) systems is to first design a set of stabilizing LTI controllers using standard LTI methods, and then design a switching law to increase performance. While such design algorithms can lead to increases in performance, they often impose restrictions that do not allow the designer to take full advantage of the switching architecture being considered.",
    "advisors": ["Munther A. Dahleh"],
    "text": "On the synthesis of switched output feedback controllers for linear, time-invariant systems The theory of switching systems has seen many advances in the past decade. Its beginnings were founded primarily due to the physical limitations in devices to implement control such as relays, but today there exists a strong interest in the development of switching systems where switching is introduced as a means of increasing performance. With the newer set of problems that arise from this viewpoint comes the need for many new tools for analysis and design. Analysis tools which include, for instance, the celebrated work on multiple Lyapunov functions are extensive. Tools for the design of switched systems also exist, but, in many cases, the method of designing stabilizing switching laws is often a separate process from the method which is used to determine the set of vector fields between which switching takes place. For instance, one typical method of designing switching controllers for linear, time-invariant (LTI) systems is to first design a set of stabilizing LTI controllers using standard LTI methods, and then design a switching law to increase performance. While such design algorithms can lead to increases in performance, they often impose restrictions that do not allow the designer to take full advantage of the switching architecture being considered."
}, {
    "id": "oai:dspace.mit.edu:1721.1/16896",
    "title": "Multiplexing, scheduling, and multicasting strategies for antenna arrays in wireless networks",
    "abstract": "A transmitter antenna array has the ability to direct data simultaneously to multiple receivers within a wireless network, creating potential for a more integrated view of algorithmic system components. In this thesis, such a perspective informs the design of two system tasks: the scheduling of packets from a number of data streams into groups; and the subsequent spatial multiplexing and encoding of these groups using array processing. We demonstrate how good system designs can help these two tasks reinforce one another, or alternatively enable tradeoffs in complexity between the two. Moreover, scheduling and array processing each benefit from a further awareness of both the fading channel state and certain properties of the data, providing information about key flexibilities, constraints and goals. Our development focuses on techniques that lead to high performance even with very low-complexity receivers. We first consider spatial precoding under simple scheduling and propose several extensions for implementation, such as a unified time-domain precoder that compensates for both cross-channel and intersymbol interfer- ence. We then show how more sophisticated, channel-aware scheduling can reduce the complexity requirements of the array processing. The scheduling algorithms presented are based on the receivers' fading channel realizations and the delay tolerances of the data streams. Finally, we address the multicasting of common data streams in terms of opportunities for reduced redundancy as well as the conflicting objectives inherent in sending to multiple receivers. Our channel-aware extensions of space-time codes for multicasting gain several dB over traditional versions that do not incorporate channel knowledge.",
    "advisors": ["George W. Wornell"],
    "text": "Multiplexing, scheduling, and multicasting strategies for antenna arrays in wireless networks A transmitter antenna array has the ability to direct data simultaneously to multiple receivers within a wireless network, creating potential for a more integrated view of algorithmic system components. In this thesis, such a perspective informs the design of two system tasks: the scheduling of packets from a number of data streams into groups; and the subsequent spatial multiplexing and encoding of these groups using array processing. We demonstrate how good system designs can help these two tasks reinforce one another, or alternatively enable tradeoffs in complexity between the two. Moreover, scheduling and array processing each benefit from a further awareness of both the fading channel state and certain properties of the data, providing information about key flexibilities, constraints and goals. Our development focuses on techniques that lead to high performance even with very low-complexity receivers. We first consider spatial precoding under simple scheduling and propose several extensions for implementation, such as a unified time-domain precoder that compensates for both cross-channel and intersymbol interfer- ence. We then show how more sophisticated, channel-aware scheduling can reduce the complexity requirements of the array processing. The scheduling algorithms presented are based on the receivers' fading channel realizations and the delay tolerances of the data streams. Finally, we address the multicasting of common data streams in terms of opportunities for reduced redundancy as well as the conflicting objectives inherent in sending to multiple receivers. Our channel-aware extensions of space-time codes for multicasting gain several dB over traditional versions that do not incorporate channel knowledge."
}, {
    "id": "oai:dspace.mit.edu:1721.1/17479",
    "title": "The design and testing of integrated circuits for submillimeter-wave spectroscopy",
    "abstract": "Optoelectronic techniques have extended the bandwidth of electronic spectroscopic systems to the submillimeter wavelengths. In a significant class of these systems the submillimeter-wave source, detector and device of interest are monolithically integrated. Such systems are attractive because of their reliability and small size and cost, because an integrated circuit is the highest-bandwidth environment for testing microelectronic devices, and because of their potential application to on-chip chemical and biological sensing. This thesis focuses on three separate topics in the field of submillimeter-wave spectroscopy with integrated circuits. The first topic is the decrease in bandwidth of photoconductive submillimeter wave emitters with increasing voltage bias, which limits the output power of these devices at frequencies near 1 THz. We performed measurements of a photoconductor made of low-temperature grown GaAs embedded in a coplanar waveguide with both static and dynamic illumination. We investigated the bandwidth decrease and an increase in de photocurrent that occurs at the same bias voltages. We attribute both phenomena to a reduction of the electron capture cross section of donor states due to electron heating and Coulomb-barrier lowering. The second topic is a novel circuit for ultrafast measurements with coplanar waveguide transmission lines. The circuit contains photoconductive switches that allow tunable generation and reception of a coplanar waveguide's two propagating modes. The circuit has fewer discontinuities than other circuits with similar capabilities and does not require air bridges. We show how the photoconductive switch can be biased to compensate for pump laser beam misalignment. The third topic is the first demonstration of an integrated circuit's use for submillimeter- wave frequency-domain spectroscopy. Such an application is attractive because of its inherently good frequency resolution, which is necessary for chemical and biological detection. The amplitude and phase of the measured spectrum of a circuit without a device under test agree with a model that takes into account circuit resonance, photoconductive-switch dynamics, and resistive loss. We discuss why photoconductive frequency-domain spectroscopy has an inherently lower output signal than similar time-domain spectroscopy, and how this drawback can be compensated for.",
    "advisors": ["Qing Hu"],
    "text": "The design and testing of integrated circuits for submillimeter-wave spectroscopy Optoelectronic techniques have extended the bandwidth of electronic spectroscopic systems to the submillimeter wavelengths. In a significant class of these systems the submillimeter-wave source, detector and device of interest are monolithically integrated. Such systems are attractive because of their reliability and small size and cost, because an integrated circuit is the highest-bandwidth environment for testing microelectronic devices, and because of their potential application to on-chip chemical and biological sensing. This thesis focuses on three separate topics in the field of submillimeter-wave spectroscopy with integrated circuits. The first topic is the decrease in bandwidth of photoconductive submillimeter wave emitters with increasing voltage bias, which limits the output power of these devices at frequencies near 1 THz. We performed measurements of a photoconductor made of low-temperature grown GaAs embedded in a coplanar waveguide with both static and dynamic illumination. We investigated the bandwidth decrease and an increase in de photocurrent that occurs at the same bias voltages. We attribute both phenomena to a reduction of the electron capture cross section of donor states due to electron heating and Coulomb-barrier lowering. The second topic is a novel circuit for ultrafast measurements with coplanar waveguide transmission lines. The circuit contains photoconductive switches that allow tunable generation and reception of a coplanar waveguide's two propagating modes. The circuit has fewer discontinuities than other circuits with similar capabilities and does not require air bridges. We show how the photoconductive switch can be biased to compensate for pump laser beam misalignment. The third topic is the first demonstration of an integrated circuit's use for submillimeter- wave frequency-domain spectroscopy. Such an application is attractive because of its inherently good frequency resolution, which is necessary for chemical and biological detection. The amplitude and phase of the measured spectrum of a circuit without a device under test agree with a model that takes into account circuit resonance, photoconductive-switch dynamics, and resistive loss. We discuss why photoconductive frequency-domain spectroscopy has an inherently lower output signal than similar time-domain spectroscopy, and how this drawback can be compensated for."
}, {
    "id": "oai:dspace.mit.edu:1721.1/35530",
    "title": "Design, modeling, and simulation of a Compact Optoelectronic Neural Coprocessor",
    "abstract": "Microprocessors have substantially increased in speed and computational power over the past two decades. However, they still are unable to solve certain classes of problems efficiently, particularly those which involve the analysis of large noisy data sets such as the case of image processing, feature extraction, and pattern recognition. Substantial research has focused on using neural network algorithms to process this type of data with much success. Most of this effort, however, has resulted in sophisticated neural network-based software algorithms rather than physical neural network hardware. Consequently, most neural network-type processing systems today consist of neural algorithms running on traditional sequential (i.e. Intel-based) microprocessors rather than on actual neurocomputers, and thus achieve less than optimal performance. The objective of the Compact Optoelectronic Neural Coprocessor (CONCOP) project is to build a compact, pixilated, parallel optoelectronic processor capable of running neural network-type algorithms in native hardware.",
    "advisors": ["Cardinal Warde"],
    "text": "Design, modeling, and simulation of a Compact Optoelectronic Neural Coprocessor Microprocessors have substantially increased in speed and computational power over the past two decades. However, they still are unable to solve certain classes of problems efficiently, particularly those which involve the analysis of large noisy data sets such as the case of image processing, feature extraction, and pattern recognition. Substantial research has focused on using neural network algorithms to process this type of data with much success. Most of this effort, however, has resulted in sophisticated neural network-based software algorithms rather than physical neural network hardware. Consequently, most neural network-type processing systems today consist of neural algorithms running on traditional sequential (i.e. Intel-based) microprocessors rather than on actual neurocomputers, and thus achieve less than optimal performance. The objective of the Compact Optoelectronic Neural Coprocessor (CONCOP) project is to build a compact, pixilated, parallel optoelectronic processor capable of running neural network-type algorithms in native hardware."
}, {
    "id": "oai:dspace.mit.edu:1721.1/30158",
    "title": "Experimental study of a 1.5-MW, 110-GHz gyrotron oscillator",
    "abstract": "This thesis reports the design, construction and testing of a 1.5 MW, 110 GHz gyrotron oscillator. This high power microwave tube has been proposed as the next evolutionary step for gyrotrons used to provide electron cyclotron heating required in fusion devices. A short pulse gyrotron based on the industrial tube design was built at MIT for experimental studies. The experiments are the first demonstration of such high powers at 110 GHz. Using a 96 kV, 40 A electron beam, over 1.4 MW was axially extracted in the design (TE22,6) mode in 3 us pulses, corresponding to a microwave efficiency of 37 %. The beam alpha, the ratio of transverse to axial velocity in the electron beam, was measured with a probe. At the high efficiency operating point the beam alpha was measured as 1.33. This value of alpha is less than the design value of 1.4, possibly accounting for the slightly reduced experimental efficiency. The output power and efficiency, as a function of magnetic field, beam voltage, and beam current, are in good agreement with nonlinear theory and simulations with the MAGY code. In another phase of the experiment, a second tube was built and tested. This tube used the same gun and cavity but also incorporated an internal mode converter to transform the generated waveguide mode into a free-space propagating beam. The gun was tested to full power and current in the experiment. Preliminary results were obtained. A mode map was generated to locate the region of operating parameters for the design mode, as well as for neighboring modes. Scans of the output microwave beam were also taken using a power-detecting diode. Future work will focus on generating high power, as well as operating the collector at a depressed voltage for even higher efficiency. A study is also presented of the 96 kV, 40 A magnetron injection gun.",
    "advisors": ["Richard J. Temkin"],
    "text": "Experimental study of a 1.5-MW, 110-GHz gyrotron oscillator This thesis reports the design, construction and testing of a 1.5 MW, 110 GHz gyrotron oscillator. This high power microwave tube has been proposed as the next evolutionary step for gyrotrons used to provide electron cyclotron heating required in fusion devices. A short pulse gyrotron based on the industrial tube design was built at MIT for experimental studies. The experiments are the first demonstration of such high powers at 110 GHz. Using a 96 kV, 40 A electron beam, over 1.4 MW was axially extracted in the design (TE22,6) mode in 3 us pulses, corresponding to a microwave efficiency of 37 %. The beam alpha, the ratio of transverse to axial velocity in the electron beam, was measured with a probe. At the high efficiency operating point the beam alpha was measured as 1.33. This value of alpha is less than the design value of 1.4, possibly accounting for the slightly reduced experimental efficiency. The output power and efficiency, as a function of magnetic field, beam voltage, and beam current, are in good agreement with nonlinear theory and simulations with the MAGY code. In another phase of the experiment, a second tube was built and tested. This tube used the same gun and cavity but also incorporated an internal mode converter to transform the generated waveguide mode into a free-space propagating beam. The gun was tested to full power and current in the experiment. Preliminary results were obtained. A mode map was generated to locate the region of operating parameters for the design mode, as well as for neighboring modes. Scans of the output microwave beam were also taken using a power-detecting diode. Future work will focus on generating high power, as well as operating the collector at a depressed voltage for even higher efficiency. A study is also presented of the 96 kV, 40 A magnetron injection gun."
}, {
    "id": "oai:dspace.mit.edu:1721.1/117841",
    "title": "Derivation, experimental verification, and applications of a new color image model",
    "abstract": "Image modeling is an important area of image processing. Good image models are useful, for example, in image restoration problems because they provide constraints that can be imposed on degraded images to retrieve better approximations of the original image. Many physical models of images are separable functions of position and wavelength, which means that images can be written as a color independent local average multiplied by a color dependent residual. We will present experimental results showing that this is the case in practice, and discuss the limitations of this model. We will also show that several commonly used observations in image processing follow from this model. Finally, we will demonstrate the results of imposing the model constraints in several image denoising problems and show that degraded images can be improved by imposing the model constraints.",
    "advisors": ["Jae S. Lim"],
    "text": "Derivation, experimental verification, and applications of a new color image model Image modeling is an important area of image processing. Good image models are useful, for example, in image restoration problems because they provide constraints that can be imposed on degraded images to retrieve better approximations of the original image. Many physical models of images are separable functions of position and wavelength, which means that images can be written as a color independent local average multiplied by a color dependent residual. We will present experimental results showing that this is the case in practice, and discuss the limitations of this model. We will also show that several commonly used observations in image processing follow from this model. Finally, we will demonstrate the results of imposing the model constraints in several image denoising problems and show that degraded images can be improved by imposing the model constraints."
}, {
    "id": "oai:dspace.mit.edu:1721.1/38924",
    "title": "Tiled microprocessors",
    "abstract": "Current-day microprocessors have reached the point of diminishing returns due to inherent scalability limitations. This thesis examines the tiled microprocessor, a class of microprocessor which is physically scalable but inherits many of the desirable properties of conventional microprocessors. Tiled microprocessors are composed of an array of replicated tiles connected by a special class of network, the Scalar Operand Network (SON), which is optimized for low-latency, low-occupancy communication between remote ALUs on different tiles. Tiled microprocessors can be constructed to scale to 100's or 1000's of functional units. This thesis identifies seven key criteria for achieving physical scalability in tiled microprocessors. It employs an archetypal tiled microprocessor to examine the challenges in achieving these criteria and to explore the properties of Scalar Operand Networks. The thesis develops the field of SONs in three major ways: it introduces the 5-tuple performance metric, it describes a complete, high-frequency <0,0,1,2,0> SON implementation, and it proposes a taxonomy, called AsTrO, for categorizing them.",
    "advisors": ["Anant Agarwal"],
    "text": "Tiled microprocessors Current-day microprocessors have reached the point of diminishing returns due to inherent scalability limitations. This thesis examines the tiled microprocessor, a class of microprocessor which is physically scalable but inherits many of the desirable properties of conventional microprocessors. Tiled microprocessors are composed of an array of replicated tiles connected by a special class of network, the Scalar Operand Network (SON), which is optimized for low-latency, low-occupancy communication between remote ALUs on different tiles. Tiled microprocessors can be constructed to scale to 100's or 1000's of functional units. This thesis identifies seven key criteria for achieving physical scalability in tiled microprocessors. It employs an archetypal tiled microprocessor to examine the challenges in achieving these criteria and to explore the properties of Scalar Operand Networks. The thesis develops the field of SONs in three major ways: it introduces the 5-tuple performance metric, it describes a complete, high-frequency <0,0,1,2,0> SON implementation, and it proposes a taxonomy, called AsTrO, for categorizing them."
}, {
    "id": "oai:dspace.mit.edu:1721.1/8228",
    "title": "Cellular computation and communications using engineered genetic regulatory networks",
    "abstract": "In this thesis, I present an engineering discipline for obtaining complex, predictable, and reliable cell behaviors by embedding biochemical logic circuits and programmed intercellular communications into cells. To accomplish this goal, I provide a well-characterized component library, a biocircuit design methodology, and software design tools. I have built and characterized an initial cellular gate library with biochemical gates that implement the NOT, IMPLIES, and AND logic functions in E. coli cells. The logic gates perform computation using DNA-binding proteins, small molecules that interact with these proteins, and segments of DNA that regulate the expression of the proteins. I introduce genetic process engineering, a methodology for modifying the DNA encoding of existing genetic elements to achieve the desired input/output behavior for constructing reliable circuits of significant complexity. I demonstrate the feasibility of digital computation in cells by building several operational in-vivo digital logic circuits, each composed of three gates that have been optimized by genetic process engineering.",
    "advisors": ["Thomas F. Knight, Jr., Gerald Jay Sussman", "Harold Abelson"],
    "text": "Cellular computation and communications using engineered genetic regulatory networks In this thesis, I present an engineering discipline for obtaining complex, predictable, and reliable cell behaviors by embedding biochemical logic circuits and programmed intercellular communications into cells. To accomplish this goal, I provide a well-characterized component library, a biocircuit design methodology, and software design tools. I have built and characterized an initial cellular gate library with biochemical gates that implement the NOT, IMPLIES, and AND logic functions in E. coli cells. The logic gates perform computation using DNA-binding proteins, small molecules that interact with these proteins, and segments of DNA that regulate the expression of the proteins. I introduce genetic process engineering, a methodology for modifying the DNA encoding of existing genetic elements to achieve the desired input/output behavior for constructing reliable circuits of significant complexity. I demonstrate the feasibility of digital computation in cells by building several operational in-vivo digital logic circuits, each composed of three gates that have been optimized by genetic process engineering."
}, {
    "id": "oai:dspace.mit.edu:1721.1/113998",
    "title": "Extreme imaging via physical model inversion : seeing around corners and imaging black holes",
    "abstract": "Imaging often plays a critical role in advancing fundamental science. However, as science continues to push the boundaries of knowledge, imaging systems are reaching the limits of what can be measured using traditional-direct approaches. By designing systems that tightly integrate novel sensor and algorithm design, it may be possible to develop imaging systems that exceed fundamental theoretical limitations to observe things previously impossible to see. However, these non-traditional imaging systems generally come with a trade-off; they produce increasingly sparse and/or noisy measurements that require incorporating additional structure to extract anything meaningful. The focus of this thesis is on using computational methods that exploit structure in our universe to move past these obstacles and reveal the invisible. In this thesis, we focus on two imaging problems that explicitly leverage structure in our universe: reconstructing images and video from a computational telescope the size of the Earth, and seeing around corners. For the first imaging problem, this thesis investigates ways to reconstruct images and video from a sparse telescope array distributed around the globe. Additionally, it presents a number of evaluation techniques developed to rigorously evaluate imaging methods in order to establish confidence in reconstructions done with real scientific data. The methods and evaluation techniques developed in this thesis will hopefully aid in ongoing work to take the first picture of a black hole. Next, this thesis presents methods developed for using the subtle spatio-temporal radiance variations that arise on the ground at the base of an edge to construct a one-dimensional video of a hidden scene. These methods may be especially valuable in remotely sensing occupants in a room during search and rescue operations, or in detecting hidden, oncoming vehicles and/or pedestrians for collision avoidance systems.",
    "advisors": ["William T. Freeman"],
    "text": "Extreme imaging via physical model inversion : seeing around corners and imaging black holes Imaging often plays a critical role in advancing fundamental science. However, as science continues to push the boundaries of knowledge, imaging systems are reaching the limits of what can be measured using traditional-direct approaches. By designing systems that tightly integrate novel sensor and algorithm design, it may be possible to develop imaging systems that exceed fundamental theoretical limitations to observe things previously impossible to see. However, these non-traditional imaging systems generally come with a trade-off; they produce increasingly sparse and/or noisy measurements that require incorporating additional structure to extract anything meaningful. The focus of this thesis is on using computational methods that exploit structure in our universe to move past these obstacles and reveal the invisible. In this thesis, we focus on two imaging problems that explicitly leverage structure in our universe: reconstructing images and video from a computational telescope the size of the Earth, and seeing around corners. For the first imaging problem, this thesis investigates ways to reconstruct images and video from a sparse telescope array distributed around the globe. Additionally, it presents a number of evaluation techniques developed to rigorously evaluate imaging methods in order to establish confidence in reconstructions done with real scientific data. The methods and evaluation techniques developed in this thesis will hopefully aid in ongoing work to take the first picture of a black hole. Next, this thesis presents methods developed for using the subtle spatio-temporal radiance variations that arise on the ground at the base of an edge to construct a one-dimensional video of a hidden scene. These methods may be especially valuable in remotely sensing occupants in a room during search and rescue operations, or in detecting hidden, oncoming vehicles and/or pedestrians for collision avoidance systems."
}, {
    "id": "oai:dspace.mit.edu:1721.1/34021",
    "title": "Dynamic resource allocation in WDM networks with optical bypass and waveband switching",
    "abstract": "In this thesis, we investigate network architecture from the twin perspectives of link resource allocation and node complexity in WDM optical networks Chapter 2 considers networks where the nodes have full wavelength accessibility, and investigates link resource allocation in ring networks in the form of the routing and wavelength assignment problem. In a ring network with N nodes and P calls allowed per node, we show that a necessary and sufficient lower bound on the number of wavelengths required for rearrangeably non-blocking traffic is PN/41 wavelengths. Two novel algorithms are presented: one that achieves this lower bound using at most two converters per wavelength, and a second requiring 2PN/71 wavelengths that requires significantly fewer wavelength converters. Chapter 3 begins our investigation of the role of reduced-complexity nodes in WDM networks by considering networks with optical bypass. The ring, torus, and tree architectures are considered. For the ring, an optical bypass architecture is constructed that requires the minimum number of locally-accessible wavelengths, with the remaining wavelengths bypassing all but a small number of hub nodes. The routing and wavelength assignment for all non-hub nodes is statically assigned, and these nodes do not require dynamic switching capability.",
    "advisors": ["Eytan Modiano"],
    "text": "Dynamic resource allocation in WDM networks with optical bypass and waveband switching In this thesis, we investigate network architecture from the twin perspectives of link resource allocation and node complexity in WDM optical networks Chapter 2 considers networks where the nodes have full wavelength accessibility, and investigates link resource allocation in ring networks in the form of the routing and wavelength assignment problem. In a ring network with N nodes and P calls allowed per node, we show that a necessary and sufficient lower bound on the number of wavelengths required for rearrangeably non-blocking traffic is PN/41 wavelengths. Two novel algorithms are presented: one that achieves this lower bound using at most two converters per wavelength, and a second requiring 2PN/71 wavelengths that requires significantly fewer wavelength converters. Chapter 3 begins our investigation of the role of reduced-complexity nodes in WDM networks by considering networks with optical bypass. The ring, torus, and tree architectures are considered. For the ring, an optical bypass architecture is constructed that requires the minimum number of locally-accessible wavelengths, with the remaining wavelengths bypassing all but a small number of hub nodes. The routing and wavelength assignment for all non-hub nodes is statically assigned, and these nodes do not require dynamic switching capability."
}, {
    "id": "oai:dspace.mit.edu:1721.1/111904",
    "title": "Energy scalable systems for 2D and 3D low-power ultrasound beamforming",
    "abstract": "In traditional ultrasound imaging systems, bulky and power-intensive mainframes are used to process the high number of waveforms acquired in parallel from a large transducer array. The computational power of these systems scales linearly with transducer count. However, there exist applications where basic functionality in low-power conditions may be favorable to an \"all-or-nothing\" system that only produces a high resolution image when enough power is supplied. This thesis presents systems designed to support energy-scalability at run-time, enabling the user to make the tradeoff between power and performance. First, a system-level energy model for a receive-side digital beamforming system is presented. Power-performance tradeoffs for the analog front-end, analog-to-digital converter, and digital beamformer are analyzed individually and then combined to account for the performance dependency between the functional components. These considerations inform a recommendation on design choices for the end-to-end system. Second, this thesis describes an energy-scalable 2-D beamformer that provides user-controlled run-time tradeoff between image quality and energy consumption. Architectural design choices that enable three operating modes are discussed. A test chip was fabricated in 65-nm low power CMOS technology. It can operate with functional correctness at 0.49 V, with a measured power of 185 [mu]W in real-time operation at 0.52 V. Finally, a software-based energy-scalable 3-D ultrasound beamformer is implemented on an embedded supercomputer. The energy consumption and corresponding imaging quality are measured and compared.",
    "advisors": ["Anantha P. Chandrakasan", "Gerald J. Sussman"],
    "text": "Energy scalable systems for 2D and 3D low-power ultrasound beamforming In traditional ultrasound imaging systems, bulky and power-intensive mainframes are used to process the high number of waveforms acquired in parallel from a large transducer array. The computational power of these systems scales linearly with transducer count. However, there exist applications where basic functionality in low-power conditions may be favorable to an \"all-or-nothing\" system that only produces a high resolution image when enough power is supplied. This thesis presents systems designed to support energy-scalability at run-time, enabling the user to make the tradeoff between power and performance. First, a system-level energy model for a receive-side digital beamforming system is presented. Power-performance tradeoffs for the analog front-end, analog-to-digital converter, and digital beamformer are analyzed individually and then combined to account for the performance dependency between the functional components. These considerations inform a recommendation on design choices for the end-to-end system. Second, this thesis describes an energy-scalable 2-D beamformer that provides user-controlled run-time tradeoff between image quality and energy consumption. Architectural design choices that enable three operating modes are discussed. A test chip was fabricated in 65-nm low power CMOS technology. It can operate with functional correctness at 0.49 V, with a measured power of 185 [mu]W in real-time operation at 0.52 V. Finally, a software-based energy-scalable 3-D ultrasound beamformer is implemented on an embedded supercomputer. The energy consumption and corresponding imaging quality are measured and compared."
}, {
    "id": "oai:dspace.mit.edu:1721.1/40514",
    "title": "Supramolecular architectures for neural prostheses",
    "abstract": "Neural prosthetic devices offer a means of restoring function that have been lost due to neural damage. The first part of this thesis investigates the design of a 15-channel, low-power, fully implantable stimulator chip. The chip is powered wirelessly and receives wireless commands. The chip features a CMOS only ASK detector, a single-differential converter based on a novel feedback loop, a low-power adaptive bandwidth DLL and 15 programmable current sources that can be controlled via four commands. Though it is feasible to build an implantable stimulator chip, the amount of power required to stimulate more than 16 channels is prohibitively large. Clearly, there is a need for a fundamentally different approach. The ultimate challenge is to design a self-sufficient neural interface. The ideal device will lend itself to seamless integration with the existing neural architecture. This necessitates that communication with the neural tissue should be performed via chemical rather than electrical messages. However, catastrophic destruction of neural tissue due to the release of large quantities of a neuroactive species, like neurotransmitters, precludes the storage of quantities large enough to suffice for the lifetime of the device. The ideal device then should actively sequester the chemical species from the body and release it upon receiving appropriate triggers in a power efficient manner. This thesis proposes the use of ionic gradients, specifically K+ ions as an alternative chemical stimulation method. The required ions can readily be sequestered from the background extracellular fluid. The parameters of using such a stimulation technique are first established by performing in-vitro experiments on rabbit retinas. The results show that modest increases (~~10mM) of K+ ions are sufficient to elicit a neural response.",
    "advisors": ["Marc A. Baldo"],
    "text": "Supramolecular architectures for neural prostheses Neural prosthetic devices offer a means of restoring function that have been lost due to neural damage. The first part of this thesis investigates the design of a 15-channel, low-power, fully implantable stimulator chip. The chip is powered wirelessly and receives wireless commands. The chip features a CMOS only ASK detector, a single-differential converter based on a novel feedback loop, a low-power adaptive bandwidth DLL and 15 programmable current sources that can be controlled via four commands. Though it is feasible to build an implantable stimulator chip, the amount of power required to stimulate more than 16 channels is prohibitively large. Clearly, there is a need for a fundamentally different approach. The ultimate challenge is to design a self-sufficient neural interface. The ideal device will lend itself to seamless integration with the existing neural architecture. This necessitates that communication with the neural tissue should be performed via chemical rather than electrical messages. However, catastrophic destruction of neural tissue due to the release of large quantities of a neuroactive species, like neurotransmitters, precludes the storage of quantities large enough to suffice for the lifetime of the device. The ideal device then should actively sequester the chemical species from the body and release it upon receiving appropriate triggers in a power efficient manner. This thesis proposes the use of ionic gradients, specifically K+ ions as an alternative chemical stimulation method. The required ions can readily be sequestered from the background extracellular fluid. The parameters of using such a stimulation technique are first established by performing in-vitro experiments on rabbit retinas. The results show that modest increases (~~10mM) of K+ ions are sufficient to elicit a neural response."
}, {
    "id": "oai:dspace.mit.edu:1721.1/89995",
    "title": "Linguistically motivated models for lightly-supervised dependency parsing",
    "abstract": "Today, the top performing parsing algorithms rely on the availability of annotated data for learning the syntactic structure of a language. Unfortunately, syntactically annotated texts are available only for a handful of languages. The research presented in this thesis aims at developing parsing models that can effectively perform in a lightly-supervised training regime. In particular we focus on formulating linguistically aware models of dependency parsing that can exploit readily available sources of linguistic knowledge such as language universals and typological features. This type of linguistic knowledge can be used to motivate model design and/or to guide inference procedure. We propose three alternative approaches for incorporating linguistic information into a lightly-supervised training setup: First, we show that linguistic information can be used in the form of rules on top of standard unsupervised parsing models to guide inference procedure. This method consistently outperforms existing monolingual and multilingual unsupervised parsers when tested on a set of 6 Indo-European languages. Next, we show that a linguistically aware model design greatly facilitates crosslingual parser transfer by leveraging syntactic connections between languages. Our transfer approach outperforms the state-of-the-art multilingual transfer parser across a set of 19 languages, achieving an average gain of 5.9%. The gains are even more pronounced - 14.4% - on non-Indo-European languages where existing transfer methods fail to perform. Finally, we propose a corpus-level Bayesian framework that allows multiple views of data in a single model. We use this framework to combine a dependency model with constituency view and universal rules, achieving a performance gain of 1.9% compared to the top-performing unsupervised parsing model.",
    "advisors": ["Regina Barzilay"],
    "text": "Linguistically motivated models for lightly-supervised dependency parsing Today, the top performing parsing algorithms rely on the availability of annotated data for learning the syntactic structure of a language. Unfortunately, syntactically annotated texts are available only for a handful of languages. The research presented in this thesis aims at developing parsing models that can effectively perform in a lightly-supervised training regime. In particular we focus on formulating linguistically aware models of dependency parsing that can exploit readily available sources of linguistic knowledge such as language universals and typological features. This type of linguistic knowledge can be used to motivate model design and/or to guide inference procedure. We propose three alternative approaches for incorporating linguistic information into a lightly-supervised training setup: First, we show that linguistic information can be used in the form of rules on top of standard unsupervised parsing models to guide inference procedure. This method consistently outperforms existing monolingual and multilingual unsupervised parsers when tested on a set of 6 Indo-European languages. Next, we show that a linguistically aware model design greatly facilitates crosslingual parser transfer by leveraging syntactic connections between languages. Our transfer approach outperforms the state-of-the-art multilingual transfer parser across a set of 19 languages, achieving an average gain of 5.9%. The gains are even more pronounced - 14.4% - on non-Indo-European languages where existing transfer methods fail to perform. Finally, we propose a corpus-level Bayesian framework that allows multiple views of data in a single model. We use this framework to combine a dependency model with constituency view and universal rules, achieving a performance gain of 1.9% compared to the top-performing unsupervised parsing model."
}, {
    "id": "oai:dspace.mit.edu:1721.1/44412",
    "title": "A comparison of parallel Gaussian elimination solvers for the computation of electrochemical battery models on the cell processor",
    "abstract": "The rising cost of fossil fuels, together with a push for more eco-friendly methods of transportation, has increased interest in and demand for electrically powered or assisted vehicles. The majority of these electric or hybrid electric vehicles will be, for the foreseeable future, powered by batteries. One of the major problems with batteries is their aging. For batteries, aging means that the maximum charge they can store decreases as number of charge/discharge cycles increases. Aging also means that after a certain number of charge/discharge cycles, the battery will fail. In lead-acid batteries, one of the major phenomenon that promotes battery failure is the development of a non-uniform concentration gradient of electrolyte along the electrodes' height. This phenomenon is known as electrolyte stratification. This thesis develops a simple two-level circuit model that can be used to model electrolyte stratification. The two-level circuit model is justified experimentally using digital Mach-Zehnder interferometry and is explained theoretically by means of two different electrochemical battery models. The experiments show how the usage of the electrode varies along its height while the simulations indicate that the high resistivity of the lead dioxide electrode plays a major role in the development of a stratified electrolyte. Finally, computational issues associated with the computation of a sophisticated two dimensional electrochemical battery model on the multicore Cell Broadband Engine processor are addressed in detail. In particular, three different banded parallel Gaussian elimination solvers are developed and compared. These three solvers vividly illustrate how performance achieved on the new multicore processors is strongly dependent on the algorithm used.",
    "advisors": ["John L. Wyatt, Jr.", "Thomas A. Keim"],
    "text": "A comparison of parallel Gaussian elimination solvers for the computation of electrochemical battery models on the cell processor The rising cost of fossil fuels, together with a push for more eco-friendly methods of transportation, has increased interest in and demand for electrically powered or assisted vehicles. The majority of these electric or hybrid electric vehicles will be, for the foreseeable future, powered by batteries. One of the major problems with batteries is their aging. For batteries, aging means that the maximum charge they can store decreases as number of charge/discharge cycles increases. Aging also means that after a certain number of charge/discharge cycles, the battery will fail. In lead-acid batteries, one of the major phenomenon that promotes battery failure is the development of a non-uniform concentration gradient of electrolyte along the electrodes' height. This phenomenon is known as electrolyte stratification. This thesis develops a simple two-level circuit model that can be used to model electrolyte stratification. The two-level circuit model is justified experimentally using digital Mach-Zehnder interferometry and is explained theoretically by means of two different electrochemical battery models. The experiments show how the usage of the electrode varies along its height while the simulations indicate that the high resistivity of the lead dioxide electrode plays a major role in the development of a stratified electrolyte. Finally, computational issues associated with the computation of a sophisticated two dimensional electrochemical battery model on the multicore Cell Broadband Engine processor are addressed in detail. In particular, three different banded parallel Gaussian elimination solvers are developed and compared. These three solvers vividly illustrate how performance achieved on the new multicore processors is strongly dependent on the algorithm used."
}, {
    "id": "oai:dspace.mit.edu:1721.1/16612",
    "title": "Predictive multiple sampling algorithm with overlapping integration intervals for linear wide dynamic range integrating image sensors",
    "abstract": "Machine vision systems are used in a wide range of applications such as security, automated quality control and intelligent transportation systems. Several of these systems need to extract information from natural scenes in the section of the electromagnetic spectrum visible to humans. These scenes can easily have intra-frame illumination ratios in excess of 10 : 1. Solid-state image sensors that can correctly process wide illumination dynamic range scenes are therefore required to ensure correct reliability and performance. This thesis describes a new algorithm to linearly increase the illumination dynamic range of integrating-type image sensors. A user-defined integration time is taken as a reference to create a potentially large set of integration intervals of different duration (the selected integration time being the longest) but with a common end. The light intensity received by each pixel in the sensing array is used to choose the optimal integration interval from the set, while a pixel saturation predictive decision is used to overlap the integration intervals within the given integration time such that only one frame using the optimal integration interval for each pixel is produced. The total integration time is never exceeded. Benefits from this approach are motion minimization, real-time operation, reduced memory requirements, programmable light intensity dynamic range increase and access to incremental light intensity information during the integration time.",
    "advisors": ["Charles G. Sodini"],
    "text": "Predictive multiple sampling algorithm with overlapping integration intervals for linear wide dynamic range integrating image sensors Machine vision systems are used in a wide range of applications such as security, automated quality control and intelligent transportation systems. Several of these systems need to extract information from natural scenes in the section of the electromagnetic spectrum visible to humans. These scenes can easily have intra-frame illumination ratios in excess of 10 : 1. Solid-state image sensors that can correctly process wide illumination dynamic range scenes are therefore required to ensure correct reliability and performance. This thesis describes a new algorithm to linearly increase the illumination dynamic range of integrating-type image sensors. A user-defined integration time is taken as a reference to create a potentially large set of integration intervals of different duration (the selected integration time being the longest) but with a common end. The light intensity received by each pixel in the sensing array is used to choose the optimal integration interval from the set, while a pixel saturation predictive decision is used to overlap the integration intervals within the given integration time such that only one frame using the optimal integration interval for each pixel is produced. The total integration time is never exceeded. Benefits from this approach are motion minimization, real-time operation, reduced memory requirements, programmable light intensity dynamic range increase and access to incremental light intensity information during the integration time."
}, {
    "id": "oai:dspace.mit.edu:1721.1/66009",
    "title": "Subspace and graph methods to leverage auxiliary data for limited target data multi-class classification, applied to speaker verification",
    "abstract": "Multi-class classification can be adversely affected by the absence of sufficient target (in-class) instances for training. Such cases arise in face recognition, speaker verification, and document classification, among others. Auxiliary data-sets, which contain a diverse sampling of non-target instances, are leveraged in this thesis using subspace and graph methods to improve classification where target data is limited. The auxiliary data is used to define a compact representation that maps instances into a vector space where inner products quantify class similarity. Within this space, an estimate of the subspace that constitutes within-class variability (e.g. the recording channel in speaker verification or the illumination conditions in face recognition) can be obtained using class-labeled auxiliary data. This thesis proposes a way to incorporate this estimate into the SVM framework to perform nuisance compensation, thus improving classification performance. Another contribution is a framework that combines mapping and compensation into a single linear comparison, which motivates computationally inexpensive and accurate comparison functions. A key aspect of the work takes advantage of efficient pairwise comparisons between the training, test, and auxiliary instances to characterize their interaction within the vector space, and exploits it for improved classification in three ways. The first uses the local variability around the train and test instances to reduce false-alarms. The second assumes the instances lie on a low-dimensional manifold and uses the distances along the manifold. The third extracts relational features from a similarity graph where nodes correspond to the training, test and auxiliary instances. To quantify the merit of the proposed techniques, results of experiments in speaker verification are presented where only a single target recording is provided to train the classifier. Experiments are preformed on standard NIST corpora and methods are compared using standard evalutation metrics: detection error trade-off curves, minimum decision costs, and equal error rates.",
    "advisors": ["William M. Campbell", "Alan V. Oppenheim"],
    "text": "Subspace and graph methods to leverage auxiliary data for limited target data multi-class classification, applied to speaker verification Multi-class classification can be adversely affected by the absence of sufficient target (in-class) instances for training. Such cases arise in face recognition, speaker verification, and document classification, among others. Auxiliary data-sets, which contain a diverse sampling of non-target instances, are leveraged in this thesis using subspace and graph methods to improve classification where target data is limited. The auxiliary data is used to define a compact representation that maps instances into a vector space where inner products quantify class similarity. Within this space, an estimate of the subspace that constitutes within-class variability (e.g. the recording channel in speaker verification or the illumination conditions in face recognition) can be obtained using class-labeled auxiliary data. This thesis proposes a way to incorporate this estimate into the SVM framework to perform nuisance compensation, thus improving classification performance. Another contribution is a framework that combines mapping and compensation into a single linear comparison, which motivates computationally inexpensive and accurate comparison functions. A key aspect of the work takes advantage of efficient pairwise comparisons between the training, test, and auxiliary instances to characterize their interaction within the vector space, and exploits it for improved classification in three ways. The first uses the local variability around the train and test instances to reduce false-alarms. The second assumes the instances lie on a low-dimensional manifold and uses the distances along the manifold. The third extracts relational features from a similarity graph where nodes correspond to the training, test and auxiliary instances. To quantify the merit of the proposed techniques, results of experiments in speaker verification are presented where only a single target recording is provided to train the classifier. Experiments are preformed on standard NIST corpora and methods are compared using standard evalutation metrics: detection error trade-off curves, minimum decision costs, and equal error rates."
}, {
    "id": "oai:dspace.mit.edu:1721.1/47777",
    "title": "Building dependability arguments for software intensive systems",
    "abstract": "A method is introduced for structuring and guiding the development of end-to-end dependability arguments. The goal is to establish high-level requirements of complex software-intensive systems, especially properties that cross-cut normal functional decomposition. The resulting argument documents and validates the justification of system-level claims by tracing them down to component-level substantiation, such as automatic code analysis or cryptographic proofs. The method is evaluated on case studies drawn from the Burr Proton Therapy Center, operating at Massachusetts General Hospital, and on the Pret a Voter cryptographic voting system, developed at the University of Newcastle.",
    "advisors": ["Daniel Jackson"],
    "text": "Building dependability arguments for software intensive systems A method is introduced for structuring and guiding the development of end-to-end dependability arguments. The goal is to establish high-level requirements of complex software-intensive systems, especially properties that cross-cut normal functional decomposition. The resulting argument documents and validates the justification of system-level claims by tracing them down to component-level substantiation, such as automatic code analysis or cryptographic proofs. The method is evaluated on case studies drawn from the Burr Proton Therapy Center, operating at Massachusetts General Hospital, and on the Pret a Voter cryptographic voting system, developed at the University of Newcastle."
}, {
    "id": "oai:dspace.mit.edu:1721.1/82367",
    "title": "Stochastic methods for large-scale linear problems, variational inequalities, and convex optimization",
    "abstract": "This thesis considers stochastic methods for large-scale linear systems, variational inequalities, and convex optimization problems. I focus on special structures that lend themselves to sampling, such as when the linear/nonlinear mapping or the objective function is an expected value or is the sum of a large number of terms, and/or the constraint is the intersection of a large number of simpler sets. For linear systems, I propose modifications to deterministic methods to allow the use of random samples and maintain the stochastic convergence, which is particularly challenging when the unknown system is singular or nearly singular. For variational inequalities and optimization problems, I propose a class of methods that combine elements of incremental constraint projection, stochastic gradient/ subgradient descent, and proximal algorithm. These methods can be applied with various sampling schemes that are suitable for applications involving distributed implementation, large data set, or online learning. I use a unified framework to analyze the convergence and the rate of convergence of these methods. This framework is based on a pair of supermartingale bounds, which control the convergence to feasibility and the convergence to optimality, respectively, and are coupled at different time scales.",
    "advisors": ["Dimitri P. Bertsekas"],
    "text": "Stochastic methods for large-scale linear problems, variational inequalities, and convex optimization This thesis considers stochastic methods for large-scale linear systems, variational inequalities, and convex optimization problems. I focus on special structures that lend themselves to sampling, such as when the linear/nonlinear mapping or the objective function is an expected value or is the sum of a large number of terms, and/or the constraint is the intersection of a large number of simpler sets. For linear systems, I propose modifications to deterministic methods to allow the use of random samples and maintain the stochastic convergence, which is particularly challenging when the unknown system is singular or nearly singular. For variational inequalities and optimization problems, I propose a class of methods that combine elements of incremental constraint projection, stochastic gradient/ subgradient descent, and proximal algorithm. These methods can be applied with various sampling schemes that are suitable for applications involving distributed implementation, large data set, or online learning. I use a unified framework to analyze the convergence and the rate of convergence of these methods. This framework is based on a pair of supermartingale bounds, which control the convergence to feasibility and the convergence to optimality, respectively, and are coupled at different time scales."
}, {
    "id": "oai:dspace.mit.edu:1721.1/8230",
    "title": "Intelligence by design : principles of modularity and coordination for engineering complex adaptive agents",
    "abstract": "All intelligence relies on search - for example, the search for an intelligent agent's next action. Search is only likely to succeed in resource-bounded agents if they have already been biased towards finding the right answer. In artificial agents, the primary source of bias is engineering. This dissertation describes an approach, Behavior-Oriented Design (BOD) for engineering complex agents. A complex agent is one that must arbitrate between potentially conflicting goals or behaviors. Behavior-oriented design builds on work in behavior-based and hybrid architectures for agents, and the object oriented approach to software engineering. The primary contributions of this dissertation are: 1. The BOD architecture: a modular architecture with each module providing specialized representations to facilitate learning. This includes one pre-specified module and representation for action selection or behavior arbitration. The specialized representation underlying BOD action selection is Parallel-rooted, Ordered, Slip-stack Hierarchical (POSH) reactive plans. 2. The BOD development process: an iterative process that alternately scales the agent's capabilities then optimizes the agent for simplicity, exploiting tradeoffs between the component representations. This ongoing process for controlling complexity not only provides bias for the behaving agent, but also facilitates its maintenance and extendibility.",
    "advisors": ["Lynn Andrea Stein"],
    "text": "Intelligence by design : principles of modularity and coordination for engineering complex adaptive agents All intelligence relies on search - for example, the search for an intelligent agent's next action. Search is only likely to succeed in resource-bounded agents if they have already been biased towards finding the right answer. In artificial agents, the primary source of bias is engineering. This dissertation describes an approach, Behavior-Oriented Design (BOD) for engineering complex agents. A complex agent is one that must arbitrate between potentially conflicting goals or behaviors. Behavior-oriented design builds on work in behavior-based and hybrid architectures for agents, and the object oriented approach to software engineering. The primary contributions of this dissertation are: 1. The BOD architecture: a modular architecture with each module providing specialized representations to facilitate learning. This includes one pre-specified module and representation for action selection or behavior arbitration. The specialized representation underlying BOD action selection is Parallel-rooted, Ordered, Slip-stack Hierarchical (POSH) reactive plans. 2. The BOD development process: an iterative process that alternately scales the agent's capabilities then optimizes the agent for simplicity, exploiting tradeoffs between the component representations. This ongoing process for controlling complexity not only provides bias for the behaving agent, but also facilitates its maintenance and extendibility."
}, {
    "id": "oai:dspace.mit.edu:1721.1/37846",
    "title": "Ferrofluid surface and volume flows in uniform rotating magnetic fields",
    "abstract": "Ferrofluid surface and volume effects in uniform dc and rotating magnetic fields are studied. Theory and corroborating measurements are presented for meniscus shapes and resulting surface driven flows, spin-up flows, and Hele-Shaw cell flows and instabilities. To characterize the water-based and oil-based ferrofluids used in experiments, measurements were made of the magnetization curve, surface tension, viscosity, density, and the speed of sound. Extensive measurements of the height and shape of ferrofluid menisci in applied uniform dc magnetic fields show that the height of the meniscus increases for vertical applied magnetic fields, whereas horizontal magnetic fields decrease meniscus height. An approximate energy minimization analysis agrees with the observed trends in ferrofluid meniscus height. The effects of ferrofluid meniscus curvature on spin-up flow were modeled under simplified assumptions. Analytical solutions were derived for two dimensional low Reynolds number flows and extended results were obtained numerically using COMSOL's Multiphysics finite element software package (FEMLAB) to solve for three dimensional recirculating flows at higher Reynolds numbers.",
    "advisors": ["Marcus Zahn"],
    "text": "Ferrofluid surface and volume flows in uniform rotating magnetic fields Ferrofluid surface and volume effects in uniform dc and rotating magnetic fields are studied. Theory and corroborating measurements are presented for meniscus shapes and resulting surface driven flows, spin-up flows, and Hele-Shaw cell flows and instabilities. To characterize the water-based and oil-based ferrofluids used in experiments, measurements were made of the magnetization curve, surface tension, viscosity, density, and the speed of sound. Extensive measurements of the height and shape of ferrofluid menisci in applied uniform dc magnetic fields show that the height of the meniscus increases for vertical applied magnetic fields, whereas horizontal magnetic fields decrease meniscus height. An approximate energy minimization analysis agrees with the observed trends in ferrofluid meniscus height. The effects of ferrofluid meniscus curvature on spin-up flow were modeled under simplified assumptions. Analytical solutions were derived for two dimensional low Reynolds number flows and extended results were obtained numerically using COMSOL's Multiphysics finite element software package (FEMLAB) to solve for three dimensional recirculating flows at higher Reynolds numbers."
}, {
    "id": "oai:dspace.mit.edu:1721.1/33083",
    "title": "A system for automated lexical mapping",
    "abstract": "Merging of clinical systems and medical databases, or aggregation of information from disparate databases, frequently requires a process where vocabularies are compared and similar concepts are mapped. Using a normalization phase followed by a novel alignment stage inspired by DNA sequence alignment methods, automated lexical mapping can map terms from various databases to standard vocabularies such as UMLS (Unified Medical Language System) and SNOMED (the Systematized Nomenclature of Medicine). This automated lexical mapping was evaluated using a real-world database of consultation letters from Children's Hospital Boston. The first phase involved extracting the reason for referral from the consultation letters. The reasons for referral were then mapped to SNOMED. The alignment algorithm was able to map 72% of equivalent concepts through lexical mapping alone. Lexical mapping can facilitate the integration of data from diverse sources and decrease the time and cost required for manual mapping and integration of clinical systems and medical databases.",
    "advisors": ["Isaac S. Kohane"],
    "text": "A system for automated lexical mapping Merging of clinical systems and medical databases, or aggregation of information from disparate databases, frequently requires a process where vocabularies are compared and similar concepts are mapped. Using a normalization phase followed by a novel alignment stage inspired by DNA sequence alignment methods, automated lexical mapping can map terms from various databases to standard vocabularies such as UMLS (Unified Medical Language System) and SNOMED (the Systematized Nomenclature of Medicine). This automated lexical mapping was evaluated using a real-world database of consultation letters from Children's Hospital Boston. The first phase involved extracting the reason for referral from the consultation letters. The reasons for referral were then mapped to SNOMED. The alignment algorithm was able to map 72% of equivalent concepts through lexical mapping alone. Lexical mapping can facilitate the integration of data from diverse sources and decrease the time and cost required for manual mapping and integration of clinical systems and medical databases."
}, {
    "id": "oai:dspace.mit.edu:1721.1/28587",
    "title": "When my patient is not my patient : inferring primary-care relationships using machine learning",
    "abstract": "This paper demonstrates that one can infer with respectable accuracy a physician's view of the therapeutic relationship that he or she has with a given patient, using data available in the patient's electronic medical record. In this study, we differentiate between the active primary relationship, the inactive primary or non-attending relationship, and the coverage relationship. We demonstrate that a single model built using the Averaged One-Dependence Estimator (AODE) classifier and learned with six attributes taken from patient visit history and physician practice characteristics can, for most of the 18 physicians tested, differentiate patients with a coverage relationship to a given physician from those with a primary relationship, achieving accuracies of 0.90 or greater as determined by the area under the receiver operating characteristic curve. Three of the 18 datasets had too few coverage patients to adequately characterize. We also demonstrate that, surprisingly, physicians are generally of like mind when assessing the therapeutic relationship that they have with a given patient. We find that for all physicians in our sample, a model built individually with any one physician's assessments performs statistically identically to the model built from the assessments of all other physicians combined. As a sub-goal of this research, we test the performance of different attribute selection methods on our dataset, comparing greedy vs. randomized search and wrapper vs. filter evaluators and finding no practical difference between them for our data. We also test the performance of several different classifiers, with AODE emerging as the best choice for this dataset. Lastly, we test the performance of linear vs. non-linear meta-learners for Stacked",
    "advisors": ["Henry C. Chueh", "G. Octo Barnett"],
    "text": "When my patient is not my patient : inferring primary-care relationships using machine learning This paper demonstrates that one can infer with respectable accuracy a physician's view of the therapeutic relationship that he or she has with a given patient, using data available in the patient's electronic medical record. In this study, we differentiate between the active primary relationship, the inactive primary or non-attending relationship, and the coverage relationship. We demonstrate that a single model built using the Averaged One-Dependence Estimator (AODE) classifier and learned with six attributes taken from patient visit history and physician practice characteristics can, for most of the 18 physicians tested, differentiate patients with a coverage relationship to a given physician from those with a primary relationship, achieving accuracies of 0.90 or greater as determined by the area under the receiver operating characteristic curve. Three of the 18 datasets had too few coverage patients to adequately characterize. We also demonstrate that, surprisingly, physicians are generally of like mind when assessing the therapeutic relationship that they have with a given patient. We find that for all physicians in our sample, a model built individually with any one physician's assessments performs statistically identically to the model built from the assessments of all other physicians combined. As a sub-goal of this research, we test the performance of different attribute selection methods on our dataset, comparing greedy vs. randomized search and wrapper vs. filter evaluators and finding no practical difference between them for our data. We also test the performance of several different classifiers, with AODE emerging as the best choice for this dataset. Lastly, we test the performance of linear vs. non-linear meta-learners for Stacked"
}, {
    "id": "oai:dspace.mit.edu:1721.1/33848",
    "title": "Design of a goal ontology for medical decision-support",
    "abstract": "Objectives: There are several ongoing efforts aimed at developing formal models of medical knowledge and reasoning to design decision-support systems. Until now, these efforts have focused primarily on representing content of clinical guidelines and their logical structure. The present study aims to develop a computable representation of health-care providers' intentions to be used as part of a framework for implementing clinical decision-support systems. Our goal is to create an ontology that supports retrieval of plans based on the intentions or goals of the clinician. Methods: We developed an ontological representation of medical goals, plans, clinical scenarios and other relevant entities in medical decision-making. We used the resulting ontology along with an external ontology inference engine to simulate selection of clinical recommendations based on goals. The ontology instances used in the simulation were modeled from two clinical guidelines. Testing the design: Thirty-two clinical recommendations were encoded in the experimental model. Nine test cases were created to verify the ability of the model to retrieve the plans. For all nine cases, plans were successfully retrieved. Conclusion: The ontological design we developed supported effective reasoning over a medical knowledge base.",
    "advisors": ["Aziz Boxwala"],
    "text": "Design of a goal ontology for medical decision-support Objectives: There are several ongoing efforts aimed at developing formal models of medical knowledge and reasoning to design decision-support systems. Until now, these efforts have focused primarily on representing content of clinical guidelines and their logical structure. The present study aims to develop a computable representation of health-care providers' intentions to be used as part of a framework for implementing clinical decision-support systems. Our goal is to create an ontology that supports retrieval of plans based on the intentions or goals of the clinician. Methods: We developed an ontological representation of medical goals, plans, clinical scenarios and other relevant entities in medical decision-making. We used the resulting ontology along with an external ontology inference engine to simulate selection of clinical recommendations based on goals. The ontology instances used in the simulation were modeled from two clinical guidelines. Testing the design: Thirty-two clinical recommendations were encoded in the experimental model. Nine test cases were created to verify the ability of the model to retrieve the plans. For all nine cases, plans were successfully retrieved. Conclusion: The ontological design we developed supported effective reasoning over a medical knowledge base."
}, {
    "id": "oai:dspace.mit.edu:1721.1/42207",
    "title": "Assessing the economic case for stratified medicine",
    "abstract": "The goal of this study is to explore the economic conditions that favor the joint development of therapeutics and companion diagnostics. I hypothesize that predictive biomarkers can generate economic value in drug development by increasing success rates. I construct an economic model of the development of a hypothetical new therapy, and devote particular attention to parameters regarding safety, efficacy, cost, and market size, within a decision-theoretic framework. The results include a characterization of the dynamic net present value trade-offs between stratum size and biomarker success, as well as the identification of two complementary concepts of stratified medicine, namely, disease reclassification and value-based reimbursement. I also identify a strong potential incentive mechanism in the hands of public policy makers that could facilitate a resolution of the tension between patient interests and the interests of pharmaceutical sponsors. The conclusion is that a biomarker can compensate for smaller stratum by increasing success probabilities. However, the effects of longer development time due to biomarker inclusion counter the effects of improved success probabilities. Longer exclusivity periods for stratified medicine may be required in order to resolve the tension between patient interests and the interests of pharmaceutical sponsors.",
    "advisors": ["Ernst Berndt", "Thomas G. Roberts, Jr"],
    "text": "Assessing the economic case for stratified medicine The goal of this study is to explore the economic conditions that favor the joint development of therapeutics and companion diagnostics. I hypothesize that predictive biomarkers can generate economic value in drug development by increasing success rates. I construct an economic model of the development of a hypothetical new therapy, and devote particular attention to parameters regarding safety, efficacy, cost, and market size, within a decision-theoretic framework. The results include a characterization of the dynamic net present value trade-offs between stratum size and biomarker success, as well as the identification of two complementary concepts of stratified medicine, namely, disease reclassification and value-based reimbursement. I also identify a strong potential incentive mechanism in the hands of public policy makers that could facilitate a resolution of the tension between patient interests and the interests of pharmaceutical sponsors. The conclusion is that a biomarker can compensate for smaller stratum by increasing success probabilities. However, the effects of longer development time due to biomarker inclusion counter the effects of improved success probabilities. Longer exclusivity periods for stratified medicine may be required in order to resolve the tension between patient interests and the interests of pharmaceutical sponsors."
}, {
    "id": "oai:dspace.mit.edu:1721.1/78159",
    "title": "Disease marketing and patient coping : a research study",
    "abstract": "BACKGROUND: There is a high prevalence of disease marketing actions in the United States that are targeted towards patients with chronic illness. However, no study has assessed the direct effects of these marketing actions on patient coping attitudes and behaviors. OBJECTIVES: This study aims to investigate whether the mere presence of disease marketing impacts patient coping and if so, how do they affect patients' coping attitudes and behaviors. METHODS: We conducted a controlled experiment using online questionnaires to assess the disease perceptions, coping decisions and disease disclosure behaviors of 108 subjects. The subjects were divided into two groups where the experimental group (N = 55) was shown marketing actions associated with a fictitious disease called Karlsen's Disease while the control group (N = 53) was not shown any marketing actions. The subjects were then asked a series of questions related to health-related coping behaviors and non-health related social behaviors. T-tests and chi-square analyses were used to analyze the behavioral differences between the experimental (high-marketing) and control (no-marketing) groups. RESULTS: Subjects in the high-marketing group were overall significantly more willing to draft a will than subjects in the no-marketing group (t(106) = 2.64, p = 0.01); High-marketing group subjects were overall significantly more likely to wear a medical ID bracelet than no-marketing group subjects (c(1, N = 108) = 3.71, p = 0.05); Among subjects who were willing to request a menu accommodation at a dinner party, those who were in the high-marketing group were significantly more likely to disclose their disease to the party host (c(1, N = 90) = 4.65, p = 0.03); Subjects in the high-marketing group were also significantly more likely to anticipate greater understanding from the party host towards their menu accommodation request. When controlled for gender, women in the high-marketing group were more likely to join a patient support group (t(61) = 1.75, p = 0.09), and less likely to ask family and friends to shave their heads in show of solidarity (t(18) = -1.97, p = 0.07) than women in the no-marketing group; Men in the high-marketing group were more likely than men in the no-marketing group to disclose their health condition to the dinner party host (c(1, N = 47) = 3.61, p = 0.06). Finally, among subjects with at least a 4-year college degree, those in the high-marketing group were more willing than those in the no-marketing group to wear a face mask to protect themselves from airborne pathogens in crowded public places (t(61) = 1.79, p = 0.08). CONCLUSIONS: Based on our results, the presence of disease marketing is anticipated to have a general positive impact on patient coping attitudes and behaviors. Chronically ill patients exposed to disease marketing actions are expected to anticipate less stigma from others, have increased willingness to disclose their illness and adopt health seeking behaviors. Disease marketing is also expected to have differential impact on patients based on their gender and level of education. Follow-up studies using real patients with chronic illness should be carried out to confirm the findings from this study.",
    "advisors": ["Rene Richardson Gosline", "Richard Rox Anderson"],
    "text": "Disease marketing and patient coping : a research study BACKGROUND: There is a high prevalence of disease marketing actions in the United States that are targeted towards patients with chronic illness. However, no study has assessed the direct effects of these marketing actions on patient coping attitudes and behaviors. OBJECTIVES: This study aims to investigate whether the mere presence of disease marketing impacts patient coping and if so, how do they affect patients' coping attitudes and behaviors. METHODS: We conducted a controlled experiment using online questionnaires to assess the disease perceptions, coping decisions and disease disclosure behaviors of 108 subjects. The subjects were divided into two groups where the experimental group (N = 55) was shown marketing actions associated with a fictitious disease called Karlsen's Disease while the control group (N = 53) was not shown any marketing actions. The subjects were then asked a series of questions related to health-related coping behaviors and non-health related social behaviors. T-tests and chi-square analyses were used to analyze the behavioral differences between the experimental (high-marketing) and control (no-marketing) groups. RESULTS: Subjects in the high-marketing group were overall significantly more willing to draft a will than subjects in the no-marketing group (t(106) = 2.64, p = 0.01); High-marketing group subjects were overall significantly more likely to wear a medical ID bracelet than no-marketing group subjects (c(1, N = 108) = 3.71, p = 0.05); Among subjects who were willing to request a menu accommodation at a dinner party, those who were in the high-marketing group were significantly more likely to disclose their disease to the party host (c(1, N = 90) = 4.65, p = 0.03); Subjects in the high-marketing group were also significantly more likely to anticipate greater understanding from the party host towards their menu accommodation request. When controlled for gender, women in the high-marketing group were more likely to join a patient support group (t(61) = 1.75, p = 0.09), and less likely to ask family and friends to shave their heads in show of solidarity (t(18) = -1.97, p = 0.07) than women in the no-marketing group; Men in the high-marketing group were more likely than men in the no-marketing group to disclose their health condition to the dinner party host (c(1, N = 47) = 3.61, p = 0.06). Finally, among subjects with at least a 4-year college degree, those in the high-marketing group were more willing than those in the no-marketing group to wear a face mask to protect themselves from airborne pathogens in crowded public places (t(61) = 1.79, p = 0.08). CONCLUSIONS: Based on our results, the presence of disease marketing is anticipated to have a general positive impact on patient coping attitudes and behaviors. Chronically ill patients exposed to disease marketing actions are expected to anticipate less stigma from others, have increased willingness to disclose their illness and adopt health seeking behaviors. Disease marketing is also expected to have differential impact on patients based on their gender and level of education. Follow-up studies using real patients with chronic illness should be carried out to confirm the findings from this study."
}, {
    "id": "oai:dspace.mit.edu:1721.1/38334",
    "title": "Investment performance of life-science venture capital investment funds, persistence, and subsector analysis",
    "abstract": "Venture capital investment performance data and performance attribution are not typically published. Venture investors articulate (and sell to LPs) conflicting strategies; the popular business literature and culture is rife with rapidly changing beliefs about the relative attractiveness of healthcare venture subsectors, particularly therapeutics and devices. To examine these issues in a more rigorous format I developed a dataset of healthcare venture deals, scored each deal with a new metric (\"jb-score\"), and assigned each portfolio company to appropriate subsectors. This dataset was then used to examine subsector performance, persistence, and fund strategy attribution (pure vs. mixed healthcare strategies.) Specifically, I found that the performance characteristics of device and therapeutic (aka biotech or drug) investments are similar: both subsectors evidence similar jb-scores and firms who invest heavily in these subsectors show similar levels of persistent overperformance with devices showing somewhat higher persistence. Firms that focus on one subsector do not perform as well as firms that follow a more balanced strategy. Finally, I examine the validity of the jb-score and offer some suggestions for future improvements.",
    "advisors": ["Antoinette Schoar", "Stanley N. Lapidus"],
    "text": "Investment performance of life-science venture capital investment funds, persistence, and subsector analysis Venture capital investment performance data and performance attribution are not typically published. Venture investors articulate (and sell to LPs) conflicting strategies; the popular business literature and culture is rife with rapidly changing beliefs about the relative attractiveness of healthcare venture subsectors, particularly therapeutics and devices. To examine these issues in a more rigorous format I developed a dataset of healthcare venture deals, scored each deal with a new metric (\"jb-score\"), and assigned each portfolio company to appropriate subsectors. This dataset was then used to examine subsector performance, persistence, and fund strategy attribution (pure vs. mixed healthcare strategies.) Specifically, I found that the performance characteristics of device and therapeutic (aka biotech or drug) investments are similar: both subsectors evidence similar jb-scores and firms who invest heavily in these subsectors show similar levels of persistent overperformance with devices showing somewhat higher persistence. Firms that focus on one subsector do not perform as well as firms that follow a more balanced strategy. Finally, I examine the validity of the jb-score and offer some suggestions for future improvements."
}, {
    "id": "oai:dspace.mit.edu:1721.1/28585",
    "title": "Applying axiomatic design methodology to enable adaptation of clinical guidelines to local contexts",
    "abstract": "Background: Local adaptation of guidelines may increase compliance with guidelines that have been developed at a national level and are often not used in practice because of contextual factors. We have developed a representation scheme known as HieroGLIF, that facilitates a two-step approach to development and implementation of guidelines. In the first step, professional medical societies create a setting-independent guideline. In the second step, the guideline is adapted to local settings. The scheme represents the setting-independent guideline knowledge in a hierarchical structure for which we use axiomatic design theory to guide a top-down design. This representation extends the Guideline Interchange Format (GLIF), a frame-based ontology that has been previously developed for representation of guidelines. Methods: We encoded conventional national guidelines in the setting-independent format which were adapted to local clinical settings of their practice by primary care physicians (PCPs). We conducted a qualitative analysis of the type of changes that were made by the local adaptors. For each of the guidelines two patients' scenarios were created for which two sets of guideline recommendations were generated, one from the adapted hierarchical guideline and one from the conventional, non-hierarchical guideline. We evaluated in a randomized controlled trial the potential impact of the local adaptation methodology on adherence to guidelines. For each recommendation, 70 PCPs responded to a questionnaire that inquired if PCPs would follow the recommendations and their ratings for several relevant attributes. We also analyzed the data to look for attributes of recommendations that are important for their acceptance by PCPs. Results: In 8 out of",
    "advisors": ["Aziz A. Boxwala"],
    "text": "Applying axiomatic design methodology to enable adaptation of clinical guidelines to local contexts Background: Local adaptation of guidelines may increase compliance with guidelines that have been developed at a national level and are often not used in practice because of contextual factors. We have developed a representation scheme known as HieroGLIF, that facilitates a two-step approach to development and implementation of guidelines. In the first step, professional medical societies create a setting-independent guideline. In the second step, the guideline is adapted to local settings. The scheme represents the setting-independent guideline knowledge in a hierarchical structure for which we use axiomatic design theory to guide a top-down design. This representation extends the Guideline Interchange Format (GLIF), a frame-based ontology that has been previously developed for representation of guidelines. Methods: We encoded conventional national guidelines in the setting-independent format which were adapted to local clinical settings of their practice by primary care physicians (PCPs). We conducted a qualitative analysis of the type of changes that were made by the local adaptors. For each of the guidelines two patients' scenarios were created for which two sets of guideline recommendations were generated, one from the adapted hierarchical guideline and one from the conventional, non-hierarchical guideline. We evaluated in a randomized controlled trial the potential impact of the local adaptation methodology on adherence to guidelines. For each recommendation, 70 PCPs responded to a questionnaire that inquired if PCPs would follow the recommendations and their ratings for several relevant attributes. We also analyzed the data to look for attributes of recommendations that are important for their acceptance by PCPs. Results: In 8 out of"
}, {
    "id": "oai:dspace.mit.edu:1721.1/28588",
    "title": "Organizational design : the integration of pharmaceutical discovery and development",
    "abstract": "The decline in Pharmaceutical R&D productivity has been attributed to high clinical failure rates suggesting that targets, leads and clinical candidates may be of lower quality in recent years. Senior R&D management generally believes that a greater integration of drug discovery and development will improve the selection and optimization of clinical candidates. I demonstrate the different nature of discovery and development with discovery tasks seen as more uncertain, having more reciprocal work flows and more under the control of management than development tasks. Discovery and development personnel have different characteristics and motivations, with discovery staff having greater creative skills and development staff greater planning skill. Following a congruence approach to organization design these differences imply that a complete merging of discovery and development functions would lead to poor fit between organizational design elements. This leaves an ongoing requirement for integrative systems which can preserve the important characteristic of discovery and development functions yet provide knowledge integration at key decision points to improve the quality of clinical candidates. A wide range of integrating mechanisms was found to be in use with an emphasis on cross functional teams. Information Technology was viewed as necessary infrastructure but not an important component of knowledge integration. No strong links were found between pipeline maturity and the integrative mechanism deployed. I speculate that company R&D performance could be better matched to internal and external circumstances by a more active approach to managing integrative systems. I propose a conceptual model of integrative systems to guide a more dynamic management approach",
    "advisors": ["Nelson Repenning", "Brian Seed"],
    "text": "Organizational design : the integration of pharmaceutical discovery and development The decline in Pharmaceutical R&D productivity has been attributed to high clinical failure rates suggesting that targets, leads and clinical candidates may be of lower quality in recent years. Senior R&D management generally believes that a greater integration of drug discovery and development will improve the selection and optimization of clinical candidates. I demonstrate the different nature of discovery and development with discovery tasks seen as more uncertain, having more reciprocal work flows and more under the control of management than development tasks. Discovery and development personnel have different characteristics and motivations, with discovery staff having greater creative skills and development staff greater planning skill. Following a congruence approach to organization design these differences imply that a complete merging of discovery and development functions would lead to poor fit between organizational design elements. This leaves an ongoing requirement for integrative systems which can preserve the important characteristic of discovery and development functions yet provide knowledge integration at key decision points to improve the quality of clinical candidates. A wide range of integrating mechanisms was found to be in use with an emphasis on cross functional teams. Information Technology was viewed as necessary infrastructure but not an important component of knowledge integration. No strong links were found between pipeline maturity and the integrative mechanism deployed. I speculate that company R&D performance could be better matched to internal and external circumstances by a more active approach to managing integrative systems. I propose a conceptual model of integrative systems to guide a more dynamic management approach"
}, {
    "id": "oai:dspace.mit.edu:1721.1/58093",
    "title": "Impact of the Massachusetts Pharmaceutical and Medical Device Manufacturer Code of Conduct on medical device physician-industry collaboration",
    "abstract": "The Massachusetts Pharmaceutical and Medical Device Manufacturer Code of Conduct (PCOC) or 105 CMR 970.000 was enacted by the Massachusetts state legislature and adopted by the Department of Public Health (DPH) in July 2009 under Chapter 305 of the Acts of 2008, An Act To Promote Cost Containment, Transparency and Efficiency in the Delivery of Quality Health Care. The state law requires pharmaceutical and medical device manufacturers to comply with a marketing code of conduct, obey specific compliance activities, and disclose payments to Massachusetts-licensed healthcare providers with a value of $50 or more in connection with sales and marketing activities. This thesis qualitatively assessed the impact of 105 CMR 970.000 on physician-industry collaboration related to technology development and physician education in the Massachusetts medical device industry, as depicted by academic physicians and representatives of medical device companies during the first quarter of calendar year 2010. A pilot study comprising interviews and surveys of stakeholders in the Massachusetts medical device industry was conducted to summarize the initial impressions of the impact of 105 CMR 970.000 on medical device physician-industry collaboration, with the intention of creating a roadmap for future analysis. Informal interviews (36) included individuals at medical device manufacturers, distributors, academic medical centers, venture capital firms, law firms, consulting firms, MassMedic, and the DPH. Formal surveys (40) included academic physicians and medical device company representatives selling to Massachusetts licensed physicians. The hypothesis was confirmed that 105 CMR 970.000 has impaired medical device physician-industry collaboration related to technology development and physician education in Massachusetts. Our results may have state and federal regulatory implications for the medical device industry and can serve as a guide for future analysis.",
    "advisors": ["T. Forcht Dagi", "Kevin L. Ohashi"],
    "text": "Impact of the Massachusetts Pharmaceutical and Medical Device Manufacturer Code of Conduct on medical device physician-industry collaboration The Massachusetts Pharmaceutical and Medical Device Manufacturer Code of Conduct (PCOC) or 105 CMR 970.000 was enacted by the Massachusetts state legislature and adopted by the Department of Public Health (DPH) in July 2009 under Chapter 305 of the Acts of 2008, An Act To Promote Cost Containment, Transparency and Efficiency in the Delivery of Quality Health Care. The state law requires pharmaceutical and medical device manufacturers to comply with a marketing code of conduct, obey specific compliance activities, and disclose payments to Massachusetts-licensed healthcare providers with a value of $50 or more in connection with sales and marketing activities. This thesis qualitatively assessed the impact of 105 CMR 970.000 on physician-industry collaboration related to technology development and physician education in the Massachusetts medical device industry, as depicted by academic physicians and representatives of medical device companies during the first quarter of calendar year 2010. A pilot study comprising interviews and surveys of stakeholders in the Massachusetts medical device industry was conducted to summarize the initial impressions of the impact of 105 CMR 970.000 on medical device physician-industry collaboration, with the intention of creating a roadmap for future analysis. Informal interviews (36) included individuals at medical device manufacturers, distributors, academic medical centers, venture capital firms, law firms, consulting firms, MassMedic, and the DPH. Formal surveys (40) included academic physicians and medical device company representatives selling to Massachusetts licensed physicians. The hypothesis was confirmed that 105 CMR 970.000 has impaired medical device physician-industry collaboration related to technology development and physician education in Massachusetts. Our results may have state and federal regulatory implications for the medical device industry and can serve as a guide for future analysis."
}, {
    "id": "oai:dspace.mit.edu:1721.1/28591",
    "title": "Health information on the Internet : strategies for assessing consumer needs and improving consumer information retrieval",
    "abstract": "Patients and their family members are increasingly turning to the Internet for health information. However, the search strategies consumers are using to obtain information are often unsuccessful. Since some patients are using the information they obtain to influence health decisions, it is increasingly important to identify strategies that aid consumer access to quality information to address their needs. Three different strategies to improve consumer health information retrieval are explored in this thesis, and suggestions for the application of these tactics and incorporation into healthcare delivery are discussed. Consumers have the option to choose between medically specific web sites and generic search engines with the whole Internet as their search space. For this project, a rigorous comparison of Internet searches in these two scopes was conducted to determine which search scope provides better returns. No statistical difference was found between the two different scopes, but several pros and cons of each were identified. Queries generated by consumers to initiate a free-text Internet search are often too short or too general to be effective. Additionally, consumers often employ vocabulary that does not match the terminology of health content. For this research, reformulation of original consumer queries using professional terminology was explored. A trend was noted towards increased search precision when substitutions were provided for lay terms, abbreviations, and acronyms, though performance often worsened when reformulated queries contained ill-fitted or arcane terminology. It is essential to study information needs to devise strategies to support consumer health information retrieval. The specific needs of asthma patients",
    "advisors": ["Qing T. Zeng"],
    "text": "Health information on the Internet : strategies for assessing consumer needs and improving consumer information retrieval Patients and their family members are increasingly turning to the Internet for health information. However, the search strategies consumers are using to obtain information are often unsuccessful. Since some patients are using the information they obtain to influence health decisions, it is increasingly important to identify strategies that aid consumer access to quality information to address their needs. Three different strategies to improve consumer health information retrieval are explored in this thesis, and suggestions for the application of these tactics and incorporation into healthcare delivery are discussed. Consumers have the option to choose between medically specific web sites and generic search engines with the whole Internet as their search space. For this project, a rigorous comparison of Internet searches in these two scopes was conducted to determine which search scope provides better returns. No statistical difference was found between the two different scopes, but several pros and cons of each were identified. Queries generated by consumers to initiate a free-text Internet search are often too short or too general to be effective. Additionally, consumers often employ vocabulary that does not match the terminology of health content. For this research, reformulation of original consumer queries using professional terminology was explored. A trend was noted towards increased search precision when substitutions were provided for lay terms, abbreviations, and acronyms, though performance often worsened when reformulated queries contained ill-fitted or arcane terminology. It is essential to study information needs to devise strategies to support consumer health information retrieval. The specific needs of asthma patients"
}, {
    "id": "oai:dspace.mit.edu:1721.1/58401",
    "title": "Biomedical data retrieval utilizing textual data in a gene expression database by Richard Lu, MD.",
    "abstract": "Background: The commoditization of high-throughput gene expression sequencing and microarrays has led to a proliferation in both the amount of genomic and clinical data that is available. Descriptive textual information deposited with gene expression data in the Gene Expression Omnibus (GEO) is an underutilized resource because the textual information is unstructured and difficult to query. Rendering this information in a structured format utilizing standard medical terms would facilitate better searching and data reuse. Such a procedure would significantly increase the clinical utility of biomedical data repositories. Methods: The thesis is divided into two sections. The first section compares how well four medical terminologies were able to represent textual information deposited in GEO. The second section implements free-text search and faceted search and evaluates how well they are able to answer clinical queries with varying levels of complexity. Part I: 120 samples were randomly extracted from samples deposited in the GEO database from six clinical domains-breast cancer, colon cancer, rheumatoid arthritis (RA), systemic lupus erythematosus (SLE), type I diabetes mellitus (IDDM), and asthma. These samples were previously annotated manually and structured textual information was obtained in a tag:value format. Data was mapped to four different controlled terminologies: NCI Thesaurus, MeSH, SNOMED-CT, and ICD- 10. The samples were assigned a score on a three-point scale that was based on how well the terminology was able to represent descriptive textual information. Part II: Faceted and free-text search tools were implemented, with 300 GEO samples included for querying. Eight natural language search questions were selected randomly from scientific journals. Academic researchers were recruited and asked to use the faceted and free-text search tools to locate samples matching the question criteria. Precision, recall, F-score, and search time were compared and analyzed for both free-text and faceted search. Results: The results show that the NCI Thesaurus consistently ranked as the most comprehensive terminology across all domains while ICD-10 consistently ranked as the least comprehensive. Using NCI Thesaurus to augment the faceted search tool, each researcher was able to reach 100% precision and recall (F-score 1.0) for each of the eight search questions. Using free-text search, test users averaged 22.8% precision, 60.7% recall, and an F-score of 0.282. The mean search time per question using faceted search and free-text search were 116.7 seconds, and 138.4 seconds, respectively. The difference between search time was not statistically significant (p=0. 734). However, paired t-test analysis showed a statistically signficant difference between the two search strategies with respect to precision (p=O.001), recall (p=O.042), and F-score (p<0. 001). Conclusion: This work demonstrates that biomedical terms included in a gene expression database can be adequately expressed using the NCI Thesaurus. It also shows that faceted searching using a controlled terminology is superior to conventional free-text searching when answering queries of varying levels of complexity.",
    "advisors": ["Ronilda Lacson"],
    "text": "Biomedical data retrieval utilizing textual data in a gene expression database by Richard Lu, MD. Background: The commoditization of high-throughput gene expression sequencing and microarrays has led to a proliferation in both the amount of genomic and clinical data that is available. Descriptive textual information deposited with gene expression data in the Gene Expression Omnibus (GEO) is an underutilized resource because the textual information is unstructured and difficult to query. Rendering this information in a structured format utilizing standard medical terms would facilitate better searching and data reuse. Such a procedure would significantly increase the clinical utility of biomedical data repositories. Methods: The thesis is divided into two sections. The first section compares how well four medical terminologies were able to represent textual information deposited in GEO. The second section implements free-text search and faceted search and evaluates how well they are able to answer clinical queries with varying levels of complexity. Part I: 120 samples were randomly extracted from samples deposited in the GEO database from six clinical domains-breast cancer, colon cancer, rheumatoid arthritis (RA), systemic lupus erythematosus (SLE), type I diabetes mellitus (IDDM), and asthma. These samples were previously annotated manually and structured textual information was obtained in a tag:value format. Data was mapped to four different controlled terminologies: NCI Thesaurus, MeSH, SNOMED-CT, and ICD- 10. The samples were assigned a score on a three-point scale that was based on how well the terminology was able to represent descriptive textual information. Part II: Faceted and free-text search tools were implemented, with 300 GEO samples included for querying. Eight natural language search questions were selected randomly from scientific journals. Academic researchers were recruited and asked to use the faceted and free-text search tools to locate samples matching the question criteria. Precision, recall, F-score, and search time were compared and analyzed for both free-text and faceted search. Results: The results show that the NCI Thesaurus consistently ranked as the most comprehensive terminology across all domains while ICD-10 consistently ranked as the least comprehensive. Using NCI Thesaurus to augment the faceted search tool, each researcher was able to reach 100% precision and recall (F-score 1.0) for each of the eight search questions. Using free-text search, test users averaged 22.8% precision, 60.7% recall, and an F-score of 0.282. The mean search time per question using faceted search and free-text search were 116.7 seconds, and 138.4 seconds, respectively. The difference between search time was not statistically significant (p=0. 734). However, paired t-test analysis showed a statistically signficant difference between the two search strategies with respect to precision (p=O.001), recall (p=O.042), and F-score (p<0. 001). Conclusion: This work demonstrates that biomedical terms included in a gene expression database can be adequately expressed using the NCI Thesaurus. It also shows that faceted searching using a controlled terminology is superior to conventional free-text searching when answering queries of varying levels of complexity."
}, {
    "id": "oai:dspace.mit.edu:1721.1/62523",
    "title": "An analysis of the differences between national and local coverage determinations of medical procedures in the US",
    "abstract": "Medicare coverage policies of medical procedures can be promulgated at a national level by the Centers of Medicare and Medicaid Services (CMS) as National Coverage Determinations (NCDs) or at a local level by Medicare contractors as Local Coverage Determinations (LCDs). Although LCDs shouldn't contradict NCDs, they can differ. In the present study, I analyze some factors that could partially account for the differences between NCDs and LCDs. Using the Medicare Coverage Database from CMS, I searched for differences between NCDs and LCDs in five benefit categories: inpatient hospital services, durable medical devices, diagnostic laboratory tests, physician's services and other diagnostic tests. There is a reasonable degree of homogeneity in coverage policies for procedures for which an NCD has been issued: 82% are exactly the same. Most of the differences took the form of exclusions from LCDs, but not from NCDs. For each state, I computed the number of times that LCDs were issued and the number of times that LCDs differed from NCDs and searched for possible linear or exponential correlation models. The following factors were initially hypothesized to account for these differences: number of Schools of Medicine, number of physicians, GDP per capita by state, state ranking according to number of Level 1 and Level 2 Trauma Centers and the profile of MEDCAC members. At a national level, I found no correlation between the number of LCDs issued or the number of differences between LCDs and NCDs and any of these variables. However, on a sub-analysis at a local level, in some regions I found a positive correlation (r2 >.94) between the following three variables: 1) number of Schools of Medicine, 2) number of physicians, and 3) state ranking according to the number of Level 1 and Level 2 Trauma Centers and the following two parameters: 1) the level of LCD issuing activity, and 2) the number of times that LCDs differ from NCDs. The correlations shown by the performed sub-analysis within regions may imply that more LCDs are issued to restrict coverage when there is a local need to control the excessive demand partially driven by the higher number of hospitals and physicians that are active in pursuing their interests. The fact that these correlations were shown only at a regional level may indicate that when local factors are disregarded, the original hypothesized factors do influence LCD activity, however, at a national level, other hypothetical local factors may have a greater influence on LCD activity and policy discrepancies. In order to have a better understanding of my results and the factors that could account for the differences between NCDs and LCDs, I interviewed four Contractor Medical Directors (CMDs). These interviews indicated that other factors could account for these differences, including the following: a history of abuse and fraud, contractor's budgets, the CMD's special interests and experience, data analysis capabilities, the number of claims and the novelty of the procedure. The impact of these variables on the differences between national and local coverage policies can be an interesting topic for future research on the subject.",
    "advisors": ["Richard Anders", "Teo Forcht Dagi"],
    "text": "An analysis of the differences between national and local coverage determinations of medical procedures in the US Medicare coverage policies of medical procedures can be promulgated at a national level by the Centers of Medicare and Medicaid Services (CMS) as National Coverage Determinations (NCDs) or at a local level by Medicare contractors as Local Coverage Determinations (LCDs). Although LCDs shouldn't contradict NCDs, they can differ. In the present study, I analyze some factors that could partially account for the differences between NCDs and LCDs. Using the Medicare Coverage Database from CMS, I searched for differences between NCDs and LCDs in five benefit categories: inpatient hospital services, durable medical devices, diagnostic laboratory tests, physician's services and other diagnostic tests. There is a reasonable degree of homogeneity in coverage policies for procedures for which an NCD has been issued: 82% are exactly the same. Most of the differences took the form of exclusions from LCDs, but not from NCDs. For each state, I computed the number of times that LCDs were issued and the number of times that LCDs differed from NCDs and searched for possible linear or exponential correlation models. The following factors were initially hypothesized to account for these differences: number of Schools of Medicine, number of physicians, GDP per capita by state, state ranking according to number of Level 1 and Level 2 Trauma Centers and the profile of MEDCAC members. At a national level, I found no correlation between the number of LCDs issued or the number of differences between LCDs and NCDs and any of these variables. However, on a sub-analysis at a local level, in some regions I found a positive correlation (r2 >.94) between the following three variables: 1) number of Schools of Medicine, 2) number of physicians, and 3) state ranking according to the number of Level 1 and Level 2 Trauma Centers and the following two parameters: 1) the level of LCD issuing activity, and 2) the number of times that LCDs differ from NCDs. The correlations shown by the performed sub-analysis within regions may imply that more LCDs are issued to restrict coverage when there is a local need to control the excessive demand partially driven by the higher number of hospitals and physicians that are active in pursuing their interests. The fact that these correlations were shown only at a regional level may indicate that when local factors are disregarded, the original hypothesized factors do influence LCD activity, however, at a national level, other hypothetical local factors may have a greater influence on LCD activity and policy discrepancies. In order to have a better understanding of my results and the factors that could account for the differences between NCDs and LCDs, I interviewed four Contractor Medical Directors (CMDs). These interviews indicated that other factors could account for these differences, including the following: a history of abuse and fraud, contractor's budgets, the CMD's special interests and experience, data analysis capabilities, the number of claims and the novelty of the procedure. The impact of these variables on the differences between national and local coverage policies can be an interesting topic for future research on the subject."
}, {
    "id": "oai:dspace.mit.edu:1721.1/55274",
    "title": "An investigation of alliances between western life-science therapeutic and Indian firms",
    "abstract": "Large pharmaceutical companies (Multinational Pharmaceutical Companies or MPCs) have struggled in recent years with the rapidly accelerating costs of drug-discovery research and development. These costs continue to rise while resulting in fewer drug leads. Several industries have realized significant cost savings by outsourcing operations to countries with low-cost labor like India and China. Several factors have traditionally kept MPCs from moving high value, patent-sensitive discovery operations to India despite these drastically lower labor costs. However recent improvements in the Indian patent system in response to WTO compliance have stimulated an increase in both domestic investment in innovative research and in deal making within the life science industry. Nonetheless, there are few systematic analyses of the quantity of deal making between international and domestic Indian firms. Based on our analysis, we conclude that MPCs are establishing alliances at a greater rate than Biotechnology-based firms. In addition, we find that the improvements in patent law have created the structures necessary to stimulate innovation-based life science companies to establish relationships with Indian firms that put their most important types of intellectual property at risk.",
    "advisors": ["Fiona Murray", "Martha Gray"],
    "text": "An investigation of alliances between western life-science therapeutic and Indian firms Large pharmaceutical companies (Multinational Pharmaceutical Companies or MPCs) have struggled in recent years with the rapidly accelerating costs of drug-discovery research and development. These costs continue to rise while resulting in fewer drug leads. Several industries have realized significant cost savings by outsourcing operations to countries with low-cost labor like India and China. Several factors have traditionally kept MPCs from moving high value, patent-sensitive discovery operations to India despite these drastically lower labor costs. However recent improvements in the Indian patent system in response to WTO compliance have stimulated an increase in both domestic investment in innovative research and in deal making within the life science industry. Nonetheless, there are few systematic analyses of the quantity of deal making between international and domestic Indian firms. Based on our analysis, we conclude that MPCs are establishing alliances at a greater rate than Biotechnology-based firms. In addition, we find that the improvements in patent law have created the structures necessary to stimulate innovation-based life science companies to establish relationships with Indian firms that put their most important types of intellectual property at risk."
}, {
    "id": "oai:dspace.mit.edu:1721.1/35555",
    "title": "The pharmaco-economics of combination therapies : a study of the effects of component and market factors on combined therapy price",
    "abstract": "For a growing number of indications, combination therapies are becoming increasingly common due in part to their superior efficacy, as compared to monotherapies. In fact, in the case of infectious diseases such as AIDS and tuberculosis, combination therapies are now the standard of care. With the emergence of drug-device combinations, genetic testing, and individualized medicine, this trend towards combination therapies is likely to continue to grow. In this context the pricing of combination therapies is a critical component that needs to be understood by medical practitioners, payors and policy makers. There are three factors to consider in the pricing of combination therapies: the characteristics and structure of the market in which the combined product is sold, the absence or presence of market exclusivity, and the prices of the components of the combined product, when sold individually. When one or more of the components of the combined product has market exclusivity, additional factors such as exclusionary bundling, tying, and double marginalization may come into play.",
    "advisors": ["Frank Douglas"],
    "text": "The pharmaco-economics of combination therapies : a study of the effects of component and market factors on combined therapy price For a growing number of indications, combination therapies are becoming increasingly common due in part to their superior efficacy, as compared to monotherapies. In fact, in the case of infectious diseases such as AIDS and tuberculosis, combination therapies are now the standard of care. With the emergence of drug-device combinations, genetic testing, and individualized medicine, this trend towards combination therapies is likely to continue to grow. In this context the pricing of combination therapies is a critical component that needs to be understood by medical practitioners, payors and policy makers. There are three factors to consider in the pricing of combination therapies: the characteristics and structure of the market in which the combined product is sold, the absence or presence of market exclusivity, and the prices of the components of the combined product, when sold individually. When one or more of the components of the combined product has market exclusivity, additional factors such as exclusionary bundling, tying, and double marginalization may come into play."
}, {
    "id": "oai:dspace.mit.edu:1721.1/28759",
    "title": "Emerin and inherited disease",
    "abstract": "(cont.) nucleus and at the nuclear surface.",
    "advisors": ["Richard Lee"],
    "text": "Emerin and inherited disease (cont.) nucleus and at the nuclear surface."
}, {
    "id": "oai:dspace.mit.edu:1721.1/65522",
    "title": "Impact of the CE mark approval on exit opportunities and validation for early stage medical device companies",
    "abstract": "The aim of this thesis was to look at the impact of acquiring the CE marking approval on the outcome of early stage medical device companies, specifically its impact on strategic acquisition opportunities and on valuation. We gathered data on acquisitions of 237 companies over the past ten years, from April 01, 2002 to March 31, 2011. These data were gathered from various sources, and information on the date of acquisition, enterprise value, funds invested to date, date of incorporation, status and dates of CE and FDA approvals, patent status, type of regulatory clearances (PMA versus 510K), type of sales models (direct versus distributorship), capitalization status and last twelve month stock returns of the acquirer was acquired. These data were then analyzed using basic statistical methods and multivariate linear regression analyses to determine the significance of the CE marking on the outcomes of these companies. Our results support the claim that the CE mark does significantly improve outcomes for early stage medical device companies, in terms of time to strategic acquisition, which is by far the commonest exit route for these companies. On the other hand, we did not find any statistically significant impact of acquisition of the CE mark on the valuation or valuation multiples of these companies. These results have potential implications for management of these early stage medical device companies in making strategic decisions and for investors who are concerned about the exit opportunities and valuations, especially as it relates to funds invested. There could also be some policy implications in terms of the effort, duration and cost of getting a CE approval versus that of an FDA approval, which is especially important given the current growing concern about increasingly stringent regulation, rising costs and increasing delays in FDA approvals for medical devices.",
    "advisors": ["Myron Spector", "Carl Berke"],
    "text": "Impact of the CE mark approval on exit opportunities and validation for early stage medical device companies The aim of this thesis was to look at the impact of acquiring the CE marking approval on the outcome of early stage medical device companies, specifically its impact on strategic acquisition opportunities and on valuation. We gathered data on acquisitions of 237 companies over the past ten years, from April 01, 2002 to March 31, 2011. These data were gathered from various sources, and information on the date of acquisition, enterprise value, funds invested to date, date of incorporation, status and dates of CE and FDA approvals, patent status, type of regulatory clearances (PMA versus 510K), type of sales models (direct versus distributorship), capitalization status and last twelve month stock returns of the acquirer was acquired. These data were then analyzed using basic statistical methods and multivariate linear regression analyses to determine the significance of the CE marking on the outcomes of these companies. Our results support the claim that the CE mark does significantly improve outcomes for early stage medical device companies, in terms of time to strategic acquisition, which is by far the commonest exit route for these companies. On the other hand, we did not find any statistically significant impact of acquisition of the CE mark on the valuation or valuation multiples of these companies. These results have potential implications for management of these early stage medical device companies in making strategic decisions and for investors who are concerned about the exit opportunities and valuations, especially as it relates to funds invested. There could also be some policy implications in terms of the effort, duration and cost of getting a CE approval versus that of an FDA approval, which is especially important given the current growing concern about increasingly stringent regulation, rising costs and increasing delays in FDA approvals for medical devices."
}, {
    "id": "oai:dspace.mit.edu:1721.1/28589",
    "title": "Managing revisions of rules and guidelines used in clinical information systems : exploring a hierarchical knowledge representation model",
    "abstract": "One important purpose for creating clinical practice guidelines is to improve quality of care by reducing variations in practice. In the current healthcare environment, guidelines are being advocated as a means to disseminate research findings, standardize care, improve quality of care, and increase the cost-effectiveness of health care services. Unfortunately, compliance with text-based clinical practice guidelines is unsatisfactory. On the other hand, adherence to guideline recommendations is increased when providers receive patient-specific recommendations during the patient-provider consultation. Guideline-based point of care decision support systems have been shown to increase provider consultation. Guideline-based point of care decision support systems have been shown to increase provider adherence to guideline recommendations. Computer-interpretable formats for clinical practice guidelines are a prerequisite for decision support systems. The development process of a text-based clinical practice guideline is long and arduous and in most cases this process is repeated when text-based guidelines are revised to include new medical knowledge. Clearly, once text-based guideline knowledge is translated into a computer-interpretable format, the computer-interpretable guideline would also require periodic revisions to maintain the integrity of its evidence-base. Therefore, representation formalisms for encoding guideline knowledge into computer-interpretable formats should enable easy revisions of the encoded guidelines. This thesis describes a study I conducted to demonstrate that modular knowledge representation of clinical practice guidelines facilitates easy guideline revisions. To test the hypothesis",
    "advisors": ["Aziz A. Boxwala"],
    "text": "Managing revisions of rules and guidelines used in clinical information systems : exploring a hierarchical knowledge representation model One important purpose for creating clinical practice guidelines is to improve quality of care by reducing variations in practice. In the current healthcare environment, guidelines are being advocated as a means to disseminate research findings, standardize care, improve quality of care, and increase the cost-effectiveness of health care services. Unfortunately, compliance with text-based clinical practice guidelines is unsatisfactory. On the other hand, adherence to guideline recommendations is increased when providers receive patient-specific recommendations during the patient-provider consultation. Guideline-based point of care decision support systems have been shown to increase provider consultation. Guideline-based point of care decision support systems have been shown to increase provider adherence to guideline recommendations. Computer-interpretable formats for clinical practice guidelines are a prerequisite for decision support systems. The development process of a text-based clinical practice guideline is long and arduous and in most cases this process is repeated when text-based guidelines are revised to include new medical knowledge. Clearly, once text-based guideline knowledge is translated into a computer-interpretable format, the computer-interpretable guideline would also require periodic revisions to maintain the integrity of its evidence-base. Therefore, representation formalisms for encoding guideline knowledge into computer-interpretable formats should enable easy revisions of the encoded guidelines. This thesis describes a study I conducted to demonstrate that modular knowledge representation of clinical practice guidelines facilitates easy guideline revisions. To test the hypothesis"
}, {
    "id": "oai:dspace.mit.edu:1721.1/54592",
    "title": "Adoption of healthcare information technology and the impact on clinician behavior",
    "abstract": "It is widely believed that healthcare information technology (health IT) can improve care and lower costs. However, the pattern and uptake of beneficial features of health IT is poorly understood, and is an important part of realizing the full benefits of health IT. This thesis examines the factors relating to adoption and use of reporting features within an outpatient practice management system. A retrospective observational study was performed utilizing web log data from a practice management and electronic health record system vendor. Two years of data were analyzed on the use of features within the system in two different scenarios: the use of a newly released custom reporting feature among existing clients, and the use of a physician-level monthly report among new clients. Among these two different populations and features, the first use and subsequent utilization exhibited similar patterns. Using the Bass model of technology diffusion to quantify the adoption of these features, it was found that adoption had a low social component (coefficient of imitation) and a high personal component (coefficient of innovation). One physician's use of a feature in his practice did not appear to influence whether a new physician joining the same practice would use the feature. In addition, the earliest users of a feature tended to utilize that feature more often. Practices and providers that used these features performed better across three of four operational and financial metrics. The purchase and installation of a health IT system by an organization does not ensure that individuals within it will fully utilize the system and realize all the benefits.",
    "advisors": ["Teo Forcht Dagi", "John Halamka"],
    "text": "Adoption of healthcare information technology and the impact on clinician behavior It is widely believed that healthcare information technology (health IT) can improve care and lower costs. However, the pattern and uptake of beneficial features of health IT is poorly understood, and is an important part of realizing the full benefits of health IT. This thesis examines the factors relating to adoption and use of reporting features within an outpatient practice management system. A retrospective observational study was performed utilizing web log data from a practice management and electronic health record system vendor. Two years of data were analyzed on the use of features within the system in two different scenarios: the use of a newly released custom reporting feature among existing clients, and the use of a physician-level monthly report among new clients. Among these two different populations and features, the first use and subsequent utilization exhibited similar patterns. Using the Bass model of technology diffusion to quantify the adoption of these features, it was found that adoption had a low social component (coefficient of imitation) and a high personal component (coefficient of innovation). One physician's use of a feature in his practice did not appear to influence whether a new physician joining the same practice would use the feature. In addition, the earliest users of a feature tended to utilize that feature more often. Practices and providers that used these features performed better across three of four operational and financial metrics. The purchase and installation of a health IT system by an organization does not ensure that individuals within it will fully utilize the system and realize all the benefits."
}, {
    "id": "oai:dspace.mit.edu:1721.1/78154",
    "title": "Economic potential of a point-of-care CD4+ T cell count diagnostic in Mexico : a case study for low-end disruption diagnostics in middle of the pyramid Latin America",
    "abstract": "Disruptive models of innovation are starting to appear in healthcare. In the US, for instance, retail medicine clinics are changing the way in which patients satisfy their basic medical needs. In Mexico, similar retail medicine models (e.g. Farmacias Similares) are also disrupting healthcare delivery for basic medical needs. Disruptive innovations, however, are not limited to healthcare delivery, but also change the face of devices and diagnostics markets. A low CD4+ T cell count is the primary clinical indicator for HIV/AIDS disease progression, and thus is used as the primary trigger to initiate antiretroviral therapy. An entire diagnostic industry has emerged around CD4+ T cell counts for the management and treatment of HIV/AIDS patients. The diagnostic gold standards of CD4+ counts are flow cytometers. These large, capital intensive devices are commonly located in central laboratory settings, typically in urban areas. In developing nations, particularly, suburban and rural regions have no access to flow cytometers and typically face logistical problems of blood sample transportation and loss to follow-up of patients. Point-of-Care (POC) diagnostics promise disruptive models in diagnostics that will increase access, enhance care, and help better allocate healthcare resources. The concept of POC embodies the trade-off of lower \"quality\" (usually in the form of lower specificity and sensitivity) in exchange for higher \"convenience\" (i.e. better accessibility and portability, and significantly lower cost). POC diagnostics promise typical low-end and new-market disruptions in medical diagnostics and devices. Cambridge-based Daktari Diagnostics is one of such companies focused in POC diagnostics. It has developed a CD4+ T cell count diagnostic device for the management and treatment of HIV/AIDS patients. It is hypothesized in this thesis that there exists a relevant unmet medical need for POC CD4 count diagnostics in the Mexican HIV/AIDS market. In order to evaluate this hypothesis, secondary sources were reviewed, as well as primary interviews conducted across the Mexican HIV/AIDS healthcare landscape. While this hypothesis was evaluated on a preliminary basis only, responses suggested a relevant, albeit not urgent, medical need for POC CD4 count diagnostics. This primary hypothesis evaluation is extended by and complemented with market size estimations, and competitive dynamic discussions, that arrive at the following preliminary conclusions: the current market opportunity in Mexico ranges from baseline of ~100,000 tests per year to an upper bound potential of ~200,000 tests per year. In the context of this potential opportunity, Daktari's CD4 count diagnostic device is well positioned, as defined by diagnostic quality, technological characteristics, and competitive offering, to obtain a portion of this estimated market opportunity in Mexico.",
    "advisors": ["Ernst Berndt", "William Rodriguez"],
    "text": "Economic potential of a point-of-care CD4+ T cell count diagnostic in Mexico : a case study for low-end disruption diagnostics in middle of the pyramid Latin America Disruptive models of innovation are starting to appear in healthcare. In the US, for instance, retail medicine clinics are changing the way in which patients satisfy their basic medical needs. In Mexico, similar retail medicine models (e.g. Farmacias Similares) are also disrupting healthcare delivery for basic medical needs. Disruptive innovations, however, are not limited to healthcare delivery, but also change the face of devices and diagnostics markets. A low CD4+ T cell count is the primary clinical indicator for HIV/AIDS disease progression, and thus is used as the primary trigger to initiate antiretroviral therapy. An entire diagnostic industry has emerged around CD4+ T cell counts for the management and treatment of HIV/AIDS patients. The diagnostic gold standards of CD4+ counts are flow cytometers. These large, capital intensive devices are commonly located in central laboratory settings, typically in urban areas. In developing nations, particularly, suburban and rural regions have no access to flow cytometers and typically face logistical problems of blood sample transportation and loss to follow-up of patients. Point-of-Care (POC) diagnostics promise disruptive models in diagnostics that will increase access, enhance care, and help better allocate healthcare resources. The concept of POC embodies the trade-off of lower \"quality\" (usually in the form of lower specificity and sensitivity) in exchange for higher \"convenience\" (i.e. better accessibility and portability, and significantly lower cost). POC diagnostics promise typical low-end and new-market disruptions in medical diagnostics and devices. Cambridge-based Daktari Diagnostics is one of such companies focused in POC diagnostics. It has developed a CD4+ T cell count diagnostic device for the management and treatment of HIV/AIDS patients. It is hypothesized in this thesis that there exists a relevant unmet medical need for POC CD4 count diagnostics in the Mexican HIV/AIDS market. In order to evaluate this hypothesis, secondary sources were reviewed, as well as primary interviews conducted across the Mexican HIV/AIDS healthcare landscape. While this hypothesis was evaluated on a preliminary basis only, responses suggested a relevant, albeit not urgent, medical need for POC CD4 count diagnostics. This primary hypothesis evaluation is extended by and complemented with market size estimations, and competitive dynamic discussions, that arrive at the following preliminary conclusions: the current market opportunity in Mexico ranges from baseline of ~100,000 tests per year to an upper bound potential of ~200,000 tests per year. In the context of this potential opportunity, Daktari's CD4 count diagnostic device is well positioned, as defined by diagnostic quality, technological characteristics, and competitive offering, to obtain a portion of this estimated market opportunity in Mexico."
}, {
    "id": "oai:dspace.mit.edu:1721.1/38660",
    "title": "Mathematical and mechanical modeling of vaso-occlusion in sickle cell disease",
    "abstract": "Vaso-occlusive crises cause most of the morbidity and mortality associated with sickle cell disease. The proximal causes of these occlusive events are not well understood. The risks and consequences of vaso-occlusion however are clear. Ten percent of sickle cell disease patients will have a stroke by the age of 20. Two thirds of sickle cell disease patients require more than one hospitalization per year for treatment of pain crises. The flow behavior of blood samples from sickle cell patients was studied in an artificial microfluidic environment. This microfluidic environment allowed modulation of the hydrostatic pressure causing flow, the ambient oxygen concentration, and the vascular channel geometry. A range of blood samples was evaluated by selecting specimens with various hematocrits and concentrations of sickle hemoglobin. Velocity profiles were calculated following sudden changes in oxygen concentration. From these profiles, it was possible to create a phase space of vaso-occlusion in the artificial microfluidic environment. This phase space characterizes the environmental conditions in which sickle cell blood will stop flowing within a given interval of time.",
    "advisors": ["L. Mahadevan"],
    "text": "Mathematical and mechanical modeling of vaso-occlusion in sickle cell disease Vaso-occlusive crises cause most of the morbidity and mortality associated with sickle cell disease. The proximal causes of these occlusive events are not well understood. The risks and consequences of vaso-occlusion however are clear. Ten percent of sickle cell disease patients will have a stroke by the age of 20. Two thirds of sickle cell disease patients require more than one hospitalization per year for treatment of pain crises. The flow behavior of blood samples from sickle cell patients was studied in an artificial microfluidic environment. This microfluidic environment allowed modulation of the hydrostatic pressure causing flow, the ambient oxygen concentration, and the vascular channel geometry. A range of blood samples was evaluated by selecting specimens with various hematocrits and concentrations of sickle hemoglobin. Velocity profiles were calculated following sudden changes in oxygen concentration. From these profiles, it was possible to create a phase space of vaso-occlusion in the artificial microfluidic environment. This phase space characterizes the environmental conditions in which sickle cell blood will stop flowing within a given interval of time."
}, {
    "id": "oai:dspace.mit.edu:1721.1/54459",
    "title": "Factors influencing superior returns achieved through mergers & acquisitions of corporate spin-outs in the life sciences",
    "abstract": "Corporate spin-outs have become more frequent in the contemporary business environment as an alternate source of risk diversification and value creation for both the parent and external investors. Once established, corporate spin-outs are often perceived to be of higher quality than their counterparts in the industry; previous studies have shown that they tend to receive higher valuations in financing, faster financing and higher preference by prestigious Wall Street investment banks when they decide to go public. The primary objective of this thesis was to compare the net proceeds associated with successful liquidity events (IPO or M&A) for US based therapeutic-focused corporate spin-outs to industry averages and test the hypothesis that corporate spin-outs generate superior returns. A database containing information on 186 corporate spin-outs within the life sciences (founded from 1990 - present) was generated for the purpose of testing this hypothesis. Net proceeds from corporate spin-out liquidity events were compared to median net proceeds of all biotech/pharmaceutical liquidity events for a given vintage year and type of liquidity event (IPO vs. M&A). Liquidity events were observed with a",
    "advisors": ["T. (Teo) Forcht Dagi", "Carl Berke"],
    "text": "Factors influencing superior returns achieved through mergers & acquisitions of corporate spin-outs in the life sciences Corporate spin-outs have become more frequent in the contemporary business environment as an alternate source of risk diversification and value creation for both the parent and external investors. Once established, corporate spin-outs are often perceived to be of higher quality than their counterparts in the industry; previous studies have shown that they tend to receive higher valuations in financing, faster financing and higher preference by prestigious Wall Street investment banks when they decide to go public. The primary objective of this thesis was to compare the net proceeds associated with successful liquidity events (IPO or M&A) for US based therapeutic-focused corporate spin-outs to industry averages and test the hypothesis that corporate spin-outs generate superior returns. A database containing information on 186 corporate spin-outs within the life sciences (founded from 1990 - present) was generated for the purpose of testing this hypothesis. Net proceeds from corporate spin-out liquidity events were compared to median net proceeds of all biotech/pharmaceutical liquidity events for a given vintage year and type of liquidity event (IPO vs. M&A). Liquidity events were observed with a"
}, {
    "id": "oai:dspace.mit.edu:1721.1/43878",
    "title": "A novel polymeric microelectrode array for highly parallel, long-term neuronal culture and stimulation",
    "abstract": "Cell-based high-throughput screening is emerging as a disruptive technology in drug discovery; however, massively parallel electrical assaying of neurons and cardiomyocites has until now been prohibitively expensive. To address this limitation, we developed a scalable, all-organic 3D microelectrode array technology. The cheap, disposable arrays would be integrated into a fixed stimulation and imaging setup, potentially amenable to automated handling and data analysis. A combination of activity-dependent plasticity, made possible by independent control of up to 64 stimulating electrodes, and, eventually, of substrate chemical patterning would be employed to constrain the neuronal culture network connectivity. In order to ensure longterm survival of the cultures, a bottom feeder layer of glial cells would be grown. In addition to high-throughput screening application, the polymeric microelectrode arrays and integrated stimulation systems were designed to allow the long-term study of synaptic plasticity, combining excellent long-term culture capabilities with a unique ability to independently control each electrode stimulation pattern. The resulting activity could be monitored optically, e,g, with calcium or voltage sensitive dyes, and the images could be stored and processed (possibly even in real time) within the same environment (LabView) as the stimulator. To fabricate the polymeric microelectrode array, we prepare a multilayered mask substrate, by reversibly bonding together two sheets of implant-grade polydimethylsiloxane (PDMS) sheets, with or without a glass coverslip between them. Thanks to PDMS self-adhesive properties the various layers are held together stably but reversibly. The mask is then laser-patterned, using either a standard CO2 laser or a 193 nm excimer laser.",
    "advisors": ["Martha Constantine-Paton", "Ian W. Hunter"],
    "text": "A novel polymeric microelectrode array for highly parallel, long-term neuronal culture and stimulation Cell-based high-throughput screening is emerging as a disruptive technology in drug discovery; however, massively parallel electrical assaying of neurons and cardiomyocites has until now been prohibitively expensive. To address this limitation, we developed a scalable, all-organic 3D microelectrode array technology. The cheap, disposable arrays would be integrated into a fixed stimulation and imaging setup, potentially amenable to automated handling and data analysis. A combination of activity-dependent plasticity, made possible by independent control of up to 64 stimulating electrodes, and, eventually, of substrate chemical patterning would be employed to constrain the neuronal culture network connectivity. In order to ensure longterm survival of the cultures, a bottom feeder layer of glial cells would be grown. In addition to high-throughput screening application, the polymeric microelectrode arrays and integrated stimulation systems were designed to allow the long-term study of synaptic plasticity, combining excellent long-term culture capabilities with a unique ability to independently control each electrode stimulation pattern. The resulting activity could be monitored optically, e,g, with calcium or voltage sensitive dyes, and the images could be stored and processed (possibly even in real time) within the same environment (LabView) as the stimulator. To fabricate the polymeric microelectrode array, we prepare a multilayered mask substrate, by reversibly bonding together two sheets of implant-grade polydimethylsiloxane (PDMS) sheets, with or without a glass coverslip between them. Thanks to PDMS self-adhesive properties the various layers are held together stably but reversibly. The mask is then laser-patterned, using either a standard CO2 laser or a 193 nm excimer laser."
}, {
    "id": "oai:dspace.mit.edu:1721.1/33081",
    "title": "ModuleFinder : a computational model for the identification of Cis regulatory modules",
    "abstract": "Regulation of gene expression occurs largely through the binding of sequence- specific transcription factors (TFs) to genomic DNA binding sites (BSs). This thesis presents a rigorous scoring scheme, implemented as a C program termed \"ModuleFinder\", that evaluates the likelihood that a given genomic region is a cis regulatory module (CRM) for an input set of TFs according to its degree of: (1) homotypic site clustering; (2) heterotypic site clustering; and (3) evolutionary conservation across multiple genomes. Importantly, ModuleFinder obtains all parameters needed to appropriately weight the relative contributions of these sequence features directly from the input sequences and TFBS motifs, and does not need to first be trained. Using two previously described collections of experimentally verified CRMs in mammals as validation datasets, we show that ModuleFinder is able to identify CRMs with great sensitivity and specificity. We also evaluated ModuleFinder on a set of DNA binding site data for the human TFs Hepatocyte Nuclear Factor HNF1 [alpha], HNF4 [alpha] and HNF6 and compared its performance with logistic regression and neural network models.",
    "advisors": ["Marth L. Bulyk"],
    "text": "ModuleFinder : a computational model for the identification of Cis regulatory modules Regulation of gene expression occurs largely through the binding of sequence- specific transcription factors (TFs) to genomic DNA binding sites (BSs). This thesis presents a rigorous scoring scheme, implemented as a C program termed \"ModuleFinder\", that evaluates the likelihood that a given genomic region is a cis regulatory module (CRM) for an input set of TFs according to its degree of: (1) homotypic site clustering; (2) heterotypic site clustering; and (3) evolutionary conservation across multiple genomes. Importantly, ModuleFinder obtains all parameters needed to appropriately weight the relative contributions of these sequence features directly from the input sequences and TFBS motifs, and does not need to first be trained. Using two previously described collections of experimentally verified CRMs in mammals as validation datasets, we show that ModuleFinder is able to identify CRMs with great sensitivity and specificity. We also evaluated ModuleFinder on a set of DNA binding site data for the human TFs Hepatocyte Nuclear Factor HNF1 [alpha], HNF4 [alpha] and HNF6 and compared its performance with logistic regression and neural network models."
}, {
    "id": "oai:dspace.mit.edu:1721.1/54458",
    "title": "Visualizing the conversation pathways of telephone linked care in a directed graph",
    "abstract": "Telephone linked care (TLC) is a telehealth intervention that has been shown to be effective in a variety of clinical settings. TLC is an interactive computerized telephone system. The system 'speaks' to patients, asking them questions and understanding their responses. There is logic built into the calls, so that a patient's response to a question will dictate the next question that is asked. This serves to personalize the call for each patient, and makes the conversation more realistic. All of the patients' responses are stored in a database. This database provides much opportunity for analysis because a single phone call contains many responses. Visualization is an important way of gaining insight into data. Visualization can make data easier to understand and process. Different aspects of data can be encoded in a visualization. The TLC data lends itself to visualization. By viewing each of the questions that the system asks as nodes, and connecting the nodes by the chronological order in which these questions are asked, a tree structure will reveal the conversational paths that are taken in the calls. By combining data from multiple calls and encoding them in this tree structure, new insights can be gained into the TLC data. For example, the frequency with which questions are answered in a particular way can be encoded to reveal the most common pathways through the tree. This paper describes a visualization application of TLC data which will allow researchers to gain new insights into the TLC conversations and into medical interviews in general.",
    "advisors": ["Robert Friedman", "Peter Szolovits"],
    "text": "Visualizing the conversation pathways of telephone linked care in a directed graph Telephone linked care (TLC) is a telehealth intervention that has been shown to be effective in a variety of clinical settings. TLC is an interactive computerized telephone system. The system 'speaks' to patients, asking them questions and understanding their responses. There is logic built into the calls, so that a patient's response to a question will dictate the next question that is asked. This serves to personalize the call for each patient, and makes the conversation more realistic. All of the patients' responses are stored in a database. This database provides much opportunity for analysis because a single phone call contains many responses. Visualization is an important way of gaining insight into data. Visualization can make data easier to understand and process. Different aspects of data can be encoded in a visualization. The TLC data lends itself to visualization. By viewing each of the questions that the system asks as nodes, and connecting the nodes by the chronological order in which these questions are asked, a tree structure will reveal the conversational paths that are taken in the calls. By combining data from multiple calls and encoding them in this tree structure, new insights can be gained into the TLC data. For example, the frequency with which questions are answered in a particular way can be encoded to reveal the most common pathways through the tree. This paper describes a visualization application of TLC data which will allow researchers to gain new insights into the TLC conversations and into medical interviews in general."
}, {
    "id": "oai:dspace.mit.edu:1721.1/57804",
    "title": "End of life resuscitation patterns : a socio-demographic study of intensive care unit patients by Sharon L. Lojun.",
    "abstract": "This study investigates the effect of age, gender, medical condition, and daily free text input on classification accuracy for resuscitation code status. Data was extracted from the MIMICII database. Natural language processing (NLP) was used to evaluate the social section of the nurses' progress notes. BoosTexter was used to predict the code-status using text, age, gender, and SAPS scoring. The relative impact of features was analyzed by feature ablation. Social text was the greatest single indicator of code status. The addition of text to medical condition features increased classification accuracy significantly (p<0.001.) N-gram frequency was analyzed. Gender differences were noted across all code-statuses, with women always more frequent (e.g. wife>husband.) Visitors and contact were more common in the less aggressive resuscitation codes. Logistic regression on medical, age, and gender features was used to determine gender bias or ageism. Evidence of bias was found; both females (OR=1.47) and patients over age 70 (OR=3.72) were more likely to be DNR. Feature ablation was also applied to the social section of physician discharge summaries, as well as to annotated features. The addition of annotated features increased classification accuracy, but the nursing social text remained the most individually predictive. The annotated features included: children; living situation; marital status; and working status. Having zero to one child; living alone or in an institution; being divorced or widow or widower; and working, working in white collar job, or being retired, were all associated with higher rates of DNR status, and lower rates of FC status. Contrarily, living with family; being married; and being unemployed, were all associated with lower rates of DNR status, and higher rates of FC status. Some of these findings were gender and/or age dependent.",
    "advisors": ["Regina Barzilay"],
    "text": "End of life resuscitation patterns : a socio-demographic study of intensive care unit patients by Sharon L. Lojun. This study investigates the effect of age, gender, medical condition, and daily free text input on classification accuracy for resuscitation code status. Data was extracted from the MIMICII database. Natural language processing (NLP) was used to evaluate the social section of the nurses' progress notes. BoosTexter was used to predict the code-status using text, age, gender, and SAPS scoring. The relative impact of features was analyzed by feature ablation. Social text was the greatest single indicator of code status. The addition of text to medical condition features increased classification accuracy significantly (p<0.001.) N-gram frequency was analyzed. Gender differences were noted across all code-statuses, with women always more frequent (e.g. wife>husband.) Visitors and contact were more common in the less aggressive resuscitation codes. Logistic regression on medical, age, and gender features was used to determine gender bias or ageism. Evidence of bias was found; both females (OR=1.47) and patients over age 70 (OR=3.72) were more likely to be DNR. Feature ablation was also applied to the social section of physician discharge summaries, as well as to annotated features. The addition of annotated features increased classification accuracy, but the nursing social text remained the most individually predictive. The annotated features included: children; living situation; marital status; and working status. Having zero to one child; living alone or in an institution; being divorced or widow or widower; and working, working in white collar job, or being retired, were all associated with higher rates of DNR status, and lower rates of FC status. Contrarily, living with family; being married; and being unemployed, were all associated with lower rates of DNR status, and higher rates of FC status. Some of these findings were gender and/or age dependent."
}, {
    "id": "oai:dspace.mit.edu:1721.1/65513",
    "title": "Optimizing a protein-RNA aptamer gene regulatory system using an engineered peptide library",
    "abstract": "For this project, N-terminal and C-terminal peptide library fusions were designed,bconstructed, and screened in order to improve the repression achievable with a novel gene regulatory system. This system, based on the interaction between proteins and proteinbinding RNA aptamers, takes advantage of the reversible interaction between TetR and its RNA aptamer binding partner 5-1.2 to modulate gene expression. With no tetracyclines present, TetR preferentially binds to aptamer 5-1.2 in the mRNA of a gene of interest with low nanomolar affinity and represses translation. Tetracyclines such as aTc induce a conformation change in TetR, prevent TetR binding to aptamer 5-1.2, and induce gene expression. Therefore, TetR binds aptamer 5-1.2 in an aTc-dependent manner, allowing inducible control of gene expression through the TetR-aptamer system. Initial characterization showed a regulatory range of 78% or approximately 5 fold in S. cerevisiae. The aim of this project is to improve repression levels achievable with the TetR-aptamer system by creating libraries of N-terminal and C-terminal peptide fusions to TetR and screening for increased repression in S. cerevisiae. The N-terminal and C-terminal library fusions were constructed from synthesized oligonucleotide fragments and a baseline TetR vector containing library insertions sites at both the N-terminal and C-terminal ends. The library fragments contain 20 random amino acids and a standard SSG linker peptide flanked by both single-cutting restriction enzyme sites and 40 bases of homology to the library insertion sites on the baseline TetR vector, allowing for construction by both restriction/ligation cloning in bacteria and yeast homologous recombination. Both libraries were constructed using restriction/ligation cloning after initial experiments determined optimized conditions for PCR, digest, purification, ligation, and electrocompetent bacterial transformation to achieve a maximum efficiency, fidelity, and purity. The N-terminal and C-terminal libraries produced have a combined diversity of 2.5x 105 variants. These library variants were screened using a plate-based assay with URA3 as a reporter gene. A selection with 5-fluoroorotic acid (5-FOA) was performed to identify library variants with improved repression. Since 5-FOA is a competitive inhibitor of URA3, cells that have URA3 expression cannot live on media containing 5-FOA. Preliminary experiments determined that 0.035% 5-FOA is the threshold for growth for the baseline",
    "advisors": ["Jacquin Niles"],
    "text": "Optimizing a protein-RNA aptamer gene regulatory system using an engineered peptide library For this project, N-terminal and C-terminal peptide library fusions were designed,bconstructed, and screened in order to improve the repression achievable with a novel gene regulatory system. This system, based on the interaction between proteins and proteinbinding RNA aptamers, takes advantage of the reversible interaction between TetR and its RNA aptamer binding partner 5-1.2 to modulate gene expression. With no tetracyclines present, TetR preferentially binds to aptamer 5-1.2 in the mRNA of a gene of interest with low nanomolar affinity and represses translation. Tetracyclines such as aTc induce a conformation change in TetR, prevent TetR binding to aptamer 5-1.2, and induce gene expression. Therefore, TetR binds aptamer 5-1.2 in an aTc-dependent manner, allowing inducible control of gene expression through the TetR-aptamer system. Initial characterization showed a regulatory range of 78% or approximately 5 fold in S. cerevisiae. The aim of this project is to improve repression levels achievable with the TetR-aptamer system by creating libraries of N-terminal and C-terminal peptide fusions to TetR and screening for increased repression in S. cerevisiae. The N-terminal and C-terminal library fusions were constructed from synthesized oligonucleotide fragments and a baseline TetR vector containing library insertions sites at both the N-terminal and C-terminal ends. The library fragments contain 20 random amino acids and a standard SSG linker peptide flanked by both single-cutting restriction enzyme sites and 40 bases of homology to the library insertion sites on the baseline TetR vector, allowing for construction by both restriction/ligation cloning in bacteria and yeast homologous recombination. Both libraries were constructed using restriction/ligation cloning after initial experiments determined optimized conditions for PCR, digest, purification, ligation, and electrocompetent bacterial transformation to achieve a maximum efficiency, fidelity, and purity. The N-terminal and C-terminal libraries produced have a combined diversity of 2.5x 105 variants. These library variants were screened using a plate-based assay with URA3 as a reporter gene. A selection with 5-fluoroorotic acid (5-FOA) was performed to identify library variants with improved repression. Since 5-FOA is a competitive inhibitor of URA3, cells that have URA3 expression cannot live on media containing 5-FOA. Preliminary experiments determined that 0.035% 5-FOA is the threshold for growth for the baseline"
}, {
    "id": "oai:dspace.mit.edu:1721.1/28760",
    "title": "Context identification in electronic medical records",
    "abstract": "In order to automate data extraction from electronic medical documents, it is important to identify the correct context of the extracted information. Context in medical documents is provided by the layout of documents, which are partitioned into sections by virtue of a medical culture instilled through common practice and the training of physicians. Unfortunately, formatting and labeling is inconsistently adhered to in practice and human experts are usually required to identify sections in medical documents. A series of experiments tested the hypothesis that section identification independent of the label on sections could be achieved by using a neural network to elucidate relationships between features of sections (like size, position from start of the document) and the content characteristic of certain sections (subject-specific strings). Results showed that certain sections can be reliably identified using two different methods, and described the costs involved. The stratification of documents by document type (such as History and Physical Examination Documents or Discharge Summaries), patient diagnoses and department influenced the accuracy of identification. Future improvements suggested by the results in order to fully outline the approach were described.",
    "advisors": ["Aziz Boxwala"],
    "text": "Context identification in electronic medical records In order to automate data extraction from electronic medical documents, it is important to identify the correct context of the extracted information. Context in medical documents is provided by the layout of documents, which are partitioned into sections by virtue of a medical culture instilled through common practice and the training of physicians. Unfortunately, formatting and labeling is inconsistently adhered to in practice and human experts are usually required to identify sections in medical documents. A series of experiments tested the hypothesis that section identification independent of the label on sections could be achieved by using a neural network to elucidate relationships between features of sections (like size, position from start of the document) and the content characteristic of certain sections (subject-specific strings). Results showed that certain sections can be reliably identified using two different methods, and described the costs involved. The stratification of documents by document type (such as History and Physical Examination Documents or Discharge Summaries), patient diagnoses and department influenced the accuracy of identification. Future improvements suggested by the results in order to fully outline the approach were described."
}, {
    "id": "oai:dspace.mit.edu:1721.1/33845",
    "title": "An ontology model for clinical documentation templates",
    "abstract": "There are various kinds of clinical documents used in a hospital or clinic setting. With the emergence of Electronic Medical Records, efforts are being made to computerize these documents in a structured fashion in order to enable decision support. With structured data entry, because each fact about the patient is stored discretely and can be retrieved separately, information can be organized and presented in different ways, depending on the needs of the user. A typical structured clinical document contains a range of findings recorded by a physician, nurse or other care. These findings can be thought of as discrete pieces of information, called observations. These observations can be grouped together to form observation sets that can be placed under relevant headers within the document. When building information systems that support structured clinical documentation, these observations and sets are created and stored in catalogs. My thesis addresses the issue of building an ontology model for clinical documentation that supports the creation and management of an observations catalog, observation sets catalog and a clinical document catalog. The ontology can be used as an organizational tool for efficient maintenance of these catalogs. By tagging observations and observation sets with relevant attributes, it is possible to generate intelligent displays of data that are more flexible and dynamic.",
    "advisors": ["Aziz Boxwala"],
    "text": "An ontology model for clinical documentation templates There are various kinds of clinical documents used in a hospital or clinic setting. With the emergence of Electronic Medical Records, efforts are being made to computerize these documents in a structured fashion in order to enable decision support. With structured data entry, because each fact about the patient is stored discretely and can be retrieved separately, information can be organized and presented in different ways, depending on the needs of the user. A typical structured clinical document contains a range of findings recorded by a physician, nurse or other care. These findings can be thought of as discrete pieces of information, called observations. These observations can be grouped together to form observation sets that can be placed under relevant headers within the document. When building information systems that support structured clinical documentation, these observations and sets are created and stored in catalogs. My thesis addresses the issue of building an ontology model for clinical documentation that supports the creation and management of an observations catalog, observation sets catalog and a clinical document catalog. The ontology can be used as an organizational tool for efficient maintenance of these catalogs. By tagging observations and observation sets with relevant attributes, it is possible to generate intelligent displays of data that are more flexible and dynamic."
}, {
    "id": "oai:dspace.mit.edu:1721.1/33080",
    "title": "The effect of smooth muscle antagonists on the sound-induced motion of the tympanic membrane",
    "abstract": "The pars tensa of the tympanic membrane is composed of three layers: an epidermal layer, a fibrous layer, and a mucosal layer. Recent studies (Kuijpers et al, 1999; Henson and Henson, 2000; Henson et al, 2005) suggest that the fibrous layer in several mammalian species contains contractile fibers, which are located primarily within the thickened border of the pars tensa known as the annulus fibrosis. These contractile fibers resemble smooth muscle fibers. Yang and Henson (2002) studied the physiological effects of pharmacological modulators on the pars tensa of the annulus fibrosis by measuring the sound-induced cochlear response. Their results suggest a dose-dependent change in cochlear response after application of sodium orthovanadate and norepinephrine. Application of saline induced no change in cochlear response. Based on their data, Yang and Henson proposed that the pharmacological agents altered the function of the smooth muscle fibers of the annulus fibrosis to produce a mechanical change in the tympanic membrane. In this study two measurements, cochlear response and Laser Doppler Vibrometry, were used to assess the sound-induced velocity of the tympanic membrane of the gerbil before and after application of saline and varying concentrations of three smooth muscle antagonists (sodium orthovanadate, norepinephrine, and carbachol) to the pars tensa. It was demonstrated that applications of saline and varying concentrations of sodium orthovanadate were associated with both increases and decreases in the magnitude of the cochlear response in two out of three ears tested. There was no evidence of a dose-dependent change in the cochlear response.",
    "advisors": ["John Rosowski"],
    "text": "The effect of smooth muscle antagonists on the sound-induced motion of the tympanic membrane The pars tensa of the tympanic membrane is composed of three layers: an epidermal layer, a fibrous layer, and a mucosal layer. Recent studies (Kuijpers et al, 1999; Henson and Henson, 2000; Henson et al, 2005) suggest that the fibrous layer in several mammalian species contains contractile fibers, which are located primarily within the thickened border of the pars tensa known as the annulus fibrosis. These contractile fibers resemble smooth muscle fibers. Yang and Henson (2002) studied the physiological effects of pharmacological modulators on the pars tensa of the annulus fibrosis by measuring the sound-induced cochlear response. Their results suggest a dose-dependent change in cochlear response after application of sodium orthovanadate and norepinephrine. Application of saline induced no change in cochlear response. Based on their data, Yang and Henson proposed that the pharmacological agents altered the function of the smooth muscle fibers of the annulus fibrosis to produce a mechanical change in the tympanic membrane. In this study two measurements, cochlear response and Laser Doppler Vibrometry, were used to assess the sound-induced velocity of the tympanic membrane of the gerbil before and after application of saline and varying concentrations of three smooth muscle antagonists (sodium orthovanadate, norepinephrine, and carbachol) to the pars tensa. It was demonstrated that applications of saline and varying concentrations of sodium orthovanadate were associated with both increases and decreases in the magnitude of the cochlear response in two out of three ears tested. There was no evidence of a dose-dependent change in the cochlear response."
}, {
    "id": "oai:dspace.mit.edu:1721.1/107333",
    "title": "Can the phased array stimulation strategy be implemented using the advanced bionics cochlear implant?",
    "abstract": "Cochlear implants are devices that aim to restore a measure of hearing to the deaf by converting acoustic signals to electric stimuli delivered to electrodes implanted in the inner ear. Theoretically, the phased array stimulation strategy described by van den Honert and Kelsall (2007) provides much better control over the neural excitation patters elicited by electric stimulation by taking advantage of potential field superposition in the implanted cochlea, to construct stimuli for optimally selective excitation of auditory nerve fibers. If the phased array strategy can be implemented using a commonly-implanted commercial cochlear implant system, the strategy could be effectively evaluated in a relatively large sample of patients to determine whether it provides better speech reception than currently available systems. This thesis investigates whether the phased array strategy can be implemented using the Advanced Bionics Clarion CH or HiRes90k cochlear implant. It is shown that for realistic cochlear implant electrode impedance magnitudes, the Advanced Bionics cochlear implant current sources will deliver monopolar current suitable for the necessary measurement of transimpedance with less than 7% error. Transimpedance matrix estimates were obtained in 11 ears in 10 cochlear implant subjects. Measurements reveal that in some test subjects, low impedance current paths exist between implanted electrodes that may cause current leakage through unintended electrodes. Researchers and clinicians should consider using this transimpedance matrix estimation technique to screen for patients or research subjects who could benefit from compensatory changes to their speech processors. The results of this thesis suggest that the phased array strategy can be implemented successfully when the limitations of the internal power supply documented in this document are taken into account. It is recommended that the transimpedance matrix in a given test subject be measured on the day of any psychophysical testing because of the potential impact of variability in transimpedance over time.",
    "advisors": ["Donald K. Eddington"],
    "text": "Can the phased array stimulation strategy be implemented using the advanced bionics cochlear implant? Cochlear implants are devices that aim to restore a measure of hearing to the deaf by converting acoustic signals to electric stimuli delivered to electrodes implanted in the inner ear. Theoretically, the phased array stimulation strategy described by van den Honert and Kelsall (2007) provides much better control over the neural excitation patters elicited by electric stimulation by taking advantage of potential field superposition in the implanted cochlea, to construct stimuli for optimally selective excitation of auditory nerve fibers. If the phased array strategy can be implemented using a commonly-implanted commercial cochlear implant system, the strategy could be effectively evaluated in a relatively large sample of patients to determine whether it provides better speech reception than currently available systems. This thesis investigates whether the phased array strategy can be implemented using the Advanced Bionics Clarion CH or HiRes90k cochlear implant. It is shown that for realistic cochlear implant electrode impedance magnitudes, the Advanced Bionics cochlear implant current sources will deliver monopolar current suitable for the necessary measurement of transimpedance with less than 7% error. Transimpedance matrix estimates were obtained in 11 ears in 10 cochlear implant subjects. Measurements reveal that in some test subjects, low impedance current paths exist between implanted electrodes that may cause current leakage through unintended electrodes. Researchers and clinicians should consider using this transimpedance matrix estimation technique to screen for patients or research subjects who could benefit from compensatory changes to their speech processors. The results of this thesis suggest that the phased array strategy can be implemented successfully when the limitations of the internal power supply documented in this document are taken into account. It is recommended that the transimpedance matrix in a given test subject be measured on the day of any psychophysical testing because of the potential impact of variability in transimpedance over time."
}, {
    "id": "oai:dspace.mit.edu:1721.1/78155",
    "title": "Market incentives for pandemic influenza vaccines",
    "abstract": "It has been estimated that 100 million plus individuals could perish if a virulent influenza pandemic were to occur. In wake of the 2009-10 H1N1 pandemic and in an era of economic austerity, however, industry lacks clear incentives to invest in vaccines for other high-risk strains. The cyclic nature of pandemics also means we can expect another influenza pandemic within the next 20 years. In this environment, design of incentive mechanisms for funding development of vaccines against strains with known pandemic potential, but for whom vaccine technology is currently lacking, would be welcomed. This research explores which novel incentive mechanisms could induce investment in and development of processes for production of vaccines for these high risk strains. Interviews with vaccine developers and funding agencies and analysis of the pipeline of influenza vaccines in development were conducted. This thesis finds that there is a dearth of vaccines against influenza strains of known pandemic potential, such as H2, H7 and H9; that current pandemic preparedness efforts are not focused on these strains; that funding for pandemic preparedness efforts in H2, H7 and H9 would help incentivize development of vaccines against these strains; and that support for seasonal influenza, regulatory changes, alignment of public and private sector goals, and increased vaccine acceptance are also required to incentivize the development of vaccines against strains of known pandemic potential such as H2, H7 and H9. Furthermore, this thesis recommends that policy makers increase funding for pandemic preparedness so that programs may be initiated or expanded to include additional high risk influenza strains; that US and EU regulatory regimes for pandemic influenza vaccines be harmonized; and that governments promote public awareness of the importance of influenza vaccination.",
    "advisors": ["Fiona E Murray", "Aaron S. Kesselheim"],
    "text": "Market incentives for pandemic influenza vaccines It has been estimated that 100 million plus individuals could perish if a virulent influenza pandemic were to occur. In wake of the 2009-10 H1N1 pandemic and in an era of economic austerity, however, industry lacks clear incentives to invest in vaccines for other high-risk strains. The cyclic nature of pandemics also means we can expect another influenza pandemic within the next 20 years. In this environment, design of incentive mechanisms for funding development of vaccines against strains with known pandemic potential, but for whom vaccine technology is currently lacking, would be welcomed. This research explores which novel incentive mechanisms could induce investment in and development of processes for production of vaccines for these high risk strains. Interviews with vaccine developers and funding agencies and analysis of the pipeline of influenza vaccines in development were conducted. This thesis finds that there is a dearth of vaccines against influenza strains of known pandemic potential, such as H2, H7 and H9; that current pandemic preparedness efforts are not focused on these strains; that funding for pandemic preparedness efforts in H2, H7 and H9 would help incentivize development of vaccines against these strains; and that support for seasonal influenza, regulatory changes, alignment of public and private sector goals, and increased vaccine acceptance are also required to incentivize the development of vaccines against strains of known pandemic potential such as H2, H7 and H9. Furthermore, this thesis recommends that policy makers increase funding for pandemic preparedness so that programs may be initiated or expanded to include additional high risk influenza strains; that US and EU regulatory regimes for pandemic influenza vaccines be harmonized; and that governments promote public awareness of the importance of influenza vaccination."
}, {
    "id": "oai:dspace.mit.edu:1721.1/111321",
    "title": "The Disque Platform for the investigation of islet differentiation to study, treat, and cure Type 1 Diabetes",
    "abstract": "There is a critical health care need to generate large numbers of beta cells for transplantation. In Type 1 Diabetes (T1D), insulin-producing beta cells in the islets of Langerhans within the pancreas, which support glucose homeostasis, are destroyed in an autoimmune attack. The ensuing loss of glycemic control leads to serious complications, requiring life-long insulin injections and close monitoring, while shortening lifespan by 11-13 years. In the face of a three percent annual increase in T1D incidence, there is a grave lack of transplantable material, and very few patients are able to receive an islet transplant each year. Recent advances in stem cell differentiation have enabled the production of large quantities of insulin-producing beta-like cells in vitro, bringing hope to the field. However, the efficiency and yield of such production methods remains unacceptably low, with high batch-to-batch variability, and the function of these cells is unstable. Moreover, the ability to probe the conditions that affect differentiation outcomes is limited by the scale, cost, and complexity of existing culture systems. The present work focuses on the Disque Platform, a biomimetic screening platform for the investigation of islet cell differentiation. The Disque Platform allows for the formation of differentiating 3D micro-tissues within an automation-friendly design, and is capable of systematically manipulating the developing stem cell niche in order to identify chemical and physical cues that enhance beta cell production. Significantly, the Disque Platform consistently differentiates beta-like cells from pancreatic progenitor cells, with similar efficacy to existing high-volume production methods. Furthermore, it achieves superior differentiation outcomes compared to the 2D culture systems tested, and is able to respond to interventions when conventional systems cannot produce a clear signal or readout. Together, these data support the ability of the Disque Platform to investigate specific interventions to enhance beta-cell differentiation. It is hoped that the Disque Platform can serve as a springboard for beta cell and islet study within the diabetes community, and that these advances can contribute towards a cure for Type 1 Diabetes.",
    "advisors": ["Jeffrey M. Karp"],
    "text": "The Disque Platform for the investigation of islet differentiation to study, treat, and cure Type 1 Diabetes There is a critical health care need to generate large numbers of beta cells for transplantation. In Type 1 Diabetes (T1D), insulin-producing beta cells in the islets of Langerhans within the pancreas, which support glucose homeostasis, are destroyed in an autoimmune attack. The ensuing loss of glycemic control leads to serious complications, requiring life-long insulin injections and close monitoring, while shortening lifespan by 11-13 years. In the face of a three percent annual increase in T1D incidence, there is a grave lack of transplantable material, and very few patients are able to receive an islet transplant each year. Recent advances in stem cell differentiation have enabled the production of large quantities of insulin-producing beta-like cells in vitro, bringing hope to the field. However, the efficiency and yield of such production methods remains unacceptably low, with high batch-to-batch variability, and the function of these cells is unstable. Moreover, the ability to probe the conditions that affect differentiation outcomes is limited by the scale, cost, and complexity of existing culture systems. The present work focuses on the Disque Platform, a biomimetic screening platform for the investigation of islet cell differentiation. The Disque Platform allows for the formation of differentiating 3D micro-tissues within an automation-friendly design, and is capable of systematically manipulating the developing stem cell niche in order to identify chemical and physical cues that enhance beta cell production. Significantly, the Disque Platform consistently differentiates beta-like cells from pancreatic progenitor cells, with similar efficacy to existing high-volume production methods. Furthermore, it achieves superior differentiation outcomes compared to the 2D culture systems tested, and is able to respond to interventions when conventional systems cannot produce a clear signal or readout. Together, these data support the ability of the Disque Platform to investigate specific interventions to enhance beta-cell differentiation. It is hoped that the Disque Platform can serve as a springboard for beta cell and islet study within the diabetes community, and that these advances can contribute towards a cure for Type 1 Diabetes."
}, {
    "id": "oai:dspace.mit.edu:1721.1/63225",
    "title": "Auditory pathway responses to parametrized vowels in autism spectrum disorders",
    "abstract": "Autism spectrum disorder (ASD) is characterized by many behavioral symptoms, including delays in social and communicative development. A cluster of symptoms concentrate on speech and language development, especially manipulation of non-verbal information conveyed in prosody. It is largely unknown whether this is due to functional or structural differences in the brain regions involved in auditory and speech processing, although recent studies have shown that ASD individuals do exhibit different activation patterns in various brain regions in response to speech stimuli. This study investigated responses in regions of the auditory pathway to short recorded and synthesized vowel stimuli. These regions were the Inferior Colliculus, the Left Thalamus, the left Posterior Insula, the Auditory Cortex, Wernicke's area, and Broca's area. The stimuli were parametrized so as to target different signal processing capabilities associated with each region. They were presented to ASD and typically developing (TD) subjects while the salient regions were subject to a functional magnetic resonance imaging (fMRI). The results suggest that there were not gross differences in how ASD individuals responded from TD individuals in the subcortical regions. Results from the Auditory Cortex, however, showed a significant hemisphere dominance in TD subjects with more temporally complex stimuli that did not appear in ASD subjects. Moreover, the results showed that it was temporally-measured periodicities in the signal that were responsible for this difference. The results also show slightly different activation patterns in cortical regions which could have implications for attentiveness, and semantic and emotional processing. These results suggest that deficiencies in the temporal processing capabilities of the left Auditory Cortex play a major role in ASD speech processing.",
    "advisors": ["Nicholas Lange"],
    "text": "Auditory pathway responses to parametrized vowels in autism spectrum disorders Autism spectrum disorder (ASD) is characterized by many behavioral symptoms, including delays in social and communicative development. A cluster of symptoms concentrate on speech and language development, especially manipulation of non-verbal information conveyed in prosody. It is largely unknown whether this is due to functional or structural differences in the brain regions involved in auditory and speech processing, although recent studies have shown that ASD individuals do exhibit different activation patterns in various brain regions in response to speech stimuli. This study investigated responses in regions of the auditory pathway to short recorded and synthesized vowel stimuli. These regions were the Inferior Colliculus, the Left Thalamus, the left Posterior Insula, the Auditory Cortex, Wernicke's area, and Broca's area. The stimuli were parametrized so as to target different signal processing capabilities associated with each region. They were presented to ASD and typically developing (TD) subjects while the salient regions were subject to a functional magnetic resonance imaging (fMRI). The results suggest that there were not gross differences in how ASD individuals responded from TD individuals in the subcortical regions. Results from the Auditory Cortex, however, showed a significant hemisphere dominance in TD subjects with more temporally complex stimuli that did not appear in ASD subjects. Moreover, the results showed that it was temporally-measured periodicities in the signal that were responsible for this difference. The results also show slightly different activation patterns in cortical regions which could have implications for attentiveness, and semantic and emotional processing. These results suggest that deficiencies in the temporal processing capabilities of the left Auditory Cortex play a major role in ASD speech processing."
}, {
    "id": "oai:dspace.mit.edu:1721.1/68468",
    "title": "Evaluation of external ventricular drain complications and the use of a procedure-targeted image-guidance system",
    "abstract": "Access to the cerebral ventricle (e.g. ventriculostomy) is required to manage multiple life-threatening ailments. It can be done either in the operating room or at the bedside to relieve increased intracranial pressure or deliver medication. At the bedside, the procedure is normally performed freehand, with the occasional use of a Ghajar guide for guidance support. In the operating room, ventriculostomy may be performed with an image-guidance system, whether optical or electromagnetic. The most common complications of ventriculostomy are hemorrhage and infection. It is unclear whether catheter placement accuracy and the number of passes of the catheter for each placement are correlated with ventriculostomy complications. Our goals are 1) to evaluate the current state of practice, including complications of ventriculostomy, and 2) to evaluate a targeted image guidance system for use with ventriculostomy - the Smart Stylet. To address these goals, an Institutional Review Board-approved retrospective cross-sectional study was conducted at the Brigham and Women's Hospital (BWH) to characterize the practice of external ventricular drain placements using data from the patient electronic medical record. Post-procedure catheter location was measured on post-procedure CT and MRI imaging studies. Most cases were performed in the operating room and the operative reports provided all procedure-related information. Microbiology reports were collected within a four-week interval following catheter placements to evaluate presence of invading pathogens. All imaging studies, microbiology reports, and operative reports were reviewed manually. The rest of the medical records were not reviewed and, therefore, cerebrospinal fluid leak and shunt malfunction were not evaluated. Catheter placement accuracy and the numbers of passes for each placement were assessed. We evaluated whether these metrics were associated with the occurrence of procedure complications. A procedure-targeted image guidance system in development stage, the Smart Stylet, was implemented for use on a ventricular phantom model with a right-sided midline shift. Smart Stylet consists of an electromagnetic tracking system and ventriculostomy catheter connected to a PC and display. The operator of the Smart Stylet can interface with the system via a custom designed module in BWH's 3DSlicer software system. The system was tested for accuracy by calculating targeting error and reporting the precision of catheter placement. Precision was measured using pair-wise distances among experimental groups. The system was reviewed and commented on by three novices and two neurosurgical residents from the Massachusetts General Hospital by using the NASA-TLX grading scale questionnaire and a targeted survey. The phantom model was designed to gauge whether further tests in animals and cadavers are warranted using Smart Stylet. Patients with trauma were more likely to have catheters misplaced (OR = 9.132.31; p<0.05). It seems there is an opportunity to improve patient care if catheter placement is made more accurate and reliable. Use of the Smart Stylet system in a phantom study provided improvements in mean pair-wise distance and accuracy for catheter placement at the sub-centimeter level. A blinded operator achieved statistically significant improvement in targeting error using the right frontal approach (p<0.0 5). The operator also significantly improved mean pairwise distances using left and right frontal approaches (p<0.05). Novice operators and neurosurgical residents both showed improvements in targeting accuracy for catheter placement when using the system for the first time. However, the improvements were not statistically significant. Novices' pair-wise distances were significantly better with Smart Stylet guidance using the left frontal approach (p<0.05). Improved guidance techniques, such as the Smart Stylet approach, can potentially decrease ventriculostomy complications if they can be easily integrated into clinical use at low cost.",
    "advisors": ["Ronilda Lacson", "Kirby Vosburgh"],
    "text": "Evaluation of external ventricular drain complications and the use of a procedure-targeted image-guidance system Access to the cerebral ventricle (e.g. ventriculostomy) is required to manage multiple life-threatening ailments. It can be done either in the operating room or at the bedside to relieve increased intracranial pressure or deliver medication. At the bedside, the procedure is normally performed freehand, with the occasional use of a Ghajar guide for guidance support. In the operating room, ventriculostomy may be performed with an image-guidance system, whether optical or electromagnetic. The most common complications of ventriculostomy are hemorrhage and infection. It is unclear whether catheter placement accuracy and the number of passes of the catheter for each placement are correlated with ventriculostomy complications. Our goals are 1) to evaluate the current state of practice, including complications of ventriculostomy, and 2) to evaluate a targeted image guidance system for use with ventriculostomy - the Smart Stylet. To address these goals, an Institutional Review Board-approved retrospective cross-sectional study was conducted at the Brigham and Women's Hospital (BWH) to characterize the practice of external ventricular drain placements using data from the patient electronic medical record. Post-procedure catheter location was measured on post-procedure CT and MRI imaging studies. Most cases were performed in the operating room and the operative reports provided all procedure-related information. Microbiology reports were collected within a four-week interval following catheter placements to evaluate presence of invading pathogens. All imaging studies, microbiology reports, and operative reports were reviewed manually. The rest of the medical records were not reviewed and, therefore, cerebrospinal fluid leak and shunt malfunction were not evaluated. Catheter placement accuracy and the numbers of passes for each placement were assessed. We evaluated whether these metrics were associated with the occurrence of procedure complications. A procedure-targeted image guidance system in development stage, the Smart Stylet, was implemented for use on a ventricular phantom model with a right-sided midline shift. Smart Stylet consists of an electromagnetic tracking system and ventriculostomy catheter connected to a PC and display. The operator of the Smart Stylet can interface with the system via a custom designed module in BWH's 3DSlicer software system. The system was tested for accuracy by calculating targeting error and reporting the precision of catheter placement. Precision was measured using pair-wise distances among experimental groups. The system was reviewed and commented on by three novices and two neurosurgical residents from the Massachusetts General Hospital by using the NASA-TLX grading scale questionnaire and a targeted survey. The phantom model was designed to gauge whether further tests in animals and cadavers are warranted using Smart Stylet. Patients with trauma were more likely to have catheters misplaced (OR = 9.132.31; p<0.05). It seems there is an opportunity to improve patient care if catheter placement is made more accurate and reliable. Use of the Smart Stylet system in a phantom study provided improvements in mean pair-wise distance and accuracy for catheter placement at the sub-centimeter level. A blinded operator achieved statistically significant improvement in targeting error using the right frontal approach (p<0.0 5). The operator also significantly improved mean pairwise distances using left and right frontal approaches (p<0.05). Novice operators and neurosurgical residents both showed improvements in targeting accuracy for catheter placement when using the system for the first time. However, the improvements were not statistically significant. Novices' pair-wise distances were significantly better with Smart Stylet guidance using the left frontal approach (p<0.05). Improved guidance techniques, such as the Smart Stylet approach, can potentially decrease ventriculostomy complications if they can be easily integrated into clinical use at low cost."
}, {
    "id": "oai:dspace.mit.edu:1721.1/42217",
    "title": "Value creation through modernizing Chinese medicine",
    "abstract": "My first hypothesis in this thesis is that there is significant value vested in traditional Chinese medicine that can be captured by converting them into ethical drugs through scientific analysis, screening and validation. Further, holistic treatment is a key difference between traditional Chinese medicine and western-type chemical drugs, which makes Chinese medicine a very valuable category of knowledge. Using mixed formula is a primary method of treatment in Chinese medicine. It is the application of distinctive medical philosophies of Chinese herbal medicines in practices, reflecting the uniqueness and advantages of Chinese medicine. For example, there are 96,592 mixed formula recorded by \"Dictionary of Chinese Medicine Mixed Formula\" published in 1997. My second hypothesis in this thesis is that value can be created and captured, under the globalization context, from mixed herbal formulas for the mainstream world market with the aid of fingerprint technologies. To enter western markets as officially approved drugs through critical pathways, both scientific and regulatory, Chinese herb drugs must demonstrate sound evidence for safety and efficacy. I address in this thesis one of the central concerns of the pharmaceutical companies and FDA, that is, how quality control and material consistency is assured and how toxicity and drug kinetics of Chinese herbal medicines, either in its raw form, its purified form, its composite extract form or its mixed formula form, may be measured with reasonable scientific certainty and what would be the likely trajectory of further research.",
    "advisors": ["Frank L. Douglas", "Anthony J. Sinskey"],
    "text": "Value creation through modernizing Chinese medicine My first hypothesis in this thesis is that there is significant value vested in traditional Chinese medicine that can be captured by converting them into ethical drugs through scientific analysis, screening and validation. Further, holistic treatment is a key difference between traditional Chinese medicine and western-type chemical drugs, which makes Chinese medicine a very valuable category of knowledge. Using mixed formula is a primary method of treatment in Chinese medicine. It is the application of distinctive medical philosophies of Chinese herbal medicines in practices, reflecting the uniqueness and advantages of Chinese medicine. For example, there are 96,592 mixed formula recorded by \"Dictionary of Chinese Medicine Mixed Formula\" published in 1997. My second hypothesis in this thesis is that value can be created and captured, under the globalization context, from mixed herbal formulas for the mainstream world market with the aid of fingerprint technologies. To enter western markets as officially approved drugs through critical pathways, both scientific and regulatory, Chinese herb drugs must demonstrate sound evidence for safety and efficacy. I address in this thesis one of the central concerns of the pharmaceutical companies and FDA, that is, how quality control and material consistency is assured and how toxicity and drug kinetics of Chinese herbal medicines, either in its raw form, its purified form, its composite extract form or its mixed formula form, may be measured with reasonable scientific certainty and what would be the likely trajectory of further research."
}, {
    "id": "oai:dspace.mit.edu:1721.1/43810",
    "title": "Characterizing monitoring for the diagnosis and resuscitation of shock patients",
    "abstract": "Many factors contribute to a company's decision to launch a product in a new market. The company must be able to identify a clinical need that the product will address and the market must be willing to pay for the new technology. This thesis explores the clinical and market need for an improved shock monitoring technology. Shock occurs when there is not enough blood flow to adequately perfuse the body's organs. In the United States, about 500,000 patients go into sudden shock every year and half of these patients die. For millions of additional patients, shock is the final stage of a terminal disease. Despite advances in many other areas of medicine, shock continues to be a serious, life threatening condition. It is my hypothesis that the limitations of the current monitoring technologies contribute to the high mortality rate associated with shock. In my research, I examined the currently available monitoring technologies and their use for the diagnosis and resuscitation of shock patients. I conducted an extensive review of the scientific literature to identify the limitations of the current monitoring technologies and to understand the challenges of diagnosing and treating shock. To supplement my research, I interviewed clinicians who treat shock patients and scientists who are trying to develop new shock monitoring technologies. The clinicians confirmed that there is a critical need for improved shock monitoring technologies. However, for a new shock monitor to be successful, it will need to address the limitations of the current technologies. A well-designed clinical trial will be necessary to demonstrate that the new technology is sensitive and specific, clinically relevant, and easy to use.",
    "advisors": ["H. Frederick Bowman", "Ernst Berndt"],
    "text": "Characterizing monitoring for the diagnosis and resuscitation of shock patients Many factors contribute to a company's decision to launch a product in a new market. The company must be able to identify a clinical need that the product will address and the market must be willing to pay for the new technology. This thesis explores the clinical and market need for an improved shock monitoring technology. Shock occurs when there is not enough blood flow to adequately perfuse the body's organs. In the United States, about 500,000 patients go into sudden shock every year and half of these patients die. For millions of additional patients, shock is the final stage of a terminal disease. Despite advances in many other areas of medicine, shock continues to be a serious, life threatening condition. It is my hypothesis that the limitations of the current monitoring technologies contribute to the high mortality rate associated with shock. In my research, I examined the currently available monitoring technologies and their use for the diagnosis and resuscitation of shock patients. I conducted an extensive review of the scientific literature to identify the limitations of the current monitoring technologies and to understand the challenges of diagnosing and treating shock. To supplement my research, I interviewed clinicians who treat shock patients and scientists who are trying to develop new shock monitoring technologies. The clinicians confirmed that there is a critical need for improved shock monitoring technologies. However, for a new shock monitor to be successful, it will need to address the limitations of the current technologies. A well-designed clinical trial will be necessary to demonstrate that the new technology is sensitive and specific, clinically relevant, and easy to use."
}, {
    "id": "oai:dspace.mit.edu:1721.1/57687",
    "title": "Using EMR transactional data for personalize clinical decision support",
    "abstract": "Collective intelligence techniques have been used to predict stock prices, customer purchasing habits, movies and books preferences for years, yet they remain unused in the medical profession. With the increasing adoption of electronic medical records, patients' medical data has grown exponentially and thus constitutes an untapped field where similar techniques could be applied. If data were collectively farmed and intelligently filtered, patient information could be added to traditional clinical decision support tools to arrive at personalized recommendations based on empiric evidence. The aim of this work is to use the collective, de facto, clinical experience to augment clinical guidelines thereby providing physicians with personalized clinical decision support. The pharmacological treatment of hypertension was chosen as the clinical domain in which to explore the feasibility of this approach. Twelve-thousand-three-hundred-forty-seven hypertensive patients were seen at the Internal Medical Associates (IMA) clinic at Massachusetts General Hospital (MGH) between July 2004 and September 2009. Their relevant clinical and demographic variables, drug regimens and blood pressure measurements were collected from the clinic's electronic medical record system and a dataset was generated. Back-end application software that draws upon case-based reasoning (CBR) was constructed and used to compute similarity between an index patient and existing hypertension patients.",
    "advisors": ["William Lester"],
    "text": "Using EMR transactional data for personalize clinical decision support Collective intelligence techniques have been used to predict stock prices, customer purchasing habits, movies and books preferences for years, yet they remain unused in the medical profession. With the increasing adoption of electronic medical records, patients' medical data has grown exponentially and thus constitutes an untapped field where similar techniques could be applied. If data were collectively farmed and intelligently filtered, patient information could be added to traditional clinical decision support tools to arrive at personalized recommendations based on empiric evidence. The aim of this work is to use the collective, de facto, clinical experience to augment clinical guidelines thereby providing physicians with personalized clinical decision support. The pharmacological treatment of hypertension was chosen as the clinical domain in which to explore the feasibility of this approach. Twelve-thousand-three-hundred-forty-seven hypertensive patients were seen at the Internal Medical Associates (IMA) clinic at Massachusetts General Hospital (MGH) between July 2004 and September 2009. Their relevant clinical and demographic variables, drug regimens and blood pressure measurements were collected from the clinic's electronic medical record system and a dataset was generated. Back-end application software that draws upon case-based reasoning (CBR) was constructed and used to compute similarity between an index patient and existing hypertension patients."
}, {
    "id": "oai:dspace.mit.edu:1721.1/35553",
    "title": "The accelerated approval process in oncology : an examination of the conversion rate of approved therapies to full approval",
    "abstract": "In 1992, Accelerated Approval, or Subpart H approval, was added to the NDA regulations so promising products that provide a meaningful therapeutic benefit for serious or life-threatening diseases could be introduced to the market sooner, particularly for diseases or conditions where there was a great unmet medical need. Accelerated Approval is based on either a surrogate endpoint that is reasonably likely to predict clinical benefit or a clinical endpoint other than survival or irreversible morbidity. After approval, the sponsor is required to perform post-marketing studies to demonstrate clinical benefit. Since the FDA expanded the use of the Accelerated Approval regulatory path to include oncology drugs in 1995, thirty drugs (both small molecule as well as biologics) have been granted accelerated approval in oncology. However, from various reports in the literature and the FDA site, it appears that only a small fraction of these approvals (four to six) have been converted into regular approvals, based on the demonstration of clinical benefit in the post-marketing studies that support the benefit seen in the pivotal studies.",
    "advisors": ["Ernst R. Berndt", "Frank L. Douglas"],
    "text": "The accelerated approval process in oncology : an examination of the conversion rate of approved therapies to full approval In 1992, Accelerated Approval, or Subpart H approval, was added to the NDA regulations so promising products that provide a meaningful therapeutic benefit for serious or life-threatening diseases could be introduced to the market sooner, particularly for diseases or conditions where there was a great unmet medical need. Accelerated Approval is based on either a surrogate endpoint that is reasonably likely to predict clinical benefit or a clinical endpoint other than survival or irreversible morbidity. After approval, the sponsor is required to perform post-marketing studies to demonstrate clinical benefit. Since the FDA expanded the use of the Accelerated Approval regulatory path to include oncology drugs in 1995, thirty drugs (both small molecule as well as biologics) have been granted accelerated approval in oncology. However, from various reports in the literature and the FDA site, it appears that only a small fraction of these approvals (four to six) have been converted into regular approvals, based on the demonstration of clinical benefit in the post-marketing studies that support the benefit seen in the pivotal studies."
}, {
    "id": "oai:dspace.mit.edu:1721.1/65523",
    "title": "Assessing the impact of tumor evolution on oncology drug development and commercialization",
    "abstract": "This thesis investigates the commercial viability of developing and commercializing targeted oncology drugs directed at a specific tumor mutation instead of all forms and mutations of a single target. While oncologic drugs targeted to aberrant or overexpressed pro-proliferative proteins have revolutionized cancer treatment, tumors treated for long periods may mutate over time, gain resistance to these drugs and proliferate rapidly again. I hypothesize that drugs developed to inhibit specific resistant tumor genotypes can be commercially viable from a pharmaceutical manufacturer's perspective. To assess this hypothesis empirically, I construct a patient flow model in order to quantify the treatment of CML, a relatively rare and indolent hematological malignancy with extensive clinical data available and well-delineated disease phases and response criteria. To represent the rate of diagnosis, patients are \"added\" to the model every month, and thereafter there is a probability that a patient may either 1) become sufficiently intolerant to his drug in order to discontinue treatment, 2) fail to respond to treatment but remain in the same disease phase, 3) fail to respond to treatment and progress to the next phase of disease, or 4) adequately respond to treatment and stay on the same drug in the same phase. Patients that fail to respond (categories 2 and 3 above) have a chance of manifesting a resistance mutation that is adequately controlled by a hypothetical drug (in addition to their current treatment) but is otherwise untreatable. The aim of this analysis is to track the number of patients that accrue the chosen resistance mutation and thus would be good candidates to receive the hypothetical drug. Patient treatment rates are converted to sales figures, and are weighed against clinical development costs, timelines, and probabilities to determine the net present value (NPV) of a project to develop the hypothetical drug. In addition, parameters are varied in order to conduct a sensitivity analysis and determine the \"boundary conditions\" that make a drug profitable or unprofitable. To supplement the model results and confirm the model dynamics, I interviewed investment analysts, clinical oncology thoughtleaders, academic cancer researchers and clinical, commercial and regulatory personnel from drug manufacturers to gauge their opinions on the CML market and the hurdles particular to developing drugs aimed at resistant genotypes. The conclusion I reach from this analysis is that development of a specific mutation-directed therapy for resistant CML is unlikely to be profitable. Given the significantly smaller patient population, favorable conditions in pricing and clinical development would be required to make the hypothetical candidate even marginally profitable.",
    "advisors": ["Ernst R. Berndt", "Mark Trusheim"],
    "text": "Assessing the impact of tumor evolution on oncology drug development and commercialization This thesis investigates the commercial viability of developing and commercializing targeted oncology drugs directed at a specific tumor mutation instead of all forms and mutations of a single target. While oncologic drugs targeted to aberrant or overexpressed pro-proliferative proteins have revolutionized cancer treatment, tumors treated for long periods may mutate over time, gain resistance to these drugs and proliferate rapidly again. I hypothesize that drugs developed to inhibit specific resistant tumor genotypes can be commercially viable from a pharmaceutical manufacturer's perspective. To assess this hypothesis empirically, I construct a patient flow model in order to quantify the treatment of CML, a relatively rare and indolent hematological malignancy with extensive clinical data available and well-delineated disease phases and response criteria. To represent the rate of diagnosis, patients are \"added\" to the model every month, and thereafter there is a probability that a patient may either 1) become sufficiently intolerant to his drug in order to discontinue treatment, 2) fail to respond to treatment but remain in the same disease phase, 3) fail to respond to treatment and progress to the next phase of disease, or 4) adequately respond to treatment and stay on the same drug in the same phase. Patients that fail to respond (categories 2 and 3 above) have a chance of manifesting a resistance mutation that is adequately controlled by a hypothetical drug (in addition to their current treatment) but is otherwise untreatable. The aim of this analysis is to track the number of patients that accrue the chosen resistance mutation and thus would be good candidates to receive the hypothetical drug. Patient treatment rates are converted to sales figures, and are weighed against clinical development costs, timelines, and probabilities to determine the net present value (NPV) of a project to develop the hypothetical drug. In addition, parameters are varied in order to conduct a sensitivity analysis and determine the \"boundary conditions\" that make a drug profitable or unprofitable. To supplement the model results and confirm the model dynamics, I interviewed investment analysts, clinical oncology thoughtleaders, academic cancer researchers and clinical, commercial and regulatory personnel from drug manufacturers to gauge their opinions on the CML market and the hurdles particular to developing drugs aimed at resistant genotypes. The conclusion I reach from this analysis is that development of a specific mutation-directed therapy for resistant CML is unlikely to be profitable. Given the significantly smaller patient population, favorable conditions in pricing and clinical development would be required to make the hypothetical candidate even marginally profitable."
}, {
    "id": "oai:dspace.mit.edu:1721.1/65521",
    "title": "Orphan drugs : future viability of current forecasting models, in light of impending changes to influential market factors",
    "abstract": "Interviews were conducted to establish a baseline for how orphan drug forecasting is currently undertaken by financial market and industry analysts with the intention of understanding the variables typically accounted for in such a model. A literature search formed the basis of subsequent interviews conducted with experts from industry, payers, providers, legislators, patient groups, and the FDA. Discussion then focused on elements of the market which are poised to change in the short-term, how such changes might be reflected in existing models, and/or how these models may instead need to be modified to adapt to the new environment. We hypothesized that impending changes in the healthcare sector would indeed impact the legitimacy of current forecasting models, and that significant changes would need to be introduced to account for these new market forces. Our hypothesis, however, was not confirmed, in that although much of the literature and, indeed, public outcry over rising healthcare costs in general and drug prices in particular make a strong case for implementing changes in the orphan market via payers, government, or other actors, an assessment of healthcare experts regarding market changes over the next five years revealed a general consensus that meaningful change will likely not occur during this timeframe for orphan products, with the exception of a possible increase in pharmacoeconomic requirements for drugs which are only marginally effective. Thus, current orphan drug forecasting models constructed for use by financial and industry analysts correctly avoid discounting for these potential changes, as they will likely not face significant changes in the US until closer to a ten year time horizon. Potential exceptions to this conclusion depend on implementation and regulatory treatment of the fields of personalized medicine and gene therapy, as developments in these areas may closely interact with existing orphan drug legislation. Our results have significant implications for all companies and stakeholders entering or currently operating in the orphan market, and open the door for further quantitative and qualitative analysis.",
    "advisors": ["T. Forcht Dagi", "Parag Meswani"],
    "text": "Orphan drugs : future viability of current forecasting models, in light of impending changes to influential market factors Interviews were conducted to establish a baseline for how orphan drug forecasting is currently undertaken by financial market and industry analysts with the intention of understanding the variables typically accounted for in such a model. A literature search formed the basis of subsequent interviews conducted with experts from industry, payers, providers, legislators, patient groups, and the FDA. Discussion then focused on elements of the market which are poised to change in the short-term, how such changes might be reflected in existing models, and/or how these models may instead need to be modified to adapt to the new environment. We hypothesized that impending changes in the healthcare sector would indeed impact the legitimacy of current forecasting models, and that significant changes would need to be introduced to account for these new market forces. Our hypothesis, however, was not confirmed, in that although much of the literature and, indeed, public outcry over rising healthcare costs in general and drug prices in particular make a strong case for implementing changes in the orphan market via payers, government, or other actors, an assessment of healthcare experts regarding market changes over the next five years revealed a general consensus that meaningful change will likely not occur during this timeframe for orphan products, with the exception of a possible increase in pharmacoeconomic requirements for drugs which are only marginally effective. Thus, current orphan drug forecasting models constructed for use by financial and industry analysts correctly avoid discounting for these potential changes, as they will likely not face significant changes in the US until closer to a ten year time horizon. Potential exceptions to this conclusion depend on implementation and regulatory treatment of the fields of personalized medicine and gene therapy, as developments in these areas may closely interact with existing orphan drug legislation. Our results have significant implications for all companies and stakeholders entering or currently operating in the orphan market, and open the door for further quantitative and qualitative analysis."
}, {
    "id": "oai:dspace.mit.edu:1721.1/35131",
    "title": "Academia versus industry as a wellspring of new ideas in drug discovery : the case of oncology",
    "abstract": "The United States population is aging, and the need for novel approaches to treat and manage disease continues to grow. Among the diseases that will impact this population, cancer remains a therapeutic area with significant unmet need. Pharmaceutical and biotechnology industries must continue to meet revenue and income growth expectations and will become increasingly dependent on novel drugs in their pipelines. In order for the pharmaceutical and biotechnology industries to meet the demands of both patients and shareholders, productivity in the research and development process will need to improve significantly. In order to understand how best to improve the drug discovery and development process, it is important to identify potential sources of innovation throughout the process. Among these, an important consideration is to understand the paths that molecules take through the discovery and development process.",
    "advisors": ["Fiona Murray"],
    "text": "Academia versus industry as a wellspring of new ideas in drug discovery : the case of oncology The United States population is aging, and the need for novel approaches to treat and manage disease continues to grow. Among the diseases that will impact this population, cancer remains a therapeutic area with significant unmet need. Pharmaceutical and biotechnology industries must continue to meet revenue and income growth expectations and will become increasingly dependent on novel drugs in their pipelines. In order for the pharmaceutical and biotechnology industries to meet the demands of both patients and shareholders, productivity in the research and development process will need to improve significantly. In order to understand how best to improve the drug discovery and development process, it is important to identify potential sources of innovation throughout the process. Among these, an important consideration is to understand the paths that molecules take through the discovery and development process."
}, {
    "id": "oai:dspace.mit.edu:1721.1/78156",
    "title": "Opportunities and challenges in oncology targeted drug development : an assessment of the use of prevalence and companion diagnostic performance thresholds to guide clinical trial strategies",
    "abstract": "Targeted, especially stratified or biomarker-guided, therapies offer significant advantages over traditional oncology therapies in certain settings. Selecting patients most likely to respond to a drug increases the therapeutic efficacy while reducing toxicities and may accelerate regulatory approval since smaller clinical trials are needed to demonstrate benefit. Several drugs, including vemurafenib and crizotinib have demonstrated these benefits along with commercial success. However, significant risk exists for the drug developer since approval may be threatened if they fail to meet unclear and differing yet parallel requirements for both the drug and the required companion diagnostic. Tumor biology is also increasingly complex since recent studies suggest that there are limited numbers of individual driver mutations, complicated interactions throughout signaling pathways as well as extensive tumor heterogeneity, all of which will challenge the effectiveness of targeted therapies. Clinical trial strategy decisions can greatly impact the success of a targeted therapy due to these challenges. While therapeutic efficacy is considered important, biomarker prevalence and companion diagnostic performance have been shown to be as important, yet more informative at the time decisions are made. I hypothesized that common prevalence and companion diagnostic performance thresholds are being used to guide biomarker-guided clinical trial strategy decisions for targeted oncology therapies. Seventeen interviews with preclinical, clinical or translational leads were conducted across a focused set of ten \"pathway-modifying\" cancer drug programs (CDK4/6, MDM2 and P13KP inhibitors) that reflect the biological complexity of future targeted therapies. These interviews provided empirical data as to how biomarkers are being incorporated into current clinical trial decisions. All respondents were planning to use a companion diagnostic for their program, however, the use of biomarkers varied significantly. For those programs with ongoing clinical trials in phase I and II, 54% (n=7/13) were pursuing a biomarker-guided strategy while 46% (n=6/13) were using an initial all-comers strategy. This fairly equal split separated when compared by phase where trials in phase I and 1/11, 60% (n=6/10) were using an all-comers strategy but for those trials in phase II (n=3), all were using biomarker-guided strategies. A key finding of the interviews was that 66.7% (n=4/6) were planning biomarker enrichment as part of expansion plans. Disproving my hypothesis, however, common thresholds for neither biomarker prevalence nor companion diagnostic performance were being used to guide these decisions. Biomarker prevalences of 50-100% were stated as potentially appropriate for an all-comers strategy. Companion diagnostic performance thresholds were even less influential as only a few respondents provided a general range of desired sensitivity and specificity. This study found that actions of drug developers are not necessarily following the emerging recommendations for targeted therapies due to the significant challenges of biomarker and companion diagnostic development.",
    "advisors": ["Ernst R. Berndt", "Keith T. Flaherty"],
    "text": "Opportunities and challenges in oncology targeted drug development : an assessment of the use of prevalence and companion diagnostic performance thresholds to guide clinical trial strategies Targeted, especially stratified or biomarker-guided, therapies offer significant advantages over traditional oncology therapies in certain settings. Selecting patients most likely to respond to a drug increases the therapeutic efficacy while reducing toxicities and may accelerate regulatory approval since smaller clinical trials are needed to demonstrate benefit. Several drugs, including vemurafenib and crizotinib have demonstrated these benefits along with commercial success. However, significant risk exists for the drug developer since approval may be threatened if they fail to meet unclear and differing yet parallel requirements for both the drug and the required companion diagnostic. Tumor biology is also increasingly complex since recent studies suggest that there are limited numbers of individual driver mutations, complicated interactions throughout signaling pathways as well as extensive tumor heterogeneity, all of which will challenge the effectiveness of targeted therapies. Clinical trial strategy decisions can greatly impact the success of a targeted therapy due to these challenges. While therapeutic efficacy is considered important, biomarker prevalence and companion diagnostic performance have been shown to be as important, yet more informative at the time decisions are made. I hypothesized that common prevalence and companion diagnostic performance thresholds are being used to guide biomarker-guided clinical trial strategy decisions for targeted oncology therapies. Seventeen interviews with preclinical, clinical or translational leads were conducted across a focused set of ten \"pathway-modifying\" cancer drug programs (CDK4/6, MDM2 and P13KP inhibitors) that reflect the biological complexity of future targeted therapies. These interviews provided empirical data as to how biomarkers are being incorporated into current clinical trial decisions. All respondents were planning to use a companion diagnostic for their program, however, the use of biomarkers varied significantly. For those programs with ongoing clinical trials in phase I and II, 54% (n=7/13) were pursuing a biomarker-guided strategy while 46% (n=6/13) were using an initial all-comers strategy. This fairly equal split separated when compared by phase where trials in phase I and 1/11, 60% (n=6/10) were using an all-comers strategy but for those trials in phase II (n=3), all were using biomarker-guided strategies. A key finding of the interviews was that 66.7% (n=4/6) were planning biomarker enrichment as part of expansion plans. Disproving my hypothesis, however, common thresholds for neither biomarker prevalence nor companion diagnostic performance were being used to guide these decisions. Biomarker prevalences of 50-100% were stated as potentially appropriate for an all-comers strategy. Companion diagnostic performance thresholds were even less influential as only a few respondents provided a general range of desired sensitivity and specificity. This study found that actions of drug developers are not necessarily following the emerging recommendations for targeted therapies due to the significant challenges of biomarker and companion diagnostic development."
}, {
    "id": "oai:dspace.mit.edu:1721.1/28582",
    "title": "DMA : a diabetes disease management system",
    "abstract": "There is a clear and present need to improve the quality of diabetes care. Information technology can be used as a means to that end. In this article, we discuss the design and implementation of a web-based diabetes application. We show the role of modeling clinical workflow in the design philosophy of our application, and summarize our application's features and usage. Next, we describe observations made during and after design and implementation, and how they relate to the informatics literature. Finally, we elaborate on the paradigm of feedback control systems, its parallels with the design philosophy of our application, and its use as an organizational framework for the roles of information technology in diabetes care.",
    "advisors": ["Henry C. Chueh", "G. Octo Barnett"],
    "text": "DMA : a diabetes disease management system There is a clear and present need to improve the quality of diabetes care. Information technology can be used as a means to that end. In this article, we discuss the design and implementation of a web-based diabetes application. We show the role of modeling clinical workflow in the design philosophy of our application, and summarize our application's features and usage. Next, we describe observations made during and after design and implementation, and how they relate to the informatics literature. Finally, we elaborate on the paradigm of feedback control systems, its parallels with the design philosophy of our application, and its use as an organizational framework for the roles of information technology in diabetes care."
}, {
    "id": "oai:dspace.mit.edu:1721.1/35550",
    "title": "Use of machine learning techniques for SNP based prediction of ancestry",
    "abstract": "Some have argued that the genetic differences between continentally defined groups are relatively small and unlikely to have biomedical significance. In this study, the extent of variation between continentally defined groups was evaluated. Small numbers of randomly selected single nucleotide polymorphisms from the International HapMap Project were used to train classifiers for prediction of ancestral continent of origin. Predictive accuracy was then tested on independent data sets. A high degree of genetic similarity implies that groups will be difficult to distinguish, especially when only a limited amount of genetic information is used. It is shown that the genetic differences between continentally defined groups are sufficiently large that one can accurately predict ancestral continent of origin using only a minute, randomly selected fraction of the genetic variation present in the human genome. Genotype data from only 50 random single nucleotide polymorphisms can be used to predict ancestral continent of origin in the primary test data set with an average accuracy of 95%.",
    "advisors": ["Isaac Kohane"],
    "text": "Use of machine learning techniques for SNP based prediction of ancestry Some have argued that the genetic differences between continentally defined groups are relatively small and unlikely to have biomedical significance. In this study, the extent of variation between continentally defined groups was evaluated. Small numbers of randomly selected single nucleotide polymorphisms from the International HapMap Project were used to train classifiers for prediction of ancestral continent of origin. Predictive accuracy was then tested on independent data sets. A high degree of genetic similarity implies that groups will be difficult to distinguish, especially when only a limited amount of genetic information is used. It is shown that the genetic differences between continentally defined groups are sufficiently large that one can accurately predict ancestral continent of origin using only a minute, randomly selected fraction of the genetic variation present in the human genome. Genotype data from only 50 random single nucleotide polymorphisms can be used to predict ancestral continent of origin in the primary test data set with an average accuracy of 95%."
}, {
    "id": "oai:dspace.mit.edu:1721.1/54591",
    "title": "Financing the \"Valley of Death\" : an evaluation of incentive schemes for global health businesses",
    "abstract": "Many early-stage biotech companies face a significant funding gap when trying to develop a new drug from preclinical development to a proof of concept clinical trial. This funding gap is sometimes referred to as the \"valley of death\", a reflection of the vast number of companies that are unable to raise the needed capital to progress into the clinic. The suggestion behind the \"valley of death\" phrase is that companies that should be able to attract investment do not get funded, because (1) the technical risks inherent in taking a new drug through clinical trials are high, (2) a significant amount of capital is needed to finance clinical development, and (3) the time horizon of investment is on the order of 6-8 years. Ultimately, the valley of death reflects the perceived imbalance of risk and reward for an investment at this stage as well as the resulting difficulty for a biotech company in raising capital during this time. For companies focused on a neglected disease, this risk/reward profile is even more skewed, with significantly greater market risks and fewer exit opportunities for an investor. As a result, the \"valley of death\" phenomenon for a global health company developing a therapeutic for a neglected disease is even more pronounced As a result, private sector funding for translational research of neglected disease therapeutics has beeri severely lacking. In an effort to spur more private sector investment into the development of neglected disease therapeutics, several market design mechanisms have been developed including Advanced Market Commitments (AMCs) and Priority Review Vouchers (PRVs). These market design mechanisms are new and unproven.",
    "advisors": ["Ernst R. Berndt", "David Berry"],
    "text": "Financing the \"Valley of Death\" : an evaluation of incentive schemes for global health businesses Many early-stage biotech companies face a significant funding gap when trying to develop a new drug from preclinical development to a proof of concept clinical trial. This funding gap is sometimes referred to as the \"valley of death\", a reflection of the vast number of companies that are unable to raise the needed capital to progress into the clinic. The suggestion behind the \"valley of death\" phrase is that companies that should be able to attract investment do not get funded, because (1) the technical risks inherent in taking a new drug through clinical trials are high, (2) a significant amount of capital is needed to finance clinical development, and (3) the time horizon of investment is on the order of 6-8 years. Ultimately, the valley of death reflects the perceived imbalance of risk and reward for an investment at this stage as well as the resulting difficulty for a biotech company in raising capital during this time. For companies focused on a neglected disease, this risk/reward profile is even more skewed, with significantly greater market risks and fewer exit opportunities for an investor. As a result, the \"valley of death\" phenomenon for a global health company developing a therapeutic for a neglected disease is even more pronounced As a result, private sector funding for translational research of neglected disease therapeutics has beeri severely lacking. In an effort to spur more private sector investment into the development of neglected disease therapeutics, several market design mechanisms have been developed including Advanced Market Commitments (AMCs) and Priority Review Vouchers (PRVs). These market design mechanisms are new and unproven."
}, {
    "id": "oai:dspace.mit.edu:1721.1/72918",
    "title": "Pricing and reimbursement challenges for fixed dose combination cardiovascular drugs and intravenous oncologies",
    "abstract": "Over the past ten years there has been increasing public concern regarding the rising costs of pharmaceuticals. Drug expenditure is the fastest growing sector of healthcare costs in the United States. The structure of the U.S. healthcare system allows pharmaceutical companies to freely price their drugs. Then payers decide whether and how to cover these drugs. Payers have at their disposal several utilization management tools, such as tiering and prior authorizations, to steer their members to less costly drugs. However, the ability of payers to implement these tools varies significantly depending on whether the drug is covered under the pharmaceutical benefit / Medicare Part D provisions of healthcare plans or the medical benefit / Medicare Part B provisions. Drugs covered under the pharmaceutical benefit / Part D are distributed via retail pharmacies and, in general, are oral pills. Drugs covered under the medical benefit / Part B are physician administered drugs and, in general, are injectables or intravenous drugs. As pharmaceutical companies increasingly price their drugs at higher and higher levels, payers must take a drug's pricing into account when determining how to cover these drugs. This thesis assesses the role pricing plays in how a drug is covered. Two different classes of drugs were chosen to examine this topic: fixed dose combination (FDC) cardiovascular drugs and intravenous oncologies. FDC cardiovascular drugs were chosen because they are covered under the pharmacy benefit / Part D and are considered to have questionable efficacious value over their individual drug components. Intravenous oncologies were chosen because they are covered under the medical benefit / Part B and represent a highly politicized therapy area. These two therapy areas are illustrative of strongly contrasting classes of drugs. Literature review and public sources were used to obtain prices for the select cardiovascular FDCs and oncologies. Medicare's Formulary Finder was used to obtain the coverage level for the cardiovascular FDCs. This preliminary information showed that the most expensive of the select FDCs, Caduet, has the worst coverage. The literature review suggested that Provenge and Avastin, the most expensive of the select oncologies, had difficulty obtaining coverage. To confirm these results, interviews were conducted with a variety of payers. These interviews focused on what factors went into the coverage decision-making process for cardiovascular FDCs and intravenous oncologies. Interviews were also conducted with an oncologic distributor to determine distributors' impact on price. We hypothesized that price was the driving reason for Caduet's, Provenge's, and Avastin's relatively poor coverage. However, our hypothesis was not entirely confirmed. Payers confirmed that price and contracting were the driving factors for Caduet's relatively poor coverage, but they indicated that the situation was not as simple for the intravenous oncologies. Although price does play a small role in the coverage decision-making process for intravenous oncologies, other factors such as public policies and the unmet need in the therapy area drive coverage decisions more than price. Additionally, payers indicated that they lack the ability to steer members to less costly intravenous oncologies due to the drug acquisition and reimbursement structure of the medical benefit. Consequently, payers are beginning to utilize new techniques such as specialty pharmacies to help control utilization of these products. Also, other organizations such as certain oncologic distributors are attempting to implement cost-effective guidelines for intravenous oncologies. Our results have significant implications for what pharmaceutical companies should be considering when pricing their drugs, and highlight the pricing and coverage issues in the current healthcare system's structure that payers and other organizations are facing.",
    "advisors": ["T. Forcht Dagi"],
    "text": "Pricing and reimbursement challenges for fixed dose combination cardiovascular drugs and intravenous oncologies Over the past ten years there has been increasing public concern regarding the rising costs of pharmaceuticals. Drug expenditure is the fastest growing sector of healthcare costs in the United States. The structure of the U.S. healthcare system allows pharmaceutical companies to freely price their drugs. Then payers decide whether and how to cover these drugs. Payers have at their disposal several utilization management tools, such as tiering and prior authorizations, to steer their members to less costly drugs. However, the ability of payers to implement these tools varies significantly depending on whether the drug is covered under the pharmaceutical benefit / Medicare Part D provisions of healthcare plans or the medical benefit / Medicare Part B provisions. Drugs covered under the pharmaceutical benefit / Part D are distributed via retail pharmacies and, in general, are oral pills. Drugs covered under the medical benefit / Part B are physician administered drugs and, in general, are injectables or intravenous drugs. As pharmaceutical companies increasingly price their drugs at higher and higher levels, payers must take a drug's pricing into account when determining how to cover these drugs. This thesis assesses the role pricing plays in how a drug is covered. Two different classes of drugs were chosen to examine this topic: fixed dose combination (FDC) cardiovascular drugs and intravenous oncologies. FDC cardiovascular drugs were chosen because they are covered under the pharmacy benefit / Part D and are considered to have questionable efficacious value over their individual drug components. Intravenous oncologies were chosen because they are covered under the medical benefit / Part B and represent a highly politicized therapy area. These two therapy areas are illustrative of strongly contrasting classes of drugs. Literature review and public sources were used to obtain prices for the select cardiovascular FDCs and oncologies. Medicare's Formulary Finder was used to obtain the coverage level for the cardiovascular FDCs. This preliminary information showed that the most expensive of the select FDCs, Caduet, has the worst coverage. The literature review suggested that Provenge and Avastin, the most expensive of the select oncologies, had difficulty obtaining coverage. To confirm these results, interviews were conducted with a variety of payers. These interviews focused on what factors went into the coverage decision-making process for cardiovascular FDCs and intravenous oncologies. Interviews were also conducted with an oncologic distributor to determine distributors' impact on price. We hypothesized that price was the driving reason for Caduet's, Provenge's, and Avastin's relatively poor coverage. However, our hypothesis was not entirely confirmed. Payers confirmed that price and contracting were the driving factors for Caduet's relatively poor coverage, but they indicated that the situation was not as simple for the intravenous oncologies. Although price does play a small role in the coverage decision-making process for intravenous oncologies, other factors such as public policies and the unmet need in the therapy area drive coverage decisions more than price. Additionally, payers indicated that they lack the ability to steer members to less costly intravenous oncologies due to the drug acquisition and reimbursement structure of the medical benefit. Consequently, payers are beginning to utilize new techniques such as specialty pharmacies to help control utilization of these products. Also, other organizations such as certain oncologic distributors are attempting to implement cost-effective guidelines for intravenous oncologies. Our results have significant implications for what pharmaceutical companies should be considering when pricing their drugs, and highlight the pricing and coverage issues in the current healthcare system's structure that payers and other organizations are facing."
}, {
    "id": "oai:dspace.mit.edu:1721.1/42215",
    "title": "Best care practices in anesthesiology : development and evaluation of an electronic feedback system to improve physician compliance with evidence-based practices",
    "abstract": "Recently, hospitals, regulatory agencies, and insurers have renewed their focus on improving patient care and safety. Outcomes based measures are being utilized and hospitals are being asked to report on whether patients are being treated according to a standard of care or a best practice guideline. As peri-operative physicians, anesthesiologists are able to evaluate and, to a great degree, affect the pre-operative, intra-operative, and post-operative course of a patient. However, several barriers exist. Although best practice guidelines exist, current models to risk stratify patients need improvement. Individual anesthesiologists currently have no uniform way to measure patient outcomes, either in an institutional or provider specific manner, and many treat patients based on anecdotal experience rather than on evidence based medicine. We addressed these issues through development of an electronic feedback system. The demonstration system targeted the problem of postoperative nausea and vomiting (PONV) in the ambulatory surgery patient population. Because performance of existing PONV risk prediction models was poor and could not be used for educational purposes, we created a new PONV risk prediction model and compared it against existing models. The new, improved risk prediction model was incorporated into an electronic system that gathered patient outcomes data related to best care practice and then fed back the information to care providers. After implementation of the electronic feedback system, we evaluated its efficacy in improving compliance with best care practices.",
    "advisors": ["Lucila Ohno-Machado"],
    "text": "Best care practices in anesthesiology : development and evaluation of an electronic feedback system to improve physician compliance with evidence-based practices Recently, hospitals, regulatory agencies, and insurers have renewed their focus on improving patient care and safety. Outcomes based measures are being utilized and hospitals are being asked to report on whether patients are being treated according to a standard of care or a best practice guideline. As peri-operative physicians, anesthesiologists are able to evaluate and, to a great degree, affect the pre-operative, intra-operative, and post-operative course of a patient. However, several barriers exist. Although best practice guidelines exist, current models to risk stratify patients need improvement. Individual anesthesiologists currently have no uniform way to measure patient outcomes, either in an institutional or provider specific manner, and many treat patients based on anecdotal experience rather than on evidence based medicine. We addressed these issues through development of an electronic feedback system. The demonstration system targeted the problem of postoperative nausea and vomiting (PONV) in the ambulatory surgery patient population. Because performance of existing PONV risk prediction models was poor and could not be used for educational purposes, we created a new PONV risk prediction model and compared it against existing models. The new, improved risk prediction model was incorporated into an electronic system that gathered patient outcomes data related to best care practice and then fed back the information to care providers. After implementation of the electronic feedback system, we evaluated its efficacy in improving compliance with best care practices."
}, {
    "id": "oai:dspace.mit.edu:1721.1/72919",
    "title": "Understanding the drivers of value creation for biopharmaceuticals around the time of drug launch",
    "abstract": "The purpose of this research is to investigate potential strategic variables that executives at small to mid-sized biopharmaceutical companies should consider during the period of a drug launch. Bringing a product to market is a critical event for any biopharmaceutical company. It marks a major turning point within the biopharmaceutical's lifecycle and the company that can successfully launch a product will be viewed as a different asset class. Therefore, it is critical to understand potential drivers of the value and to encourage executives to raise probing questions when they are considering the next round of financing or whether to provide guidance. This study analyzed forty-six non-generic, therapeutic drugs launched in the US during January 2000- December 2009 by small to mid-sized biopharmaceutical companies with market capitalizations less than $20 billion at the time of launch. Predictor variables that were initially considered in the analysis are the following: management providing a sales guidance (binary), partnership (binary), market size of the partner(s) at the time of launch, specialty/primary care indication (binary), difference between year two actual sales number and that of pre-launch estimate, difference between year two actual sales number and that of post-launch estimate, financing activity prior to launch (binary), financing activity after launch (binary), average prelaunch file-to-offer discount, average post-launch file-to-offer discount, number of drugs launched by the same company (control variable) and NBI performance (control variable). Multiple linear regression analyses were then performed to determine which of these parameters were predictive of changes in stock price and changes in market capitalization. Those companies that did not provide guidance at the time of launch and raised additional capital within two years after launch performed better than those that did otherwise. Neither a partnership nor the market size of the partner contributed to either of the outcome measures. Whether or not the product is a specialty product also did not make any significant contribution to the models. The results from this study suggest several possible strategic and actionable items that can guide management to ask the right questions during the period around a drug launch.",
    "advisors": ["Brian Pereira", "Jonathan D. Fleming"],
    "text": "Understanding the drivers of value creation for biopharmaceuticals around the time of drug launch The purpose of this research is to investigate potential strategic variables that executives at small to mid-sized biopharmaceutical companies should consider during the period of a drug launch. Bringing a product to market is a critical event for any biopharmaceutical company. It marks a major turning point within the biopharmaceutical's lifecycle and the company that can successfully launch a product will be viewed as a different asset class. Therefore, it is critical to understand potential drivers of the value and to encourage executives to raise probing questions when they are considering the next round of financing or whether to provide guidance. This study analyzed forty-six non-generic, therapeutic drugs launched in the US during January 2000- December 2009 by small to mid-sized biopharmaceutical companies with market capitalizations less than $20 billion at the time of launch. Predictor variables that were initially considered in the analysis are the following: management providing a sales guidance (binary), partnership (binary), market size of the partner(s) at the time of launch, specialty/primary care indication (binary), difference between year two actual sales number and that of pre-launch estimate, difference between year two actual sales number and that of post-launch estimate, financing activity prior to launch (binary), financing activity after launch (binary), average prelaunch file-to-offer discount, average post-launch file-to-offer discount, number of drugs launched by the same company (control variable) and NBI performance (control variable). Multiple linear regression analyses were then performed to determine which of these parameters were predictive of changes in stock price and changes in market capitalization. Those companies that did not provide guidance at the time of launch and raised additional capital within two years after launch performed better than those that did otherwise. Neither a partnership nor the market size of the partner contributed to either of the outcome measures. Whether or not the product is a specialty product also did not make any significant contribution to the models. The results from this study suggest several possible strategic and actionable items that can guide management to ask the right questions during the period around a drug launch."
}, {
    "id": "oai:dspace.mit.edu:1721.1/28592",
    "title": "The drug development process : evaluation of PDUFA I/II and investigation into reducing drug development times",
    "abstract": "Published findings report that it takes approximately eight years to bring a novel drug to market at an average cost of $800 million. Over the last ten years, the Food and Drug Administration (FDA) has helped to reduce the time from filing a new drug application (NDA) to granting marketing approval (i.e. the approval phase). However, there has been no alteration in the time required to progress from an investigational new drug application (IND) to an NDA filing (i.e. the clinical phase) over this same period. Since approval times began to decrease upon the initiation of the Prescription Drug User Fee Act (PDUFA), in this thesis I analyze the impact of PDUFA and calculate its benefits to companies. Due to the importance of getting new drugs to the market faster, I also investigate why there has been no significant change in the time required to test a drug clinically, and attempt to identify steps that could be taken to improve the clinical trial process. To investigate this, I evaluated ways in which the FDA and industry can work together to reduce clinical development times, without compromising safety. The results from this study show that PDUFA has had a significant impact on reducing approval times. More importantly, I determined that the direct costs of PDUFA are small in irmlparison to its benefits. In addition, my analysis of the early clinical phases (pre-clinical to Phase II) of drug benefits. In addition, my analysis of the early clinical phases (pre-clinical to Phase II) of drug development has revealed potential steps both the FDA and industry can take to facilitate a more efficient process for assessing the safety and efficacy of drugs. Thus, this study represents an important step towards improving the development of medicines for the world.",
    "advisors": ["Ernst R. Berndt", "Joseph V. Bonventre"],
    "text": "The drug development process : evaluation of PDUFA I/II and investigation into reducing drug development times Published findings report that it takes approximately eight years to bring a novel drug to market at an average cost of $800 million. Over the last ten years, the Food and Drug Administration (FDA) has helped to reduce the time from filing a new drug application (NDA) to granting marketing approval (i.e. the approval phase). However, there has been no alteration in the time required to progress from an investigational new drug application (IND) to an NDA filing (i.e. the clinical phase) over this same period. Since approval times began to decrease upon the initiation of the Prescription Drug User Fee Act (PDUFA), in this thesis I analyze the impact of PDUFA and calculate its benefits to companies. Due to the importance of getting new drugs to the market faster, I also investigate why there has been no significant change in the time required to test a drug clinically, and attempt to identify steps that could be taken to improve the clinical trial process. To investigate this, I evaluated ways in which the FDA and industry can work together to reduce clinical development times, without compromising safety. The results from this study show that PDUFA has had a significant impact on reducing approval times. More importantly, I determined that the direct costs of PDUFA are small in irmlparison to its benefits. In addition, my analysis of the early clinical phases (pre-clinical to Phase II) of drug benefits. In addition, my analysis of the early clinical phases (pre-clinical to Phase II) of drug development has revealed potential steps both the FDA and industry can take to facilitate a more efficient process for assessing the safety and efficacy of drugs. Thus, this study represents an important step towards improving the development of medicines for the world."
}, {
    "id": "oai:dspace.mit.edu:1721.1/68465",
    "title": "Evaluation of the medical device approval lag between the United States and the European Union",
    "abstract": "The United States is the world leader in development and manufacture of medical devices. Even with this leadership position, there is evidence that the US is often not the first country to have new medical technology approved for patient use. In many cases, the European Union is the first geographic region to approve a new medical technology for sale, with US approval coming later. This delay in approval of new devices between the EU and US is referred to as the \"device lag.\" However, the extent or history of this lag over time and for different device types has not been examined. This thesis evaluated if a device approval lag has developed between US and EU at any time over the past 20 years and whether a device lag continues to exist today. US and EU regulatory approval data for 135 medical devices in three innovative medical device segments were collected and analyzed to evaluate the extent and history of the approval lag between the European Union and the United States. The collected approval data revealed a consistent approval lag between the US and EU in each of the three medical device segments explored in this study. Throughout the entire 20+ years of study, the United States had an average approval lag to the European Union in each of the three device segments, and an average lag for all devices of 21 months or almost 2 years (Ho: p. = 0, p = 8.2E-12). Furthermore, the device lag in these three segments has grown in recent years. These data are striking because they show, perhaps for the first time, that an approval lag has existed for medical devices between the US and EU for the past 20 years - since the beginning of the pan-European device regulatory system in the mid-1990s. The device lag is a useful metric for comparing the attractiveness of two markets for medical technology and may signal important changes in the medical technology industry. Furthermore, the existence of a persistent device approval lag in the United States may have significant implications for patients and their caregivers.",
    "advisors": ["Ernst Berndt"],
    "text": "Evaluation of the medical device approval lag between the United States and the European Union The United States is the world leader in development and manufacture of medical devices. Even with this leadership position, there is evidence that the US is often not the first country to have new medical technology approved for patient use. In many cases, the European Union is the first geographic region to approve a new medical technology for sale, with US approval coming later. This delay in approval of new devices between the EU and US is referred to as the \"device lag.\" However, the extent or history of this lag over time and for different device types has not been examined. This thesis evaluated if a device approval lag has developed between US and EU at any time over the past 20 years and whether a device lag continues to exist today. US and EU regulatory approval data for 135 medical devices in three innovative medical device segments were collected and analyzed to evaluate the extent and history of the approval lag between the European Union and the United States. The collected approval data revealed a consistent approval lag between the US and EU in each of the three medical device segments explored in this study. Throughout the entire 20+ years of study, the United States had an average approval lag to the European Union in each of the three device segments, and an average lag for all devices of 21 months or almost 2 years (Ho: p. = 0, p = 8.2E-12). Furthermore, the device lag in these three segments has grown in recent years. These data are striking because they show, perhaps for the first time, that an approval lag has existed for medical devices between the US and EU for the past 20 years - since the beginning of the pan-European device regulatory system in the mid-1990s. The device lag is a useful metric for comparing the attractiveness of two markets for medical technology and may signal important changes in the medical technology industry. Furthermore, the existence of a persistent device approval lag in the United States may have significant implications for patients and their caregivers."
}, {
    "id": "oai:dspace.mit.edu:1721.1/54674",
    "title": "The economic and ethical considerations and implications of the stratification of future oncology therapeutics",
    "abstract": "This thesis investigates the economic impact of stratified medicine on industry and the subsequent ethical implications for patients. Stratified medicine involves the use of clinical biomarkers to indicate differential response among patients in efficacy or potential side effects of therapeutic agents. The advent of stratified medicine should, in theory, result in the safer, more effective use of therapeutic agents to treat cancer. However, reluctance remains within the broader life sciences community, in particular within the pharmaceutical industry, to embrace stratified medicine. I hypothesize that this is due to economic concerns. Firstly, an historical analysis of the rate of market adoption of stratified therapeutics is conducted by comparing the adoption velocity and time to peak sales of stratified therapeutics relative to traditional chemotherapeutics. The aim is to analyze whether historically, stratified medicines have been more or less successful in terms of speed of market adoption. To supplement this analysis interviews are conducted with investment analysts who cover pharmaceutical and diagnostics companies to gauge their views on stratified medicine. This is important due to the fact that publicly traded companies have an obligation to their shareholders, and shareholder views are shaped by the analyses of these individuals. In order to assess the future economic impact of stratified medicine on industry, particularly given that clinical biomarkers are now being developed much earlier in the R&D timeline, a model was constructed to predict economic outcomes based on various parameters associated with biomarker development.",
    "advisors": ["Ernst R. Berndt", "Mark Trusheim"],
    "text": "The economic and ethical considerations and implications of the stratification of future oncology therapeutics This thesis investigates the economic impact of stratified medicine on industry and the subsequent ethical implications for patients. Stratified medicine involves the use of clinical biomarkers to indicate differential response among patients in efficacy or potential side effects of therapeutic agents. The advent of stratified medicine should, in theory, result in the safer, more effective use of therapeutic agents to treat cancer. However, reluctance remains within the broader life sciences community, in particular within the pharmaceutical industry, to embrace stratified medicine. I hypothesize that this is due to economic concerns. Firstly, an historical analysis of the rate of market adoption of stratified therapeutics is conducted by comparing the adoption velocity and time to peak sales of stratified therapeutics relative to traditional chemotherapeutics. The aim is to analyze whether historically, stratified medicines have been more or less successful in terms of speed of market adoption. To supplement this analysis interviews are conducted with investment analysts who cover pharmaceutical and diagnostics companies to gauge their views on stratified medicine. This is important due to the fact that publicly traded companies have an obligation to their shareholders, and shareholder views are shaped by the analyses of these individuals. In order to assess the future economic impact of stratified medicine on industry, particularly given that clinical biomarkers are now being developed much earlier in the R&D timeline, a model was constructed to predict economic outcomes based on various parameters associated with biomarker development."
}, {
    "id": "oai:dspace.mit.edu:1721.1/28758",
    "title": "Lung tissue engineering : in vitro synthesis of lung tissue from neonatal and fetal rat lung cells cultured in a three-dimensional collagen matrix",
    "abstract": "The focus of this study was to investigate the histology of tissue formed when fetal (16-20 days gestation) and neonatal (2 days old) rat lung cells were grown in a collagen-glycosaminoglycan scaffold. This project employed a collagen-GAG scaffold specifically developed for tissue engineering and investigated the effect of this substratum on the formation of lung histotypic structures in vitro. A cell isolation procedure was developed whereby 19-days gestation type II alveolar cells reaggregated to form alveolar-like structures. The effects of selected scaffold design variables including pore diameter and degradation rate of the substratum on lung tissue regeneration were explored. Lung cell behavior revealed as the cells interact with an analog of the extracellular matrix was also examined. Differences in fetal and neonatal lung cell behavior were identified using histological analysis. Lung cells were obtained from Sprague-Dawley rats after 16-, 19-, and 20-days of gestation and at 2 days after term. These cells were seeded into type I collagen-GAG matrices, sized 8mm in diameter by 2mm in thickness. The medium used, F12K and Ham's nutrient mixture, was supplemented with 10% fetal bovine serum. A seeding density between 1 to 5 million cells per sponge sample was used. Histology studies were performed at termination periods of 2, 14, and 28 days. This paper describes the in vitro formation and long-term maintenance of alveolar-like structures from enzymatically dissociated 19-days gestation fetal rat lung cells cultured on a collagen sponge substrate as a model system for lung tissue engineering.",
    "advisors": ["Myron Spector", "Ioannis V. Yannas"],
    "text": "Lung tissue engineering : in vitro synthesis of lung tissue from neonatal and fetal rat lung cells cultured in a three-dimensional collagen matrix The focus of this study was to investigate the histology of tissue formed when fetal (16-20 days gestation) and neonatal (2 days old) rat lung cells were grown in a collagen-glycosaminoglycan scaffold. This project employed a collagen-GAG scaffold specifically developed for tissue engineering and investigated the effect of this substratum on the formation of lung histotypic structures in vitro. A cell isolation procedure was developed whereby 19-days gestation type II alveolar cells reaggregated to form alveolar-like structures. The effects of selected scaffold design variables including pore diameter and degradation rate of the substratum on lung tissue regeneration were explored. Lung cell behavior revealed as the cells interact with an analog of the extracellular matrix was also examined. Differences in fetal and neonatal lung cell behavior were identified using histological analysis. Lung cells were obtained from Sprague-Dawley rats after 16-, 19-, and 20-days of gestation and at 2 days after term. These cells were seeded into type I collagen-GAG matrices, sized 8mm in diameter by 2mm in thickness. The medium used, F12K and Ham's nutrient mixture, was supplemented with 10% fetal bovine serum. A seeding density between 1 to 5 million cells per sponge sample was used. Histology studies were performed at termination periods of 2, 14, and 28 days. This paper describes the in vitro formation and long-term maintenance of alveolar-like structures from enzymatically dissociated 19-days gestation fetal rat lung cells cultured on a collagen sponge substrate as a model system for lung tissue engineering."
}, {
    "id": "oai:dspace.mit.edu:1721.1/28593",
    "title": "Patents and licensing and the commercialization of academic biomedical research",
    "abstract": "This thesis is part of a larger body of research being undertaken by Dr. Fiona Murray and colleagues examining value creation and sharing between and among the three principal players in the commercialization of academic biomedical research: universities, biotech firms, and big pharma. The Recombinant Capital database provided access to contracts for biomedical technology licensed from academe to biotech, and also subsequent contracts that included that same technology from biotech to big pharma. These two contracts comprise a contract \"pair\". Importantly, these contract \"pairs\" were unredacted, that is., all parts of the contracts, including the commercial terms, were available. This thesis will lay the foundation for later work by examining the contracts between university and biotech, from the University's point of view. The goal is to identify factors that give the university more power in a pricing negotiation, and that predict higher economic value for the contract. The Specific Aim is to determine if certain University factors have a significant effect on predicting the economic value of the university-biotech licensing agreement. Four groups of readily quantifiable factors that contain attributes that might add power to the University in its pricing negotiation with the Biotech firm were identified: Institutional factors, Single Inventor factors, Aggregate factors, and Invention factors. The hypothesis is that at least one of these factors will have a significant effect on predicting the value of the licensing agreement, as determined using ordinary- and multiple-linear regression models. In formulistic terms, the null- and test-hypotheses are: (HO) no factor has a significant effect on predicting economic value, and (HI) at least one",
    "advisors": ["Fiona E. Murray", "R. Rox Anderson"],
    "text": "Patents and licensing and the commercialization of academic biomedical research This thesis is part of a larger body of research being undertaken by Dr. Fiona Murray and colleagues examining value creation and sharing between and among the three principal players in the commercialization of academic biomedical research: universities, biotech firms, and big pharma. The Recombinant Capital database provided access to contracts for biomedical technology licensed from academe to biotech, and also subsequent contracts that included that same technology from biotech to big pharma. These two contracts comprise a contract \"pair\". Importantly, these contract \"pairs\" were unredacted, that is., all parts of the contracts, including the commercial terms, were available. This thesis will lay the foundation for later work by examining the contracts between university and biotech, from the University's point of view. The goal is to identify factors that give the university more power in a pricing negotiation, and that predict higher economic value for the contract. The Specific Aim is to determine if certain University factors have a significant effect on predicting the economic value of the university-biotech licensing agreement. Four groups of readily quantifiable factors that contain attributes that might add power to the University in its pricing negotiation with the Biotech firm were identified: Institutional factors, Single Inventor factors, Aggregate factors, and Invention factors. The hypothesis is that at least one of these factors will have a significant effect on predicting the value of the licensing agreement, as determined using ordinary- and multiple-linear regression models. In formulistic terms, the null- and test-hypotheses are: (HO) no factor has a significant effect on predicting economic value, and (HI) at least one"
}, {
    "id": "oai:dspace.mit.edu:1721.1/63014",
    "title": "Brokering strategic partnerships between Asian and western biopharmaceutical companies in the global biologics market : assessment of capabilities of Asian participants in the biologics contract manufacturing organization marketplace",
    "abstract": "It has become increasingly important for companies in the biopharmaceutical industry to maximize the clinical, commercial and economic impact of their products on a global scale. In this context, both Western and Asian firms have been engaging in international merger and acquisition (M&A) activities to improve global capabilities and competiveness. The M&A activities in the sector are driven by near-term expiration of blockbuster drug patents and marketplace pricing constraints, resulting in a perceived need to attain improved economies of scale. Across the industry, one can see an increased emphasis on biotechnology medicines (or biologics). Recent large business deals that have seen Pfizer acquire Wyeth, Merck acquire Schering-Plough and Roche complete the acquisition of Genentech all have some element of positioning around the exploitation of biologics for future growth. These trends are thought to put pressure on medium-/small-sized R&D firms to come up with competitive strategies in the global biologics market. Furthermore, the biologics market faces the threat of biosimilars (biogenerics or follow-on biologics). With the advent of expected changes in the US government healthcare policy, a number of companies will be facing competition from biosimilars in the near term. Mitigating the impact of the threat of biosimilars, to some extent, is the fact that manufacturing of most non-vaccine biologics is challenging because of the structural and biological complexity of the commercial product as well as the significant differences in the manufacturing process from one product to the next. Technical capacity and the ability to respond to shifting demands are likely to be one of the critical determinants for the success of individual companies in the biologics (and biosimilars) market. To meet the perceived needs, companies have either expanded their manufacturing capacity and capabilities by building inhouse facilities or by striking long-term supply deals through contract manufacturing organizations (CMOs). Utilizing highly efficient and cost-effective overseas biologics, CMOs could be a value-added business model for Western participants. The most dramatic cost-saving strategy would likely result from outsourcing operations to firms in emerging Asian countries like India and China. However, intellectual property protection and quality control issues have been considered problematic in these countries. In this context, other relatively well developed Asian countries-Japan, South Korea (referred to as Korea) and Singapore, which have relatively strong intellectual property protection and sophisticated manufacturing environments, might be strategic partners for Western firms in the contract biomanufacturing markets. In this research study, the current biopharmaceutical industry trends and global strategies of companies in Japan, Korea and Singapore were explored. As a sub-segment of the biopharmaceutical industry, the geographical features and defining characteristics of the biologics CMO market were examined. The framework for analysis was based on an assessment of the key contributing factors: capacity, capital and cost. The potential capabilities among emerging Asian participants in the global biologics CMO markets were assessed through personal interviews with senior/executive corporate managers of Asian domestic biopharmaceutical companies (principally Japanese and Korean firms). As the results indicate, the biopharmaceutical industry of each country has been influenced both by corporate strategy and government policy. The quantitative analyses show that the current biologics CMO market in these countries is underdeveloped with a few existing participants focusing on high-tech biomanufacturing of commercial products. In addition, the macro-and micro-environment of the biotechnology industry in these countries appears to be unfavorable for the development of a global biologics CMO market. Through individual interviews, it was found that biopharmaceutical corporate managers believe that the opportunities for growth/development of an Asian emerging CMO at the global level are modest, expressing the view that their direct presence in the biologics markets as global-scale CMO participants was unlikely to take place because of financial concerns (high risk investment and profit margin sharing), absence of a global network (no proven track record) and existence of an R&D-intensive corporate culture. In conclusion, while the capabilities of large and established domestic biopharmaceutical firms in these Asian countries certainly can meet regulatory, legal and technical requirements as emerging global CMO participants, the possibilities for development of a global CMO capability in these countries are likely to be small. Strategic considerations for the possible/likely development paths of the CMO market in these Asian countries are provided.",
    "advisors": ["Ernst Berndt", "Brian Seed"],
    "text": "Brokering strategic partnerships between Asian and western biopharmaceutical companies in the global biologics market : assessment of capabilities of Asian participants in the biologics contract manufacturing organization marketplace It has become increasingly important for companies in the biopharmaceutical industry to maximize the clinical, commercial and economic impact of their products on a global scale. In this context, both Western and Asian firms have been engaging in international merger and acquisition (M&A) activities to improve global capabilities and competiveness. The M&A activities in the sector are driven by near-term expiration of blockbuster drug patents and marketplace pricing constraints, resulting in a perceived need to attain improved economies of scale. Across the industry, one can see an increased emphasis on biotechnology medicines (or biologics). Recent large business deals that have seen Pfizer acquire Wyeth, Merck acquire Schering-Plough and Roche complete the acquisition of Genentech all have some element of positioning around the exploitation of biologics for future growth. These trends are thought to put pressure on medium-/small-sized R&D firms to come up with competitive strategies in the global biologics market. Furthermore, the biologics market faces the threat of biosimilars (biogenerics or follow-on biologics). With the advent of expected changes in the US government healthcare policy, a number of companies will be facing competition from biosimilars in the near term. Mitigating the impact of the threat of biosimilars, to some extent, is the fact that manufacturing of most non-vaccine biologics is challenging because of the structural and biological complexity of the commercial product as well as the significant differences in the manufacturing process from one product to the next. Technical capacity and the ability to respond to shifting demands are likely to be one of the critical determinants for the success of individual companies in the biologics (and biosimilars) market. To meet the perceived needs, companies have either expanded their manufacturing capacity and capabilities by building inhouse facilities or by striking long-term supply deals through contract manufacturing organizations (CMOs). Utilizing highly efficient and cost-effective overseas biologics, CMOs could be a value-added business model for Western participants. The most dramatic cost-saving strategy would likely result from outsourcing operations to firms in emerging Asian countries like India and China. However, intellectual property protection and quality control issues have been considered problematic in these countries. In this context, other relatively well developed Asian countries-Japan, South Korea (referred to as Korea) and Singapore, which have relatively strong intellectual property protection and sophisticated manufacturing environments, might be strategic partners for Western firms in the contract biomanufacturing markets. In this research study, the current biopharmaceutical industry trends and global strategies of companies in Japan, Korea and Singapore were explored. As a sub-segment of the biopharmaceutical industry, the geographical features and defining characteristics of the biologics CMO market were examined. The framework for analysis was based on an assessment of the key contributing factors: capacity, capital and cost. The potential capabilities among emerging Asian participants in the global biologics CMO markets were assessed through personal interviews with senior/executive corporate managers of Asian domestic biopharmaceutical companies (principally Japanese and Korean firms). As the results indicate, the biopharmaceutical industry of each country has been influenced both by corporate strategy and government policy. The quantitative analyses show that the current biologics CMO market in these countries is underdeveloped with a few existing participants focusing on high-tech biomanufacturing of commercial products. In addition, the macro-and micro-environment of the biotechnology industry in these countries appears to be unfavorable for the development of a global biologics CMO market. Through individual interviews, it was found that biopharmaceutical corporate managers believe that the opportunities for growth/development of an Asian emerging CMO at the global level are modest, expressing the view that their direct presence in the biologics markets as global-scale CMO participants was unlikely to take place because of financial concerns (high risk investment and profit margin sharing), absence of a global network (no proven track record) and existence of an R&D-intensive corporate culture. In conclusion, while the capabilities of large and established domestic biopharmaceutical firms in these Asian countries certainly can meet regulatory, legal and technical requirements as emerging global CMO participants, the possibilities for development of a global CMO capability in these countries are likely to be small. Strategic considerations for the possible/likely development paths of the CMO market in these Asian countries are provided."
}, {
    "id": "oai:dspace.mit.edu:1721.1/63226",
    "title": "Comparing long-term antiplatelet strategies to prevent morbidity and mortality in patients with drug-eluting coronary stents",
    "abstract": "Background: The optimal long-term antiplatelet therapy (APT) that balances the benefit of preventing myocardial infarction (MI) with the risk of severe bleeding is unknown in patients greater than one year after drug-eluting stent (DES) placement. Methods: We modeled life expectancy (LE) using published data by building a Markov model to compare several APT strategies composed of aspirin and clopidogrel, both as monotherapy and in various clinically plausible combinations. The base case examined a 65-year old person treated with a DES then continuous aspirin plus clopidogrel (Dual-Rx) for one year without complications. We considered risk of mortality from myocardial infarction and severe bleeding. We used a lifetime horizon and projected LE without quality-adjustment. Results: In the base-case analysis, APT yielding greatest LE was a toss-up between Dual-Rx indefinitely (LE of 13.48 years), clopidogrel indefinitely (LE of 13.45 years), and aspirin indefinitely (LE of 13.42 years); of the strategies considered, no APT was least preferred (LE of 13.36 years). All parameters were varied over plausible ranges in sensitivity analyses, including the duration of future treatment with clopidogrel (base-case, life long). The choice of APT remained a toss-up unless: the annual probability of MI fell below 0.0087 (base-case, 0.013) or the relative risk of systemic bleeding exceeded 1.52 (base case, 1.00), in which case clopidogrel indefinitely was preferred; or the efficacy of clopidogrel to prevent MI fell below 0.09 (base case, 0.20) or the relative risk of clopidogrel for severe gastrointestinal hemorrhage exceeded 3.33 (base case, 2.01), in which case aspirin indefinitely was preferred. Conclusions: For patients with a drug-eluting stent placed greater than one year ago, the antiplatelet therapy which yields the greatest life expectancy is a toss-up between dual antiplatelet therapy (clopidogrel plus aspirin indefinitely), clopidogrel indefinitely, and aspirin indefinitely. However, additional research (including a clinical trial, subgroup analysis, and modeling) is needed.",
    "advisors": ["Alexa T. McCray, Stephen G. Pauker", "John B. Wong"],
    "text": "Comparing long-term antiplatelet strategies to prevent morbidity and mortality in patients with drug-eluting coronary stents Background: The optimal long-term antiplatelet therapy (APT) that balances the benefit of preventing myocardial infarction (MI) with the risk of severe bleeding is unknown in patients greater than one year after drug-eluting stent (DES) placement. Methods: We modeled life expectancy (LE) using published data by building a Markov model to compare several APT strategies composed of aspirin and clopidogrel, both as monotherapy and in various clinically plausible combinations. The base case examined a 65-year old person treated with a DES then continuous aspirin plus clopidogrel (Dual-Rx) for one year without complications. We considered risk of mortality from myocardial infarction and severe bleeding. We used a lifetime horizon and projected LE without quality-adjustment. Results: In the base-case analysis, APT yielding greatest LE was a toss-up between Dual-Rx indefinitely (LE of 13.48 years), clopidogrel indefinitely (LE of 13.45 years), and aspirin indefinitely (LE of 13.42 years); of the strategies considered, no APT was least preferred (LE of 13.36 years). All parameters were varied over plausible ranges in sensitivity analyses, including the duration of future treatment with clopidogrel (base-case, life long). The choice of APT remained a toss-up unless: the annual probability of MI fell below 0.0087 (base-case, 0.013) or the relative risk of systemic bleeding exceeded 1.52 (base case, 1.00), in which case clopidogrel indefinitely was preferred; or the efficacy of clopidogrel to prevent MI fell below 0.09 (base case, 0.20) or the relative risk of clopidogrel for severe gastrointestinal hemorrhage exceeded 3.33 (base case, 2.01), in which case aspirin indefinitely was preferred. Conclusions: For patients with a drug-eluting stent placed greater than one year ago, the antiplatelet therapy which yields the greatest life expectancy is a toss-up between dual antiplatelet therapy (clopidogrel plus aspirin indefinitely), clopidogrel indefinitely, and aspirin indefinitely. However, additional research (including a clinical trial, subgroup analysis, and modeling) is needed."
}, {
    "id": "oai:dspace.mit.edu:1721.1/47854",
    "title": "Medication recommendations vs. peer practice in pediatric levothyroxine dosing : a study of collective intelligence from a clinical data warehouse as a potential model for clinical decision support",
    "abstract": "Clinical decision support systems (CDSS) are developed primarily from knowledge gleaned from evidence-based research, guidelines, trusted resources and domain experts. While these resources generally represent information that is research proven, time-tested and consistent with current medical knowledge, they lack some qualities that would be desirable in a CDSS. For instance, the information is presented as generalized recommendations that are not specific to particular patients and may not consider certain subpopulations. In addition, the knowledge base that produces the guidelines may be outdated and may not reflect real-world practice. Ideally, resources for decision support should be timely, patient-specific, and represent current practice. Patient-oriented clinical decision support is particularly important in the practice of pediatrics because it addresses a population in constant flux. Every age represents a different set of physiological and developmental concerns and considerations, especially in medication dosing patterns. Patient clinical data warehouses (CDW) may be able to bridge the knowledge gap. CDWs contain the collective intelligence of various contributors (i.e. clinicians, administrators, etc.) where each data entry provides information regarding medical care for a patient in the real world. CDWs have the potential to provide information as current as the latest upload, be focused to specific subpopulations and reflect current clinical practice. In this paper, I study the potential of a well-known patient clinical data warehouse to provide information regarding pediatric levothyroxine dosing as a form of clinical decision support. I study the state of the stored data, the necessary data transformations and options for representing the data to effectively summarize and communicate the findings.",
    "advisors": ["Anil K. Dubey"],
    "text": "Medication recommendations vs. peer practice in pediatric levothyroxine dosing : a study of collective intelligence from a clinical data warehouse as a potential model for clinical decision support Clinical decision support systems (CDSS) are developed primarily from knowledge gleaned from evidence-based research, guidelines, trusted resources and domain experts. While these resources generally represent information that is research proven, time-tested and consistent with current medical knowledge, they lack some qualities that would be desirable in a CDSS. For instance, the information is presented as generalized recommendations that are not specific to particular patients and may not consider certain subpopulations. In addition, the knowledge base that produces the guidelines may be outdated and may not reflect real-world practice. Ideally, resources for decision support should be timely, patient-specific, and represent current practice. Patient-oriented clinical decision support is particularly important in the practice of pediatrics because it addresses a population in constant flux. Every age represents a different set of physiological and developmental concerns and considerations, especially in medication dosing patterns. Patient clinical data warehouses (CDW) may be able to bridge the knowledge gap. CDWs contain the collective intelligence of various contributors (i.e. clinicians, administrators, etc.) where each data entry provides information regarding medical care for a patient in the real world. CDWs have the potential to provide information as current as the latest upload, be focused to specific subpopulations and reflect current clinical practice. In this paper, I study the potential of a well-known patient clinical data warehouse to provide information regarding pediatric levothyroxine dosing as a form of clinical decision support. I study the state of the stored data, the necessary data transformations and options for representing the data to effectively summarize and communicate the findings."
}, {
    "id": "oai:dspace.mit.edu:1721.1/35517",
    "title": "Use of location data for the surveillance, analysis, and optimization of clinical processes",
    "abstract": "Location tracking systems in healthcare produce a wealth of data applicable across many aspects of care and management. However, since dedicated location tracking systems, such as the oft mentioned RFID tracking system, are still sparsely deployed, a number of other data sources may be utilized to serve as a proxy for physical location, such as barcodes and manual timestamp entry, and may be better suited to indicate progress through clinical workflows. INCOMING!, a web-based platform that monitors and tracks patient progress from the operating room to the post-anesthesia care unit (PACU), is one such system that utilizes manual timestamps routinely entered as standard process of care in the operating room in order to track a patient's progress through the post-operative period. This integrated real time system facilitates patient flow between the PACU and the surgical ward and eases PACU workload by reducing the effort of discharging patients.",
    "advisors": ["William T. Lester"],
    "text": "Use of location data for the surveillance, analysis, and optimization of clinical processes Location tracking systems in healthcare produce a wealth of data applicable across many aspects of care and management. However, since dedicated location tracking systems, such as the oft mentioned RFID tracking system, are still sparsely deployed, a number of other data sources may be utilized to serve as a proxy for physical location, such as barcodes and manual timestamp entry, and may be better suited to indicate progress through clinical workflows. INCOMING!, a web-based platform that monitors and tracks patient progress from the operating room to the post-anesthesia care unit (PACU), is one such system that utilizes manual timestamps routinely entered as standard process of care in the operating room in order to track a patient's progress through the post-operative period. This integrated real time system facilitates patient flow between the PACU and the surgical ward and eases PACU workload by reducing the effort of discharging patients."
}, {
    "id": "oai:dspace.mit.edu:1721.1/33846",
    "title": "Effect of time horizon on incremental cost-effectiveness ratios",
    "abstract": "Background: Estimation of cost-effectiveness of a therapy as compared with another, in healthcare, is often based on a single perspective and a single time horizon. In this thesis, I explored methods of extrapolating the survival effect of different interventions and the effect of time horizon on incremental cost-effectiveness ratios when comparing two strategies. Methods: Two strategies for a patient are compared: new or usual treatment. A hypothetical model based on US life tables (for a 64-year old) assumed that the new and usual treatment strategies resulted in patient survivals identical to a person who is 5 and 10 years older, respectively, than the patient's chronologic age. The hazard rates over time were calculated and transformed to linear equations for least-squares linear regression to fit exponential, linear exponential, Weibull and Gompertz distributions. The survival model yielding the maximal likelihood estimate was extrapolated over different time horizons: 5, 10 and 15-year in addition to lifetime. In addition, I extracted survival data from a published trial evaluating thrombolysis in patients with myocardial infarction and applied this methodology over different time horizons.",
    "advisors": ["Robert A. Greenes,. John B. Wong", "Stephen G. Pauker"],
    "text": "Effect of time horizon on incremental cost-effectiveness ratios Background: Estimation of cost-effectiveness of a therapy as compared with another, in healthcare, is often based on a single perspective and a single time horizon. In this thesis, I explored methods of extrapolating the survival effect of different interventions and the effect of time horizon on incremental cost-effectiveness ratios when comparing two strategies. Methods: Two strategies for a patient are compared: new or usual treatment. A hypothetical model based on US life tables (for a 64-year old) assumed that the new and usual treatment strategies resulted in patient survivals identical to a person who is 5 and 10 years older, respectively, than the patient's chronologic age. The hazard rates over time were calculated and transformed to linear equations for least-squares linear regression to fit exponential, linear exponential, Weibull and Gompertz distributions. The survival model yielding the maximal likelihood estimate was extrapolated over different time horizons: 5, 10 and 15-year in addition to lifetime. In addition, I extracted survival data from a published trial evaluating thrombolysis in patients with myocardial infarction and applied this methodology over different time horizons."
}, {
    "id": "oai:dspace.mit.edu:1721.1/78244",
    "title": "Synthesis of composite hydrogels incorporating D,L-cyclic peptide nanotubes as a platform for materials engineering",
    "abstract": "Composite hydrogels find increasing use as biomaterials because the addition of a filler often improves on the material properties of the original matrix, or provides new optical, magnetic, conductive or bioactive functionalities not inherent to the hydrogel. In this work we synthesized nanocomposite gelatin methacrylate (GelMA) hydrogels that incorporate D,L-cyclic peptide nanotubes. These nanotubes are biocompatible, stiff and their physical and chemical properties can be tailored simply by changing the amino acid sequence of the peptide. We show that the nanotubes successfully integrated into the hydrogel matrix and provided some mechanical reinforcement, without affecting hydrogel porosity or hydration characteristics. We will be using this composite system as a platform for engineering hydrogels with unique physical and biological properties to the hydrogel, for application as biological scaffolds.",
    "advisors": ["Neel Joshi"],
    "text": "Synthesis of composite hydrogels incorporating D,L-cyclic peptide nanotubes as a platform for materials engineering Composite hydrogels find increasing use as biomaterials because the addition of a filler often improves on the material properties of the original matrix, or provides new optical, magnetic, conductive or bioactive functionalities not inherent to the hydrogel. In this work we synthesized nanocomposite gelatin methacrylate (GelMA) hydrogels that incorporate D,L-cyclic peptide nanotubes. These nanotubes are biocompatible, stiff and their physical and chemical properties can be tailored simply by changing the amino acid sequence of the peptide. We show that the nanotubes successfully integrated into the hydrogel matrix and provided some mechanical reinforcement, without affecting hydrogel porosity or hydration characteristics. We will be using this composite system as a platform for engineering hydrogels with unique physical and biological properties to the hydrogel, for application as biological scaffolds."
}, {
    "id": "oai:dspace.mit.edu:1721.1/37979",
    "title": "Using otoacoustic emissions to measure the transmission matrix of the middle-ear",
    "abstract": "Here we describe an experimental method for measuring the acoustic transmission matrix of the middle-ear using otoacoustic emissions. The experiment builds on previous work that uses distortion product otoacoustic emissions (DPOAEs) as an intracochlear sound source to drive the middle-ear in reverse. This technique eliminates the complications introduced by needing to place an acoustic transducer inside the cochlea. Previous authors have shown how the complete 4x3 system response matrix, with its 12 unknowns, can be simplified to a 2x2 transmission matrix by de-coupling the middle-ear cavity and assuming the cochlear fluids are incompressible. This simplified description of middle-ear mechanics assumes that the input-output response at the tympanic membrane and stapes footplate is linear, one dimensional and time invariant. The technique allows for estimating the acoustic pressure and volume velocity at the tympanic membrane and the volume velocity of the stapes footplate, in both the forward and reverse direction, and under different boundary conditions at the stapes. The technique was applied to deeply anesthetized cats with widely opened middle-ear cavities over a frequency range of 200Hz to 10kHz. Results on three animals are reported and generally agree with previous data and a published middle-ear model.",
    "advisors": ["Christopher A. Shera"],
    "text": "Using otoacoustic emissions to measure the transmission matrix of the middle-ear Here we describe an experimental method for measuring the acoustic transmission matrix of the middle-ear using otoacoustic emissions. The experiment builds on previous work that uses distortion product otoacoustic emissions (DPOAEs) as an intracochlear sound source to drive the middle-ear in reverse. This technique eliminates the complications introduced by needing to place an acoustic transducer inside the cochlea. Previous authors have shown how the complete 4x3 system response matrix, with its 12 unknowns, can be simplified to a 2x2 transmission matrix by de-coupling the middle-ear cavity and assuming the cochlear fluids are incompressible. This simplified description of middle-ear mechanics assumes that the input-output response at the tympanic membrane and stapes footplate is linear, one dimensional and time invariant. The technique allows for estimating the acoustic pressure and volume velocity at the tympanic membrane and the volume velocity of the stapes footplate, in both the forward and reverse direction, and under different boundary conditions at the stapes. The technique was applied to deeply anesthetized cats with widely opened middle-ear cavities over a frequency range of 200Hz to 10kHz. Results on three animals are reported and generally agree with previous data and a published middle-ear model."
}, {
    "id": "oai:dspace.mit.edu:1721.1/35516",
    "title": "Market application of a novel stent-based patency monitor to the management of ischemic vascular disease",
    "abstract": "The use of stents following angioplasty in ischemic arterial beds is limited by complications and continuing vascular deterioration. A phenomenon called stent restenosis post procedure exists which puts patients at a relatively high risk for vessel stenosis and occlusion. Stent restenosis may eventually lead to clinical symptoms such as myocardial infarction, stroke or limb loss, and if overlooked might lead to death. Within five years of stenting, a significant portion of patients require additional surgical intervention. A novel stent-based, implantable, and wireless approach for real-time monitoring of vessel patency at the site of coronary stents is proposed, will provide a measure of efficacy of stenting and of the pharmacologic regiment to mitigate the risk of vessel stenosis and narrowing due to the underlying. The purpose of this thesis is to explore and test the Hypotheses that there is a market for a direct, non-invasive monitoring of vessel patency at the site of a coronary stent; and that an implantable, wireless, stent-based device to monitor blood flow rate through a coronary stent can be designed and built.",
    "advisors": ["Richard J. Cohen", "T. Forchet Dagi"],
    "text": "Market application of a novel stent-based patency monitor to the management of ischemic vascular disease The use of stents following angioplasty in ischemic arterial beds is limited by complications and continuing vascular deterioration. A phenomenon called stent restenosis post procedure exists which puts patients at a relatively high risk for vessel stenosis and occlusion. Stent restenosis may eventually lead to clinical symptoms such as myocardial infarction, stroke or limb loss, and if overlooked might lead to death. Within five years of stenting, a significant portion of patients require additional surgical intervention. A novel stent-based, implantable, and wireless approach for real-time monitoring of vessel patency at the site of coronary stents is proposed, will provide a measure of efficacy of stenting and of the pharmacologic regiment to mitigate the risk of vessel stenosis and narrowing due to the underlying. The purpose of this thesis is to explore and test the Hypotheses that there is a market for a direct, non-invasive monitoring of vessel patency at the site of a coronary stent; and that an implantable, wireless, stent-based device to monitor blood flow rate through a coronary stent can be designed and built."
}, {
    "id": "oai:dspace.mit.edu:1721.1/35552",
    "title": "Characterizing MIT's serial scientist-entrepreneurs in life sciences",
    "abstract": "Since the Bayh-Dole Act of 1980, the commercialization of ideas generated in academia has driven significant startup activity and expansion in the life sciences. This commercial transformation has been shown by others to be concentrated among a relatively small number of elite academic institutions. However, within these institutions, we find that a small number of prestigious scientists are disproportionately responsible for entrepreneurial and commercial activity. To date, limited research has been conducted which aims to understand the characteristics of such serial scientist-entrepreneurs or their significance in early commercial ventures. This study identifies and characterizes 18 serial scientist-entrepreneurs (defined as faculty who have founded or served on the board of directors of 3 or more startups) on the basis of academic impact, patenting, and social network centrality, as compared to their first-time entrepreneur (i.e., faculty who founded or directed 1-2 companies) and noncommercial peers. These individuals constitute a subset of 66 scientist-entrepreneurs from a population of the 493 scientists who served as faculty in life sciences-related departments at MIT, during the period of 1981 to 2005 (representing the primary commercialization period for biotechnology).",
    "advisors": ["Robert Langer"],
    "text": "Characterizing MIT's serial scientist-entrepreneurs in life sciences Since the Bayh-Dole Act of 1980, the commercialization of ideas generated in academia has driven significant startup activity and expansion in the life sciences. This commercial transformation has been shown by others to be concentrated among a relatively small number of elite academic institutions. However, within these institutions, we find that a small number of prestigious scientists are disproportionately responsible for entrepreneurial and commercial activity. To date, limited research has been conducted which aims to understand the characteristics of such serial scientist-entrepreneurs or their significance in early commercial ventures. This study identifies and characterizes 18 serial scientist-entrepreneurs (defined as faculty who have founded or served on the board of directors of 3 or more startups) on the basis of academic impact, patenting, and social network centrality, as compared to their first-time entrepreneur (i.e., faculty who founded or directed 1-2 companies) and noncommercial peers. These individuals constitute a subset of 66 scientist-entrepreneurs from a population of the 493 scientists who served as faculty in life sciences-related departments at MIT, during the period of 1981 to 2005 (representing the primary commercialization period for biotechnology)."
}, {
    "id": "oai:dspace.mit.edu:1721.1/83969",
    "title": "A contrast agent for MRI of calcifications in breast cancer",
    "abstract": "Clinical x-ray mammography cannot delineate between hydroxyapatite and calcium oxalate, the respective forms of calcification in malignant and benign breast tumors. The water-poor nature of solid calcifications makes them difficult to image by conventional MRI. Recently, ultra-short echo time (UTE) MRI has enabled detection of solid calcified structures, but it is not specific to the underlying chemical composition. This thesis presents a hydroxyapatite-targeted gadolinium contrast agent for UTE MRI of calcification in malignant breast cancer. The hydroxyapatite-targeted contrast agent was synthesized by conjugating a bisphosphonate, pamidronate, to a gadolinium chelate. Binding specificity was tested by UTE MRI of the contrast agent reacted with hydroxyapatite, calcium oxalate, and other calcium-based crystals. The sensitivity of the contrast agent for hydroxyapatite was evaluated by UTE MRI: the lowest detectable concentration of hydroxyapatite-adsorbed contrast was 1 pM. Longitudinal relaxation time measurements were used to estimate the apparent relaxivity of the hydroxyapatite contrast agent to be >1000 s-I/mM. The targeted agent relaxivity is enhanced more than a 100-fold compared to conventional untargeted gadolinium contrast agents due to the restricted rotational motion of the contrast agent upon binding to a solid surface. In-vivo MRI of systemic delivery of the contrast agent was demonstrated in an animal model for breast cancer with hydroxyapatite calcifications. Pre- and post-contrast UTE MRI were acquired with systemic contrast agent injections. Dual-echo UTE subtraction images between short and long echoes showed specific uptake of the contrast agent to the calcifications. The mean signal intensity of the calcified regions enhanced by 200% between pre- and post-contrast images, posing the hydroxyapatite-targeted contrast agent as a clinical diagnostic for distinguishing benign and malignant calcification forms in breast cancer.",
    "advisors": ["Robert E. Lenkinski", "Bruce R. Rosen"],
    "text": "A contrast agent for MRI of calcifications in breast cancer Clinical x-ray mammography cannot delineate between hydroxyapatite and calcium oxalate, the respective forms of calcification in malignant and benign breast tumors. The water-poor nature of solid calcifications makes them difficult to image by conventional MRI. Recently, ultra-short echo time (UTE) MRI has enabled detection of solid calcified structures, but it is not specific to the underlying chemical composition. This thesis presents a hydroxyapatite-targeted gadolinium contrast agent for UTE MRI of calcification in malignant breast cancer. The hydroxyapatite-targeted contrast agent was synthesized by conjugating a bisphosphonate, pamidronate, to a gadolinium chelate. Binding specificity was tested by UTE MRI of the contrast agent reacted with hydroxyapatite, calcium oxalate, and other calcium-based crystals. The sensitivity of the contrast agent for hydroxyapatite was evaluated by UTE MRI: the lowest detectable concentration of hydroxyapatite-adsorbed contrast was 1 pM. Longitudinal relaxation time measurements were used to estimate the apparent relaxivity of the hydroxyapatite contrast agent to be >1000 s-I/mM. The targeted agent relaxivity is enhanced more than a 100-fold compared to conventional untargeted gadolinium contrast agents due to the restricted rotational motion of the contrast agent upon binding to a solid surface. In-vivo MRI of systemic delivery of the contrast agent was demonstrated in an animal model for breast cancer with hydroxyapatite calcifications. Pre- and post-contrast UTE MRI were acquired with systemic contrast agent injections. Dual-echo UTE subtraction images between short and long echoes showed specific uptake of the contrast agent to the calcifications. The mean signal intensity of the calcified regions enhanced by 200% between pre- and post-contrast images, posing the hydroxyapatite-targeted contrast agent as a clinical diagnostic for distinguishing benign and malignant calcification forms in breast cancer."
}, {
    "id": "oai:dspace.mit.edu:1721.1/68469",
    "title": "Understanding risk in a biopharmaceutical portfolio",
    "abstract": "Investors have difficulty funding the life sciences because of the high risks involved in research and development and commercialization of new products. Risk in the biopharmaceutical industry is the result of scientific, regulatory and economic uncertainty. The nature of the biopharmaceutical industry introduces many challenges. Each of these challenges incorporates a measure of risk into drug development. The level of understanding of technical success interdependencies has not been fully investigated. These interdependencies (correlations) could lead to an overall greater risk to the company's portfolio than previously expected. A better understanding of the risks that lead to success or failure in drug development might encourage more investment in the life sciences and specifically in the biopharmaceutical industry, and a greater awareness of the correlations between risks and products might lead to more informed decision making on a biopharmaceutical portfolio leading increased productivity. A dataset was collected from Thomson Reuters. The dataset is the oncology portfolio from a biopharmaceutical company, Genentech Inc. Logistic regression was used to determine if any of the defined variables contributed to the success or failure of the oncology products. The chi-square value was 7.738 with the degrees of freedom equal to 5 and with a p-value of 0.17. Therefore, none of the variables significantly contributed to the outcome. More research should be performed in this area in order to better understand the risk in a biopharmaceutical portfolio.",
    "advisors": ["Fiona Murray", "Andrew W. Lo"],
    "text": "Understanding risk in a biopharmaceutical portfolio Investors have difficulty funding the life sciences because of the high risks involved in research and development and commercialization of new products. Risk in the biopharmaceutical industry is the result of scientific, regulatory and economic uncertainty. The nature of the biopharmaceutical industry introduces many challenges. Each of these challenges incorporates a measure of risk into drug development. The level of understanding of technical success interdependencies has not been fully investigated. These interdependencies (correlations) could lead to an overall greater risk to the company's portfolio than previously expected. A better understanding of the risks that lead to success or failure in drug development might encourage more investment in the life sciences and specifically in the biopharmaceutical industry, and a greater awareness of the correlations between risks and products might lead to more informed decision making on a biopharmaceutical portfolio leading increased productivity. A dataset was collected from Thomson Reuters. The dataset is the oncology portfolio from a biopharmaceutical company, Genentech Inc. Logistic regression was used to determine if any of the defined variables contributed to the success or failure of the oncology products. The chi-square value was 7.738 with the degrees of freedom equal to 5 and with a p-value of 0.17. Therefore, none of the variables significantly contributed to the outcome. More research should be performed in this area in order to better understand the risk in a biopharmaceutical portfolio."
}, {
    "id": "oai:dspace.mit.edu:1721.1/78158",
    "title": "From bench to bedside : impact of conflict-of-interest restrictions at academic medical centers on clinical trials",
    "abstract": "Successful translation of scientific discovery into new medicines is most successful with collaboration between academics - scientists and physicians - and industry. In recent years, there has been increasing concern at academic medical centers about the impact of relationships with industry on patient care and student education. This has generally resulted in more stringent conflict-of-interest rules. This paper seeks to better understand the impact of these conflict-of-interest rules. In the first part, it explores research to-date on the importance of relationships between industry and academia and discusses some of the concerns that have arisen. In the second part, this relationship is better characterized with clinical trial data. The findings suggest that there is a strong trend towards schools with higher conflict-of-interest rules having fewer clinical trials. This suggests that although there may be benefits to stricter regulation, there are trade-offs in terms of clinical translation.",
    "advisors": ["Fiona Murray", "Anthony Sinskey"],
    "text": "From bench to bedside : impact of conflict-of-interest restrictions at academic medical centers on clinical trials Successful translation of scientific discovery into new medicines is most successful with collaboration between academics - scientists and physicians - and industry. In recent years, there has been increasing concern at academic medical centers about the impact of relationships with industry on patient care and student education. This has generally resulted in more stringent conflict-of-interest rules. This paper seeks to better understand the impact of these conflict-of-interest rules. In the first part, it explores research to-date on the importance of relationships between industry and academia and discusses some of the concerns that have arisen. In the second part, this relationship is better characterized with clinical trial data. The findings suggest that there is a strong trend towards schools with higher conflict-of-interest rules having fewer clinical trials. This suggests that although there may be benefits to stricter regulation, there are trade-offs in terms of clinical translation."
}, {
    "id": "oai:dspace.mit.edu:1721.1/39573",
    "title": "Genomics research and cultivating serendipity in pharmaceutical drug discovery : assessing the competitiveness of R&D productivity between the West and Asia",
    "abstract": "It has been widely reported that pharmaceutical drug discovery innovation began its major decline somewhere in the last decade of the 20th century. After reaching a historical high of 53 new molecular entities (NMEs) in 1996, the industry has since witnessed a steady decline of NME filings (down to 18 in 2006) with the Center for Drug Evaluation and Research (CDER)---despite rapidly escalating R&D spending among the world's major pharmaceutical firms (the \"majors\"). Industry leaders, researchers, and observers have all but acknowledged this drug discovery productivity crisis, much of it attributed to the industry's preference for and eventual exhaustion of simple, single molecular targets-the so-called \"low-hanging fruit\" whose discovery is characteristically attributed to serendipity. Collectively, pharmacological compounds were identified that targeted the products of -400-500 genes in the human body over the past five decades. These single-molecular targets-the majority of which are mechanistically overrepresented by the G-protein coupled receptors and key enzymes--are now believed to have been mostly discovered and commercialized into the ubiquitous blockbuster drugs on the market, ranging from statins to proton-pump inhibitors (PPIs).",
    "advisors": ["Frank L. Douglas", "Anthony J. Sinskey"],
    "text": "Genomics research and cultivating serendipity in pharmaceutical drug discovery : assessing the competitiveness of R&D productivity between the West and Asia It has been widely reported that pharmaceutical drug discovery innovation began its major decline somewhere in the last decade of the 20th century. After reaching a historical high of 53 new molecular entities (NMEs) in 1996, the industry has since witnessed a steady decline of NME filings (down to 18 in 2006) with the Center for Drug Evaluation and Research (CDER)---despite rapidly escalating R&D spending among the world's major pharmaceutical firms (the \"majors\"). Industry leaders, researchers, and observers have all but acknowledged this drug discovery productivity crisis, much of it attributed to the industry's preference for and eventual exhaustion of simple, single molecular targets-the so-called \"low-hanging fruit\" whose discovery is characteristically attributed to serendipity. Collectively, pharmacological compounds were identified that targeted the products of -400-500 genes in the human body over the past five decades. These single-molecular targets-the majority of which are mechanistically overrepresented by the G-protein coupled receptors and key enzymes--are now believed to have been mostly discovered and commercialized into the ubiquitous blockbuster drugs on the market, ranging from statins to proton-pump inhibitors (PPIs)."
}, {
    "id": "oai:dspace.mit.edu:1721.1/63227",
    "title": "A comprehensive guide to the three biosimilar markets (Europe, US, Japan) and the regulatory pathways",
    "abstract": "Generics in the pharmaceutical industry have been instrumental in reducing overall healthcare cost and allowing for greater dispersal of life saving drugs to the general population. The Hatch-Waxman Act of 1984 played a critical role in changing the landscape of the pharmaceutical industry and providing legislation for an abbreviated regulatory pathway for generic drugs. The conversation has shifted to the need to implement similar regulatory paths for generics of biologics. First generation biologic patents have or are geared to expire within the next five years, providing a great opportunity for generic companies in this space to enter. Biologic generics, termed biosimilars or follow-on biologics, are more difficult to evaluate due to the complex nature of the molecule and the variables involved in the development and manufacturing process. This research seeks to understand the current debate in the biosimilar conversation, and examine whether there is a clear regulatory path to market for biosimilars using epoetin as a case example across the three main markets; US, Europe and Japan.",
    "advisors": ["Fiona Murray", "Gary Pisano"],
    "text": "A comprehensive guide to the three biosimilar markets (Europe, US, Japan) and the regulatory pathways Generics in the pharmaceutical industry have been instrumental in reducing overall healthcare cost and allowing for greater dispersal of life saving drugs to the general population. The Hatch-Waxman Act of 1984 played a critical role in changing the landscape of the pharmaceutical industry and providing legislation for an abbreviated regulatory pathway for generic drugs. The conversation has shifted to the need to implement similar regulatory paths for generics of biologics. First generation biologic patents have or are geared to expire within the next five years, providing a great opportunity for generic companies in this space to enter. Biologic generics, termed biosimilars or follow-on biologics, are more difficult to evaluate due to the complex nature of the molecule and the variables involved in the development and manufacturing process. This research seeks to understand the current debate in the biosimilar conversation, and examine whether there is a clear regulatory path to market for biosimilars using epoetin as a case example across the three main markets; US, Europe and Japan."
}, {
    "id": "oai:dspace.mit.edu:1721.1/54594",
    "title": "Turning quicksand into bedrock : understanding the dynamic effects of disease-focused global health aid on health systems",
    "abstract": "This thesis asks one basic question: how do \"vertical\" disease- or intervention-focused global health programs impact the underlying health systems of the nations they serve? Vertical programs-health aid focused on a particular disease, such as HIV, or type of intervention, such as immunization-receive the lion's share of global health aid dollars, and yet we know uncomfortably little about their long-run impact on broader health systems. Many speculate that vertical aid undermines health worker effectiveness, distorts national policies, and disrupts the supply chain for drugs and medical products. Unfortunately, a lack of hard data makes quantitative analysis extremely difficult. Using the tools of system dynamics, this thesis consolidates the collective wisdom of previously published investigations and anecdotal observations to reveal the field's prevailing \"mental model\" of the dynamic in question. The result is a set of diagrams that describe the known impacts of vertical programs on health systems, and also reveal dynamic effects not yet explicitly identified in the literature. These effects fall into four sub-systems of impact: care delivery specialization and fragmentation, care delivery development and mediocritization, health policy development and mismatch, and market development and distortion. These models are then used to better understand the effects of recent contextual developments-the HIV/AIDS epidemic and the emergence of large Global Health Initiatives.",
    "advisors": ["Ernst R. Berndt", "Anjali Sastry"],
    "text": "Turning quicksand into bedrock : understanding the dynamic effects of disease-focused global health aid on health systems This thesis asks one basic question: how do \"vertical\" disease- or intervention-focused global health programs impact the underlying health systems of the nations they serve? Vertical programs-health aid focused on a particular disease, such as HIV, or type of intervention, such as immunization-receive the lion's share of global health aid dollars, and yet we know uncomfortably little about their long-run impact on broader health systems. Many speculate that vertical aid undermines health worker effectiveness, distorts national policies, and disrupts the supply chain for drugs and medical products. Unfortunately, a lack of hard data makes quantitative analysis extremely difficult. Using the tools of system dynamics, this thesis consolidates the collective wisdom of previously published investigations and anecdotal observations to reveal the field's prevailing \"mental model\" of the dynamic in question. The result is a set of diagrams that describe the known impacts of vertical programs on health systems, and also reveal dynamic effects not yet explicitly identified in the literature. These effects fall into four sub-systems of impact: care delivery specialization and fragmentation, care delivery development and mediocritization, health policy development and mismatch, and market development and distortion. These models are then used to better understand the effects of recent contextual developments-the HIV/AIDS epidemic and the emergence of large Global Health Initiatives."
}, {
    "id": "oai:dspace.mit.edu:1721.1/16699",
    "title": "Improving the efficiency of the later stages of the drug development process : survey results from the industry, academia, and the FDA",
    "abstract": "Drug development in the United States is a lengthy and expensive endeavor. It is estimated that average development times range from eleven to fifteen years and exceed costs of one billion dollars. The development pathway includes basic scientific discovery, pre-clinical testing in animals, clinical development in humans, and an application process. The Food and Drug Administration is responsible for the oversight and approval of drugs going through this process. Numerous financial and economic studies have been conducted that show the benefits to accelerating the drug development process. In 1992, the United States Congress enacted the Prescription Drug User Fee Act I, which mandated faster response times from the FDA in return for user fee payments to the FDA by the drug developing companies. Data on approval times for new drugs indicate that this process was indeed shortened. In contrast, the average drug development process prior to the filing of an application has been increasing in cost and time. The first purpose of this research is to quantify the benefits of accelerated new drug application review time under the Prescription Drug User Fee Acts I and II. The second purpose of the research is to investigate what industry and the FDA can do together to reduce the development process time between the IND and NDA without compromising patient safety and welfare, specifically the Phase II, Phase III, and NDA components. The research indicates that PDUFA has improved approval times in a statistically significant way. Furthermore, the financial and social benefits as measured using net present value have far exceeded the PDUFA costs. Quantitative and qualitative surveys of fifty individuals in large pharmaceutical and biotech companies",
    "advisors": ["Ernest R. Berndt", "Joseph V. Bonventre"],
    "text": "Improving the efficiency of the later stages of the drug development process : survey results from the industry, academia, and the FDA Drug development in the United States is a lengthy and expensive endeavor. It is estimated that average development times range from eleven to fifteen years and exceed costs of one billion dollars. The development pathway includes basic scientific discovery, pre-clinical testing in animals, clinical development in humans, and an application process. The Food and Drug Administration is responsible for the oversight and approval of drugs going through this process. Numerous financial and economic studies have been conducted that show the benefits to accelerating the drug development process. In 1992, the United States Congress enacted the Prescription Drug User Fee Act I, which mandated faster response times from the FDA in return for user fee payments to the FDA by the drug developing companies. Data on approval times for new drugs indicate that this process was indeed shortened. In contrast, the average drug development process prior to the filing of an application has been increasing in cost and time. The first purpose of this research is to quantify the benefits of accelerated new drug application review time under the Prescription Drug User Fee Acts I and II. The second purpose of the research is to investigate what industry and the FDA can do together to reduce the development process time between the IND and NDA without compromising patient safety and welfare, specifically the Phase II, Phase III, and NDA components. The research indicates that PDUFA has improved approval times in a statistically significant way. Furthermore, the financial and social benefits as measured using net present value have far exceeded the PDUFA costs. Quantitative and qualitative surveys of fifty individuals in large pharmaceutical and biotech companies"
}, {
    "id": "oai:dspace.mit.edu:1721.1/33087",
    "title": "Influence of mitochondrial membrane potential on the cryopreservation survival of hepatocytes",
    "abstract": "Hepatocytes are widely used in the pharmaceutical and medical fields for drug metabolism studies, bioartificial liver devices, and repopulation of damaged livers as an alternative to transplantation. However, these cells are scarce and difficult to maintain in culture for prolonged periods of time. Banks of cryopreserved liver cells would significantly alleviate issues of hepatocyte availability, and efforts are being made to improve the viability and functionality of frozen hepatocytes. Previously, most work on improving post-thaw viability has hinged on limiting the physical damage of freezing by adding cryoprotective agents and optimizing cooling rates. Membrane-permeable cryoprotectants, such as dimethyl sulfoxide, though widely used, can be extremely toxic to the cell. More natural, non-membrane-permeable cryoprotectants, inspired by freeze-tolerant animals have also been used. A non-metabolizable glucose analog, 3-0-methyl- glucose (30MG), has shown promise with hepatocytes and was used in this study. Kinetics of the rGLUT2 cellular transporter used for 30MG uptake were quantified; Km and Vmax were determined to be 27.6 mM and 1.38 mM/s, respectively, by Lineweaver-Burk analysis and 70.0 mM and 1.82 mM/s, respectively, by Eadie-Hofstee analysis. This study also aimed to investigate the role of mitochondria in cell death induced by freezing. In particular, mitochondrial membrane potential (MMP) was investigated as a predictor of a cell's likelihood to avoid apoptosis from freeze-induced stress. Cells were sorted into high and low MMP subpopulations, frozen, thawed, and cultured for 24 hours.",
    "advisors": ["Mehmet Toner", "Gregory Stephanopoulos"],
    "text": "Influence of mitochondrial membrane potential on the cryopreservation survival of hepatocytes Hepatocytes are widely used in the pharmaceutical and medical fields for drug metabolism studies, bioartificial liver devices, and repopulation of damaged livers as an alternative to transplantation. However, these cells are scarce and difficult to maintain in culture for prolonged periods of time. Banks of cryopreserved liver cells would significantly alleviate issues of hepatocyte availability, and efforts are being made to improve the viability and functionality of frozen hepatocytes. Previously, most work on improving post-thaw viability has hinged on limiting the physical damage of freezing by adding cryoprotective agents and optimizing cooling rates. Membrane-permeable cryoprotectants, such as dimethyl sulfoxide, though widely used, can be extremely toxic to the cell. More natural, non-membrane-permeable cryoprotectants, inspired by freeze-tolerant animals have also been used. A non-metabolizable glucose analog, 3-0-methyl- glucose (30MG), has shown promise with hepatocytes and was used in this study. Kinetics of the rGLUT2 cellular transporter used for 30MG uptake were quantified; Km and Vmax were determined to be 27.6 mM and 1.38 mM/s, respectively, by Lineweaver-Burk analysis and 70.0 mM and 1.82 mM/s, respectively, by Eadie-Hofstee analysis. This study also aimed to investigate the role of mitochondria in cell death induced by freezing. In particular, mitochondrial membrane potential (MMP) was investigated as a predictor of a cell's likelihood to avoid apoptosis from freeze-induced stress. Cells were sorted into high and low MMP subpopulations, frozen, thawed, and cultured for 24 hours."
}, {
    "id": "oai:dspace.mit.edu:1721.1/54672",
    "title": "Economic potential for clinically significant in vitro diagnostics",
    "abstract": "In recent years, significant advances have been made in the realm of in vitro diagnostics with the development of novel tests which are able to meaningfully impact the course of a patients' disease management. This transformation has strained the traditional in vitro diagnostic business model and raised questions as to whether the economics support the commercial development of these tests. The goal of this study is to evaluate the economics of in vitro diagnostics from development to commercialization, with a focus on a specific a class of novel and complex tests called In Vitro Diagnostic Multivariate Index Assays (IVDMIA). My hypothesis is that the current dynamics of the market can only sustain a small number of such novel tests. To evaluate this hypothesis, I construct an economic model of the development of a hypothetical new in vitro diagnostics which focuses on both the cost of development and commercialization together with market potential and adoption. The analysis reviews specific break-even scenarios to determine the parameters which would allow for an economically viable complex in vitro diagnostic. The conclusion I reach based on this analysis is that only a very small number of medical conditions could economically support the development of a novel in vitro diagnostic. The medical conditions which could support the development of a novel test are governed by complexity, severity and prevalence of the disease. Given the dramatic impact these new tests may have on disease management, incentives may be required to offset the risks associated with expanding novel diagnostics into smaller but medically significant disease areas.",
    "advisors": ["Ernst Berndt", "Mark Trusheim"],
    "text": "Economic potential for clinically significant in vitro diagnostics In recent years, significant advances have been made in the realm of in vitro diagnostics with the development of novel tests which are able to meaningfully impact the course of a patients' disease management. This transformation has strained the traditional in vitro diagnostic business model and raised questions as to whether the economics support the commercial development of these tests. The goal of this study is to evaluate the economics of in vitro diagnostics from development to commercialization, with a focus on a specific a class of novel and complex tests called In Vitro Diagnostic Multivariate Index Assays (IVDMIA). My hypothesis is that the current dynamics of the market can only sustain a small number of such novel tests. To evaluate this hypothesis, I construct an economic model of the development of a hypothetical new in vitro diagnostics which focuses on both the cost of development and commercialization together with market potential and adoption. The analysis reviews specific break-even scenarios to determine the parameters which would allow for an economically viable complex in vitro diagnostic. The conclusion I reach based on this analysis is that only a very small number of medical conditions could economically support the development of a novel in vitro diagnostic. The medical conditions which could support the development of a novel test are governed by complexity, severity and prevalence of the disease. Given the dramatic impact these new tests may have on disease management, incentives may be required to offset the risks associated with expanding novel diagnostics into smaller but medically significant disease areas."
}, {
    "id": "oai:dspace.mit.edu:1721.1/97830",
    "title": "Novel endoscopes for microscopic assessment of airway clearance using micro-optical coherence tomography",
    "abstract": "The health of the human respiratory system depends critically on airway clearance via motile hair-like structures (cilia), which transport and eliminate unwanted particles trapped within mucus. Impairment of mucociliary clearance (MCC) can lead to life-threatening airway narrowing and lung infections, and is a major cause of morbidity and mortality in patients with cystic fibrosis, primary ciliary dyskinesia and chronic obstructive lung disease. However, no tool for microscopic in-vivo visualization of ciliary function is currently available, limiting studies of disease pathogenesis, refined diagnosis and phenotyping, and the development of novel therapeutics. In this thesis, a novel, 1-pm resolution, optical interferometric imaging technique termed Micro-OCT was incorporated into miniaturized common-path endoscopes and mucociliary transport was visualized in vivo for the first time. The first-generation Micro-OCT probe had a rigid design with outer diameter of 4 mm and a two-prism configuration providing beam splitting and sample beam shaping into an annular profile. Image quality of the probe allowed visualization of the periodic pattern of ciliary beating, measurement of airway surface liquid depth (ASL) and visualization of mucociliary transport. Unaltered ciliary function was demonstrated in a living, spontaneously breathing swine model. Newer generation common-path endoscope designs were demonstrated that improve, among other limitations, the stability of the reference reflector position and provide greater potential for miniaturization. The presented work opens unprecedented avenues for studying MCC and the effect of novel therapeutics within the complexity of a living organism. Further, it lays the groundwork for the development of a human probe with the potential to revolutionize diagnosis, phenotyping, and therapy management for all patients with respiratory disease involving the mucociliary escalator.",
    "advisors": ["Guillermo J. Tearney"],
    "text": "Novel endoscopes for microscopic assessment of airway clearance using micro-optical coherence tomography The health of the human respiratory system depends critically on airway clearance via motile hair-like structures (cilia), which transport and eliminate unwanted particles trapped within mucus. Impairment of mucociliary clearance (MCC) can lead to life-threatening airway narrowing and lung infections, and is a major cause of morbidity and mortality in patients with cystic fibrosis, primary ciliary dyskinesia and chronic obstructive lung disease. However, no tool for microscopic in-vivo visualization of ciliary function is currently available, limiting studies of disease pathogenesis, refined diagnosis and phenotyping, and the development of novel therapeutics. In this thesis, a novel, 1-pm resolution, optical interferometric imaging technique termed Micro-OCT was incorporated into miniaturized common-path endoscopes and mucociliary transport was visualized in vivo for the first time. The first-generation Micro-OCT probe had a rigid design with outer diameter of 4 mm and a two-prism configuration providing beam splitting and sample beam shaping into an annular profile. Image quality of the probe allowed visualization of the periodic pattern of ciliary beating, measurement of airway surface liquid depth (ASL) and visualization of mucociliary transport. Unaltered ciliary function was demonstrated in a living, spontaneously breathing swine model. Newer generation common-path endoscope designs were demonstrated that improve, among other limitations, the stability of the reference reflector position and provide greater potential for miniaturization. The presented work opens unprecedented avenues for studying MCC and the effect of novel therapeutics within the complexity of a living organism. Further, it lays the groundwork for the development of a human probe with the potential to revolutionize diagnosis, phenotyping, and therapy management for all patients with respiratory disease involving the mucociliary escalator."
}, {
    "id": "oai:dspace.mit.edu:1721.1/68466",
    "title": "Best antibiotics for buccal delivery",
    "abstract": "The purpose of the research was to identify the clinical and commercial benefits of switching from intravenous (IV) to buccal delivery of antibiotics. then, the research continued to select 3-5 antibiotics that best met the buccal delivery and market requirements. Methods: The research began with the hypothesis that some injectable antibiotics are good candidates for buccal delivery even with the limitations imposed by the buccal tissue. The thesis captures a two-year research period encompassing three critical fronts - the clinical viability of switching from IV to buccal delivery for antibiotics, the market's desire and readiness to switch, and the antibiotic brands available for commercialization. Then the research moved to drug identification and selection in order to assess the antibiotics that would best function in the buccal delivery model. Results: Intravenous (IV) antibiotics are usually reserved for severe infections that require faster treatment. Less aggressive bacterial growths are treated with oral antibiotics, which has fewer side effects and complications. In the past two decades, the understanding of drug transport across different tissues has increased resulting in improved patient adherence to the therapeutic regimen and pharmacologic response. The administration of drugs by transdermal or transmucosal routes are relatively painless, offers patients more choices, and reduces the need to establish intravenous access, which is a particular benefit for children and elderly. These alternative methods also provide clinical care providers with more choices to better manage their patient's course of treatment. In the past, clinicians administered sedatives, narcotics, and a variety of other medications by transdermal, sublingual, nasal, rectal, and even tracheal-mucosal routes. These delivery options have provided flexible practice settings and this paper intends to show that antibiotics could be the next set of drugs to be administered in variety of ways to provide patients and clinicians the best array of choices. Conclusion: A few years ago, the buccal delivery method was fairly unknown. However, advances in nano encapsulation, physiology, toxicity, and the availability of certain drugs make the timing ideal for introducing antibiotics that have undergone a highly selective process for delivering through the buccal tissue.",
    "advisors": ["Robert S. Langer, Jr.", "Frederick H. Bowman"],
    "text": "Best antibiotics for buccal delivery The purpose of the research was to identify the clinical and commercial benefits of switching from intravenous (IV) to buccal delivery of antibiotics. then, the research continued to select 3-5 antibiotics that best met the buccal delivery and market requirements. Methods: The research began with the hypothesis that some injectable antibiotics are good candidates for buccal delivery even with the limitations imposed by the buccal tissue. The thesis captures a two-year research period encompassing three critical fronts - the clinical viability of switching from IV to buccal delivery for antibiotics, the market's desire and readiness to switch, and the antibiotic brands available for commercialization. Then the research moved to drug identification and selection in order to assess the antibiotics that would best function in the buccal delivery model. Results: Intravenous (IV) antibiotics are usually reserved for severe infections that require faster treatment. Less aggressive bacterial growths are treated with oral antibiotics, which has fewer side effects and complications. In the past two decades, the understanding of drug transport across different tissues has increased resulting in improved patient adherence to the therapeutic regimen and pharmacologic response. The administration of drugs by transdermal or transmucosal routes are relatively painless, offers patients more choices, and reduces the need to establish intravenous access, which is a particular benefit for children and elderly. These alternative methods also provide clinical care providers with more choices to better manage their patient's course of treatment. In the past, clinicians administered sedatives, narcotics, and a variety of other medications by transdermal, sublingual, nasal, rectal, and even tracheal-mucosal routes. These delivery options have provided flexible practice settings and this paper intends to show that antibiotics could be the next set of drugs to be administered in variety of ways to provide patients and clinicians the best array of choices. Conclusion: A few years ago, the buccal delivery method was fairly unknown. However, advances in nano encapsulation, physiology, toxicity, and the availability of certain drugs make the timing ideal for introducing antibiotics that have undergone a highly selective process for delivering through the buccal tissue."
}, {
    "id": "oai:dspace.mit.edu:1721.1/39576",
    "title": "Mergers and acquisitions in the medical device industry",
    "abstract": "Mergers and acquisitions in the Medical Device Industry are the primary mode of exit for early stage companies. The focus of this thesis is to examine factors which influence the value of these M&A transactions from the target and acquiring firm perspectives and to understand the value creation that occurs. Publicly available electronic and published data sources were used to build a database of 674 M&A transactions and 113 IPO events for deals with published deal values and terms between January 1996 and October 2006. In this work, we demonstrate that transaction deal value varies between various medical device industry sectors. Factors that were shown to significantly correlate with M&A transaction deal value included the Sales of the target company, Market Capitalization value of the acquiring company, type of regulatory approval, and whether the company had venture backing prior to acquisition. M&A transactions that involved targets that were Public companies had significantly higher deal values than those that were private. Using 3-day event window analysis, returns of acquiring companies were shown to be slightly negative and significantly less than the S&P composite index returns over the same period.",
    "advisors": ["Anthony J. Sinskey", "Antoinette Schoar"],
    "text": "Mergers and acquisitions in the medical device industry Mergers and acquisitions in the Medical Device Industry are the primary mode of exit for early stage companies. The focus of this thesis is to examine factors which influence the value of these M&A transactions from the target and acquiring firm perspectives and to understand the value creation that occurs. Publicly available electronic and published data sources were used to build a database of 674 M&A transactions and 113 IPO events for deals with published deal values and terms between January 1996 and October 2006. In this work, we demonstrate that transaction deal value varies between various medical device industry sectors. Factors that were shown to significantly correlate with M&A transaction deal value included the Sales of the target company, Market Capitalization value of the acquiring company, type of regulatory approval, and whether the company had venture backing prior to acquisition. M&A transactions that involved targets that were Public companies had significantly higher deal values than those that were private. Using 3-day event window analysis, returns of acquiring companies were shown to be slightly negative and significantly less than the S&P composite index returns over the same period."
}, {
    "id": "oai:dspace.mit.edu:1721.1/30276",
    "title": "Trends in U.S. regulatory approvals of the biopharmaceutical therapeutic entities",
    "abstract": "Pharmaceutical productivity, as measured by annual output of new molecular entities and new therapeutic biologics, has fallen significantly since reaching a peak in 1996. According to Food and Drug Administration (FDA) data, the number of new drug approvals (new molecular entities and new biologics) fell from 50 in 1996 to 29 in 2003 (FDA-BEP database 2004). Meanwhile, non-inflation adjusted expenditures for research and development have almost doubled (PhRMA 2004). This thesis uses time series analysis to characterize historical trends in new drug introductions. Linear modeling and ARIMA modeling are employed to show that the large increase in new drug approvals in 1996 is inconsistent with previous trends. The hypothesis that the 1996 increase in new drug approvals is the consequence of additional FDA processing capacity pursuant to the implementation of the Prescription Drug User Fee Act (PDUFA) is considered and rejected, based on an analysis of the underlying causes of the increase. Next, approval trends before and after the implementation of PDUFA are compared. Notably, the percentage of new drug applications resulting in approval has increased since the implementation of PDUFA while the number of applications reviewed per year has not changed significantly. The relationship between the success ratio and drug withdrawal rates is examined, with inconclusive results.",
    "advisors": ["Ernst R. Berndt", "Issac S. Kohane"],
    "text": "Trends in U.S. regulatory approvals of the biopharmaceutical therapeutic entities Pharmaceutical productivity, as measured by annual output of new molecular entities and new therapeutic biologics, has fallen significantly since reaching a peak in 1996. According to Food and Drug Administration (FDA) data, the number of new drug approvals (new molecular entities and new biologics) fell from 50 in 1996 to 29 in 2003 (FDA-BEP database 2004). Meanwhile, non-inflation adjusted expenditures for research and development have almost doubled (PhRMA 2004). This thesis uses time series analysis to characterize historical trends in new drug introductions. Linear modeling and ARIMA modeling are employed to show that the large increase in new drug approvals in 1996 is inconsistent with previous trends. The hypothesis that the 1996 increase in new drug approvals is the consequence of additional FDA processing capacity pursuant to the implementation of the Prescription Drug User Fee Act (PDUFA) is considered and rejected, based on an analysis of the underlying causes of the increase. Next, approval trends before and after the implementation of PDUFA are compared. Notably, the percentage of new drug applications resulting in approval has increased since the implementation of PDUFA while the number of applications reviewed per year has not changed significantly. The relationship between the success ratio and drug withdrawal rates is examined, with inconclusive results."
}, {
    "id": "oai:dspace.mit.edu:1721.1/28590",
    "title": "A computational framework for the identification, cataloging, and classification of evolutionary conserved genomic DNA",
    "abstract": "Evolutionarily conserved genomic regions (ecores) are understudied, and yet comprise a very large percentage of the Human Genome. Highly conserved human-mouse non-coding ecores, for example, are more abundant within the Human Genome than those regions, which are currently estimated to encode for proteins. Subsets of these ecores also exhibit conservation that extends across several species. These genomic regions have managed to survive millions of years of evolution despite the fact that they do not appear to directly encode for proteins. The survival of these regions compels us to investigate their potential function. Development of a computational framework for the classification and clustering of these regions may be the first step in understanding their function. The need for a standardized framework is underscored by the explosive growth in the number of publicly available, fully sequenced genomes, and the diverse set of methodologies used to generate cross-species alignments. This project describes the design and implementation of a system for the identification, classification and cataloguing of ecores across multiple species. A key feature of this system is its ability to quickly incorporate new genomes and assemblies as they become available. Additionally, this system provides investigators with a feature rich user interface, which facilitates the retrieval of ecores based on a wide range of parameters. The system returns a dynamically annotated list of evolutionarily conserved regions, which is used as input to several classification schemes, aimed at identifying families of ecores that share similar features, including depth of evolutionary conservation, position relative to known genes, sequence similarity,",
    "advisors": ["Isaac S. Kohane"],
    "text": "A computational framework for the identification, cataloging, and classification of evolutionary conserved genomic DNA Evolutionarily conserved genomic regions (ecores) are understudied, and yet comprise a very large percentage of the Human Genome. Highly conserved human-mouse non-coding ecores, for example, are more abundant within the Human Genome than those regions, which are currently estimated to encode for proteins. Subsets of these ecores also exhibit conservation that extends across several species. These genomic regions have managed to survive millions of years of evolution despite the fact that they do not appear to directly encode for proteins. The survival of these regions compels us to investigate their potential function. Development of a computational framework for the classification and clustering of these regions may be the first step in understanding their function. The need for a standardized framework is underscored by the explosive growth in the number of publicly available, fully sequenced genomes, and the diverse set of methodologies used to generate cross-species alignments. This project describes the design and implementation of a system for the identification, classification and cataloguing of ecores across multiple species. A key feature of this system is its ability to quickly incorporate new genomes and assemblies as they become available. Additionally, this system provides investigators with a feature rich user interface, which facilitates the retrieval of ecores based on a wide range of parameters. The system returns a dynamically annotated list of evolutionarily conserved regions, which is used as input to several classification schemes, aimed at identifying families of ecores that share similar features, including depth of evolutionary conservation, position relative to known genes, sequence similarity,"
}, {
    "id": "oai:dspace.mit.edu:1721.1/33089",
    "title": "Personalized medicine, population genetics and privacy : an empirical study of international gene banks",
    "abstract": "The promise of personalized medicine lies in its potential to fundamentally change healthcare. In the past, pharmaceuticals were prescribed on a \"one size fits all\" basis-patients with certain disease phenotypes were given what were thought to be appropriate drugs. There is growing evidence however that the effectiveness of these drugs may differ by individual and by sub-group; presumably due to fundamental genetic differences in disease and metabolic pathways. Drugs like Herceptin, Gleevec and Iressa are part of an emerging trend in the biopharmaceutical arena of drugs that are accompanied by genetic diagnostic tests and prescribed only for patients with genotypes in which the agents are most effective.",
    "advisors": ["Fiona E. Murray", "Anthony J. Sinskey"],
    "text": "Personalized medicine, population genetics and privacy : an empirical study of international gene banks The promise of personalized medicine lies in its potential to fundamentally change healthcare. In the past, pharmaceuticals were prescribed on a \"one size fits all\" basis-patients with certain disease phenotypes were given what were thought to be appropriate drugs. There is growing evidence however that the effectiveness of these drugs may differ by individual and by sub-group; presumably due to fundamental genetic differences in disease and metabolic pathways. Drugs like Herceptin, Gleevec and Iressa are part of an emerging trend in the biopharmaceutical arena of drugs that are accompanied by genetic diagnostic tests and prescribed only for patients with genotypes in which the agents are most effective."
}, {
    "id": "oai:dspace.mit.edu:1721.1/35551",
    "title": "Medication concepts, records, and lists in electronic medical record systems",
    "abstract": "A well-designed implementation of medication concepts, records, and lists in an electronic medical record (EMR) system allows it to successfully perform many functions vital for the provision of quality health care. A controlled medication terminology provides the foundation for decision support services, such as duplication checking, allergy checking, and drug-drug interaction alerts. Clever modeling of medication records makes it easy to provide a history of any medication the patient is on and to generate the patient's medication list for any arbitrary point in time. Medication lists that distinguish between description and prescription and that are exportable in a standard format can play an essential role in medication reconciliation and contribute to the reduction of medication errors. At present, there is no general agreement on how to best implement medication concepts, records, and lists. The underlying implementation in an EMR often reflects the needs, culture, and history of both the developers and the local users. survey of a sample of medication terminologies (COSTAR Directory, the MDD, NDDF Plus, and RxNorm) and EMR implementations of medication records (OnCall, LMR, and the Benedum EMR) reveals the advantages and disadvantages of each. There is no medication system that would fit perfectly in every single context, but some features should strongly be considered in the development of any new system.",
    "advisors": ["Henry C. Chueh"],
    "text": "Medication concepts, records, and lists in electronic medical record systems A well-designed implementation of medication concepts, records, and lists in an electronic medical record (EMR) system allows it to successfully perform many functions vital for the provision of quality health care. A controlled medication terminology provides the foundation for decision support services, such as duplication checking, allergy checking, and drug-drug interaction alerts. Clever modeling of medication records makes it easy to provide a history of any medication the patient is on and to generate the patient's medication list for any arbitrary point in time. Medication lists that distinguish between description and prescription and that are exportable in a standard format can play an essential role in medication reconciliation and contribute to the reduction of medication errors. At present, there is no general agreement on how to best implement medication concepts, records, and lists. The underlying implementation in an EMR often reflects the needs, culture, and history of both the developers and the local users. survey of a sample of medication terminologies (COSTAR Directory, the MDD, NDDF Plus, and RxNorm) and EMR implementations of medication records (OnCall, LMR, and the Benedum EMR) reveals the advantages and disadvantages of each. There is no medication system that would fit perfectly in every single context, but some features should strongly be considered in the development of any new system."
}, {
    "id": "oai:dspace.mit.edu:1721.1/68173",
    "title": "Fulfilling the promise of targeted therapeutics in oncology via companion diagnostics : a perspective on pipeline trends and co-development strategies",
    "abstract": "Herceptin was the poster child of personalized medicine that brought forward the notion that contemporaneously developed companion diagnostics (CDx) could lead to more efficacious use of a cancer therapeutic in a selected population. Despite a gap of 12 years, the recent approvals of Zelboraf and Xalkori in quick succession by the FDA are a testament to the fact that the age of cancer therapeutics co-developed with a companion diagnostic is finally upon us. The purpose of this thesis was to test the hypotheses, that the trend for CDx based therapy launches in oncology is NOT headed towards a dramatic upturn in the next 5 years and in the view of biopharmaceutical executives - increasing price and market share of launched drugs are the dominant drivers for investing in companion diagnostics, and that the other features of CDx, such as improving the productivity of oncology drug development and reducing development costs are essentially dispensable. These hypotheses were tested using a study design that involved conducting a pilot study comprising of 18 interviews of stakeholders directly involved with the decision making of oncology drug development - to synthesize the extent of contemporaneously developed CDx to be launched with a cancer therapeutic in the coming 5 years. An analysis of the results obtained from the survey indicate, that a significant number of oncology drug launches within this decade would feature a co-developed companion diagnostic, and that despite challenges and initial trepidations over this business model - the higher probability of success, lower development costs, shorter time to market and pricing power associated with this approach, are incentives that are increasingly attracting more oncology firms to adopt this strategy for developing targeted therapeutics. Based on these findings, the original hypotheses were rejected.",
    "advisors": ["Richard J. Cohen, Keith Flaherty", "Mara Aspinall"],
    "text": "Fulfilling the promise of targeted therapeutics in oncology via companion diagnostics : a perspective on pipeline trends and co-development strategies Herceptin was the poster child of personalized medicine that brought forward the notion that contemporaneously developed companion diagnostics (CDx) could lead to more efficacious use of a cancer therapeutic in a selected population. Despite a gap of 12 years, the recent approvals of Zelboraf and Xalkori in quick succession by the FDA are a testament to the fact that the age of cancer therapeutics co-developed with a companion diagnostic is finally upon us. The purpose of this thesis was to test the hypotheses, that the trend for CDx based therapy launches in oncology is NOT headed towards a dramatic upturn in the next 5 years and in the view of biopharmaceutical executives - increasing price and market share of launched drugs are the dominant drivers for investing in companion diagnostics, and that the other features of CDx, such as improving the productivity of oncology drug development and reducing development costs are essentially dispensable. These hypotheses were tested using a study design that involved conducting a pilot study comprising of 18 interviews of stakeholders directly involved with the decision making of oncology drug development - to synthesize the extent of contemporaneously developed CDx to be launched with a cancer therapeutic in the coming 5 years. An analysis of the results obtained from the survey indicate, that a significant number of oncology drug launches within this decade would feature a co-developed companion diagnostic, and that despite challenges and initial trepidations over this business model - the higher probability of success, lower development costs, shorter time to market and pricing power associated with this approach, are incentives that are increasingly attracting more oncology firms to adopt this strategy for developing targeted therapeutics. Based on these findings, the original hypotheses were rejected."
}, {
    "id": "oai:dspace.mit.edu:1721.1/62524",
    "title": "Effect of patent law changes on the innovation strategy of Chinese and Indian Life Science companies as reflected in US patent filings",
    "abstract": "In this paper we evaluate how harmonization of patent laws in China and India to developed world standards has affected innovative research and development activity in the life sciences industry of those countries. The patents listed in the United States Patent and Trademark Office were used as a proxy to measure innovative activity. The number and types of patents filed over the period from 1976 through 2008 were analyzed for trends towards innovation. At a high level, we found that 'Drugs and Medical' account for only 6% of Chinese patents but make up 20% of the universe of Indian patents. When evaluating patent activity over time, we found that filings rose exponentially in the mid-nineties corresponding to the creation and implementation of product patent laws in both countries. India exhibited a much higher and steeper increase, likely due to its previously established capabilities as a generics manufacturer. When segmenting the data based on type of firms (academic, foreign multinationals and local private) we found that post product patent laws, local private firms exhibit more activity in India whereas local firms and multinationals show similar amounts of activity in China. In both countries, academic institutions show the greatest amount of activity compared to the multinationals and local private companies. We conclude that stronger IP laws have resulted in greater innovative activity as seen in the exponential rise in patent filings in the life sciences industry in both China and India. Although India has shown greater activity compared to China possibly due to its established capabilities in the generics space as a result of its protective patent regime prior to the harmonization.",
    "advisors": ["Fiona Murray", "Martha L. Gray"],
    "text": "Effect of patent law changes on the innovation strategy of Chinese and Indian Life Science companies as reflected in US patent filings In this paper we evaluate how harmonization of patent laws in China and India to developed world standards has affected innovative research and development activity in the life sciences industry of those countries. The patents listed in the United States Patent and Trademark Office were used as a proxy to measure innovative activity. The number and types of patents filed over the period from 1976 through 2008 were analyzed for trends towards innovation. At a high level, we found that 'Drugs and Medical' account for only 6% of Chinese patents but make up 20% of the universe of Indian patents. When evaluating patent activity over time, we found that filings rose exponentially in the mid-nineties corresponding to the creation and implementation of product patent laws in both countries. India exhibited a much higher and steeper increase, likely due to its previously established capabilities as a generics manufacturer. When segmenting the data based on type of firms (academic, foreign multinationals and local private) we found that post product patent laws, local private firms exhibit more activity in India whereas local firms and multinationals show similar amounts of activity in China. In both countries, academic institutions show the greatest amount of activity compared to the multinationals and local private companies. We conclude that stronger IP laws have resulted in greater innovative activity as seen in the exponential rise in patent filings in the life sciences industry in both China and India. Although India has shown greater activity compared to China possibly due to its established capabilities in the generics space as a result of its protective patent regime prior to the harmonization."
}, {
    "id": "oai:dspace.mit.edu:1721.1/42208",
    "title": "What future physicians want : a comparative analysis of the perception of medical students and pharmaceutical industry executives regarding the future of pharmaceutical sales",
    "abstract": "The leading publicly traded pharmaceutical companies (\"Big Pharma) in the US are facing a commercial crisis - their sales structure collectively consisting of more than 100,000 pharmaceutical sales representatives, originally setup to launch blockbusters, is suffering as a result of shrinking pipelines and low NME approvals. Although sales and marketing constitutes by far the largest corporate expense at 33% of revenues, sales productivity continues to decline. The goal of this study is to explore how pharmaceutical sales will change over the next 5 - 7 years and more specifically explore the role technology (including the internet) will play in the sales process. The study focuses on testing the perceptions of two key stakeholders -pharmaceutical executives and current medical students (future physicians) regarding the future of pharmaceutical sales process. Accordingly, 33 individuals were interviewed of which 18 were pharmaceutical executives and 15 were future physicians. The study tests three hypotheses: 1. Pharma executives believe that sales representative based detailing will continue to be the predominant method to engage and sell to physician customers while future physicians believe that technology will play a dominant role in the pharmaceutical detailing process. 2. Pharmaceutical executives agree that the most effective and ethical method to convey the benefits and challenges of an ethical pharmaceutical product are via a trained sales representative while future physicians believe that the sales representative does not effectively and ethically convey the merits of the relevant pharmaceutical therapy. 3. Person to person contact is not essential in conveying the merits of a particular ethical therapy - pharmaceutical executives disagree with this premise while future physicians agree.",
    "advisors": ["Teo Forcht Dagi", "Pedro Huertas"],
    "text": "What future physicians want : a comparative analysis of the perception of medical students and pharmaceutical industry executives regarding the future of pharmaceutical sales The leading publicly traded pharmaceutical companies (\"Big Pharma) in the US are facing a commercial crisis - their sales structure collectively consisting of more than 100,000 pharmaceutical sales representatives, originally setup to launch blockbusters, is suffering as a result of shrinking pipelines and low NME approvals. Although sales and marketing constitutes by far the largest corporate expense at 33% of revenues, sales productivity continues to decline. The goal of this study is to explore how pharmaceutical sales will change over the next 5 - 7 years and more specifically explore the role technology (including the internet) will play in the sales process. The study focuses on testing the perceptions of two key stakeholders -pharmaceutical executives and current medical students (future physicians) regarding the future of pharmaceutical sales process. Accordingly, 33 individuals were interviewed of which 18 were pharmaceutical executives and 15 were future physicians. The study tests three hypotheses: 1. Pharma executives believe that sales representative based detailing will continue to be the predominant method to engage and sell to physician customers while future physicians believe that technology will play a dominant role in the pharmaceutical detailing process. 2. Pharmaceutical executives agree that the most effective and ethical method to convey the benefits and challenges of an ethical pharmaceutical product are via a trained sales representative while future physicians believe that the sales representative does not effectively and ethically convey the merits of the relevant pharmaceutical therapy. 3. Person to person contact is not essential in conveying the merits of a particular ethical therapy - pharmaceutical executives disagree with this premise while future physicians agree."
}, {
    "id": "oai:dspace.mit.edu:1721.1/58179",
    "title": "Mapping healthcare information technology",
    "abstract": "In this thesis I have developed a map of Healthcare Information Technology applications used in the United States for care delivery, healthcare enterprise management, clinical support, research and patient engagement. No attempt has previously been made to develop such a taxonomy for use by healthcare policy makers and on-the-spot decision makers. Using my own fifteen years of experience in HIT, along with an extensive set of literature reviews, interviews and on-site research I assembled lists of applications and organized them into categories based on primary workflows. Seven categories of HIT systems emerged, which are Practice Tools, Advisory Tools, Financial Tools, Remote Healthcare Tools, Clinical Research Tools, Health 2.0 Tools and Enterprise Clinical Analytics, each of which have different operational characteristics and user communities. The results of this pilot study demonstrate that a map is possible. The draft map presented here will allow researchers and investors to focus on developing the next generation of HIT tools, including software platforms that orchestrate a variety of healthcare transactions, and will support policy makers as they consider the impact of Federal funding for HIT deployment and adoption. Further studies will refine the map, adding an additional level of detail below the seven categories established here, thus supporting tactical decision making at the hospital and medical practice level.",
    "advisors": ["T. Forcht Dagi", "Ernst R. Berndt"],
    "text": "Mapping healthcare information technology In this thesis I have developed a map of Healthcare Information Technology applications used in the United States for care delivery, healthcare enterprise management, clinical support, research and patient engagement. No attempt has previously been made to develop such a taxonomy for use by healthcare policy makers and on-the-spot decision makers. Using my own fifteen years of experience in HIT, along with an extensive set of literature reviews, interviews and on-site research I assembled lists of applications and organized them into categories based on primary workflows. Seven categories of HIT systems emerged, which are Practice Tools, Advisory Tools, Financial Tools, Remote Healthcare Tools, Clinical Research Tools, Health 2.0 Tools and Enterprise Clinical Analytics, each of which have different operational characteristics and user communities. The results of this pilot study demonstrate that a map is possible. The draft map presented here will allow researchers and investors to focus on developing the next generation of HIT tools, including software platforms that orchestrate a variety of healthcare transactions, and will support policy makers as they consider the impact of Federal funding for HIT deployment and adoption. Further studies will refine the map, adding an additional level of detail below the seven categories established here, thus supporting tactical decision making at the hospital and medical practice level."
}, {
    "id": "oai:dspace.mit.edu:1721.1/32249",
    "title": "Probabilistic modeling of the drug development domain: A Bayesian domain-knowledge application for pharmacovigilance",
    "abstract": "A recent analysis by the Tufts Center for the Study of Drug Development estimates that the cost of developing a single new chemical entity (NCE) into a successful therapeutic agent is $802 million. This figure is largely dependent on the expense of investigating NCEs that ultimately fail to be approved for use: between 70 - 90% of NCEs do not achieve New Drug Application (NDA) approval, and many of these failures are identified during the later, more costly phases of drug development. The exponential growth in the number of putative NCEs as a result of combinatorial chemistry and high-throughput screening has only confounded this problem by significantly increasing the number of early-phase NCEs under consideration for further costly development in human clinical trials. It is widely agreed upon that there are 3 major categories of reasons for drug failure: safety (toxicity), efficacy, and economics. This thesis is concerned with developing a Bayesian domain-knowledge probabilistic model (called Pharminator) to address the first two of these categories, with a goal of predicting clinical success of an NCE. Pharmacoeconomic modeling is a vastly different domain compared to Pharminator's clinical trial domain, and is beyond the scope of this thesis. While several clinical predictive models have been described in the literature over the past 10 years, the ongoing costly failure rate in drug development warrants developing more reliable predictors of NCE clinical success. The number of NDA approvals in 2002 fell to a 5-year low of 18, compared to 30, 35, 27, and 24 in 1998, 1999, 2000, and 2001 respectively, despite rapidly increasing numbers of NCEs as a result of high-throughput screening and combinatorial chemistry.",
    "advisors": ["Isaac S. Kohane"],
    "text": "Probabilistic modeling of the drug development domain: A Bayesian domain-knowledge application for pharmacovigilance A recent analysis by the Tufts Center for the Study of Drug Development estimates that the cost of developing a single new chemical entity (NCE) into a successful therapeutic agent is $802 million. This figure is largely dependent on the expense of investigating NCEs that ultimately fail to be approved for use: between 70 - 90% of NCEs do not achieve New Drug Application (NDA) approval, and many of these failures are identified during the later, more costly phases of drug development. The exponential growth in the number of putative NCEs as a result of combinatorial chemistry and high-throughput screening has only confounded this problem by significantly increasing the number of early-phase NCEs under consideration for further costly development in human clinical trials. It is widely agreed upon that there are 3 major categories of reasons for drug failure: safety (toxicity), efficacy, and economics. This thesis is concerned with developing a Bayesian domain-knowledge probabilistic model (called Pharminator) to address the first two of these categories, with a goal of predicting clinical success of an NCE. Pharmacoeconomic modeling is a vastly different domain compared to Pharminator's clinical trial domain, and is beyond the scope of this thesis. While several clinical predictive models have been described in the literature over the past 10 years, the ongoing costly failure rate in drug development warrants developing more reliable predictors of NCE clinical success. The number of NDA approvals in 2002 fell to a 5-year low of 18, compared to 30, 35, 27, and 24 in 1998, 1999, 2000, and 2001 respectively, despite rapidly increasing numbers of NCEs as a result of high-throughput screening and combinatorial chemistry."
}, {
    "id": "oai:dspace.mit.edu:1721.1/84413",
    "title": "The contribution of disease focused nonprofits to biomedical research and development",
    "abstract": "Patient-centered, disease-focused nonprofits are playing an increasingly prominent role in accelerating the development of new diagnostics, drugs, and therapies. They are engaging in a variety of complex venture philanthropic activities as they seek to bridge the \"valley of death\" gap between basic and clinical research. Examples of such activities include developing preclinical research tools, supporting clinical trials infrastructure, and investing in private biotechnology companies. In this thesis, 1: 1) quantify the financial contribution of US-based nonprofits to biomedical research and development (R&D) and the allocation to therapeutic areas; and 2) propose a framework for understanding the core functions of biomedical venture philanthropies. I find that US-based nonprofits contributed $3.7 billion to biomedical R&D in 2011, and that within certain disease areas nonprofit spending is comparable to or exceeds spending by the National Institutes of Health (NIH). I catalogue nonprofit activities and place them in a framework of five core functions: bridging gaps, enabling research, directing pipelines, informing stakeholders, and shaping markets. I present several case studies via this framework, discuss opportunities, and point out challenges such as a lack of conflict of interest standards. Methods included recording and analyzing publically available financial data from over 400 biomedical nonprofits, and conducting a series of in depth interviews with nonprofit executives and other related professionals.",
    "advisors": ["Ernst Berndt"],
    "text": "The contribution of disease focused nonprofits to biomedical research and development Patient-centered, disease-focused nonprofits are playing an increasingly prominent role in accelerating the development of new diagnostics, drugs, and therapies. They are engaging in a variety of complex venture philanthropic activities as they seek to bridge the \"valley of death\" gap between basic and clinical research. Examples of such activities include developing preclinical research tools, supporting clinical trials infrastructure, and investing in private biotechnology companies. In this thesis, 1: 1) quantify the financial contribution of US-based nonprofits to biomedical research and development (R&D) and the allocation to therapeutic areas; and 2) propose a framework for understanding the core functions of biomedical venture philanthropies. I find that US-based nonprofits contributed $3.7 billion to biomedical R&D in 2011, and that within certain disease areas nonprofit spending is comparable to or exceeds spending by the National Institutes of Health (NIH). I catalogue nonprofit activities and place them in a framework of five core functions: bridging gaps, enabling research, directing pipelines, informing stakeholders, and shaping markets. I present several case studies via this framework, discuss opportunities, and point out challenges such as a lack of conflict of interest standards. Methods included recording and analyzing publically available financial data from over 400 biomedical nonprofits, and conducting a series of in depth interviews with nonprofit executives and other related professionals."
}, {
    "id": "oai:dspace.mit.edu:1721.1/68462",
    "title": "Characteristics of disruptive innovation within the medical device industry",
    "abstract": "Innovation within the medical device industry had led to tremendous advances in the provision of care for patients worldwide. Continued progress in the treatment of disease will require effective processes for managing and analyzing innovation within this industry. Popular models of innovation exist for many industries outside of the medical realm; however, an extensive literature search uncovered a limited body of work related to innovation within the medical device industry. Specifically, literature that examines the application of the principles of disruptive innovation to the medical device industry is limited in scope and in quantity. It is theorized that the medical device industry may have unique characteristics for disruptive innovation due to the unique economic and regulatory structures that exist within this industry. This thesis applies the principles of disruptive innovation that were popularized by Clayton Christenson's seminal work, \"The Innovator's Dilemma\", to the medical device industry. These characteristics are subsequently delineated and evaluated through examination of the prosthetic cardiac valve industry. This industry serves as an effective case study due to the long history of innovation and the emergence of new disruptive technology within this specialty. The categorization of a \"disruptive\" innovation was made when a given technology altered the value proposition for treating a disease, relative to incumbent technology. This case study was evaluated along metrics of performance characteristics, the perception of leading customers, the ability to prospectively analyze markets, and the profitability of disruptive innovation for the incumbent firm. Conclusions were reached based on an examination of relevant literature and primary research conducted with thought leaders in this area. This research supports the conclusion that the cardiac valve industry has experienced unique characteristics in the development and commercialization of disruptive innovations. Specifically, incentives appear to exist within this industry that support development and commercialization of disruptive innovations by industry incumbents. Furthermore, the importance of understanding what value proposition is being disrupted is paramount in effectively understanding the incentives of manufacturers to innovate. When a technology is developed that is disruptive to a procedure, then the manufacturer tends to behave similar to a \"newentrant\" within the Christenson framework. This appears to also be true when the innovation is disruptive to that manufacturer's legacy products. Additional research is warranted in extrapolating this finding to the broader medical device industry.",
    "advisors": ["Frederick J. Schoen", "Josh Tolkoff"],
    "text": "Characteristics of disruptive innovation within the medical device industry Innovation within the medical device industry had led to tremendous advances in the provision of care for patients worldwide. Continued progress in the treatment of disease will require effective processes for managing and analyzing innovation within this industry. Popular models of innovation exist for many industries outside of the medical realm; however, an extensive literature search uncovered a limited body of work related to innovation within the medical device industry. Specifically, literature that examines the application of the principles of disruptive innovation to the medical device industry is limited in scope and in quantity. It is theorized that the medical device industry may have unique characteristics for disruptive innovation due to the unique economic and regulatory structures that exist within this industry. This thesis applies the principles of disruptive innovation that were popularized by Clayton Christenson's seminal work, \"The Innovator's Dilemma\", to the medical device industry. These characteristics are subsequently delineated and evaluated through examination of the prosthetic cardiac valve industry. This industry serves as an effective case study due to the long history of innovation and the emergence of new disruptive technology within this specialty. The categorization of a \"disruptive\" innovation was made when a given technology altered the value proposition for treating a disease, relative to incumbent technology. This case study was evaluated along metrics of performance characteristics, the perception of leading customers, the ability to prospectively analyze markets, and the profitability of disruptive innovation for the incumbent firm. Conclusions were reached based on an examination of relevant literature and primary research conducted with thought leaders in this area. This research supports the conclusion that the cardiac valve industry has experienced unique characteristics in the development and commercialization of disruptive innovations. Specifically, incentives appear to exist within this industry that support development and commercialization of disruptive innovations by industry incumbents. Furthermore, the importance of understanding what value proposition is being disrupted is paramount in effectively understanding the incentives of manufacturers to innovate. When a technology is developed that is disruptive to a procedure, then the manufacturer tends to behave similar to a \"newentrant\" within the Christenson framework. This appears to also be true when the innovation is disruptive to that manufacturer's legacy products. Additional research is warranted in extrapolating this finding to the broader medical device industry."
}, {
    "id": "oai:dspace.mit.edu:1721.1/46807",
    "title": "Silencing the host : the role of intronic microRNAs",
    "abstract": "Fifteen years ago lin-4 was reported to be the first endogenous small non-coding, but interfering RNA structure involved in developmental timing in C. elegans. First thought not, or only rarely, to occur in mammals, microRNAs are now among the major players in up-to-date genomic research. The mature molecules are ~22 nucleotides in length and, by targeting predominantly the 3' UTR of mRNAs, lead to translational repression or degradation of the target message, hence controlling important cellular mechanisms, including division, differentiation and death. This key role makes them excellent targets for cancer research. In fact they have been shown to have a major impact on cancer development in many cases. However, miRNAs are not a homogeneous class and can be sub classified into intragenic and intergenic, depending on their genomic position. Whereas intergenic miRNAs are expected to be independent transcriptional units, intragenic miRNAs are commonly believed to be regulated through their host gene. Despite of the growing knowledge on how miRNAs integrate into cellular regulatory networks, our current knowledge about the specific role of intragenic miRNAs is rather limited. In this work we integrated current miRNA knowledge bases, ranging from miRNA sequence and genomic localization information to target prediction, with biochemical pathway information and publicly available expression data to investigate functional properties of intragenic miRNAs and their relationship to their host genes. To the best of our knowledge, we are the first to show in a large-scale analysis that intragenic miRNAs seem to act as negative feedback regulators on multiple levels. We furthermore investigated the impact of this model on the potential role of intronic miRNAs in cancer pathogenesis.",
    "advisors": ["Lucila Ohno-Machado"],
    "text": "Silencing the host : the role of intronic microRNAs Fifteen years ago lin-4 was reported to be the first endogenous small non-coding, but interfering RNA structure involved in developmental timing in C. elegans. First thought not, or only rarely, to occur in mammals, microRNAs are now among the major players in up-to-date genomic research. The mature molecules are ~22 nucleotides in length and, by targeting predominantly the 3' UTR of mRNAs, lead to translational repression or degradation of the target message, hence controlling important cellular mechanisms, including division, differentiation and death. This key role makes them excellent targets for cancer research. In fact they have been shown to have a major impact on cancer development in many cases. However, miRNAs are not a homogeneous class and can be sub classified into intragenic and intergenic, depending on their genomic position. Whereas intergenic miRNAs are expected to be independent transcriptional units, intragenic miRNAs are commonly believed to be regulated through their host gene. Despite of the growing knowledge on how miRNAs integrate into cellular regulatory networks, our current knowledge about the specific role of intragenic miRNAs is rather limited. In this work we integrated current miRNA knowledge bases, ranging from miRNA sequence and genomic localization information to target prediction, with biochemical pathway information and publicly available expression data to investigate functional properties of intragenic miRNAs and their relationship to their host genes. To the best of our knowledge, we are the first to show in a large-scale analysis that intragenic miRNAs seem to act as negative feedback regulators on multiple levels. We furthermore investigated the impact of this model on the potential role of intronic miRNAs in cancer pathogenesis."
}, {
    "id": "oai:dspace.mit.edu:1721.1/98336",
    "title": "Use of wearable ambulatory monitor in the classification of movement states in Parkinson's disease",
    "abstract": "For Parkinson's patients to function at their best, their medications need to be optimally adjusted to the diurnal variation of symptoms. For this to occur, it is important for the managing clinician to have an accurate picture of how the patient's bradykinesia/hypokinesia and dyskinesia fluctuate throughout the normal daily activities. This thesis proposes the use of wearable accelerometers coupled with machine learning and statistical techniques in order to classify the movement states of Parkinson's patients and to provide a timeline of how the patients fluctuate throughout the day. A pilot study was performed using 2 patients with the goal of assessing the ability to classify dyskinesia and bradykinesia/hypokinesia based on accelerometric data. The patients were observed and videotaped. Clinical observations of bradykinesia/hypokinesia and dyskinesia were noted every minute. Neural networks were able to classify better than classification trees with an average c-index (equivalent to the area under the ROC curve) of 0.905 for bradykinesia/hypokinesia and 0.926 for dyskinesia. A separate group of 5 patients were observed with the additional goal of building models that can classify the movement of a patient without requiring clinically annotated training data for the same patient. An enhanced protocol was used in the final study. Dichotomized linear regression was found to classify well with an average c-index of 0.8219 for body bradykinesia/hypokinesia and 0.8799 using as the gold-standard the patient's diary. Dyskinesia was classified at a c-index of 0.7522. Neural networks did not perform as well, possibly because of restrictions placed on adjusting parameters. The two most clinically important problems: predicting",
    "advisors": ["Lucila Ohno-Machado"],
    "text": "Use of wearable ambulatory monitor in the classification of movement states in Parkinson's disease For Parkinson's patients to function at their best, their medications need to be optimally adjusted to the diurnal variation of symptoms. For this to occur, it is important for the managing clinician to have an accurate picture of how the patient's bradykinesia/hypokinesia and dyskinesia fluctuate throughout the normal daily activities. This thesis proposes the use of wearable accelerometers coupled with machine learning and statistical techniques in order to classify the movement states of Parkinson's patients and to provide a timeline of how the patients fluctuate throughout the day. A pilot study was performed using 2 patients with the goal of assessing the ability to classify dyskinesia and bradykinesia/hypokinesia based on accelerometric data. The patients were observed and videotaped. Clinical observations of bradykinesia/hypokinesia and dyskinesia were noted every minute. Neural networks were able to classify better than classification trees with an average c-index (equivalent to the area under the ROC curve) of 0.905 for bradykinesia/hypokinesia and 0.926 for dyskinesia. A separate group of 5 patients were observed with the additional goal of building models that can classify the movement of a patient without requiring clinically annotated training data for the same patient. An enhanced protocol was used in the final study. Dichotomized linear regression was found to classify well with an average c-index of 0.8219 for body bradykinesia/hypokinesia and 0.8799 using as the gold-standard the patient's diary. Dyskinesia was classified at a c-index of 0.7522. Neural networks did not perform as well, possibly because of restrictions placed on adjusting parameters. The two most clinically important problems: predicting"
}, {
    "id": "oai:dspace.mit.edu:1721.1/54595",
    "title": "What makes personalized medicine work? : an empirical analysis of the role of product attributes, medical professional societies and patient groups in the diffusion of four breast cancer genetic tests",
    "abstract": "Personalized medicine is the science and practice of customizing medical screening and treatment plans for an individual based on his or her genomic profile. Breast cancer is one of the first disease areas to serve as an example of this approach, where most patients have experienced its benefits through the use of genetic tests that provide decision support for health care workers regarding the likely effectiveness of specific drugs and, more broadly, the urgency of particular treatment options (for example, chemoprevention versus prophylactic surgery). Little is known about the diffusion of such personalized approaches to medical practice, particularly the factors shaping the adoption of genetic tests. While numerous medical diffusion studies have been published over the past few decades, most were univariate analyses and did not consider the unique aspects of genetic testing versus drugs. Moreover, they mainly focused on the characteristics and behaviors of physicians, patients, product manufacturers, and social networks, and did not explore the role of potentially important third parties like professional medical societies and patient groups (e.g. disease foundations and patient advocacy organizations). The aim of this thesis was to analyze the relationship between seven attributes of four breast cancer genetic tests and clinical adoption to show that standard diffusion frameworks can be enhanced through previously unstudied dimensions when evaluating personalized medicine-related innovations.",
    "advisors": ["Fiona E. Murray", "Anthony J. Sinskey"],
    "text": "What makes personalized medicine work? : an empirical analysis of the role of product attributes, medical professional societies and patient groups in the diffusion of four breast cancer genetic tests Personalized medicine is the science and practice of customizing medical screening and treatment plans for an individual based on his or her genomic profile. Breast cancer is one of the first disease areas to serve as an example of this approach, where most patients have experienced its benefits through the use of genetic tests that provide decision support for health care workers regarding the likely effectiveness of specific drugs and, more broadly, the urgency of particular treatment options (for example, chemoprevention versus prophylactic surgery). Little is known about the diffusion of such personalized approaches to medical practice, particularly the factors shaping the adoption of genetic tests. While numerous medical diffusion studies have been published over the past few decades, most were univariate analyses and did not consider the unique aspects of genetic testing versus drugs. Moreover, they mainly focused on the characteristics and behaviors of physicians, patients, product manufacturers, and social networks, and did not explore the role of potentially important third parties like professional medical societies and patient groups (e.g. disease foundations and patient advocacy organizations). The aim of this thesis was to analyze the relationship between seven attributes of four breast cancer genetic tests and clinical adoption to show that standard diffusion frameworks can be enhanced through previously unstudied dimensions when evaluating personalized medicine-related innovations."
}, {
    "id": "oai:dspace.mit.edu:1721.1/42209",
    "title": "Assessing decision inputs in drug development between small, early stage companies and big pharma : is there is a difference?",
    "abstract": "The pipeline productivity challenge facing large, publicly traded pharmaceutical companies, collectively referred to as \"Big Pharma,\" is well known. The unprecedented success Big Pharma achieved over the past few decades in commercializing blockbuster products means that it is now faced with near-term patent expirations on such products, representing billions of dollars in lost sales and profits. In order to maintain its economic momentum, Big Pharma is increasingly relying on the universe of smaller, early stage biotechnology and pharmaceutical companies as a source of new products. Early stage companies may offer Big Pharma something beyond simply more product bets. Several recent consulting studies have shown that economic returns to Big Pharma of products sourced externally are greater than those developed internally, which raises the question: What, if anything, are early stage companies doing differently from Big Pharma in their product development programs? The goal of this thesis is to evaluate product development programs (\"projects\") and compare qualitatively and quantitatively the decisions for projects at key decision points between early stage pharmaceutical and biotechnology companies and Big Pharma. Given that much of the critical discovery and R&D work on pharmaceutical products happens both before and during a product's entry into human clinical trials, this thesis focuses on those areas of the development continuum where R&D plays a central role. The key decision points are therefore: lead candidate selection/optimization, moving a project from pre-clinical trials into Phase I human clinical trials, and moving a project from Phase I to Phase II clinical trials in humans.",
    "advisors": ["Fiona Murray", "Kimberly M. Thompson"],
    "text": "Assessing decision inputs in drug development between small, early stage companies and big pharma : is there is a difference? The pipeline productivity challenge facing large, publicly traded pharmaceutical companies, collectively referred to as \"Big Pharma,\" is well known. The unprecedented success Big Pharma achieved over the past few decades in commercializing blockbuster products means that it is now faced with near-term patent expirations on such products, representing billions of dollars in lost sales and profits. In order to maintain its economic momentum, Big Pharma is increasingly relying on the universe of smaller, early stage biotechnology and pharmaceutical companies as a source of new products. Early stage companies may offer Big Pharma something beyond simply more product bets. Several recent consulting studies have shown that economic returns to Big Pharma of products sourced externally are greater than those developed internally, which raises the question: What, if anything, are early stage companies doing differently from Big Pharma in their product development programs? The goal of this thesis is to evaluate product development programs (\"projects\") and compare qualitatively and quantitatively the decisions for projects at key decision points between early stage pharmaceutical and biotechnology companies and Big Pharma. Given that much of the critical discovery and R&D work on pharmaceutical products happens both before and during a product's entry into human clinical trials, this thesis focuses on those areas of the development continuum where R&D plays a central role. The key decision points are therefore: lead candidate selection/optimization, moving a project from pre-clinical trials into Phase I human clinical trials, and moving a project from Phase I to Phase II clinical trials in humans."
}, {
    "id": "oai:dspace.mit.edu:1721.1/33844",
    "title": "Auditory-nerve fiber responses to amplitude modulated tones and multi-tonal stimuli",
    "abstract": "In normal-hearing ears, sound waves are amplified within the cochlea and a small fraction of the sound energy travels backward out into the ear canal, producing sounds known as \"otoacoustic emissions\" (OAE) that can be measured with a sensitive microphone. One class of OAE, called \"stimulus-frequency-otoacoustic-emissions\" (SFOAEs), has been hypothesized to be produced by a process known as \"coherent reflection filtering\" (CRF). The CRF theory provides a prediction between the SFOAE group delay and the group delays of tone responses on the basilar membrane within the cochlea. Using single and multiple-tone stimuli, we collected data from the firing patterns of single auditory-nerve-fibers (ANFs) from which basilar-membrane tone-response group delays can be calculated for both high and low best-frequency (BF) positions along the basilar membrane. These calculated basilar-membrane group delays were compared to published SFOAE group delays. Our results suggest that group delays calculated from the tip, the lower-frequency tail, or the above-BF region of ANF tuning curves do not match the CRF theory prediction. In obtaining the data to the test the CRF theory, we used two methods for obtaining ANF group delays at frequencies above BF: a previously published method and a simpler new method based on the same principle.",
    "advisors": ["John J. Guinan, Jr"],
    "text": "Auditory-nerve fiber responses to amplitude modulated tones and multi-tonal stimuli In normal-hearing ears, sound waves are amplified within the cochlea and a small fraction of the sound energy travels backward out into the ear canal, producing sounds known as \"otoacoustic emissions\" (OAE) that can be measured with a sensitive microphone. One class of OAE, called \"stimulus-frequency-otoacoustic-emissions\" (SFOAEs), has been hypothesized to be produced by a process known as \"coherent reflection filtering\" (CRF). The CRF theory provides a prediction between the SFOAE group delay and the group delays of tone responses on the basilar membrane within the cochlea. Using single and multiple-tone stimuli, we collected data from the firing patterns of single auditory-nerve-fibers (ANFs) from which basilar-membrane tone-response group delays can be calculated for both high and low best-frequency (BF) positions along the basilar membrane. These calculated basilar-membrane group delays were compared to published SFOAE group delays. Our results suggest that group delays calculated from the tip, the lower-frequency tail, or the above-BF region of ANF tuning curves do not match the CRF theory prediction. In obtaining the data to the test the CRF theory, we used two methods for obtaining ANF group delays at frequencies above BF: a previously published method and a simpler new method based on the same principle."
}, {
    "id": "oai:dspace.mit.edu:1721.1/55277",
    "title": "Innovative Alzheimer's disease clinical trial design in the coming age of biomarkers",
    "abstract": "Alzheimer's disease (AD) is a field with huge unmet need and only a few symptomatic treatments with limited efficacy have been made available to patients. With the testing of disease-modifying drugs in recent years, the length of AD clinical trials has tripled and the enrollment has gone up drastically. These investigational disease-modifying drugs address new targets including the amyloid beta and tau protein aggregation pathways in the brain. They have opened up a whole research field on biomarkers specific to these pathways. These biomarkers have however never been used to select a subpopulation that would enroll in clinical trials. This thesis defines a framework for assessing any AD biomarker's quality as a selection tool for enrolling a subpopulation into an AD clinical trial. Carefully selecting the patient population with appropriate biomarkers can lead to a reduction in required enrollment in a study to show statistical significance. In turn, the decreased patient enrollment helps sponsors reduce costs and allows them to test several drugs with the same budget. In order to test our framework in an applied and relevant setting, we established from www.clinicaltrials.gov that for disease-modifying drugs the primary endpoint is change in ADAS-cog points at 18 months and that the trials enrolled on average 337 patients per treatment group. These disease-modifying AD trials use the inference on means statistical model.",
    "advisors": ["Joseph V. Bonventre", "A. Gregory Sorensen"],
    "text": "Innovative Alzheimer's disease clinical trial design in the coming age of biomarkers Alzheimer's disease (AD) is a field with huge unmet need and only a few symptomatic treatments with limited efficacy have been made available to patients. With the testing of disease-modifying drugs in recent years, the length of AD clinical trials has tripled and the enrollment has gone up drastically. These investigational disease-modifying drugs address new targets including the amyloid beta and tau protein aggregation pathways in the brain. They have opened up a whole research field on biomarkers specific to these pathways. These biomarkers have however never been used to select a subpopulation that would enroll in clinical trials. This thesis defines a framework for assessing any AD biomarker's quality as a selection tool for enrolling a subpopulation into an AD clinical trial. Carefully selecting the patient population with appropriate biomarkers can lead to a reduction in required enrollment in a study to show statistical significance. In turn, the decreased patient enrollment helps sponsors reduce costs and allows them to test several drugs with the same budget. In order to test our framework in an applied and relevant setting, we established from www.clinicaltrials.gov that for disease-modifying drugs the primary endpoint is change in ADAS-cog points at 18 months and that the trials enrolled on average 337 patients per treatment group. These disease-modifying AD trials use the inference on means statistical model."
}, {
    "id": "oai:dspace.mit.edu:1721.1/68470",
    "title": "Does decreased research funding from the National Institutes of Health to local academic hospitals cause an increase in industry sponsored research funding?",
    "abstract": "The National Institutes of Health (NIH) has been the stalwart of research funding at universities and academic teaching hospitals. However, since the start of the last decade NIH funding has contracted in real terms. Anticipating future Federal Government fiscal austerity, the situation appears unlikely to improve and most likely will become worse. Local area teaching hospitals have explored other funding to support their large research infrastructure such as industry-sponsored research. This thesis qualitatively assessed whether the Federal Government and local area academic hospital fiscal data over the last six years support the hypothesis: Yes, industry funding has been received to support research at local area teaching hospitals to substitute for decreased availability of NIH funds. To test the hypothesis, Federal and local hospital fiscal data were extracted and statistical analysis was performed in three key areas to challenge the hypothesis and eliminate confounding data. First, is National Institutes of Health funding decreasing in real terms? Second, have local area teaching hospitals compensated by soliciting and receiving greater levels of industry sponsored research dollars? Third, has industry increased support in light of decreased NIH funding or are industry research commitments uncorrelated? The test questions were evaluated across two different hospitals and against various economic benchmarks. The hypothesis was rejected. Decreased NIH research funding granted to local academic hospitals has not caused a corresponding increase in industry sponsored research funding. Given the structural difficulties of industry and academic hospital collaboration, this likely impacts the level of industry sponsored research funding. Unfortunately, upcoming long-term Federal fiscal austerity may severely curtail NIH budgets. Academic hospitals will either need to consider greater industry collaboration or reduce the size and scope of their research activities.",
    "advisors": ["T. (Teo) Forcht Dagi", "Jeffrey Karp"],
    "text": "Does decreased research funding from the National Institutes of Health to local academic hospitals cause an increase in industry sponsored research funding? The National Institutes of Health (NIH) has been the stalwart of research funding at universities and academic teaching hospitals. However, since the start of the last decade NIH funding has contracted in real terms. Anticipating future Federal Government fiscal austerity, the situation appears unlikely to improve and most likely will become worse. Local area teaching hospitals have explored other funding to support their large research infrastructure such as industry-sponsored research. This thesis qualitatively assessed whether the Federal Government and local area academic hospital fiscal data over the last six years support the hypothesis: Yes, industry funding has been received to support research at local area teaching hospitals to substitute for decreased availability of NIH funds. To test the hypothesis, Federal and local hospital fiscal data were extracted and statistical analysis was performed in three key areas to challenge the hypothesis and eliminate confounding data. First, is National Institutes of Health funding decreasing in real terms? Second, have local area teaching hospitals compensated by soliciting and receiving greater levels of industry sponsored research dollars? Third, has industry increased support in light of decreased NIH funding or are industry research commitments uncorrelated? The test questions were evaluated across two different hospitals and against various economic benchmarks. The hypothesis was rejected. Decreased NIH research funding granted to local academic hospitals has not caused a corresponding increase in industry sponsored research funding. Given the structural difficulties of industry and academic hospital collaboration, this likely impacts the level of industry sponsored research funding. Unfortunately, upcoming long-term Federal fiscal austerity may severely curtail NIH budgets. Academic hospitals will either need to consider greater industry collaboration or reduce the size and scope of their research activities."
}, {
    "id": "oai:dspace.mit.edu:1721.1/39569",
    "title": "Enhancing productivity through effective collaborations : the barriers and enablers of collaboration within geographic bioclusters",
    "abstract": "Increasing competition and specialization of firms in the life sciences industry has led to recognition of the need for collaboration. Bioclusters, the co-location of life sciences entities in a specific geographic area, have therefore emerged as a global trend. While it is assumed that such clusters allow stakeholders to realize synergies through participation and presence in the local area, the collaborative behavior within these clusters has yet to be explored. The goal of this study was to characterize the barriers and enablers of effective collaboration within bioclusters, and amongst their key stakeholder groups. This study directly compared the bioclusters of San Diego and Singapore to gain an understanding of their relative collaborative environments. San Diego, with cluster longevity of over 40 years, provided an example of organic growth, given its roots in entrepreneurial activities. The Singapore cluster, still in an embryonic state, has a history of organized growth due to the leadership, support, and funding of the Singaporean government. The study of clusters that differ in history of formation and longevity of presence provided the breadth of information needed for an effective comparison of their collaborative environments and approach to collaborative endeavors.",
    "advisors": ["Ernst Berndt", "Frank Douglas"],
    "text": "Enhancing productivity through effective collaborations : the barriers and enablers of collaboration within geographic bioclusters Increasing competition and specialization of firms in the life sciences industry has led to recognition of the need for collaboration. Bioclusters, the co-location of life sciences entities in a specific geographic area, have therefore emerged as a global trend. While it is assumed that such clusters allow stakeholders to realize synergies through participation and presence in the local area, the collaborative behavior within these clusters has yet to be explored. The goal of this study was to characterize the barriers and enablers of effective collaboration within bioclusters, and amongst their key stakeholder groups. This study directly compared the bioclusters of San Diego and Singapore to gain an understanding of their relative collaborative environments. San Diego, with cluster longevity of over 40 years, provided an example of organic growth, given its roots in entrepreneurial activities. The Singapore cluster, still in an embryonic state, has a history of organized growth due to the leadership, support, and funding of the Singaporean government. The study of clusters that differ in history of formation and longevity of presence provided the breadth of information needed for an effective comparison of their collaborative environments and approach to collaborative endeavors."
}, {
    "id": "oai:dspace.mit.edu:1721.1/35679",
    "title": "Valuation of the use of biomarkers predictive of drug efficacy to enrich responders in oncology drug clinical development",
    "abstract": "I study several aspects of the value in performing oncology clinical trials using screening biomarkers to preferentially select and enroll responders. From trial reports and investigational reports on potential biomarkers, I construct a series of six cases comparing the trial as conducted to a hypothetical trial using different screening and eligibility criteria. These cases illustrate, within limits of the model, what difference the use of a plausible biomarker test may have on trial size, cost, number of patients screened, and number of patients exposed to experimental treatment without benefit.",
    "advisors": ["A. Gregory Sorensen", "Teodro Forcht Dagi"],
    "text": "Valuation of the use of biomarkers predictive of drug efficacy to enrich responders in oncology drug clinical development I study several aspects of the value in performing oncology clinical trials using screening biomarkers to preferentially select and enroll responders. From trial reports and investigational reports on potential biomarkers, I construct a series of six cases comparing the trial as conducted to a hypothetical trial using different screening and eligibility criteria. These cases illustrate, within limits of the model, what difference the use of a plausible biomarker test may have on trial size, cost, number of patients screened, and number of patients exposed to experimental treatment without benefit."
}, {
    "id": "oai:dspace.mit.edu:1721.1/90177",
    "title": "Retrieval mechanisms in sentence comprehension",
    "abstract": "This work investigates the nature of the memory mechanisms utilized in language comprehension. Through the use of the Speed Accuracy Tradeoff (SAT) paradigm (Wickelgren, 1977), healthy young adults were studied for the use of parallel or serial search mechanisms to understand syntactically complex sentences with multiple embeddings. Systematically designed sentence stimuli tested whether the relevant memory mechanism differs when reanalysis is required. Results indicated that sentence length and syntactic ambiguity affected overall accuracy of sentence comprehension. The rate in which information was retrieved did not vary for most sentence types, but may have been affected by length in one type of sentence (ambiguous \"early closure\" sentences). The data support a parallel, content-addressable retrieval mechanism for information in most sentences but may provide evidence for serial search in ambiguous sentences that require complex syntactic reanalysis.",
    "advisors": ["David Caplan"],
    "text": "Retrieval mechanisms in sentence comprehension This work investigates the nature of the memory mechanisms utilized in language comprehension. Through the use of the Speed Accuracy Tradeoff (SAT) paradigm (Wickelgren, 1977), healthy young adults were studied for the use of parallel or serial search mechanisms to understand syntactically complex sentences with multiple embeddings. Systematically designed sentence stimuli tested whether the relevant memory mechanism differs when reanalysis is required. Results indicated that sentence length and syntactic ambiguity affected overall accuracy of sentence comprehension. The rate in which information was retrieved did not vary for most sentence types, but may have been affected by length in one type of sentence (ambiguous \"early closure\" sentences). The data support a parallel, content-addressable retrieval mechanism for information in most sentences but may provide evidence for serial search in ambiguous sentences that require complex syntactic reanalysis."
}, {
    "id": "oai:dspace.mit.edu:1721.1/54675",
    "title": "Novel strategies for characterizing T Cell responses in SIV-infected rhesus monkeys",
    "abstract": "Human Immunodeficiency Virus (HIV) is the cause of Acquired Immune Deficiency Syndrome (AIDS) and has killed over 25 million people since the disease was first recognized in 1981. As of 2007, 33 million people globally are infected with HIV and this number is growing. HIV infects and depletes CD4+ helper T cells, affecting the ability of the immune system to defend the host against common infections. While anti-retroviral therapy has decreased morbidity and mortality, these drugs are not curative. In addition, they are beyond the financial reach of many HIV infected patients. Thus, the development of strategies to control HIV spread is a high priority. The most relevant animal model for studying HIV is the Simian Immunodeficiency Virus (SIV) - infected rhesus monkey. While HIV research has focused on studying peripheral blood specimens, mucosal sites have recently been identified as a focal point for HIV replication and tissue destruction. They are usually the sites of primary infection in the setting of sexual transmission and they are also important sites of immune depletion. If methods for controlling the replication of the virus early after infection in mucosal sites are available, it may be possible to eliminate the virus prior to systemic spread. While strategies for generating strong neutralizing antibody responses have not yet been developed, emerging data suggest that CD8+ cytotoxic T cells can contribute substantially to early virus control. It is important to study CD8+ T cells in the setting of SIV infection in rhesus monkeys, particularly in mucosal sites, using functional as well as transcriptional assays.",
    "advisors": ["Norman L. Letvin"],
    "text": "Novel strategies for characterizing T Cell responses in SIV-infected rhesus monkeys Human Immunodeficiency Virus (HIV) is the cause of Acquired Immune Deficiency Syndrome (AIDS) and has killed over 25 million people since the disease was first recognized in 1981. As of 2007, 33 million people globally are infected with HIV and this number is growing. HIV infects and depletes CD4+ helper T cells, affecting the ability of the immune system to defend the host against common infections. While anti-retroviral therapy has decreased morbidity and mortality, these drugs are not curative. In addition, they are beyond the financial reach of many HIV infected patients. Thus, the development of strategies to control HIV spread is a high priority. The most relevant animal model for studying HIV is the Simian Immunodeficiency Virus (SIV) - infected rhesus monkey. While HIV research has focused on studying peripheral blood specimens, mucosal sites have recently been identified as a focal point for HIV replication and tissue destruction. They are usually the sites of primary infection in the setting of sexual transmission and they are also important sites of immune depletion. If methods for controlling the replication of the virus early after infection in mucosal sites are available, it may be possible to eliminate the virus prior to systemic spread. While strategies for generating strong neutralizing antibody responses have not yet been developed, emerging data suggest that CD8+ cytotoxic T cells can contribute substantially to early virus control. It is important to study CD8+ T cells in the setting of SIV infection in rhesus monkeys, particularly in mucosal sites, using functional as well as transcriptional assays."
}, {
    "id": "oai:dspace.mit.edu:1721.1/65518",
    "title": "Regulatory roles of endothelial cells in cancer",
    "abstract": "This thesis describes the biochemical regulatory impact of endothelial cells, the cells that line all blood vessels, in cancer. Our work draws from concepts in vascular repair and tissue engineering and extends the view of tumor vessels from perfusing tubes to delivery platforms lined with potent paracrine regulatory cells. We focus on how the endothelial cells themselves regulate tumor biology in a state-dependent fashion. We found that healthy endothelial cells inhibit cancer cell proliferation, invasiveness, and inflammatory signaling and that a defined perturbation of the healthy endothelial cell state - silencing of the gene encoding perlecan - causes loss of the invasion-inhibitory capabilities of endothelial cells by transcriptional upregulation of IL-6. The use of matrixembedded endothelial implants enabled the effects in cell culture to be expanded and validated in animal models. Moreover, endothelial cells exposed to a pathologically activating and inflammatory culture environment, similar to endothelial cells exposed to the atherosclerotic milieu, were leaky and inflamed, with dysregulated proliferative and leukocyte binding properties. Unlike healthy endothelial cells, which suppress cancer cell proliferation and metastasis, these dysfunctional endothelial cells instead aggressively stimulated cancer cell inflammatory signaling and invasiveness, which correlated with stimulation of spontaneous metastasis when implanted as matrixembedded cell implants adjacent to tumors. Fascinatingly we were able to identify markers of endothelial dysfunction, including reduction of endothelial perlecan expression, in human non-small cell lung carcinoma specimens. The state-dependent impact of endothelial cells on cancer biology adds another element to stromal regulation of cancer and brings together a range of disciplines and disparate findings regarding vascular control of tumors. That healthy endothelial cells suppress and dysfunctional cells promote tumor aggression may help to explain undesired effects of therapies that target tumor blood vessels. The harnessing of tissue engineering to regulate vascular and cancer biology may motivate the development of innovative pharmacologic and cell-based therapies for cancer.",
    "advisors": ["Elazer R. Edelman"],
    "text": "Regulatory roles of endothelial cells in cancer This thesis describes the biochemical regulatory impact of endothelial cells, the cells that line all blood vessels, in cancer. Our work draws from concepts in vascular repair and tissue engineering and extends the view of tumor vessels from perfusing tubes to delivery platforms lined with potent paracrine regulatory cells. We focus on how the endothelial cells themselves regulate tumor biology in a state-dependent fashion. We found that healthy endothelial cells inhibit cancer cell proliferation, invasiveness, and inflammatory signaling and that a defined perturbation of the healthy endothelial cell state - silencing of the gene encoding perlecan - causes loss of the invasion-inhibitory capabilities of endothelial cells by transcriptional upregulation of IL-6. The use of matrixembedded endothelial implants enabled the effects in cell culture to be expanded and validated in animal models. Moreover, endothelial cells exposed to a pathologically activating and inflammatory culture environment, similar to endothelial cells exposed to the atherosclerotic milieu, were leaky and inflamed, with dysregulated proliferative and leukocyte binding properties. Unlike healthy endothelial cells, which suppress cancer cell proliferation and metastasis, these dysfunctional endothelial cells instead aggressively stimulated cancer cell inflammatory signaling and invasiveness, which correlated with stimulation of spontaneous metastasis when implanted as matrixembedded cell implants adjacent to tumors. Fascinatingly we were able to identify markers of endothelial dysfunction, including reduction of endothelial perlecan expression, in human non-small cell lung carcinoma specimens. The state-dependent impact of endothelial cells on cancer biology adds another element to stromal regulation of cancer and brings together a range of disciplines and disparate findings regarding vascular control of tumors. That healthy endothelial cells suppress and dysfunctional cells promote tumor aggression may help to explain undesired effects of therapies that target tumor blood vessels. The harnessing of tissue engineering to regulate vascular and cancer biology may motivate the development of innovative pharmacologic and cell-based therapies for cancer."
}, {
    "id": "oai:dspace.mit.edu:1721.1/111333",
    "title": "Quantifiable MRI changes in cerebral white matter and their importance to aging, cognition, and Alzheimer's disease",
    "abstract": "Alzheimer's disease (AD) is a neurodegenerative disease for which there are no preventative or therapeutic interventions. It is currently understood to be linked to the accumulation of pathologic proteins in the brain. In the past several decades, a strong body of evidence has accumulated that is suggestive of a vascular-related pathway in AD. A deeper understanding of this phenomenon is critical in advancing our understanding of the AD biological process as well and may lead to the discovery of novel therapeutic targets. A common age-related change in the brain is the development of white matter signal abnormalities (WMSA) as seen on magnetic resonance imaging (MRI). These lesions are related to cognitive function and are thought to be due to compromised integrity of the brain's vascular system. Despite evidence that WMSA are known to influence the clinical progression of AD, we do not currently view AD as a vascular disease nor do we use WMSA as a clinical indicator of AD. This is because we still do not know whether or not WMSA are a distinct phenomenon in AD, their relationship to traditional AD biomarkers, and how they independently contribute to clinical status. In this work, we examine if and how WMSA are related to AD conversion, whether they differ in their spatial distribution between typical aging and AD, and how they are linked to classic pathologic markers of AD. This work also includes technical development for WMSA quantification and baseline studies of WMSA in cognitively healthy aging. The main findings of this work suggest that WMSA are distinctly different in AD than in typical aging and have a unique role in AD progression. This not only motivates the utility of WMSA in our clinical treatment of AD, but also provides insight into the biological underpinnings of the disease process that may lead to novel therapeutic targets.",
    "advisors": ["David H. Salat"],
    "text": "Quantifiable MRI changes in cerebral white matter and their importance to aging, cognition, and Alzheimer's disease Alzheimer's disease (AD) is a neurodegenerative disease for which there are no preventative or therapeutic interventions. It is currently understood to be linked to the accumulation of pathologic proteins in the brain. In the past several decades, a strong body of evidence has accumulated that is suggestive of a vascular-related pathway in AD. A deeper understanding of this phenomenon is critical in advancing our understanding of the AD biological process as well and may lead to the discovery of novel therapeutic targets. A common age-related change in the brain is the development of white matter signal abnormalities (WMSA) as seen on magnetic resonance imaging (MRI). These lesions are related to cognitive function and are thought to be due to compromised integrity of the brain's vascular system. Despite evidence that WMSA are known to influence the clinical progression of AD, we do not currently view AD as a vascular disease nor do we use WMSA as a clinical indicator of AD. This is because we still do not know whether or not WMSA are a distinct phenomenon in AD, their relationship to traditional AD biomarkers, and how they independently contribute to clinical status. In this work, we examine if and how WMSA are related to AD conversion, whether they differ in their spatial distribution between typical aging and AD, and how they are linked to classic pathologic markers of AD. This work also includes technical development for WMSA quantification and baseline studies of WMSA in cognitively healthy aging. The main findings of this work suggest that WMSA are distinctly different in AD than in typical aging and have a unique role in AD progression. This not only motivates the utility of WMSA in our clinical treatment of AD, but also provides insight into the biological underpinnings of the disease process that may lead to novel therapeutic targets."
}, {
    "id": "oai:dspace.mit.edu:1721.1/79250",
    "title": "Bacteria-targeting nanoparticles for managing infections",
    "abstract": "Bacterial infections continue to be a significant concern particularly in healthcare settings and in the developing world. Current challenges include the increasing spread of drug resistant (DR) organisms, the side effects of antibiotic therapy, the negative consequences of clearing the commensal bacterial flora, and difficulties in developing prophylactic vaccines. This thesis was an investigation of the potential of a class of polymeric nanoparticles (NP) to contribute to the management of bacterial infections. More specifically, steps were taken towards using these NPs (1) to achieve greater spatiotemporal control over drug therapy by more targeted antibiotic delivery to bacteria, and (2) to develop a prophylactic vaccine formulation against the common bacterial sexually transmitted disease (STD) caused by Chlamydia trachomatis. In the first part, we synthesized polymeric NPs containing poly(lactic-co-glycolic acid)- block-poly(L-histidine)-block-poly(ethylene glycol) (PLGA-PLH-PEG). We show that these NPs are able to bind to bacteria under model acidic infection conditions and are able to encapsulate and deliver vancomycin to inhibit the growth of Staphylococcus aureus bacteria in vitro. Further work showed that the PLGA-PLH-PEG-based NPs demonstrated the potential for competition for binding bacteria at a site of infection from soluble protein and model phagocytic and tissue-resident cells in a NP composition dependent manner. The NPs demonstrated low toxicity in vitro, were well tolerated by mice in vivo, and circulated in the blood on timescales comparable to control PLGA-PEG NPs. In the second part, we used PLGA-PLH-PEG-based NPs to design a prophylactic vaccine against the obligate intracellular bacterium Chlamydia trachomatis, the most common cause of bacterial STD in the world. Currently, no vaccines against this pathogen are approved for use in humans. We first formulated NPs encapsulating the TLR7 agonist R848 conjugated to poly(lactic acid) (R848-PLA) in PLGA-PLH-PEG-based NPs, then incubated these R848-NPs with UV-inactivated C. trachomatis bacteria in acidity, forming a construct. Mice immunized with this vaccine via genital or intranasal routes demonstrated protection from genital infection post immunization in a primarily CD4 T cell-dependent manner. These results may suggest avenues for future work in designing and developing more targeted drug therapies or vaccine formulations for managing bacterial infections using polymeric nanoparticles.",
    "advisors": ["Robert Langer", "Omid C. Farokhzad"],
    "text": "Bacteria-targeting nanoparticles for managing infections Bacterial infections continue to be a significant concern particularly in healthcare settings and in the developing world. Current challenges include the increasing spread of drug resistant (DR) organisms, the side effects of antibiotic therapy, the negative consequences of clearing the commensal bacterial flora, and difficulties in developing prophylactic vaccines. This thesis was an investigation of the potential of a class of polymeric nanoparticles (NP) to contribute to the management of bacterial infections. More specifically, steps were taken towards using these NPs (1) to achieve greater spatiotemporal control over drug therapy by more targeted antibiotic delivery to bacteria, and (2) to develop a prophylactic vaccine formulation against the common bacterial sexually transmitted disease (STD) caused by Chlamydia trachomatis. In the first part, we synthesized polymeric NPs containing poly(lactic-co-glycolic acid)- block-poly(L-histidine)-block-poly(ethylene glycol) (PLGA-PLH-PEG). We show that these NPs are able to bind to bacteria under model acidic infection conditions and are able to encapsulate and deliver vancomycin to inhibit the growth of Staphylococcus aureus bacteria in vitro. Further work showed that the PLGA-PLH-PEG-based NPs demonstrated the potential for competition for binding bacteria at a site of infection from soluble protein and model phagocytic and tissue-resident cells in a NP composition dependent manner. The NPs demonstrated low toxicity in vitro, were well tolerated by mice in vivo, and circulated in the blood on timescales comparable to control PLGA-PEG NPs. In the second part, we used PLGA-PLH-PEG-based NPs to design a prophylactic vaccine against the obligate intracellular bacterium Chlamydia trachomatis, the most common cause of bacterial STD in the world. Currently, no vaccines against this pathogen are approved for use in humans. We first formulated NPs encapsulating the TLR7 agonist R848 conjugated to poly(lactic acid) (R848-PLA) in PLGA-PLH-PEG-based NPs, then incubated these R848-NPs with UV-inactivated C. trachomatis bacteria in acidity, forming a construct. Mice immunized with this vaccine via genital or intranasal routes demonstrated protection from genital infection post immunization in a primarily CD4 T cell-dependent manner. These results may suggest avenues for future work in designing and developing more targeted drug therapies or vaccine formulations for managing bacterial infections using polymeric nanoparticles."
}, {
    "id": "oai:dspace.mit.edu:1721.1/97822",
    "title": "Medial olivocochlear efferent (MOC) effects on stimulus frequency otoacoustic emissions (SFOAEs) and auditory-nerve compound action potentials (CAP) in guinea pigs",
    "abstract": "In humans, SFOAEs can non-invasively assess MOC strength and, may predict the MOC reduction of damage from traumatic sounds. However, the functionally important MOC effect is inhibition of auditory-nerve (AN) responses. Understanding the relationship between MOC effects on SFOAEs and AN CAPs is important for understanding SFOAE generation and for development of clinical tools that use these measures. This thesis presents several novel data sets that address MOC effects on SFOAEs, CAPs and the relationship between them in guinea pigs. Classic theory indicates that SFOAEs come from cochlear irregularities that coherently reflect energy at the peak of the traveling wave (TW), and that reflected energy arrives in the ear canal as a single wave at certain delay. Contrary to theory, in humans and chinchillas there have been reports of SFOAEs having multiple components with different delays, and that lowfrequency SFOAE delays are too short. The first thesis aim used time-frequency analysis to show that guinea pigs have frequency regions over which SFOAEs appear to have multiple components. However, we argue that the multiple components can be a simple result of variations in the patters of irregularities near the TW peak and are not necessarily indicative of multiple SFAOE sources. From comparison of our SFOAE delays with previously reported neural delays, we hypothesize that short SFOAE delays at low frequencies arise from a cochlear motion with a group delay shorter than the TW group delay. Aim 2 investigated how SFOAEs are affected by brainstem electrical stimulation of MOC fibers and found that MOC activation sometimes inhibited and sometimes enhanced SFOAEs. MOC stimulation always decreased CAP sensitivity which rules out SFOAE enhancement from increased cochlear amplification. We propose that shock-evoked MOC activity increases cochlear irregularity which results in increased SFOAE amplitudes. Aim 3 investigated the relationship between MOC effects on SFOAEs and tone-pip-evoked AN CAPs at same frequency and sound level. The ratio of the MOC effect on the SFOAE to the MOC effect on the CAP showed a highly-significant decrease (p<0.001) as the strength of MOC stimulation was increased. Although this observation was unexpected, several hypothesis to explain it are presented.",
    "advisors": ["John. J. Guinan Jr"],
    "text": "Medial olivocochlear efferent (MOC) effects on stimulus frequency otoacoustic emissions (SFOAEs) and auditory-nerve compound action potentials (CAP) in guinea pigs In humans, SFOAEs can non-invasively assess MOC strength and, may predict the MOC reduction of damage from traumatic sounds. However, the functionally important MOC effect is inhibition of auditory-nerve (AN) responses. Understanding the relationship between MOC effects on SFOAEs and AN CAPs is important for understanding SFOAE generation and for development of clinical tools that use these measures. This thesis presents several novel data sets that address MOC effects on SFOAEs, CAPs and the relationship between them in guinea pigs. Classic theory indicates that SFOAEs come from cochlear irregularities that coherently reflect energy at the peak of the traveling wave (TW), and that reflected energy arrives in the ear canal as a single wave at certain delay. Contrary to theory, in humans and chinchillas there have been reports of SFOAEs having multiple components with different delays, and that lowfrequency SFOAE delays are too short. The first thesis aim used time-frequency analysis to show that guinea pigs have frequency regions over which SFOAEs appear to have multiple components. However, we argue that the multiple components can be a simple result of variations in the patters of irregularities near the TW peak and are not necessarily indicative of multiple SFAOE sources. From comparison of our SFOAE delays with previously reported neural delays, we hypothesize that short SFOAE delays at low frequencies arise from a cochlear motion with a group delay shorter than the TW group delay. Aim 2 investigated how SFOAEs are affected by brainstem electrical stimulation of MOC fibers and found that MOC activation sometimes inhibited and sometimes enhanced SFOAEs. MOC stimulation always decreased CAP sensitivity which rules out SFOAE enhancement from increased cochlear amplification. We propose that shock-evoked MOC activity increases cochlear irregularity which results in increased SFOAE amplitudes. Aim 3 investigated the relationship between MOC effects on SFOAEs and tone-pip-evoked AN CAPs at same frequency and sound level. The ratio of the MOC effect on the SFOAE to the MOC effect on the CAP showed a highly-significant decrease (p<0.001) as the strength of MOC stimulation was increased. Although this observation was unexpected, several hypothesis to explain it are presented."
}, {
    "id": "oai:dspace.mit.edu:1721.1/54452",
    "title": "Neural correlates and mechanisms of sound localization in everyday reverberant settings",
    "abstract": "Nearly all listening environments-indoors and outdoors alike-are full of boundary surfaces (e.g., walls, trees, and rocks) that produce acoustic reflections. These reflections interfere with the direct sound arriving at a listener's ears, distorting the binaural cues for sound localization. Yet, human listeners have little difficulty localizing sounds in most settings. This thesis addresses fundamental questions regarding the neural basis of sound localization in everyday reverberant environments. In the first set of experiments, we investigate the effects of reverberation on the directional sensitivity of low-frequency auditory neurons sensitive to interaural time differences (ITD), the principal cue for localizing sound containing low frequency energy. Because reverberant energy builds up over time, the source location is represented relatively faithfully during the early portion of a sound, but this representation becomes increasingly degraded later in the stimulus. We show that the directional sensitivity of ITD-sensitive neurons in the auditory midbrain of anesthetized cats and awake rabbits follows a similar time course. However, the tendency of neurons to fire preferentially at the onset of a stimulus results in more robust directional sensitivity than expected, suggesting a simple mechanism for improving directional sensitivity in reverberation. To probe the role of temporal response dynamics, we use a conditioning paradigm to systematically alter temporal response patterns of single neurons. Results suggest that making temporal response patterns less onset-dominated typically leads to poorer directional sensitivity in reverberation. In parallel behavioral experiments, we show that human lateralization judgments are consistent with predictions from a population rate model for decoding the observed midbrain responses, suggesting a subcortical origin for robust sound localization in reverberant environments. In the second part of the thesis we examine the effects of reverberation on directional sensitivity of neurons across the tonotopic axis in the awake rabbit auditory midbrain. We find that reverberation degrades the directional sensitivity of single neurons, although the amount of degradation depends on the characteristic frequency and the type of binaural cues available. When ITD is the only available directional cue, low frequency neurons sensitive to ITD in the fine-time structure maintain better directional sensitivity in reverberation than high frequency neurons sensitive to ITD in the envelope. On the other hand, when both ITD and interaural level differences (ILD) cues are available, directional sensitivity is comparable throughout the tonotopic axis, suggesting that, at high frequencies, ILDs provide better directional information than envelope ITDs in reverberation. These findings can account for results from human psychophysical studies of spatial hearing in reverberant environments. This thesis marks fundamental progress towards elucidating the neural basis for spatial hearing in everyday settings. Overall, our results suggest that the information contained in the rate responses of neurons in the auditory midbrain is sufficient to account for human sound localization in reverberant environments.",
    "advisors": ["Bertrand Delgutte"],
    "text": "Neural correlates and mechanisms of sound localization in everyday reverberant settings Nearly all listening environments-indoors and outdoors alike-are full of boundary surfaces (e.g., walls, trees, and rocks) that produce acoustic reflections. These reflections interfere with the direct sound arriving at a listener's ears, distorting the binaural cues for sound localization. Yet, human listeners have little difficulty localizing sounds in most settings. This thesis addresses fundamental questions regarding the neural basis of sound localization in everyday reverberant environments. In the first set of experiments, we investigate the effects of reverberation on the directional sensitivity of low-frequency auditory neurons sensitive to interaural time differences (ITD), the principal cue for localizing sound containing low frequency energy. Because reverberant energy builds up over time, the source location is represented relatively faithfully during the early portion of a sound, but this representation becomes increasingly degraded later in the stimulus. We show that the directional sensitivity of ITD-sensitive neurons in the auditory midbrain of anesthetized cats and awake rabbits follows a similar time course. However, the tendency of neurons to fire preferentially at the onset of a stimulus results in more robust directional sensitivity than expected, suggesting a simple mechanism for improving directional sensitivity in reverberation. To probe the role of temporal response dynamics, we use a conditioning paradigm to systematically alter temporal response patterns of single neurons. Results suggest that making temporal response patterns less onset-dominated typically leads to poorer directional sensitivity in reverberation. In parallel behavioral experiments, we show that human lateralization judgments are consistent with predictions from a population rate model for decoding the observed midbrain responses, suggesting a subcortical origin for robust sound localization in reverberant environments. In the second part of the thesis we examine the effects of reverberation on directional sensitivity of neurons across the tonotopic axis in the awake rabbit auditory midbrain. We find that reverberation degrades the directional sensitivity of single neurons, although the amount of degradation depends on the characteristic frequency and the type of binaural cues available. When ITD is the only available directional cue, low frequency neurons sensitive to ITD in the fine-time structure maintain better directional sensitivity in reverberation than high frequency neurons sensitive to ITD in the envelope. On the other hand, when both ITD and interaural level differences (ILD) cues are available, directional sensitivity is comparable throughout the tonotopic axis, suggesting that, at high frequencies, ILDs provide better directional information than envelope ITDs in reverberation. These findings can account for results from human psychophysical studies of spatial hearing in reverberant environments. This thesis marks fundamental progress towards elucidating the neural basis for spatial hearing in everyday settings. Overall, our results suggest that the information contained in the rate responses of neurons in the auditory midbrain is sufficient to account for human sound localization in reverberant environments."
}, {
    "id": "oai:dspace.mit.edu:1721.1/65512",
    "title": "Collagen scaffolds in full- and hemi- resection spinal cord injury models",
    "abstract": "Basic scientific research over the past few decades has shown some light on the complex pathophysiology of SCI and has enhanced our understanding of some of the important factors that contribute to the lack of regeneration following the initial traumatic injury and secondary injury response in the adult spinal cord. These factors include both intrinsic limitations in the regeneration capacity of mature neurons, but also a host of environmental factors which inhibit spontaneous attempts of axon regeneration. These environmental factors include physical barriers to axon regeneration such as fibrous and glial scars as well as molecules which actively inhibit regeneration. Even when these barriers are removed, axons in the central nervous system still require the appropriate stimulatory signals in order for significant regeneration to occur. These signals may include a substrate to provide directional guidance of the extending axon growth cones as well as neurotrophic factors to promote axon growth and survival. Thus, to achieve a clinically meaningful regenerative response following spinal cord injury a combinatorial therapeutic approach is likely necessary. This thesis investigated the use of a porous collagen scaffold with aligned pores to serve as a substrate for axon guidance and a delivery vehicle for select therapeutic agents in both fulland hemi- resection injury models in adult rats. In the hemi-resection injury model, the treatment groups included: a dehydrothermally (DHT) crosslinked scaffold, a carbodiimide (EDAC) crosslinked scaffold, and an EDAC-treated scaffold delivering either soluble Nogo receptor (sNgR), chondroitinase ABC (ChABC), or bone marrow derived mesenchymal stem cells (MSCs). Improvement in hindlimb function over four weeks following injury was seen in the DHT scaffold, EDAC scaffold + ChABC, and EDAC scaffold + MSCs groups, but not in the untreated control group. Immunohistochemical evaluation of the tissue revealed a few regenerating axons reaching the center of the scaffold in the DHT scaffold and EDAC scaffold + ChABC groups at 4 week post injury. Histological evaluation at 4 weeks showed the defect area of all groups to be largely comprised of loosely organized fibrous tissue. Many macrophages were seen surrounding the defect area of all groups; however, there were significantly more macrophages within the defect area of the control group compared to the treatment groups. In the full-resection injury model, at 6 weeks post injury, implanted collagen scaffolds tended to reduce the number of cystic cavities in the defect and to better align the reparative tissue with the long axis of the spinal cord. These results show the potential benefit of collagen scaffolds alone or in combination with select therapeutic agents as a means to modulate the healing response following spinal cord injury.",
    "advisors": ["Myron Spector"],
    "text": "Collagen scaffolds in full- and hemi- resection spinal cord injury models Basic scientific research over the past few decades has shown some light on the complex pathophysiology of SCI and has enhanced our understanding of some of the important factors that contribute to the lack of regeneration following the initial traumatic injury and secondary injury response in the adult spinal cord. These factors include both intrinsic limitations in the regeneration capacity of mature neurons, but also a host of environmental factors which inhibit spontaneous attempts of axon regeneration. These environmental factors include physical barriers to axon regeneration such as fibrous and glial scars as well as molecules which actively inhibit regeneration. Even when these barriers are removed, axons in the central nervous system still require the appropriate stimulatory signals in order for significant regeneration to occur. These signals may include a substrate to provide directional guidance of the extending axon growth cones as well as neurotrophic factors to promote axon growth and survival. Thus, to achieve a clinically meaningful regenerative response following spinal cord injury a combinatorial therapeutic approach is likely necessary. This thesis investigated the use of a porous collagen scaffold with aligned pores to serve as a substrate for axon guidance and a delivery vehicle for select therapeutic agents in both fulland hemi- resection injury models in adult rats. In the hemi-resection injury model, the treatment groups included: a dehydrothermally (DHT) crosslinked scaffold, a carbodiimide (EDAC) crosslinked scaffold, and an EDAC-treated scaffold delivering either soluble Nogo receptor (sNgR), chondroitinase ABC (ChABC), or bone marrow derived mesenchymal stem cells (MSCs). Improvement in hindlimb function over four weeks following injury was seen in the DHT scaffold, EDAC scaffold + ChABC, and EDAC scaffold + MSCs groups, but not in the untreated control group. Immunohistochemical evaluation of the tissue revealed a few regenerating axons reaching the center of the scaffold in the DHT scaffold and EDAC scaffold + ChABC groups at 4 week post injury. Histological evaluation at 4 weeks showed the defect area of all groups to be largely comprised of loosely organized fibrous tissue. Many macrophages were seen surrounding the defect area of all groups; however, there were significantly more macrophages within the defect area of the control group compared to the treatment groups. In the full-resection injury model, at 6 weeks post injury, implanted collagen scaffolds tended to reduce the number of cystic cavities in the defect and to better align the reparative tissue with the long axis of the spinal cord. These results show the potential benefit of collagen scaffolds alone or in combination with select therapeutic agents as a means to modulate the healing response following spinal cord injury."
}, {
    "id": "oai:dspace.mit.edu:1721.1/95844",
    "title": "Algorithms for enhanced spatiotemporal imaging of human brain function",
    "abstract": "Studies of human brain function require technologies to non-invasively image neuronal dynamics with high spatiotemporal resolution. The electroencephalogram (EEG) and magnetoencephalogram (MEG) measure neuronal activity with high temporal resolution, and provide clinically accessible signatures of brain states. However, they have limited spatial resolution for regional dynamics. Combinations of M/EEG with functional and anatomical magnetic resonance imaging (MRI) can enable jointly high temporal and spatial resolution. In this thesis, we address two critical challenges limiting multimodal imaging studies of spatiotemporal brain dynamics. First, simultaneous EEG-fMRI offers a promising means to relate rapidly evolving EEG signatures with slower regional dynamics measured on fMRI. However, the potential of this technique is undermined by MRI-related ballistocardiogram artifacts that corrupt the EEG. We identify a harmonic basis for these artifacts, develop a local likelihood estimation algorithm to remove them, and demonstrate enhanced recovery of oscillatory and evoked EEG dynamics in the MRI scanner. Second, M/EEG source imaging offers a means to characterize rapidly evolving regional dynamics within an estimation framework informed by anatomical MRI. However, existing approaches are limited to cortical structures. Crucial dynamics in subcortical structures, which generate weaker M/EEG signals, are largely unexplored. We identify robust distinctions in M/EEG field patterns arising from subcortical and cortical structures, and develop a hierarchical subspace pursuit algorithm to estimate neural currents in subcortical structures. We validate efficacy for recovering thalamic and brainstem contributions in simulated and experimental studies. These results establish the feasibility of using non-invasive M/EEG measurements to estimate millisecond-scale dynamics involving subcortical structures. Finally, we illustrate the potential of these techniques for novel studies in cognitive and clinical neuroscience. Within an EEG-fMRI study of auditory stimulus processing under propofol anesthesia, we observed EEG signatures accompanying distinct changes in thalamocortical dynamics at loss of consciousness and subsequently, at deeper levels of anesthesia. These results suggest neurophysiologic correlates to better interpret clinical EEG signatures demarcating brain dynamics under anesthesia. Overall, the algorithms developed in this thesis provide novel opportunities to non-invasively relate fast timescale measures of neuronal activity with their underlying regional brain dynamics, thus paving a way for enhanced spatiotemporal imaging of human brain function.",
    "advisors": ["Emery N. Brown", "Patrick L. Purdon"],
    "text": "Algorithms for enhanced spatiotemporal imaging of human brain function Studies of human brain function require technologies to non-invasively image neuronal dynamics with high spatiotemporal resolution. The electroencephalogram (EEG) and magnetoencephalogram (MEG) measure neuronal activity with high temporal resolution, and provide clinically accessible signatures of brain states. However, they have limited spatial resolution for regional dynamics. Combinations of M/EEG with functional and anatomical magnetic resonance imaging (MRI) can enable jointly high temporal and spatial resolution. In this thesis, we address two critical challenges limiting multimodal imaging studies of spatiotemporal brain dynamics. First, simultaneous EEG-fMRI offers a promising means to relate rapidly evolving EEG signatures with slower regional dynamics measured on fMRI. However, the potential of this technique is undermined by MRI-related ballistocardiogram artifacts that corrupt the EEG. We identify a harmonic basis for these artifacts, develop a local likelihood estimation algorithm to remove them, and demonstrate enhanced recovery of oscillatory and evoked EEG dynamics in the MRI scanner. Second, M/EEG source imaging offers a means to characterize rapidly evolving regional dynamics within an estimation framework informed by anatomical MRI. However, existing approaches are limited to cortical structures. Crucial dynamics in subcortical structures, which generate weaker M/EEG signals, are largely unexplored. We identify robust distinctions in M/EEG field patterns arising from subcortical and cortical structures, and develop a hierarchical subspace pursuit algorithm to estimate neural currents in subcortical structures. We validate efficacy for recovering thalamic and brainstem contributions in simulated and experimental studies. These results establish the feasibility of using non-invasive M/EEG measurements to estimate millisecond-scale dynamics involving subcortical structures. Finally, we illustrate the potential of these techniques for novel studies in cognitive and clinical neuroscience. Within an EEG-fMRI study of auditory stimulus processing under propofol anesthesia, we observed EEG signatures accompanying distinct changes in thalamocortical dynamics at loss of consciousness and subsequently, at deeper levels of anesthesia. These results suggest neurophysiologic correlates to better interpret clinical EEG signatures demarcating brain dynamics under anesthesia. Overall, the algorithms developed in this thesis provide novel opportunities to non-invasively relate fast timescale measures of neuronal activity with their underlying regional brain dynamics, thus paving a way for enhanced spatiotemporal imaging of human brain function."
}, {
    "id": "oai:dspace.mit.edu:1721.1/8628",
    "title": "A biomechanical investigation of the structure--function relationships in the human tongue",
    "abstract": "The human tongue is a versatile, lithe and structurally complex muscular organ that is of paramount importance for many physiological tasks. The lingual musculature is composed of various orthogonally oriented myofiber populations. Furthermore, coupling this knowledge of tissue myoarchitecture with patterns of regional deformation offers the ability to explore complex structure-function relationships in the organ. Tongue myoarchitecture was studied with Diffusion Tensor MRI (DTI), which derived the spatial diffusion tensor field in the tongue. Since, diffusivity relates directly to myofiber orientation, this in vivo technique successfully produced a virtual anatomical atlas. In order to relate this 3D myoarchitecture to physiological deformations, in vivo strain was quantified by an MRI tagging technique. This technique tagged lingual tissue with a rectilinear grid, which was subsequently imaged to track and quantify deformation through 3D strain measures. Anterior protrusion, sagittal bending, and oral stage deglutition were studied with this technique. The results demonstrated that synergistic co-contraction between various muscle populations produced the necessary deformations in global tongue shape. In order to delineate specific muscular contributions to sagittal bending, the tongue was modeled by a thermal bimetal strip analog wherein thermal contraction approximated muscle fiber activation.",
    "advisors": ["Richard J. Gilbert"],
    "text": "A biomechanical investigation of the structure--function relationships in the human tongue The human tongue is a versatile, lithe and structurally complex muscular organ that is of paramount importance for many physiological tasks. The lingual musculature is composed of various orthogonally oriented myofiber populations. Furthermore, coupling this knowledge of tissue myoarchitecture with patterns of regional deformation offers the ability to explore complex structure-function relationships in the organ. Tongue myoarchitecture was studied with Diffusion Tensor MRI (DTI), which derived the spatial diffusion tensor field in the tongue. Since, diffusivity relates directly to myofiber orientation, this in vivo technique successfully produced a virtual anatomical atlas. In order to relate this 3D myoarchitecture to physiological deformations, in vivo strain was quantified by an MRI tagging technique. This technique tagged lingual tissue with a rectilinear grid, which was subsequently imaged to track and quantify deformation through 3D strain measures. Anterior protrusion, sagittal bending, and oral stage deglutition were studied with this technique. The results demonstrated that synergistic co-contraction between various muscle populations produced the necessary deformations in global tongue shape. In order to delineate specific muscular contributions to sagittal bending, the tongue was modeled by a thermal bimetal strip analog wherein thermal contraction approximated muscle fiber activation."
}, {
    "id": "oai:dspace.mit.edu:1721.1/97824",
    "title": "Multi-scale imaging and informatics pipeline for in situ pluripotent stem cell analysis",
    "abstract": "Human pluripotent stem (hPS) cells have the ability to reproduce indefinitely and differentiate into any cell type of the body, making them a potential source of cells for medical therapy and an ideal system to study fate decisions in early development. However, hPS cells exhibit a high degree of heterogeneity, which may be an obstacle to their clinical translation. Heterogeneity is at least partially induced as an artifact of removing the cells from the embryo and culturing them on a plastic dish. hPS cells grow in spatially patterned colony structures, which necessitates in situ quantitative single-cell image analysis. This dissertation offers a tool for analyzing the spatial population context of hPS cells that integrates automated fluorescent microscopy with an analysis pipeline. It enables high-throughput detection of colonies at low resolution, with single-cellular and sub-cellular analysis at high resolutions, generating seamless in situ maps of single-cellular data organized by colony. We demonstrate the tool's utility by analyzing inter- and intra-colony heterogeneity of hPS cell cycle regulation and pluripotency marker expression. We measured the heterogeneity within individual colonies by analyzing cell cycle as a function of distance. Cells loosely associated with the outside of the colony are more likely to be in G1, reflecting a less pluripotent state, while cells within the first pluripotent layer are more likely to be in G2, possibly reflecting a G2/M block. Our analysis tool can group colony regions into density classes, and cells belonging to those classes have distinct distributions of pluripotency markers and respond differently to DNA damage induction. Our platform also enabled noninvasive texture analysis of live hPS colonies, which was applied to monitoring subtle changes in differentiation state. Lastly, we demonstrate that our pipeline can robustly handle high-content, high-resolution single molecular mRNA FISH data by using novel image processing techniques. Overall, the imaging informatics pipeline presented offers a novel approach to the analysis of hPS cells, which includes not only single cell features but also spatial configuration across multiple length scales.",
    "advisors": ["Paul H. Lerou"],
    "text": "Multi-scale imaging and informatics pipeline for in situ pluripotent stem cell analysis Human pluripotent stem (hPS) cells have the ability to reproduce indefinitely and differentiate into any cell type of the body, making them a potential source of cells for medical therapy and an ideal system to study fate decisions in early development. However, hPS cells exhibit a high degree of heterogeneity, which may be an obstacle to their clinical translation. Heterogeneity is at least partially induced as an artifact of removing the cells from the embryo and culturing them on a plastic dish. hPS cells grow in spatially patterned colony structures, which necessitates in situ quantitative single-cell image analysis. This dissertation offers a tool for analyzing the spatial population context of hPS cells that integrates automated fluorescent microscopy with an analysis pipeline. It enables high-throughput detection of colonies at low resolution, with single-cellular and sub-cellular analysis at high resolutions, generating seamless in situ maps of single-cellular data organized by colony. We demonstrate the tool's utility by analyzing inter- and intra-colony heterogeneity of hPS cell cycle regulation and pluripotency marker expression. We measured the heterogeneity within individual colonies by analyzing cell cycle as a function of distance. Cells loosely associated with the outside of the colony are more likely to be in G1, reflecting a less pluripotent state, while cells within the first pluripotent layer are more likely to be in G2, possibly reflecting a G2/M block. Our analysis tool can group colony regions into density classes, and cells belonging to those classes have distinct distributions of pluripotency markers and respond differently to DNA damage induction. Our platform also enabled noninvasive texture analysis of live hPS colonies, which was applied to monitoring subtle changes in differentiation state. Lastly, we demonstrate that our pipeline can robustly handle high-content, high-resolution single molecular mRNA FISH data by using novel image processing techniques. Overall, the imaging informatics pipeline presented offers a novel approach to the analysis of hPS cells, which includes not only single cell features but also spatial configuration across multiple length scales."
}, {
    "id": "oai:dspace.mit.edu:1721.1/117894",
    "title": "Quantifying fluid overload with portable magnetic resonance sensors",
    "abstract": "The objective of this work was to translate the diagnostic capabilities of magnetic resonance imaging (MRI) to the patient bedside, specifically for the purpose of quantifying fluid overload. MRI is used extensively in clinical medicine, but it is still not used for routine diagnostics due to high cost, limited availability, and long scan times. Many of these impracticalities come from the hardware requirements associated with generating images. Images, however, are not necessary to harness some of magnetic resonance's (MR's) diagnostic potential. This thesis demonstrates that that a single-voxel MR sensor can obtain the same results as a traditional MRI in both phantoms and humans. A clinical study with hemodialysis patients and age-matched healthy controls was performed at MGH. The T2 relaxation times of study participants' legs were quantified at multiple time points with both a 1.5T clinical MRI scanner and a custom 0.27T single-voxel MR sensor. The results showed that the first sign of fluid overload is an increase in the relative fraction of extracellular fluid in the muscle. The relaxation time of the extracellular fluid in the muscle eventually increases after more fluid accumulates. Importantly, these MR findings occur before signs of lower-extremity edema are detectable on physical exam. Two healthy control subjects became dehydrated over the course of the study and the relative fraction of their extracellular fluid decreased. This incidental finding suggests MR can measure the full spectrum of hydration states. Furthermore, a single MRI measurement at a single time point can distinguish fluid overloaded patients from healthy controls. The amplitude associated with extracellular fluid most closely correlates to fluid loss, and these amplitude decreases are detectable with both the MRI and MR sensor. The results of this work point towards a promising future of using cheaper, faster MR sensors for bedside diagnostics.",
    "advisors": ["Michael J. Cima"],
    "text": "Quantifying fluid overload with portable magnetic resonance sensors The objective of this work was to translate the diagnostic capabilities of magnetic resonance imaging (MRI) to the patient bedside, specifically for the purpose of quantifying fluid overload. MRI is used extensively in clinical medicine, but it is still not used for routine diagnostics due to high cost, limited availability, and long scan times. Many of these impracticalities come from the hardware requirements associated with generating images. Images, however, are not necessary to harness some of magnetic resonance's (MR's) diagnostic potential. This thesis demonstrates that that a single-voxel MR sensor can obtain the same results as a traditional MRI in both phantoms and humans. A clinical study with hemodialysis patients and age-matched healthy controls was performed at MGH. The T2 relaxation times of study participants' legs were quantified at multiple time points with both a 1.5T clinical MRI scanner and a custom 0.27T single-voxel MR sensor. The results showed that the first sign of fluid overload is an increase in the relative fraction of extracellular fluid in the muscle. The relaxation time of the extracellular fluid in the muscle eventually increases after more fluid accumulates. Importantly, these MR findings occur before signs of lower-extremity edema are detectable on physical exam. Two healthy control subjects became dehydrated over the course of the study and the relative fraction of their extracellular fluid decreased. This incidental finding suggests MR can measure the full spectrum of hydration states. Furthermore, a single MRI measurement at a single time point can distinguish fluid overloaded patients from healthy controls. The amplitude associated with extracellular fluid most closely correlates to fluid loss, and these amplitude decreases are detectable with both the MRI and MR sensor. The results of this work point towards a promising future of using cheaper, faster MR sensors for bedside diagnostics."
}, {
    "id": "oai:dspace.mit.edu:1721.1/58298",
    "title": "Ex vivo perfusion optimization of donor liver grafts for transplantation and cell isolation",
    "abstract": "There is a constant demand for enormous numbers of high quality hepatocytes in the fields of cell transplantation, pharmacotoxicology, tissue engineering, and bioartificial assist devices. The scarcity of viable hepatocytes necessitates the use of suboptimal sources including damaged donor organs that are not transplantable. Many of these organs have potentially reversible pathologies however, that could be treated via ex vivo perfusion thereby increasing their cell yield. With the intent to translate organ recovery by perfusion into the clinic, we engineered a very simple room temperature-operated ex vivo organ perfusion system to test a rat liver model of uncontrolled non-heart beating donors. Seventeen times as many hepatocytes were recovered from livers exposed to an hour of warm ischemia (WI, 34*C) compared to untreated WI livers in only 3 hours of perfusion. Further, fresh liver hepatocyte yields were also increased by 32% postperfusion, demonstrating that both damaged and healthy donor livers could benefit from this methodology. A linear correlation between cell yield and tissue ATP content was established. This enables an accurate prediction of cell recovery during preservation and can be used as a direct measure of organ viability and the trajectory of organ recovery during perfusion resuscitation. Further, a strong correlation between perfusion flow rate and cell yield was also established supporting the use of flow rates as low as possible without causing hypoperfusion or oxygen deprivation. Morphologically and functionally, perfusion-isolated hepatocytes generally performed comparably or better than fresh hepatocytes in cell suspension and plate culture. Cumulatively, these findings strongly support the ubiquitous use of organ perfusion systems in the clinic for optimal enhancement of donor grafts.",
    "advisors": ["Martin L. Yarmush"],
    "text": "Ex vivo perfusion optimization of donor liver grafts for transplantation and cell isolation There is a constant demand for enormous numbers of high quality hepatocytes in the fields of cell transplantation, pharmacotoxicology, tissue engineering, and bioartificial assist devices. The scarcity of viable hepatocytes necessitates the use of suboptimal sources including damaged donor organs that are not transplantable. Many of these organs have potentially reversible pathologies however, that could be treated via ex vivo perfusion thereby increasing their cell yield. With the intent to translate organ recovery by perfusion into the clinic, we engineered a very simple room temperature-operated ex vivo organ perfusion system to test a rat liver model of uncontrolled non-heart beating donors. Seventeen times as many hepatocytes were recovered from livers exposed to an hour of warm ischemia (WI, 34*C) compared to untreated WI livers in only 3 hours of perfusion. Further, fresh liver hepatocyte yields were also increased by 32% postperfusion, demonstrating that both damaged and healthy donor livers could benefit from this methodology. A linear correlation between cell yield and tissue ATP content was established. This enables an accurate prediction of cell recovery during preservation and can be used as a direct measure of organ viability and the trajectory of organ recovery during perfusion resuscitation. Further, a strong correlation between perfusion flow rate and cell yield was also established supporting the use of flow rates as low as possible without causing hypoperfusion or oxygen deprivation. Morphologically and functionally, perfusion-isolated hepatocytes generally performed comparably or better than fresh hepatocytes in cell suspension and plate culture. Cumulatively, these findings strongly support the ubiquitous use of organ perfusion systems in the clinic for optimal enhancement of donor grafts."
}, {
    "id": "oai:dspace.mit.edu:1721.1/47850",
    "title": "A microwell array cytometry system for high throughput single cell biology and bioinformatics",
    "abstract": "Recent advances in systems biology and bioinformatics have highlighted that no cell population is truly uniform and that stochastic behavior is an inherent property of many biological systems. As a result, bulk measurements can be misleading even when particular care has been taken to isolate a single cell type, and measurements averaged over multiple cell populations in a tissue can be as misleading as the average height at an elementary school. Unfortunately, there are relatively few experimental systems available at present that can provide a combination of single cell resolution, large cell populations, and the ability to track individual cells over multiple time points. Those systems that do exist are often difficult to automate and require extensive user intervention simply to generate the raw data sets for later analysis. The goal of this thesis project was to develop a powerful, inexpensive, and easy-to-use system that meets the above requirements and can serve as a platform for single cell bioinformatics. Our current system design is composed of two basic parts: 1) a customizable PDMS device consisting of one or more microwell arrays, each with associated alignment and identification features, and 2) a suite of custom software tools for automated image processing and data analysis. The system has a number of significant advantages over competing technologies such as flow cytometry and standard image cytometry. Unlike flow cytometry, the cells are not in suspension, and individual cells can be tracked across multiple time points or examined before and after a treatment.",
    "advisors": ["Mehmet Toner"],
    "text": "A microwell array cytometry system for high throughput single cell biology and bioinformatics Recent advances in systems biology and bioinformatics have highlighted that no cell population is truly uniform and that stochastic behavior is an inherent property of many biological systems. As a result, bulk measurements can be misleading even when particular care has been taken to isolate a single cell type, and measurements averaged over multiple cell populations in a tissue can be as misleading as the average height at an elementary school. Unfortunately, there are relatively few experimental systems available at present that can provide a combination of single cell resolution, large cell populations, and the ability to track individual cells over multiple time points. Those systems that do exist are often difficult to automate and require extensive user intervention simply to generate the raw data sets for later analysis. The goal of this thesis project was to develop a powerful, inexpensive, and easy-to-use system that meets the above requirements and can serve as a platform for single cell bioinformatics. Our current system design is composed of two basic parts: 1) a customizable PDMS device consisting of one or more microwell arrays, each with associated alignment and identification features, and 2) a suite of custom software tools for automated image processing and data analysis. The system has a number of significant advantages over competing technologies such as flow cytometry and standard image cytometry. Unlike flow cytometry, the cells are not in suspension, and individual cells can be tracked across multiple time points or examined before and after a treatment."
}, {
    "id": "oai:dspace.mit.edu:1721.1/9372",
    "title": "Perturbed equilibria of myosin binding in airway smooth muscle and its implications in airway hyperresponsiveness and asthma",
    "abstract": "In asthma, the key effector driving acute airway narrowing is thought to be airway smooth muscle (ASM); as the muscle surrounding the airways shortens, the airway lumen narrows. Airway hyperresponsiveness (AHR) - the excessive narrowing of the airways - is one of the cardinal features of asthma. Yet, the mechanism(s) regulating the airway lumenal radius, and perhaps the failure of these mechanisms to prevent excessive airway constriction, remains largely unexplained. This thesis shows that the regulation of ASM length corresponds to a dynamically equilibrated steady-state, not the static mechanical equilibrium that had been previously assumed. This dynamic steady state requires as an essential feature a continuous supply of external mechanical energy (derived from tidal lung inflations) that act to perturb the interactions of myosin with actin, drive the molecular state of the system far away from thermodynamic equilibrium, and bias the muscle toward lengthening. This mechanism leads naturally to the suggestion that excessive airway narrowing in asthma may be associated with the destabilization of that dynamic process and its resulting collapse back to static equilibrium. With this collapse the muscle undergoes a phase transition and virtually freezes at its static equilibrium length. This mechanism may help to elucidate several unexplained phenomena including the multi-factorial origins of AHR, how allergen sensitization leads to AHR, and the inability in asthma of deep inspiration to relax ASM.",
    "advisors": ["Jeffrey J. Fredberg"],
    "text": "Perturbed equilibria of myosin binding in airway smooth muscle and its implications in airway hyperresponsiveness and asthma In asthma, the key effector driving acute airway narrowing is thought to be airway smooth muscle (ASM); as the muscle surrounding the airways shortens, the airway lumen narrows. Airway hyperresponsiveness (AHR) - the excessive narrowing of the airways - is one of the cardinal features of asthma. Yet, the mechanism(s) regulating the airway lumenal radius, and perhaps the failure of these mechanisms to prevent excessive airway constriction, remains largely unexplained. This thesis shows that the regulation of ASM length corresponds to a dynamically equilibrated steady-state, not the static mechanical equilibrium that had been previously assumed. This dynamic steady state requires as an essential feature a continuous supply of external mechanical energy (derived from tidal lung inflations) that act to perturb the interactions of myosin with actin, drive the molecular state of the system far away from thermodynamic equilibrium, and bias the muscle toward lengthening. This mechanism leads naturally to the suggestion that excessive airway narrowing in asthma may be associated with the destabilization of that dynamic process and its resulting collapse back to static equilibrium. With this collapse the muscle undergoes a phase transition and virtually freezes at its static equilibrium length. This mechanism may help to elucidate several unexplained phenomena including the multi-factorial origins of AHR, how allergen sensitization leads to AHR, and the inability in asthma of deep inspiration to relax ASM."
}, {
    "id": "oai:dspace.mit.edu:1721.1/31105",
    "title": "Spatially-localized correlation of MRI and mechanical stiffness to assess cartilage integrity in the human tibial plateau",
    "abstract": "Osteoarthritis is a painful degenerative joint disease affecting millions of people in the U.S. The pathogenesis of articular cartilage disease is characterized by softening of cartilage and loss and disruption of constituent macromolecules including proteoglycans and collagen. In current orthopaedic surgical practice, the gold standard for evaluating articular cartilage integrity is the use of a hand probe during arthroscopy. Mechanical probing, however, is invasive and requires anesthesia. Tightly confined areas of the articular surface can be difficult to reach and assess, and manual probing provides a subjective rather than a quantitative assessment of cartilage mechanical integrity. This thesis was motivated by the desire for a noninvasive and nondestructive means to map the variation in mechanical stiffness of an articular surface. Such a method could potentially have application to guiding surgeons during procedures and quantitatively assessing the efficacy of medical and surgical therapies. It could also potentially provide patient-specific, in vivo tissue mechanical property data for surgical simulation and preoperative procedure planning. The macromolecule glycosaminoglycan (GAG) is a significant determinant of cartilage stiffness. GAG content can be assessed noninvasively in vivo and in vitro by an MRI-based technique known as delayed gadolinium-enhanced magnetic resonance imaging of cartilage (dGEMRIC), which measures the MRI parameter TI after equilibration with the ionic contrast agent Gd(DTPA)2-. With dGEMRIC, TlGd serves as an index of GAG content: we therefore examined whether cartilage stiffness could be related to dGEMRIC-measured TlGd in samples of human tibial plateaus.",
    "advisors": ["W. Eric L. Grimson"],
    "text": "Spatially-localized correlation of MRI and mechanical stiffness to assess cartilage integrity in the human tibial plateau Osteoarthritis is a painful degenerative joint disease affecting millions of people in the U.S. The pathogenesis of articular cartilage disease is characterized by softening of cartilage and loss and disruption of constituent macromolecules including proteoglycans and collagen. In current orthopaedic surgical practice, the gold standard for evaluating articular cartilage integrity is the use of a hand probe during arthroscopy. Mechanical probing, however, is invasive and requires anesthesia. Tightly confined areas of the articular surface can be difficult to reach and assess, and manual probing provides a subjective rather than a quantitative assessment of cartilage mechanical integrity. This thesis was motivated by the desire for a noninvasive and nondestructive means to map the variation in mechanical stiffness of an articular surface. Such a method could potentially have application to guiding surgeons during procedures and quantitatively assessing the efficacy of medical and surgical therapies. It could also potentially provide patient-specific, in vivo tissue mechanical property data for surgical simulation and preoperative procedure planning. The macromolecule glycosaminoglycan (GAG) is a significant determinant of cartilage stiffness. GAG content can be assessed noninvasively in vivo and in vitro by an MRI-based technique known as delayed gadolinium-enhanced magnetic resonance imaging of cartilage (dGEMRIC), which measures the MRI parameter TI after equilibration with the ionic contrast agent Gd(DTPA)2-. With dGEMRIC, TlGd serves as an index of GAG content: we therefore examined whether cartilage stiffness could be related to dGEMRIC-measured TlGd in samples of human tibial plateaus."
}, {
    "id": "oai:dspace.mit.edu:1721.1/54629",
    "title": "An optical smart needle : point-of-care technologies for integrated needle guidance using optical frequency domain ranging",
    "abstract": "Obtaining accurate needle placement is of critical importance in many medical scenarios. In the setting of fine needle aspiration biopsy (FNAB), manual palpation is often the only cue for determining the optimal position of the needle. As a result, FNAB procedures frequently yield non-diagnostic tissue. When not guided by an imaging modality, breast and thyroid FNAB's only obtain diagnostic tissue in approximately 65% of cases. Although the addition of noninvasive imaging technology has been shown to increase FNAB yield, it is time-consuming, relatively expensive, and often requires additional personnel with specialized expertise. A need exists for low-cost, small, simple to use technologies that can provide active feedback during needle placement. One promising method for guiding needle placement would be to integrate an optical sensor that could identify tissue type at the tip of the needle in order to avoid non]diagnostic sampling. Optical technologies are well suited to this challenge because sensors can be made using optical fiber which is as thin a human hair. Optical frequency domain ranging (OFDR) is an optical ranging technique that is capable of measuring depth-resolved (axial, z) tissue structure, birefringence, flow (Doppler shift), and spectra at a micrometer level resolution. Analysis of the OFDR depth reflectivity profiles yields information about the nature of the tissue being interrogated at the tip of the probe and algorithms can be developed to automatically differentiate between tissue types. The overall goal of this thesis is to develop a small, portable, point-of-care optical system that can be used to differentiate human breast tissue and guide needle placement in the setting of FNAB. We will investigate enabling technologies that allow for efficient simplification and miniaturization of an OFDR system including signal processing algorithms for automatically differentiating tissue type, a miniature battery-powered laser, and a study of the effect of reduced-bit depth acquisition for OFDR systems. Throughout, we will focus on trade offs between size and performance while taking into account usability, robustness, and overall cost which are key features of point-of-care technologies.",
    "advisors": ["Guillermo J. Tearney"],
    "text": "An optical smart needle : point-of-care technologies for integrated needle guidance using optical frequency domain ranging Obtaining accurate needle placement is of critical importance in many medical scenarios. In the setting of fine needle aspiration biopsy (FNAB), manual palpation is often the only cue for determining the optimal position of the needle. As a result, FNAB procedures frequently yield non-diagnostic tissue. When not guided by an imaging modality, breast and thyroid FNAB's only obtain diagnostic tissue in approximately 65% of cases. Although the addition of noninvasive imaging technology has been shown to increase FNAB yield, it is time-consuming, relatively expensive, and often requires additional personnel with specialized expertise. A need exists for low-cost, small, simple to use technologies that can provide active feedback during needle placement. One promising method for guiding needle placement would be to integrate an optical sensor that could identify tissue type at the tip of the needle in order to avoid non]diagnostic sampling. Optical technologies are well suited to this challenge because sensors can be made using optical fiber which is as thin a human hair. Optical frequency domain ranging (OFDR) is an optical ranging technique that is capable of measuring depth-resolved (axial, z) tissue structure, birefringence, flow (Doppler shift), and spectra at a micrometer level resolution. Analysis of the OFDR depth reflectivity profiles yields information about the nature of the tissue being interrogated at the tip of the probe and algorithms can be developed to automatically differentiate between tissue types. The overall goal of this thesis is to develop a small, portable, point-of-care optical system that can be used to differentiate human breast tissue and guide needle placement in the setting of FNAB. We will investigate enabling technologies that allow for efficient simplification and miniaturization of an OFDR system including signal processing algorithms for automatically differentiating tissue type, a miniature battery-powered laser, and a study of the effect of reduced-bit depth acquisition for OFDR systems. Throughout, we will focus on trade offs between size and performance while taking into account usability, robustness, and overall cost which are key features of point-of-care technologies."
}, {
    "id": "oai:dspace.mit.edu:1721.1/68517",
    "title": "Discovery of novel anti-inflammatory proteins inspired by bone marrow mesenchymal stem cell secretions",
    "abstract": "Bone marrow mesenchymal stem cells (MSCs) may soon become the first FDA-approved stem cell therapy for autoimmune and inflammatory disease. Our lab originally hypothesized that much of the therapeutic activity of MSCs may be attributed to molecules secreted by these cells. This thesis will test this hypothesis, with an emphasis on translational steps towards clinical product development, including the identification of novel proteins secreted by MSCs. The first part of the thesis consists of studies we performed to test whether MSC conditioned medium (MSC-CM) can treat rats undergoing cisplatin-induced acute kidney injury (AKI). When AKI rats were treated with MSC-CM, we observed a survival benefit and significant protection of renal function compared to controls. The second part of the thesis will describe the development of a device designed for sustained delivery of MSC secreted factors to dialysis-dependent AKI subjects. We tested these devices for cell function, stability and viability when subjected to conditions that model future clinical operation. Finally, inspired by the therapeutic capacity of MSC secreted factors, this thesis will conclude with the introduction of a new method that we developed to uncover novel anti-inflammatory proteins from MSCs. This method revealed four previously unidentified cytokine modulators, two of which we found significantly promote IL-1 0 and suppress TNF-a in mice challenged with endotoxin. When leveraged as novel therapeutics for lethal endotoxemic shock, these two most potent modulators protected mice and provided for a significant survival benefit compared to vehicle controls. Together, these results demonstrate the power of MSC secreted factors in the context of inflammatory disease, and propose new tactics for elucidating potent secreted products from cells.",
    "advisors": ["Martin Yarmush"],
    "text": "Discovery of novel anti-inflammatory proteins inspired by bone marrow mesenchymal stem cell secretions Bone marrow mesenchymal stem cells (MSCs) may soon become the first FDA-approved stem cell therapy for autoimmune and inflammatory disease. Our lab originally hypothesized that much of the therapeutic activity of MSCs may be attributed to molecules secreted by these cells. This thesis will test this hypothesis, with an emphasis on translational steps towards clinical product development, including the identification of novel proteins secreted by MSCs. The first part of the thesis consists of studies we performed to test whether MSC conditioned medium (MSC-CM) can treat rats undergoing cisplatin-induced acute kidney injury (AKI). When AKI rats were treated with MSC-CM, we observed a survival benefit and significant protection of renal function compared to controls. The second part of the thesis will describe the development of a device designed for sustained delivery of MSC secreted factors to dialysis-dependent AKI subjects. We tested these devices for cell function, stability and viability when subjected to conditions that model future clinical operation. Finally, inspired by the therapeutic capacity of MSC secreted factors, this thesis will conclude with the introduction of a new method that we developed to uncover novel anti-inflammatory proteins from MSCs. This method revealed four previously unidentified cytokine modulators, two of which we found significantly promote IL-1 0 and suppress TNF-a in mice challenged with endotoxin. When leveraged as novel therapeutics for lethal endotoxemic shock, these two most potent modulators protected mice and provided for a significant survival benefit compared to vehicle controls. Together, these results demonstrate the power of MSC secreted factors in the context of inflammatory disease, and propose new tactics for elucidating potent secreted products from cells."
}, {
    "id": "oai:dspace.mit.edu:1721.1/38247",
    "title": "Transitive inference in healthy humans and implications for schizophrenia",
    "abstract": "Transitive inference (TI) refers to inferences on relations between items based on other known relations of those items. Using a paradigm where participants first learn a series of four overlapping pairs that constitute the ordered sequence A>B>C>D>E and are then tested on the novel TI pair BD and non-TI pair AE, animal experiments demonstrated that intact function of the hippocampus is necessary for TI but not for non-TI. We performed three functional magnetic resonance imaging (fMRI) experiments to identify neural correlates of TI in healthy humans. First, we show hippocampal activation in learning overlapping pairs that constitute an ordered sequence but not non-overlapping individual pairs. Second, we demonstrate hippocampal recruitment in inferences on the ordered sequence of overlapping pairs (TI) but not on non-overlapping pairs (non-TI, e.g., if a>b and c>d then a>d). We then demonstrate the specificity of hippocampal activation to TI on pairs that are devoid of sequence end-items (e.g., B>D vs. A>C). The results support the relational flexibility account of hippocampal function.",
    "advisors": ["Stephan H. Heckers"],
    "text": "Transitive inference in healthy humans and implications for schizophrenia Transitive inference (TI) refers to inferences on relations between items based on other known relations of those items. Using a paradigm where participants first learn a series of four overlapping pairs that constitute the ordered sequence A>B>C>D>E and are then tested on the novel TI pair BD and non-TI pair AE, animal experiments demonstrated that intact function of the hippocampus is necessary for TI but not for non-TI. We performed three functional magnetic resonance imaging (fMRI) experiments to identify neural correlates of TI in healthy humans. First, we show hippocampal activation in learning overlapping pairs that constitute an ordered sequence but not non-overlapping individual pairs. Second, we demonstrate hippocampal recruitment in inferences on the ordered sequence of overlapping pairs (TI) but not on non-overlapping pairs (non-TI, e.g., if a>b and c>d then a>d). We then demonstrate the specificity of hippocampal activation to TI on pairs that are devoid of sequence end-items (e.g., B>D vs. A>C). The results support the relational flexibility account of hippocampal function."
}, {
    "id": "oai:dspace.mit.edu:1721.1/58393",
    "title": "A novel nanoscale delivery system for spatio-temporal delivery of combination chemotherapy",
    "abstract": "In the continuing search for effective treatments for cancer, the emerging model is the combination of traditional chemotherapy with anti-angiogenesis agents that inhibit blood vessel growth. However, the implementation of this strategy has faced two major obstacles. First, the long-term shutdown of tumor blood vessels by the anti-angiogenesis agent can prevent the tumor from receiving a therapeutic concentration of the chemotherapy agent. Second, inhibiting blood supply drives the formation of intra-tumoral hypoxia, or a lack of oxygen, which has been correlated with increased tumor invasiveness and resistance to chemotherapy. In this thesis we report the disease-driven engineering of a drug delivery system, a 'nanocell', which overcomes these barriers unique to solid tumors. The nanocell comprises a nuclear nanoparticle encapsulated within a lipid membrane and is preferentially taken up by the tumor. The nanocell delivers a temporal release of two drugs within the tumor core: the outer lipid envelope first releases an anti-angiogenesis agent, causing a vascular shutdown; the inner nanoparticle, which is trapped inside the tumor, then releases a chemotherapy agent. This focal release within the tumor targets cells most at risk for hypoxia and results in improved therapeutic index with reduced toxicity. The technology can be extended to additional agents, so as to target multiple signaling pathways or distinct tumor compartments, enabling the model of an 'integrative' approach in cancer therapy.",
    "advisors": ["Ram Sasisekharan"],
    "text": "A novel nanoscale delivery system for spatio-temporal delivery of combination chemotherapy In the continuing search for effective treatments for cancer, the emerging model is the combination of traditional chemotherapy with anti-angiogenesis agents that inhibit blood vessel growth. However, the implementation of this strategy has faced two major obstacles. First, the long-term shutdown of tumor blood vessels by the anti-angiogenesis agent can prevent the tumor from receiving a therapeutic concentration of the chemotherapy agent. Second, inhibiting blood supply drives the formation of intra-tumoral hypoxia, or a lack of oxygen, which has been correlated with increased tumor invasiveness and resistance to chemotherapy. In this thesis we report the disease-driven engineering of a drug delivery system, a 'nanocell', which overcomes these barriers unique to solid tumors. The nanocell comprises a nuclear nanoparticle encapsulated within a lipid membrane and is preferentially taken up by the tumor. The nanocell delivers a temporal release of two drugs within the tumor core: the outer lipid envelope first releases an anti-angiogenesis agent, causing a vascular shutdown; the inner nanoparticle, which is trapped inside the tumor, then releases a chemotherapy agent. This focal release within the tumor targets cells most at risk for hypoxia and results in improved therapeutic index with reduced toxicity. The technology can be extended to additional agents, so as to target multiple signaling pathways or distinct tumor compartments, enabling the model of an 'integrative' approach in cancer therapy."
}, {
    "id": "oai:dspace.mit.edu:1721.1/87503",
    "title": "Single molecule techniques to probe decision-making processes in developmental biology",
    "abstract": "This work investigates the fundamental processes used by mammalian cells and organisms to make decisions during embryonic development. Current technologies that evaluate biological phenomenon often force a compromise between quantification of gene expression via bulk assays and qualitative imaging of cell and tissue heterogeneity. There are few options that allow for quantitative, high-resolution, single-cell analysis that is robust but not associated with a high degree of technical difficulty or obscured by amplification. Here, we address these issues using two model systems, the developing mammalian inner ear and single mouse embryonic stem cells (mESCs) during the process of X inactivation, to demonstrate our ability to perform single-cell, single-molecule assays that reveal both highly quantitative and spatial information. Accordingly, we adapted a high resolution, single-molecule RNA fluorescent in situ hybridization technique (smFISH) to study gene expression in the inner ear and perform allele-specific detection of the X chromosome in mESCs. We used previously-published smFISH procedures as our initial template for investigating biological signaling phenomena in these two systems. To study gene expression in the mouse inner ear, we developed a modified smFISH strategy to investigate mRNA transcript expression patterns in the cochlea during auditory hair cell development. The mammalian cochlea, a highly specialized and complex organ, beautifully demonstrates both the depth and breadth of the smFISH technique. To assay signaling behavior and topological changes of the X chromosome prior to X inactivation, we incorporated a novel allele-specific modification into the smFISH technique. We investigate the allele-specific expression patterns of eight genes that tile the X chromosome, which were chosen for their varied putative roles before, during and after X chromosome inactivation. Taken together, these two systems recapitulate the strength of the smFISH technique and its adaptations. The goals of this thesis were twofold: (1) expand the smFISH technique to work in specialized mammalian systems such as the cochlea and (2) demonstrate allele-specific DNA topological changes and expression patterns in mESCs. Elucidating high-resolution, single-molecule quantifiable imaging methods for application to complex tissues or allele-specific probing will have profound impacts on future investigations and promote a deeper comprehension of these systems.",
    "advisors": ["Alexander van Oudenaarden"],
    "text": "Single molecule techniques to probe decision-making processes in developmental biology This work investigates the fundamental processes used by mammalian cells and organisms to make decisions during embryonic development. Current technologies that evaluate biological phenomenon often force a compromise between quantification of gene expression via bulk assays and qualitative imaging of cell and tissue heterogeneity. There are few options that allow for quantitative, high-resolution, single-cell analysis that is robust but not associated with a high degree of technical difficulty or obscured by amplification. Here, we address these issues using two model systems, the developing mammalian inner ear and single mouse embryonic stem cells (mESCs) during the process of X inactivation, to demonstrate our ability to perform single-cell, single-molecule assays that reveal both highly quantitative and spatial information. Accordingly, we adapted a high resolution, single-molecule RNA fluorescent in situ hybridization technique (smFISH) to study gene expression in the inner ear and perform allele-specific detection of the X chromosome in mESCs. We used previously-published smFISH procedures as our initial template for investigating biological signaling phenomena in these two systems. To study gene expression in the mouse inner ear, we developed a modified smFISH strategy to investigate mRNA transcript expression patterns in the cochlea during auditory hair cell development. The mammalian cochlea, a highly specialized and complex organ, beautifully demonstrates both the depth and breadth of the smFISH technique. To assay signaling behavior and topological changes of the X chromosome prior to X inactivation, we incorporated a novel allele-specific modification into the smFISH technique. We investigate the allele-specific expression patterns of eight genes that tile the X chromosome, which were chosen for their varied putative roles before, during and after X chromosome inactivation. Taken together, these two systems recapitulate the strength of the smFISH technique and its adaptations. The goals of this thesis were twofold: (1) expand the smFISH technique to work in specialized mammalian systems such as the cochlea and (2) demonstrate allele-specific DNA topological changes and expression patterns in mESCs. Elucidating high-resolution, single-molecule quantifiable imaging methods for application to complex tissues or allele-specific probing will have profound impacts on future investigations and promote a deeper comprehension of these systems."
}, {
    "id": "oai:dspace.mit.edu:1721.1/38593",
    "title": "Detection power, temporal response, and spatial resolution of IRON fMRI in awake, behaving monkeys at 3 Tesla",
    "abstract": "The main goal of this thesis was to systematically characterize the detection sensitivity, temporal response, and spatial resolution of IRON contrast for fMRI within the awake, behaving monkey. Understanding these issues provides insights into the physiology of the functional response to local changes in brain activity, enables researchers to optimize experimental designs, and delineates the advantages and limitations of neuroimaging within this important animal model. The injection of the iron oxide contrast agent (MION) provided a 9-fold increase in efficiency for block designs relatively to BOLD contrast. Because the hemodynamic response function acts as a low-pass filter on neural activation to attenuate the size of differential responses to alternate stimuli, this factor dropped to approximately 2 for rapidly presented stimuli. Detection efficiency for event-related stimulus designs for BOLD and IRON contrasts could be optimized using random or semi-random distributions for interstimulus intervals. Small increases in predictability could be traded for large gains in efficiency, particularly for the IRON method. A general linear model was successfully employed to describe IRON and BOLD impulse response functions. Both responses were accurately described by a bimodal exponential model with similar time constants, a fast (4.5 sec) and a slow (13.5 sec).",
    "advisors": ["Joseph B. Mandeville"],
    "text": "Detection power, temporal response, and spatial resolution of IRON fMRI in awake, behaving monkeys at 3 Tesla The main goal of this thesis was to systematically characterize the detection sensitivity, temporal response, and spatial resolution of IRON contrast for fMRI within the awake, behaving monkey. Understanding these issues provides insights into the physiology of the functional response to local changes in brain activity, enables researchers to optimize experimental designs, and delineates the advantages and limitations of neuroimaging within this important animal model. The injection of the iron oxide contrast agent (MION) provided a 9-fold increase in efficiency for block designs relatively to BOLD contrast. Because the hemodynamic response function acts as a low-pass filter on neural activation to attenuate the size of differential responses to alternate stimuli, this factor dropped to approximately 2 for rapidly presented stimuli. Detection efficiency for event-related stimulus designs for BOLD and IRON contrasts could be optimized using random or semi-random distributions for interstimulus intervals. Small increases in predictability could be traded for large gains in efficiency, particularly for the IRON method. A general linear model was successfully employed to describe IRON and BOLD impulse response functions. Both responses were accurately described by a bimodal exponential model with similar time constants, a fast (4.5 sec) and a slow (13.5 sec)."
}, {
    "id": "oai:dspace.mit.edu:1721.1/117897",
    "title": "Methods for bounding genetic nonlinearities",
    "abstract": "Complex hierarchical structures are a hallmark of life. Within multicellular organisms, the building blocks of these structures are cells; within cells, they are genes. The interdependence of these building blocks is difficult to measure but is integral to the biological processes of health and disease, which emerge from the dynamism of thousands of interacting genes. This cooperativity manifests in particular mutations which accumulate over the course of cancer progression, gender-specific medical conditions, and transcription factor cocktails used to reprogram differentiated cells into stem cells. However, it is experimentally intractable to test the significance of perturbing every unique combination of genes. Instead, we explore gross features of this interaction space to determine how prevalent these synergies are. We take a top-down approach, creating new methods to measure the effects of removing genes from the full set. In the first, we develop a method to measure the transcriptional response to genetic perturbations across hundreds of thousands of cells revealing opposing classes of transcription factors regulating the immune response of dendritic cells. In the second, we create a method to measure how millions of combinations of genetic perturbations impact the growth rate of cancer cell lines.",
    "advisors": ["Aviv Regev"],
    "text": "Methods for bounding genetic nonlinearities Complex hierarchical structures are a hallmark of life. Within multicellular organisms, the building blocks of these structures are cells; within cells, they are genes. The interdependence of these building blocks is difficult to measure but is integral to the biological processes of health and disease, which emerge from the dynamism of thousands of interacting genes. This cooperativity manifests in particular mutations which accumulate over the course of cancer progression, gender-specific medical conditions, and transcription factor cocktails used to reprogram differentiated cells into stem cells. However, it is experimentally intractable to test the significance of perturbing every unique combination of genes. Instead, we explore gross features of this interaction space to determine how prevalent these synergies are. We take a top-down approach, creating new methods to measure the effects of removing genes from the full set. In the first, we develop a method to measure the transcriptional response to genetic perturbations across hundreds of thousands of cells revealing opposing classes of transcription factors regulating the immune response of dendritic cells. In the second, we create a method to measure how millions of combinations of genetic perturbations impact the growth rate of cancer cell lines."
}, {
    "id": "oai:dspace.mit.edu:1721.1/98722",
    "title": "Integrative genomic approaches to dissecting host-tumor and host-pathogen immune processes",
    "abstract": "Two parallel research efforts were pursued. First, we conducted a systematic exploration of how the genomic landscape of cancer shapes and is shaped by anti-tumor immunity. Using large-scale genomic data sets of solid tissue tumor biopsies, we quantified the cytolytic activity of the local immune infiltrate and identified associated properties across 18 tumor types. The number of predicted MHC Class I-associated neoantigens was correlated with cytolytic activity and was lower than expected in colorectal and other tumors, suggesting immune-mediated elimination. We identified recurrently mutated genes that showed positive association with cytolytic activity, including beta-2- microglobulin (B2M), HLA-A, -B and -C and Caspase 8 (CASP8), highlighting loss of antigen presentation and blockade of extrinsic apoptosis as key strategies of resistance to cytolytic activity. Genetic amplifications were also associated with high cytolytic activity, including immunosuppressive factors such as PDL1/2 and ALOX12B/15B. Our genetic findings thus provide evidence for immunoediting in tumors and uncover mechanisms of tumor-intrinsic resistance to cytolytic activity. Second, we combined measurements of protein production and degradation and mRNA dynamics so as to build a quantitative genomic model of the differential regulation of gene expression in lipopolysaccharide-stimulated mouse dendritic cells. Changes in mRNA abundance play a dominant role in determining most dynamic fold changes in protein levels. Conversely, the preexisting proteome of proteins performing basic cellular functions is remodeled primarily through changes in protein production or degradation, accounting for more than half of the absolute change in protein molecules in the cell. Thus, the proteome is regulated by transcriptional induction for newly activated cellular functions and by protein lifecycle changes for remodeling of preexisting functions.",
    "advisors": ["Nir Hacohen"],
    "text": "Integrative genomic approaches to dissecting host-tumor and host-pathogen immune processes Two parallel research efforts were pursued. First, we conducted a systematic exploration of how the genomic landscape of cancer shapes and is shaped by anti-tumor immunity. Using large-scale genomic data sets of solid tissue tumor biopsies, we quantified the cytolytic activity of the local immune infiltrate and identified associated properties across 18 tumor types. The number of predicted MHC Class I-associated neoantigens was correlated with cytolytic activity and was lower than expected in colorectal and other tumors, suggesting immune-mediated elimination. We identified recurrently mutated genes that showed positive association with cytolytic activity, including beta-2- microglobulin (B2M), HLA-A, -B and -C and Caspase 8 (CASP8), highlighting loss of antigen presentation and blockade of extrinsic apoptosis as key strategies of resistance to cytolytic activity. Genetic amplifications were also associated with high cytolytic activity, including immunosuppressive factors such as PDL1/2 and ALOX12B/15B. Our genetic findings thus provide evidence for immunoediting in tumors and uncover mechanisms of tumor-intrinsic resistance to cytolytic activity. Second, we combined measurements of protein production and degradation and mRNA dynamics so as to build a quantitative genomic model of the differential regulation of gene expression in lipopolysaccharide-stimulated mouse dendritic cells. Changes in mRNA abundance play a dominant role in determining most dynamic fold changes in protein levels. Conversely, the preexisting proteome of proteins performing basic cellular functions is remodeled primarily through changes in protein production or degradation, accounting for more than half of the absolute change in protein molecules in the cell. Thus, the proteome is regulated by transcriptional induction for newly activated cellular functions and by protein lifecycle changes for remodeling of preexisting functions."
}, {
    "id": "oai:dspace.mit.edu:1721.1/98724",
    "title": "Controlled local delivery of RNA : regulating tissue morphogenesis",
    "abstract": "RNA interference (RNAi) is a powerful technology that provides a means to alter the expression of a specific protein based on a targeted RNA sequence. This is done by taking advantage of existing cellular machinery present within all eukaryotic cells which use short double-stranded RNA sequences as guides for RNA induced silencing. The potential for RNAi in medicine is enormous, providing a new approach to treat the complex biological dysregulation underlying many diseases. This promise of a new branch of therapeutics however has been mired with difficulties. RNA is quickly degraded by nucleases that are prevalent in the blood and throughout the body, it is highly immunogenic, and systemic delivery is complicated by high clearance rates. As such, developing formulations for the effective delivery of short RNAs presents significant hurdles. Local delivery can limit numerous unwanted systemic side effects of therapies and it maintains the highest therapeutic index possible in a targeted area before clearance. As such, the local delivery of siRNA may hold just as much potential as systemic delivery with significantly reduced complications. Layer-by-layer (LbL) assembly is a robust method that has been successfully demonstrated for the localized and sustained delivery of many biologic therapeutics and biomolecules. Developing an LbL film capable of delivering siRNA locally would offer a powerful new approach to the treatment of local disorders. This approach could be combined with existing medical devices to improve patient outcomes by directly addressing pathologic dysregulation in the area of interest. One field where the local treatment of dysregulation could be of particular interest is that of wound healing. Wound healing is a complex and highly synchronized process of multiple biological pathways, consisting of an assortment of cytokines, growth factors, and varied cell types which evolves over time. The development of a drug delivery system that can locally modify cell behavior on the basic level of gene transcription would be a powerful tool to alter the dynamics of wound healing. There are many known complications of wound healing, ranging from chronic ulcerative wounds to hypertrophic contractile scars, which dramatically affect the lives of tens of millions of patients every year. Through RNA interference, one could specifically target the key mediators of these complications, providing a means to more effectively regulate the wound healing process in vivo. The capability to deliver siRNA locally to address these complications is a significant advance in the current state of wound treatment. As such, this work presents an opportunity to substantially improve the current standard of treatment for patients and their wound healing outcomes. Herein, we present the design and preclinical evaluation of a number of strategies to develop ultra-thin polymer coatings for the controlled delivery of RNA both in vitro and in vivo. We used Layer-by-Layer assembly to create siRNA containing polymer-based films that can sustain the release of complexed siRNA over physiologically relevant timescales for local delivery into tissues. We describe the development of the first high-throughput approach for LbL assembly and screening and its use to identify lead candidate film architectures for RNA delivery. We then apply these findings to treat dysregulation in two distinct animal models; a chronic diabetic mouse wound model and a third-degree bum model in rats, targeting three different genes of interest independently. These coatings were demonstrated to effectively coat a number of medically relevant substrates including bandages, sutures, surgical staples, nanoparticles, microparticles, and microneedles. This body of work provides insight into how siRNA can be incorporated into thin film assemblies and the design criteria to achieve successful gene knockdown in vitro and in vivo.",
    "advisors": ["Paula T. Hammond"],
    "text": "Controlled local delivery of RNA : regulating tissue morphogenesis RNA interference (RNAi) is a powerful technology that provides a means to alter the expression of a specific protein based on a targeted RNA sequence. This is done by taking advantage of existing cellular machinery present within all eukaryotic cells which use short double-stranded RNA sequences as guides for RNA induced silencing. The potential for RNAi in medicine is enormous, providing a new approach to treat the complex biological dysregulation underlying many diseases. This promise of a new branch of therapeutics however has been mired with difficulties. RNA is quickly degraded by nucleases that are prevalent in the blood and throughout the body, it is highly immunogenic, and systemic delivery is complicated by high clearance rates. As such, developing formulations for the effective delivery of short RNAs presents significant hurdles. Local delivery can limit numerous unwanted systemic side effects of therapies and it maintains the highest therapeutic index possible in a targeted area before clearance. As such, the local delivery of siRNA may hold just as much potential as systemic delivery with significantly reduced complications. Layer-by-layer (LbL) assembly is a robust method that has been successfully demonstrated for the localized and sustained delivery of many biologic therapeutics and biomolecules. Developing an LbL film capable of delivering siRNA locally would offer a powerful new approach to the treatment of local disorders. This approach could be combined with existing medical devices to improve patient outcomes by directly addressing pathologic dysregulation in the area of interest. One field where the local treatment of dysregulation could be of particular interest is that of wound healing. Wound healing is a complex and highly synchronized process of multiple biological pathways, consisting of an assortment of cytokines, growth factors, and varied cell types which evolves over time. The development of a drug delivery system that can locally modify cell behavior on the basic level of gene transcription would be a powerful tool to alter the dynamics of wound healing. There are many known complications of wound healing, ranging from chronic ulcerative wounds to hypertrophic contractile scars, which dramatically affect the lives of tens of millions of patients every year. Through RNA interference, one could specifically target the key mediators of these complications, providing a means to more effectively regulate the wound healing process in vivo. The capability to deliver siRNA locally to address these complications is a significant advance in the current state of wound treatment. As such, this work presents an opportunity to substantially improve the current standard of treatment for patients and their wound healing outcomes. Herein, we present the design and preclinical evaluation of a number of strategies to develop ultra-thin polymer coatings for the controlled delivery of RNA both in vitro and in vivo. We used Layer-by-Layer assembly to create siRNA containing polymer-based films that can sustain the release of complexed siRNA over physiologically relevant timescales for local delivery into tissues. We describe the development of the first high-throughput approach for LbL assembly and screening and its use to identify lead candidate film architectures for RNA delivery. We then apply these findings to treat dysregulation in two distinct animal models; a chronic diabetic mouse wound model and a third-degree bum model in rats, targeting three different genes of interest independently. These coatings were demonstrated to effectively coat a number of medically relevant substrates including bandages, sutures, surgical staples, nanoparticles, microparticles, and microneedles. This body of work provides insight into how siRNA can be incorporated into thin film assemblies and the design criteria to achieve successful gene knockdown in vitro and in vivo."
}, {
    "id": "oai:dspace.mit.edu:1721.1/45909",
    "title": "Coil performance evaluation based on electrodynamics : tools for hardware design and validation in magnetic resonance imaging",
    "abstract": "Parallel MRI techniques allow acceleration of MR imaging beyond traditional speed limits. In parallel MRI, radiofrequency (RF) detector coil arrays are used to perform some degree of spatial encoding which complements traditional encoding using magnetic field gradients. As the acceleration factor increases, coil design becomes critical to the overall image quality. The quality of a design is commonly judged on how it compares with other coil configurations. A procedure to evaluate the absolute performance of RF coil arrays is proposed. Electromagnetic calculations to compute the ultimate intrinsic signal-to-noise ratio (SNR) available for any physically realizable coil array are shown, and coil performance maps are generated based on the ratio of experimentally measured SNR to this ultimate intrinsic SNR. Parallel excitation, which involves independent transmission with multiple RF coils distributed around the body, can be used to improve the homogeneity of RF excitations and minimize the RF energy deposited in tissues - both critical issues for MRI at high magnetic field strength. As its use is explored further, it will be important to investigate the intrinsic constraints of the technique. We studied the trade-off between transmit homogeneity and specific absorption rate (SAR) reduction with respect to main magnetic field strength, object size and acceleration. We introduced the concept of ultimate intrinsic SAR, the theoretical smallest RF energy deposition for a target flip angle distribution, and we calculated the corresponding ideal current patterns. Knowledge of these optimal current patterns will serve as an important guide for future high-field coil designs.",
    "advisors": ["Daniel K. Sodickson"],
    "text": "Coil performance evaluation based on electrodynamics : tools for hardware design and validation in magnetic resonance imaging Parallel MRI techniques allow acceleration of MR imaging beyond traditional speed limits. In parallel MRI, radiofrequency (RF) detector coil arrays are used to perform some degree of spatial encoding which complements traditional encoding using magnetic field gradients. As the acceleration factor increases, coil design becomes critical to the overall image quality. The quality of a design is commonly judged on how it compares with other coil configurations. A procedure to evaluate the absolute performance of RF coil arrays is proposed. Electromagnetic calculations to compute the ultimate intrinsic signal-to-noise ratio (SNR) available for any physically realizable coil array are shown, and coil performance maps are generated based on the ratio of experimentally measured SNR to this ultimate intrinsic SNR. Parallel excitation, which involves independent transmission with multiple RF coils distributed around the body, can be used to improve the homogeneity of RF excitations and minimize the RF energy deposited in tissues - both critical issues for MRI at high magnetic field strength. As its use is explored further, it will be important to investigate the intrinsic constraints of the technique. We studied the trade-off between transmit homogeneity and specific absorption rate (SAR) reduction with respect to main magnetic field strength, object size and acceleration. We introduced the concept of ultimate intrinsic SAR, the theoretical smallest RF energy deposition for a target flip angle distribution, and we calculated the corresponding ideal current patterns. Knowledge of these optimal current patterns will serve as an important guide for future high-field coil designs."
}, {
    "id": "oai:dspace.mit.edu:1721.1/28762",
    "title": "Sources of difference frequency sound in a dual-frequency imaging system with implications for monitoring thermal surgery",
    "abstract": "(cont.) parametric effect, which can be considered an imaging artifact. Additionally, it may be possible to use the nonlinear interaction of scattered waves to form images that rely on the presence of small scatterers; a technique that may be enhanced with the use of contrast agents containing small scattering micro-bubbles in vivo.",
    "advisors": ["Kullervo Hynynen", "Nicholas Makris"],
    "text": "Sources of difference frequency sound in a dual-frequency imaging system with implications for monitoring thermal surgery (cont.) parametric effect, which can be considered an imaging artifact. Additionally, it may be possible to use the nonlinear interaction of scattered waves to form images that rely on the presence of small scatterers; a technique that may be enhanced with the use of contrast agents containing small scattering micro-bubbles in vivo."
}, {
    "id": "oai:dspace.mit.edu:1721.1/90171",
    "title": "Estimating evolutionary parameters and detecting signals of natural selection from genetic data",
    "abstract": "Even prior to the elucidation of the structure of DNA, the theoretical foundations of population genetics had been well developed. Advances made by Sewall Wright, John B.S. Haldane, and Ronald A. Fisher form the basis with which we understand the statistical dynamics of evolution and inheritance. Using this foundation, recent advances in DNA profiling technologies have enabled genome-wide analysis of thousands of individuals from a diverse array of human populations. These new analyses can answer fundamental questions about human population differences, natural selection, and admixture. However, with this deluge of newly available data, confusion about statistical methods may lead to misleading conclusions about human population history and natural selection. We view it as imperative to put analyses of population differences on sound statistical footing. In the course of this thesis, we have developed methods and reanalyzed existing results in two related areas: the detection of natural selection and estimation of genetic distance. Throughout our work, we have strived for statistical rigor, attempting to understand variation in previously reported results and provide a resource for other researchers in our field. Where necessary, we have made simplifying assumptions about evolutionary processes but have attempted to state these clearly and validate their reasonableness using simulations. Our efforts have culminated in three projects that will be described in the subsequent chapters: (1) A model based approach to detect natural selection in 3 populations (2) A protocol to generate consistent estimates of FST and, (3) Reanalysis of previously reports of selection in African Americans since the arrival of their ancestors in the Americas. We note that our work is just part of a rich literature on population and evolutionary genetics. We have attempted to cite this literature in detail and have published our own methods to enable others to utilize and improve upon them.",
    "advisors": ["Alkes L. Price"],
    "text": "Estimating evolutionary parameters and detecting signals of natural selection from genetic data Even prior to the elucidation of the structure of DNA, the theoretical foundations of population genetics had been well developed. Advances made by Sewall Wright, John B.S. Haldane, and Ronald A. Fisher form the basis with which we understand the statistical dynamics of evolution and inheritance. Using this foundation, recent advances in DNA profiling technologies have enabled genome-wide analysis of thousands of individuals from a diverse array of human populations. These new analyses can answer fundamental questions about human population differences, natural selection, and admixture. However, with this deluge of newly available data, confusion about statistical methods may lead to misleading conclusions about human population history and natural selection. We view it as imperative to put analyses of population differences on sound statistical footing. In the course of this thesis, we have developed methods and reanalyzed existing results in two related areas: the detection of natural selection and estimation of genetic distance. Throughout our work, we have strived for statistical rigor, attempting to understand variation in previously reported results and provide a resource for other researchers in our field. Where necessary, we have made simplifying assumptions about evolutionary processes but have attempted to state these clearly and validate their reasonableness using simulations. Our efforts have culminated in three projects that will be described in the subsequent chapters: (1) A model based approach to detect natural selection in 3 populations (2) A protocol to generate consistent estimates of FST and, (3) Reanalysis of previously reports of selection in African Americans since the arrival of their ancestors in the Americas. We note that our work is just part of a rich literature on population and evolutionary genetics. We have attempted to cite this literature in detail and have published our own methods to enable others to utilize and improve upon them."
}, {
    "id": "oai:dspace.mit.edu:1721.1/54449",
    "title": "Mitochondrial parts, pathways, and pathogenesis",
    "abstract": "Mitochondria are cellular compartments that perform essential roles in energy metabolism, ion homeostasis, and apoptosis. Mitochondrial dysfunction causes disease in 1 in 5,000 live births and also has been associated with aging, neurodegeneration, cancer, and diabetes. To systematically explore the function of mitochondria in health and in disease, it is necessary to identify all of the proteins resident in this organelle and to understand how they integrate into pathways. However, traditional molecular and biochemistry methods have identified only half of the estimated 1200 mitochondrial proteins, including the 13 encoded by the tiny mitochondrial genome. Now, newly available genomic technologies make it possible to identify the remainder and explore their roles in cellular pathways and disease. Toward this goal, we performed mass spectrometry, GFP tagging, and machine learning on multiple genomic datasets to create a mitochondrial compendium of 1098 genes and their protein expression across 14 mouse tissues. We linked poorly characterized proteins in this inventory to known mitochondrial pathways by virtue of shared evolutionary history. We additionally used our matched mRNA and protein measurements to demonstrate a widespread role of upstream open reading frames (uORFs) in blunting translation of mitochondrial and other cellular proteins. Next we used the mitochondrial protein inventory to identify genes underlying inherited diseases of mitochondrial dysfunction. In collaboration with clinicians, we identified causal mutations in five genes underlying diseases including hepatocerebral mtDNA depletion syndrome, autosomal dominant mitochondrial myopathy, and several forms of inherited complex I deficiency. These discoveries have enabled the development of diagnostic tests now widely available. More broadly, the mitochondrial compendium provides a foundation for systematically exploring the organelle's contribution to both basic cellular biology and human disease.",
    "advisors": ["Vamsi K. Mootha"],
    "text": "Mitochondrial parts, pathways, and pathogenesis Mitochondria are cellular compartments that perform essential roles in energy metabolism, ion homeostasis, and apoptosis. Mitochondrial dysfunction causes disease in 1 in 5,000 live births and also has been associated with aging, neurodegeneration, cancer, and diabetes. To systematically explore the function of mitochondria in health and in disease, it is necessary to identify all of the proteins resident in this organelle and to understand how they integrate into pathways. However, traditional molecular and biochemistry methods have identified only half of the estimated 1200 mitochondrial proteins, including the 13 encoded by the tiny mitochondrial genome. Now, newly available genomic technologies make it possible to identify the remainder and explore their roles in cellular pathways and disease. Toward this goal, we performed mass spectrometry, GFP tagging, and machine learning on multiple genomic datasets to create a mitochondrial compendium of 1098 genes and their protein expression across 14 mouse tissues. We linked poorly characterized proteins in this inventory to known mitochondrial pathways by virtue of shared evolutionary history. We additionally used our matched mRNA and protein measurements to demonstrate a widespread role of upstream open reading frames (uORFs) in blunting translation of mitochondrial and other cellular proteins. Next we used the mitochondrial protein inventory to identify genes underlying inherited diseases of mitochondrial dysfunction. In collaboration with clinicians, we identified causal mutations in five genes underlying diseases including hepatocerebral mtDNA depletion syndrome, autosomal dominant mitochondrial myopathy, and several forms of inherited complex I deficiency. These discoveries have enabled the development of diagnostic tests now widely available. More broadly, the mitochondrial compendium provides a foundation for systematically exploring the organelle's contribution to both basic cellular biology and human disease."
}, {
    "id": "oai:dspace.mit.edu:1721.1/54590",
    "title": "Adipogenesis and angiogenesis : roles in tissue engineering and glucose metabolism",
    "abstract": "Adipose tissue serves two main functions in the body: (1) it is the body's primary energy depot; and (2) it also serves as an important endocrine organ, producing and secreting various enzymes, growth factors, cytokines, and hormones. Both of these functions require ample access to circulating blood. Many aspects of angiogenesis during adipose tissue expansion remain poorly understood. Adipocytes produce a large variety of molecules involved in angiogenesis, and obesity is associated with elevated circulating levels of Vascular Endothelial Growth Factor (VEGF). Our lab has previously shown that angiogenesis and adipogenesis are mutually dependent via a VEGF receptor 2 (VEGFR2)-mediated mechanism. Since then several other studies have reinforced a role for the VEGF-VEGFR system in energy metabolism. For example, genetically obese mice treated with anti-VEGF antibody had lower fat pad weights, but the VEGF receptor responsible for this observation is not known. There is also disagreement on the cell type(s) responsible for fat tissue's angiogenic capability, with some studies supporting a dominant role for adipocytes, while others attribute most of the angiogenic capacity to the adipose tissue stromal cells (ASC). This thesis project aimed to fill some of these gaps by examining the angiogenic capacity of adipose tissue relative to other tissues, the effects of VEGFR-1 and R-2 blockade in mouse models of adipogenesis and diet-induced obesity, the respective angiogenic capabilities of adipocytes and ASC, and the possibility of harnessing the angiogenic potential of adipose tissue for vascular tissue engineering.",
    "advisors": ["Rakesh K. Jain"],
    "text": "Adipogenesis and angiogenesis : roles in tissue engineering and glucose metabolism Adipose tissue serves two main functions in the body: (1) it is the body's primary energy depot; and (2) it also serves as an important endocrine organ, producing and secreting various enzymes, growth factors, cytokines, and hormones. Both of these functions require ample access to circulating blood. Many aspects of angiogenesis during adipose tissue expansion remain poorly understood. Adipocytes produce a large variety of molecules involved in angiogenesis, and obesity is associated with elevated circulating levels of Vascular Endothelial Growth Factor (VEGF). Our lab has previously shown that angiogenesis and adipogenesis are mutually dependent via a VEGF receptor 2 (VEGFR2)-mediated mechanism. Since then several other studies have reinforced a role for the VEGF-VEGFR system in energy metabolism. For example, genetically obese mice treated with anti-VEGF antibody had lower fat pad weights, but the VEGF receptor responsible for this observation is not known. There is also disagreement on the cell type(s) responsible for fat tissue's angiogenic capability, with some studies supporting a dominant role for adipocytes, while others attribute most of the angiogenic capacity to the adipose tissue stromal cells (ASC). This thesis project aimed to fill some of these gaps by examining the angiogenic capacity of adipose tissue relative to other tissues, the effects of VEGFR-1 and R-2 blockade in mouse models of adipogenesis and diet-induced obesity, the respective angiogenic capabilities of adipocytes and ASC, and the possibility of harnessing the angiogenic potential of adipose tissue for vascular tissue engineering."
}, {
    "id": "oai:dspace.mit.edu:1721.1/54453",
    "title": "Computational studies of tau protein : implications for the pathogenesis and treatment of neurodegenerative diseases",
    "abstract": "Tau protein is the primary constituent of protein aggregates known as neurofibrillary tangles, a pathological hallmark of Alzheimer's disease (AD). Previous studies suggest that tau protein may play a contributing role in neurodegenerative diseases such as AD. Thus characterizing the structural properties of tau is critical to understanding disease pathogenesis. However, obtaining a detailed structural description of tau protein has been difficult because it belongs to a class of heteropolymers known as intrinsically disordered proteins (IDPs). Unlike most proteins, IDPs adopt many distinct conformations under physiological conditions. In spite of their disordered nature, evidence exists that such proteins may exhibit residual structural preferences. In this work, models of tau are constructed to characterize these structural preferences. We begin by performing molecular dynamics simulations to study the inherent conformational preferences of the minimal tau subsequence required for in vitro aggregation. To model residual structure in larger regions of tau, we developed a novel method called Energy-minima Mapping and Weighting (EMW). The method samples energetically favorable",
    "advisors": ["Collin M. Stultz"],
    "text": "Computational studies of tau protein : implications for the pathogenesis and treatment of neurodegenerative diseases Tau protein is the primary constituent of protein aggregates known as neurofibrillary tangles, a pathological hallmark of Alzheimer's disease (AD). Previous studies suggest that tau protein may play a contributing role in neurodegenerative diseases such as AD. Thus characterizing the structural properties of tau is critical to understanding disease pathogenesis. However, obtaining a detailed structural description of tau protein has been difficult because it belongs to a class of heteropolymers known as intrinsically disordered proteins (IDPs). Unlike most proteins, IDPs adopt many distinct conformations under physiological conditions. In spite of their disordered nature, evidence exists that such proteins may exhibit residual structural preferences. In this work, models of tau are constructed to characterize these structural preferences. We begin by performing molecular dynamics simulations to study the inherent conformational preferences of the minimal tau subsequence required for in vitro aggregation. To model residual structure in larger regions of tau, we developed a novel method called Energy-minima Mapping and Weighting (EMW). The method samples energetically favorable"
}, {
    "id": "oai:dspace.mit.edu:1721.1/104611",
    "title": "Localized and disease-selective drug delivery using adhesive hydrogels for treatment of locally advanced TNBC",
    "abstract": "Triple negative breast cancer (TNBC) is an aggressive form of cancer that represents 20% of invasive breast cancers, and about 15% are locally advanced at time of presentation. TNBC is negative for estrogen and progesterone receptor, as well as for HER2, and hence it is not treatable with common endocrine treatment such as tamoxifen or Herceptin. Systemic neoadjuvant therapy has been established as the preferred therapeutic approach for locally advanced breast cancer, downstaging the disease and preventing mastectomy. However, complications of systemic chemotherapy are devastating. Local therapy would prevent high concentrations of circulating drug and reduce off-target tissue retention. Yet, the means to attain ideal release kinetics and selective uptake remain elusive. I developed a novel class of biocompatible and biodegradable adhesive materials based on dendrimer and dextran that can coat the tumor and locally release doxorubicin in a controlled manner. Doxorubicin was conjugated to the dendritic component of the adhesive hydrogel to form a pro-drug capable of being released over time as the hydrogel degrades at a pre-programmed rate. The pro-drug was further modified with a ligand capable of sensing and discerning between healthy and cancer cells and facilitating uptake through receptor-mediated endocytosis (RME). The platform developed herein provides a paradigm shift in the way we treat cancer, in a local, selective and targeted manner, to impart optimal clinical outcome.",
    "advisors": ["Elazer R. Edelman", "Natalie Artzi"],
    "text": "Localized and disease-selective drug delivery using adhesive hydrogels for treatment of locally advanced TNBC Triple negative breast cancer (TNBC) is an aggressive form of cancer that represents 20% of invasive breast cancers, and about 15% are locally advanced at time of presentation. TNBC is negative for estrogen and progesterone receptor, as well as for HER2, and hence it is not treatable with common endocrine treatment such as tamoxifen or Herceptin. Systemic neoadjuvant therapy has been established as the preferred therapeutic approach for locally advanced breast cancer, downstaging the disease and preventing mastectomy. However, complications of systemic chemotherapy are devastating. Local therapy would prevent high concentrations of circulating drug and reduce off-target tissue retention. Yet, the means to attain ideal release kinetics and selective uptake remain elusive. I developed a novel class of biocompatible and biodegradable adhesive materials based on dendrimer and dextran that can coat the tumor and locally release doxorubicin in a controlled manner. Doxorubicin was conjugated to the dendritic component of the adhesive hydrogel to form a pro-drug capable of being released over time as the hydrogel degrades at a pre-programmed rate. The pro-drug was further modified with a ligand capable of sensing and discerning between healthy and cancer cells and facilitating uptake through receptor-mediated endocytosis (RME). The platform developed herein provides a paradigm shift in the way we treat cancer, in a local, selective and targeted manner, to impart optimal clinical outcome."
}, {
    "id": "oai:dspace.mit.edu:1721.1/72914",
    "title": "Tumor-penetrating delivery of small interfering RNA therapeutics",
    "abstract": "Efforts to sequence cancer genomes have begun to uncover comprehensive lists of genes altered in cancer. Unfortunately, the number and complexity of identified alterations has made dissecting the underlying biology of cancer difficult, as many genes are not amenable to manipulation by small molecules or antibodies. RNA interference (RNAi) provides a direct way to assess and act on putative cancer targets. However, the translation of RNAi into the clinic has been thwarted by the \"delivery\" challenge, as small interfering RNA (siRNA) therapeutics must overcome clearance mechanisms and penetrate into tumor tissues to access cancer cells. This thesis sought to develop nanotechnology-based platforms to rapidly discover and validate cancer targets in vivo. First, we developed versatile surface chemistries for nanoparticle tumor targeting. Leveraging new discoveries in amplified transvascular transport, we designed a siRNA delivery system that integrates the tumor specificity and tissue-penetrating ability of tumor-penetrating peptides with membrane penetration properties of protein transduction domains to direct siRNA to tumors in vivo. Second, we utilized this delivery system to bridge the gap between cancer genomic discovery and in vivo target validation. Comprehensive analysis of ovarian cancer genomes identified candidate targets that are undruggable by traditional approaches. Tumor-penetrating delivery of siRNA against these genes potently impeded the growth of ovarian tumors in mice and improved survival, thereby credentialing their roles in tumor initiation and maintenance. Lastly, we described efforts extending this platform for clinical translation. Mechanistic studies identified functional properties that favored receptor-specific siRNA delivery. We also explored a strategy to improve the microdistribution of successively dosed siRNA therapeutics through modulating the tumor microenvironment. Finally, we investigated the utility of the system in primary human tumors derived from patients with ovarian cancer. Together, these findings illustrate that the combination of cancer genomics with the engineering of siRNA delivery nanomaterials establishes a platform for discovering genes amenable to RNAi therapies. As efforts in genome sequencing accelerate, this platform illustrates a path to clinical translation in humans.",
    "advisors": ["Sangeeta N. Bhatia"],
    "text": "Tumor-penetrating delivery of small interfering RNA therapeutics Efforts to sequence cancer genomes have begun to uncover comprehensive lists of genes altered in cancer. Unfortunately, the number and complexity of identified alterations has made dissecting the underlying biology of cancer difficult, as many genes are not amenable to manipulation by small molecules or antibodies. RNA interference (RNAi) provides a direct way to assess and act on putative cancer targets. However, the translation of RNAi into the clinic has been thwarted by the \"delivery\" challenge, as small interfering RNA (siRNA) therapeutics must overcome clearance mechanisms and penetrate into tumor tissues to access cancer cells. This thesis sought to develop nanotechnology-based platforms to rapidly discover and validate cancer targets in vivo. First, we developed versatile surface chemistries for nanoparticle tumor targeting. Leveraging new discoveries in amplified transvascular transport, we designed a siRNA delivery system that integrates the tumor specificity and tissue-penetrating ability of tumor-penetrating peptides with membrane penetration properties of protein transduction domains to direct siRNA to tumors in vivo. Second, we utilized this delivery system to bridge the gap between cancer genomic discovery and in vivo target validation. Comprehensive analysis of ovarian cancer genomes identified candidate targets that are undruggable by traditional approaches. Tumor-penetrating delivery of siRNA against these genes potently impeded the growth of ovarian tumors in mice and improved survival, thereby credentialing their roles in tumor initiation and maintenance. Lastly, we described efforts extending this platform for clinical translation. Mechanistic studies identified functional properties that favored receptor-specific siRNA delivery. We also explored a strategy to improve the microdistribution of successively dosed siRNA therapeutics through modulating the tumor microenvironment. Finally, we investigated the utility of the system in primary human tumors derived from patients with ovarian cancer. Together, these findings illustrate that the combination of cancer genomics with the engineering of siRNA delivery nanomaterials establishes a platform for discovering genes amenable to RNAi therapies. As efforts in genome sequencing accelerate, this platform illustrates a path to clinical translation in humans."
}, {
    "id": "oai:dspace.mit.edu:1721.1/62520",
    "title": "Identification of a gap junction communication pathway critical in innate immunity",
    "abstract": "The innate immune system is the first line of host defense, and its ability to propagate antimicrobial and inflammatory signals from the cellular microenvironment to the tissue at-large is critical for survival. In a remarkably complex microenvironment, cells are constantly processing external cues, initiating convoluted intracellular signaling cascades, and interacting with neighboring cells to generate a global, unified response. At the onset of infection or sterile injury, individual cells sense danger or damage signals and elicit innate immune responses that spread from the challenged cells to surrounding cells, thereby establishing an overall inflammatory state. However, little is known about how these dynamic spatiotemporal responses unfold. Through the use GFP reporters, in vitro transplant coculture systems, and in vivo models of infection and sterile injury, this thesis describes identification of a gap junction intercellular communication pathway for amplifying immune and inflammatory responses, and demonstrates its importance in host innate immunity. The first section describes development of stable GFP reporters to study the spatiotemporal activation patterns of two key transcription factors in inflammation and innate immunity: Nuclear factor-KappaB (NFKB) and Interferon regulatory factor 3 (IRF3). Stimulation of NFKB-GFP reporters resulted in a spatially homogeneous pattern of activation, found to be largely mediated by paracrine action of the pro-inflammatory cytokine TNFa. In contrast, the activation of IRF3 was spatially heterogeneous, resulting in the formation of multicellular colonies of activated cells in an otherwise latent background. This pattern of activation was demonstrated to be dependent on cell-cell contact mediated communication between neighboring cells, and not on paracrine signaling. The second section describes the discovery of a gap junction intercellular communication pathway responsible for the formation of IRF3 active colonies in response to immune activation. Cell sorting and gene expression profiling revealed that the activated reporter colonies, collectively, serve as the major source of critical antimicrobial and inflammatory cytokines. Using in vitro transplant coculture systems, colony formation was found to be dependent on gap junction communication. Blocking gap junctions with genetic specificity severely compromised the innate immune system's ability to mount antiviral and inflammatory responses. The third section illustrates an application of the gap junction-induced amplification of innate immunity phenomenon in an animal model of sterile injury. Drug-induced liver injury was shown to be dependent on gap junction communication for amplifying sterile inflammatory signals. Mice deficient in hepatic gap junction protein connexin 32 (Cx32) were protected against liver damage, inflammation, and death in response to hepatotoxic drugs. Co-administration of a selective pharmacologic Cx32 inhibitor with hepatotoxic drugs significantly limited hepatocyte damage and sterile inflammation, and completely abrogated mortality. These finds suggests that co-formulation of gap junction inhibitors with hepatotoxic drugs may prevent liver failure in humans, and potentially limit other forms of sterile injury. In summary, this thesis demonstrates the development of novel tools for investigating the spatiotemporal dynamics of cellular responses, describes how these tools were utilized to discover a basic gap junction communication pathway critical in innate immunity, and provides evidence for the clinical relevance of this pathway in sterile inflammatory injury.",
    "advisors": ["Martin L. Yarmush"],
    "text": "Identification of a gap junction communication pathway critical in innate immunity The innate immune system is the first line of host defense, and its ability to propagate antimicrobial and inflammatory signals from the cellular microenvironment to the tissue at-large is critical for survival. In a remarkably complex microenvironment, cells are constantly processing external cues, initiating convoluted intracellular signaling cascades, and interacting with neighboring cells to generate a global, unified response. At the onset of infection or sterile injury, individual cells sense danger or damage signals and elicit innate immune responses that spread from the challenged cells to surrounding cells, thereby establishing an overall inflammatory state. However, little is known about how these dynamic spatiotemporal responses unfold. Through the use GFP reporters, in vitro transplant coculture systems, and in vivo models of infection and sterile injury, this thesis describes identification of a gap junction intercellular communication pathway for amplifying immune and inflammatory responses, and demonstrates its importance in host innate immunity. The first section describes development of stable GFP reporters to study the spatiotemporal activation patterns of two key transcription factors in inflammation and innate immunity: Nuclear factor-KappaB (NFKB) and Interferon regulatory factor 3 (IRF3). Stimulation of NFKB-GFP reporters resulted in a spatially homogeneous pattern of activation, found to be largely mediated by paracrine action of the pro-inflammatory cytokine TNFa. In contrast, the activation of IRF3 was spatially heterogeneous, resulting in the formation of multicellular colonies of activated cells in an otherwise latent background. This pattern of activation was demonstrated to be dependent on cell-cell contact mediated communication between neighboring cells, and not on paracrine signaling. The second section describes the discovery of a gap junction intercellular communication pathway responsible for the formation of IRF3 active colonies in response to immune activation. Cell sorting and gene expression profiling revealed that the activated reporter colonies, collectively, serve as the major source of critical antimicrobial and inflammatory cytokines. Using in vitro transplant coculture systems, colony formation was found to be dependent on gap junction communication. Blocking gap junctions with genetic specificity severely compromised the innate immune system's ability to mount antiviral and inflammatory responses. The third section illustrates an application of the gap junction-induced amplification of innate immunity phenomenon in an animal model of sterile injury. Drug-induced liver injury was shown to be dependent on gap junction communication for amplifying sterile inflammatory signals. Mice deficient in hepatic gap junction protein connexin 32 (Cx32) were protected against liver damage, inflammation, and death in response to hepatotoxic drugs. Co-administration of a selective pharmacologic Cx32 inhibitor with hepatotoxic drugs significantly limited hepatocyte damage and sterile inflammation, and completely abrogated mortality. These finds suggests that co-formulation of gap junction inhibitors with hepatotoxic drugs may prevent liver failure in humans, and potentially limit other forms of sterile injury. In summary, this thesis demonstrates the development of novel tools for investigating the spatiotemporal dynamics of cellular responses, describes how these tools were utilized to discover a basic gap junction communication pathway critical in innate immunity, and provides evidence for the clinical relevance of this pathway in sterile inflammatory injury."
}, {
    "id": "oai:dspace.mit.edu:1721.1/54670",
    "title": "A metabolic profiling approach to human disorders of energy metabolism",
    "abstract": "The integrated network of biochemical reactions known collectively as metabolism is essential for life, and dysfunction in parts of this network causes human disease - both rare, inherited disorders and common diseases such as diabetes mellitus. The study of metabolic disease depends upon quantitative methods which are traditionally custom-tailored to a given compound. Recent advances in technologies such as mass spectrometry now enable the simultaneous measurement of a diverse metabolite collection spanning multiple biological pathways, an approach known as metabolic profiling or metabolomics. This dissertation describes the development of one such metabolic profiling system and its application to the study of two major topics in human energy metabolism: the fasting:feeding transition and mitochondrial disease. In the first study, we profile human plasma in response to glucose ingestion, detecting dozens of metabolite changes and identifying several distinct effects of insulin. Based on these observations, we propose a multivariate view of insulin sensitivity, and show that individuals at risk for developing diabetes mellitus can differ in their insulin response profile, a concept of potential value for estimating disease risk and progression. In the second study, we elucidate a metabolic signature of human mitochondrial disease that reflects substrate oxidation, biosynthesis and energy charge.",
    "advisors": ["Vamsi K. Mootha"],
    "text": "A metabolic profiling approach to human disorders of energy metabolism The integrated network of biochemical reactions known collectively as metabolism is essential for life, and dysfunction in parts of this network causes human disease - both rare, inherited disorders and common diseases such as diabetes mellitus. The study of metabolic disease depends upon quantitative methods which are traditionally custom-tailored to a given compound. Recent advances in technologies such as mass spectrometry now enable the simultaneous measurement of a diverse metabolite collection spanning multiple biological pathways, an approach known as metabolic profiling or metabolomics. This dissertation describes the development of one such metabolic profiling system and its application to the study of two major topics in human energy metabolism: the fasting:feeding transition and mitochondrial disease. In the first study, we profile human plasma in response to glucose ingestion, detecting dozens of metabolite changes and identifying several distinct effects of insulin. Based on these observations, we propose a multivariate view of insulin sensitivity, and show that individuals at risk for developing diabetes mellitus can differ in their insulin response profile, a concept of potential value for estimating disease risk and progression. In the second study, we elucidate a metabolic signature of human mitochondrial disease that reflects substrate oxidation, biosynthesis and energy charge."
}, {
    "id": "oai:dspace.mit.edu:1721.1/103500",
    "title": "A single-cell perspective on infection",
    "abstract": "The clinical course of infection is ultimately determined by a series of cellular interactions between invading pathogens and host immune cells. It has long been understood that these interactions, even when they occur in tissue culture models, give rise to a wide variety of different outcomes, some beneficial to the host, others to the pathogen. These cellular interactions, however, are typically studied at a bulk level; masking this cell-to-cell variation, losing important information about the full range of possible host-pathogen interactions, and leaving the mechanistic basis for these different outcomes largely unexplored. Here, we present a system that combines single-cell RNA sequencing with fluorescent markers of infection outcome to directly correlate host transcription signatures with infection outcome at the single cell level. Applying this system to the well-characterized model of Salmonella enterica infection of mouse macrophages, we found: 1) Unique transcription signatures associated with bacterial exposure and bacterial infection, 2) Sustained high levels of heterogeneity in immune pathways in infected macrophages, and 3) A novel subpopulation of macrophages characterized by high expression of the Type I Interferon response after infection. Upon further investigation we found that this heterogeneity in the host Type I Interferon response was the result of heterogeneity in the population of infecting bacteria, namely in the extent of PhoPQ-mediated LPS modifications. This work highlights the importance of heterogeneity as a characteristic of bacterial populations that can influence the host immune response. It also demonstrates the benefits of examining infection with single-cell resolution.",
    "advisors": ["Deborah T. Hung"],
    "text": "A single-cell perspective on infection The clinical course of infection is ultimately determined by a series of cellular interactions between invading pathogens and host immune cells. It has long been understood that these interactions, even when they occur in tissue culture models, give rise to a wide variety of different outcomes, some beneficial to the host, others to the pathogen. These cellular interactions, however, are typically studied at a bulk level; masking this cell-to-cell variation, losing important information about the full range of possible host-pathogen interactions, and leaving the mechanistic basis for these different outcomes largely unexplored. Here, we present a system that combines single-cell RNA sequencing with fluorescent markers of infection outcome to directly correlate host transcription signatures with infection outcome at the single cell level. Applying this system to the well-characterized model of Salmonella enterica infection of mouse macrophages, we found: 1) Unique transcription signatures associated with bacterial exposure and bacterial infection, 2) Sustained high levels of heterogeneity in immune pathways in infected macrophages, and 3) A novel subpopulation of macrophages characterized by high expression of the Type I Interferon response after infection. Upon further investigation we found that this heterogeneity in the host Type I Interferon response was the result of heterogeneity in the population of infecting bacteria, namely in the extent of PhoPQ-mediated LPS modifications. This work highlights the importance of heterogeneity as a characteristic of bacterial populations that can influence the host immune response. It also demonstrates the benefits of examining infection with single-cell resolution."
}, {
    "id": "oai:dspace.mit.edu:1721.1/87504",
    "title": "Somatic retrotransposition in the cancer genome",
    "abstract": "Cancer is a complex disease of the genome exhibiting myriad somatic mutations, from single nucleotide changes to various chromosomal rearrangements. The technological advances of next-generation sequencing enable high-throughput identification and characterization of these events genome-wide using computational algorithms. Retrotransposons comprise 42% of the human genome and have the capacity to \"jump\" across the genome in a copy-and-paste manner. Recent studies have identified families of retrotransposable elements that are currently active. In fact, retrotransposons constitute a major source of human genetic variation, and somatic retrotransposon insertions have been implicated in several cancers, including an insertion into the APC tumor suppressor in a colorectal tumor. Because of the highly repetitive nature of these elements, however, the full extent of somatic retrotransposon movement across cancer remains largely unexplored. To this end, we developed TranspoSeq, a computational framework that identifies retrotransposon insertions from paired-end whole genome sequencing data, and TranspoSeq- Exome, a tool that localizes these insertions from whole-exome data. TranspoSeq identifies novel somatic retrotransposon insertions with high sensitivity and specificity in simulated data and with a 94% validation rate via site-specific PCR. Next, we applied these methods to wholegenomes from 200 tumor/normal pairs and whole-exomes from 767 tumor/normal pairs across 11 tumor types as part of The Cancer Genome Atlas (TCGA) Pan-Cancer Project. We discover more than 800 somatic retrotransposon insertions primarily in lung squamous, head and neck, colorectal and endometrial carcinomas, while glioblastoma multiforme and acute myeloid leukemia show no evidence of somatic retrotransposition. Moreover, many somatic retrotransposon insertions occur in known cancer genes. TranspoSeq-Exome uncovers 35 additional somatic retrotransposon insertions into exonic regions, including an insertion into an exon of the PTEN tumor suppressor in endometrial cancer. Finally, we integrate orthogonal genomic and clinical data to characterize features of retrotransposon insertion and samples that exhibit extensive somatic retrotransposition. We present a large-scale, comprehensive analysis of retrotransposon movement across tumor types using next-generation sequencing data. Our results suggest that somatic retrotransposon insertions may represent an important class of tumor-specific structural variation in cancer and future studies should incorporate this form of somatic genome aberration.",
    "advisors": ["Matthew Meyerson"],
    "text": "Somatic retrotransposition in the cancer genome Cancer is a complex disease of the genome exhibiting myriad somatic mutations, from single nucleotide changes to various chromosomal rearrangements. The technological advances of next-generation sequencing enable high-throughput identification and characterization of these events genome-wide using computational algorithms. Retrotransposons comprise 42% of the human genome and have the capacity to \"jump\" across the genome in a copy-and-paste manner. Recent studies have identified families of retrotransposable elements that are currently active. In fact, retrotransposons constitute a major source of human genetic variation, and somatic retrotransposon insertions have been implicated in several cancers, including an insertion into the APC tumor suppressor in a colorectal tumor. Because of the highly repetitive nature of these elements, however, the full extent of somatic retrotransposon movement across cancer remains largely unexplored. To this end, we developed TranspoSeq, a computational framework that identifies retrotransposon insertions from paired-end whole genome sequencing data, and TranspoSeq- Exome, a tool that localizes these insertions from whole-exome data. TranspoSeq identifies novel somatic retrotransposon insertions with high sensitivity and specificity in simulated data and with a 94% validation rate via site-specific PCR. Next, we applied these methods to wholegenomes from 200 tumor/normal pairs and whole-exomes from 767 tumor/normal pairs across 11 tumor types as part of The Cancer Genome Atlas (TCGA) Pan-Cancer Project. We discover more than 800 somatic retrotransposon insertions primarily in lung squamous, head and neck, colorectal and endometrial carcinomas, while glioblastoma multiforme and acute myeloid leukemia show no evidence of somatic retrotransposition. Moreover, many somatic retrotransposon insertions occur in known cancer genes. TranspoSeq-Exome uncovers 35 additional somatic retrotransposon insertions into exonic regions, including an insertion into an exon of the PTEN tumor suppressor in endometrial cancer. Finally, we integrate orthogonal genomic and clinical data to characterize features of retrotransposon insertion and samples that exhibit extensive somatic retrotransposition. We present a large-scale, comprehensive analysis of retrotransposon movement across tumor types using next-generation sequencing data. Our results suggest that somatic retrotransposon insertions may represent an important class of tumor-specific structural variation in cancer and future studies should incorporate this form of somatic genome aberration."
}, {
    "id": "oai:dspace.mit.edu:1721.1/8084",
    "title": "Functional analysis of middle temporal visual area and its associated behavior",
    "abstract": "Our lab's long-term goal is to elucidate the circuitry of the visual cortex, to develop quantitative computational models of neuronal function in the visual cortex, and to establish how these models may relate to visual perception and visually guided behavior. Central to this goal is the analysis of functional architecture, which is crucial to an understanding of how the brain works. In my thesis research, I applied behavioral and microstimulation techniques to demonstrate the causal connections between neural activity and behavior. Understanding these relationships is one of the fundamental issues needed to be addressed in Neurobiology. Specifically, I focused on the functional analysis of the middle temporal visual area (MT) and the behavior associated with it. MT is an extrastriate area that is primarily involved in visual motion processing. A very important function within MT is a segregation of center-surround interactions which plays a critical role in processing visual motion cues. There are two types of motion center-surround interactions in MT neurons: surrounds may reinforce (at wide-field sites) or suppress (at local-motion sites) the centers' directional responses. They are important in representing the initial stages of a functional segregation between wide-field and local-contrast motion processing. To further study the computational model used by the brain to readout sensory information, I conducted microstimulation experiments in MT by changing stimulation amplitudes (from 10/LA to 160tA) and frequencies (from 25Hz to 500Hz). Microstimulation can introduce an additional velocity signal into MT and the pursuit and saccadic systems usually compute a vector average of the visually evoked and microstimulation-induced velocity signals.",
    "advisors": ["Richard T. Born"],
    "text": "Functional analysis of middle temporal visual area and its associated behavior Our lab's long-term goal is to elucidate the circuitry of the visual cortex, to develop quantitative computational models of neuronal function in the visual cortex, and to establish how these models may relate to visual perception and visually guided behavior. Central to this goal is the analysis of functional architecture, which is crucial to an understanding of how the brain works. In my thesis research, I applied behavioral and microstimulation techniques to demonstrate the causal connections between neural activity and behavior. Understanding these relationships is one of the fundamental issues needed to be addressed in Neurobiology. Specifically, I focused on the functional analysis of the middle temporal visual area (MT) and the behavior associated with it. MT is an extrastriate area that is primarily involved in visual motion processing. A very important function within MT is a segregation of center-surround interactions which plays a critical role in processing visual motion cues. There are two types of motion center-surround interactions in MT neurons: surrounds may reinforce (at wide-field sites) or suppress (at local-motion sites) the centers' directional responses. They are important in representing the initial stages of a functional segregation between wide-field and local-contrast motion processing. To further study the computational model used by the brain to readout sensory information, I conducted microstimulation experiments in MT by changing stimulation amplitudes (from 10/LA to 160tA) and frequencies (from 25Hz to 500Hz). Microstimulation can introduce an additional velocity signal into MT and the pursuit and saccadic systems usually compute a vector average of the visually evoked and microstimulation-induced velocity signals."
}, {
    "id": "oai:dspace.mit.edu:1721.1/68452",
    "title": "Analysis of alterations in the human cancer genome",
    "abstract": "Aneuploidy, an abnormal complement of chromosomes, is present in approximately 90% of human malignancies. Despite over 100 years of research, many questions remain regarding the contribution of aneuploidy to the cancer phenotype. In this thesis, we develop computational methods to infer the presence and specific patterns of aneuploidy across thousands of primary cancer tissue specimens. We then combine these inferences with clinical and genomic features of the cancer samples to refine our understanding of both the clinical implications of aneuploidy, and how it evolves in various human cancers. We identified a signature of chromosomal instability from specific genes whose expression was consistently correlated with aneuploidy in several cancer types, and which was predictive of poor clinical outcome multiple cancer types. Current genomic characterization techniques measure somatic alterations in a cancer sample in units of genomes (DNA mass). The meaning of such measurements is highly dependent on the tumors purity and its overall ploidy; they are hence complicated to interpret and compare across samples. Ideally, copy-number should be measured in copies-per-cancer-cell. Such measurements are straightforward to interpret and, for alterations that are fixed in the cancer cell population, are simple integer values. We develop two computational methods to infer tumor purity and malignant cell ploidy directly from allelic analysis of DNA. First we describe HAPSEG, a probabilistic method to interpret bi-allelic marker data in cancer samples in order to produce genome-wide estimates of homologue specific copy-ratios. Second, we describe ABSOLUTE, a method that infers purity, ploidy, and absolute copy-numbers from the estimates produced by HAPSEG. In addition, ABSOLUTE can analyze point mutations to detect subclonal heterogeneity and somatic homozygosity. We used ABSOLUTE to analyze ovarian cancer data and discovered that 54% of somatic point mutations were, in fact, subclonal. In contrast, mutations occurring in key tumor suppressor genes, TP53 and NF1 were predominantly clonal and homozygous. Analysis of absolute allelic copy-number profiles from 3,155 cancer specimens revealed that genome-doubling events are common in human cancer, and likely occur in already aneuploid cells in many cancer types. By correlating genome-doubling status with mutation data, we found that homozygous mutations in NF1 occurred predominantly in non-doubled samples. This finding suggests that genome doubling influences the pathways of tumor progression, with recessive inactivation being less common after genome doubling.",
    "advisors": ["Gad Getz", "Matthew Meyerson"],
    "text": "Analysis of alterations in the human cancer genome Aneuploidy, an abnormal complement of chromosomes, is present in approximately 90% of human malignancies. Despite over 100 years of research, many questions remain regarding the contribution of aneuploidy to the cancer phenotype. In this thesis, we develop computational methods to infer the presence and specific patterns of aneuploidy across thousands of primary cancer tissue specimens. We then combine these inferences with clinical and genomic features of the cancer samples to refine our understanding of both the clinical implications of aneuploidy, and how it evolves in various human cancers. We identified a signature of chromosomal instability from specific genes whose expression was consistently correlated with aneuploidy in several cancer types, and which was predictive of poor clinical outcome multiple cancer types. Current genomic characterization techniques measure somatic alterations in a cancer sample in units of genomes (DNA mass). The meaning of such measurements is highly dependent on the tumors purity and its overall ploidy; they are hence complicated to interpret and compare across samples. Ideally, copy-number should be measured in copies-per-cancer-cell. Such measurements are straightforward to interpret and, for alterations that are fixed in the cancer cell population, are simple integer values. We develop two computational methods to infer tumor purity and malignant cell ploidy directly from allelic analysis of DNA. First we describe HAPSEG, a probabilistic method to interpret bi-allelic marker data in cancer samples in order to produce genome-wide estimates of homologue specific copy-ratios. Second, we describe ABSOLUTE, a method that infers purity, ploidy, and absolute copy-numbers from the estimates produced by HAPSEG. In addition, ABSOLUTE can analyze point mutations to detect subclonal heterogeneity and somatic homozygosity. We used ABSOLUTE to analyze ovarian cancer data and discovered that 54% of somatic point mutations were, in fact, subclonal. In contrast, mutations occurring in key tumor suppressor genes, TP53 and NF1 were predominantly clonal and homozygous. Analysis of absolute allelic copy-number profiles from 3,155 cancer specimens revealed that genome-doubling events are common in human cancer, and likely occur in already aneuploid cells in many cancer types. By correlating genome-doubling status with mutation data, we found that homozygous mutations in NF1 occurred predominantly in non-doubled samples. This finding suggests that genome doubling influences the pathways of tumor progression, with recessive inactivation being less common after genome doubling."
}, {
    "id": "oai:dspace.mit.edu:1721.1/40871",
    "title": "Drug deposition and distribution in healthy and atherosclerotic arteries and in models of atherosclerosis following bulk or stent-based drug delivery",
    "abstract": "Drug eluting stents have revolutionized the practice of medicine and the landscape of medical devices. Yet, more than four years after introduction clinical trial data and clinical use have still not fully clarified what drives the safety and efficacy of these devices. The goal of this thesis was to help fill this void by describing the mechanisms by which stent-eluted drugs are distributed within healthy and atherosclerotic vascular models. In the first part of the thesis we investigated the effect of drug physicochemical properties on drug deposition, retention, and distribution in a healthy vascular model. We found that hydrophobic drugs are deposited to a far greater degree than hydrophilic drugs, with longer retention times, and distribution patterns that likely track with specific and general binding sites. The second part of the thesis investigated how arterial ultrastructure in health and disease modulates the arterial deposition and distribution of hydrophobic antiproliferative drugs used with drug-eluting stents. We tracked the distribution of radiolabeled and FITC-labeled compounds and demonstrated that macrostructural changes in arterial architecture led to profound changes in drug deposition. Paclitaxel in particular was sensitive to tissue state.",
    "advisors": ["Elazer R. Edelman"],
    "text": "Drug deposition and distribution in healthy and atherosclerotic arteries and in models of atherosclerosis following bulk or stent-based drug delivery Drug eluting stents have revolutionized the practice of medicine and the landscape of medical devices. Yet, more than four years after introduction clinical trial data and clinical use have still not fully clarified what drives the safety and efficacy of these devices. The goal of this thesis was to help fill this void by describing the mechanisms by which stent-eluted drugs are distributed within healthy and atherosclerotic vascular models. In the first part of the thesis we investigated the effect of drug physicochemical properties on drug deposition, retention, and distribution in a healthy vascular model. We found that hydrophobic drugs are deposited to a far greater degree than hydrophilic drugs, with longer retention times, and distribution patterns that likely track with specific and general binding sites. The second part of the thesis investigated how arterial ultrastructure in health and disease modulates the arterial deposition and distribution of hydrophobic antiproliferative drugs used with drug-eluting stents. We tracked the distribution of radiolabeled and FITC-labeled compounds and demonstrated that macrostructural changes in arterial architecture led to profound changes in drug deposition. Paclitaxel in particular was sensitive to tissue state."
}, {
    "id": "oai:dspace.mit.edu:1721.1/18065",
    "title": "Characteristics of syntactic processing : an examination utilizing behavioral and fMRI techniques",
    "abstract": "This thesis explores two important factors that constrain the syntactic parser of the sentence processing mechanism, syntactic storage costs and plausibility information. It uses behavioral methods to explore the characteristics of the two factors and neuroimaging to explore the underlying neurological substrates associated with these aspects of syntactic processing. Experiment 1 behaviorally demonstrated the presence of syntactic storage costs for predictions of verbs, filler-gaps, and subcategorized prepositional phrases. It is argued that the data support the Dependency Locality Theory (Gibson, 2000) supposition of stored predicted heads as well as a theory of syntax that includes empty categories. Experiment 2 demonstrated brain regions associated with storage and integration cost demands in the contrast of subject-object (SO) and object-subject (OS) sentence structures. The results indicate that the inferior parietal cortex is part of a larger network of cortex, including inferior frontal perisylvian areas, that is involved in the processing of SO vs. OS sentences. However, the involvement is not identical to that of the inferior frontal areas and has a distinct hemodynamic character. Experiment 3 explored regions of the brain involved in the resolution of the main verb/reduced relative (MV/RR) ambiguity. Activation was seen in portions of the angular gyrus and the middle temporal gyrus for a contrast in subject noun plausibility, but not structure ambiguity, indicating that the MV interpretation was still considered even in unambiguously relative clause sentence structures. The unexpected results could imply that syntax is not the only factor that determines [theta]-role assignment and ultimately provide evidence about the brain regions involved in",
    "advisors": ["David Caplan"],
    "text": "Characteristics of syntactic processing : an examination utilizing behavioral and fMRI techniques This thesis explores two important factors that constrain the syntactic parser of the sentence processing mechanism, syntactic storage costs and plausibility information. It uses behavioral methods to explore the characteristics of the two factors and neuroimaging to explore the underlying neurological substrates associated with these aspects of syntactic processing. Experiment 1 behaviorally demonstrated the presence of syntactic storage costs for predictions of verbs, filler-gaps, and subcategorized prepositional phrases. It is argued that the data support the Dependency Locality Theory (Gibson, 2000) supposition of stored predicted heads as well as a theory of syntax that includes empty categories. Experiment 2 demonstrated brain regions associated with storage and integration cost demands in the contrast of subject-object (SO) and object-subject (OS) sentence structures. The results indicate that the inferior parietal cortex is part of a larger network of cortex, including inferior frontal perisylvian areas, that is involved in the processing of SO vs. OS sentences. However, the involvement is not identical to that of the inferior frontal areas and has a distinct hemodynamic character. Experiment 3 explored regions of the brain involved in the resolution of the main verb/reduced relative (MV/RR) ambiguity. Activation was seen in portions of the angular gyrus and the middle temporal gyrus for a contrast in subject noun plausibility, but not structure ambiguity, indicating that the MV interpretation was still considered even in unambiguously relative clause sentence structures. The unexpected results could imply that syntax is not the only factor that determines [theta]-role assignment and ultimately provide evidence about the brain regions involved in"
}, {
    "id": "oai:dspace.mit.edu:1721.1/103503",
    "title": "Distinct contribution of white matter damage to the clinical syndrome of Alzheimer's disease",
    "abstract": "Alzheimer's disease is a neurodegenerative disorder affecting over 5.1 million individuals in the United States today. The dementia exhibited with the disease is currently thought to be primarily due to amyloid plaques and neurofibrillary tangles. However, several other changes occur, including severe white matter damage that is yet to be fully understood. Such white matter damage includes white matter lesions (WML), which are more common in individuals with Alzheimer's disease than in non-demented individuals. WML are of presumed vascular origin because they show features of small-vessel disease and are more prevalent in individuals with vascular risk. It is currently unclear whether WML are linked to the neurodegenerative pathologies of Alzheimer's disease or are an independent factor that influences clinical course. In this work, we used sensitive diffusion MRI measures to determine that the tissue properties of WML slightly differed microstructurally between individuals with Alzheimer's disease and non-demented controls, and were strongly related to ventricular enlargement. In order to further understand the role of WML, we factored the volume of WML with four other neuroimaging markers affected in Alzheimer's disease and discovered two statistically distinct factors presumed to be due to differing underlying disease processes. One process strongly related to volume and tissue properties of WML, ventricular enlargement, age and cerebral perfusion, while the other process related to imaging markers associated with neurodegeneration. A decrease over time in the first process, interpreted as the age- and vascular-related factor, led to similar cognitive decline as the neurodegenerative factor independently, demonstrating the potential added therapeutic benefit of targeting this disease process that is distinct from the classical neurodegenerative component of the disease.",
    "advisors": ["David H. Salat"],
    "text": "Distinct contribution of white matter damage to the clinical syndrome of Alzheimer's disease Alzheimer's disease is a neurodegenerative disorder affecting over 5.1 million individuals in the United States today. The dementia exhibited with the disease is currently thought to be primarily due to amyloid plaques and neurofibrillary tangles. However, several other changes occur, including severe white matter damage that is yet to be fully understood. Such white matter damage includes white matter lesions (WML), which are more common in individuals with Alzheimer's disease than in non-demented individuals. WML are of presumed vascular origin because they show features of small-vessel disease and are more prevalent in individuals with vascular risk. It is currently unclear whether WML are linked to the neurodegenerative pathologies of Alzheimer's disease or are an independent factor that influences clinical course. In this work, we used sensitive diffusion MRI measures to determine that the tissue properties of WML slightly differed microstructurally between individuals with Alzheimer's disease and non-demented controls, and were strongly related to ventricular enlargement. In order to further understand the role of WML, we factored the volume of WML with four other neuroimaging markers affected in Alzheimer's disease and discovered two statistically distinct factors presumed to be due to differing underlying disease processes. One process strongly related to volume and tissue properties of WML, ventricular enlargement, age and cerebral perfusion, while the other process related to imaging markers associated with neurodegeneration. A decrease over time in the first process, interpreted as the age- and vascular-related factor, led to similar cognitive decline as the neurodegenerative factor independently, demonstrating the potential added therapeutic benefit of targeting this disease process that is distinct from the classical neurodegenerative component of the disease."
}, {
    "id": "oai:dspace.mit.edu:1721.1/79247",
    "title": "Microfluidic enabling technologies for measurement of the selective permeability of the mucus barrier",
    "abstract": "Mucus is a biological hydrogel which lines the wet (non-keratinized) epithelia of the body. Mucus provides a gateway between the cells of the epithelium and the outside world, and is postulated to provide a selective filtering function which is critical to physiological functioning and has been implicated in diseases. Currently, much of the mechanisms and criteria of this selective filtering function is not well understood. In this thesis, we contribute novel microfluidic devices to characterize the selective permeability properties of the mucus barrier. Microfluidics provides the engineering ability to create channels with precise geometries, fluid flow capability, and allow chemical concentration gradients. Our devices mimic the physiological environment of the mucosa and enable improved measurements of the mucus layer selective permeability. The first microfluidic device mimics the acid barrier function of the stomach mucus layer. This device reproduces on-chip the secretion of mucus by the gastric mucosa into an acidic stomach lumen. We use this device to demonstrate that the secretion of mucins, the glycoprotein structural component of mucus, contributes significantly to the acid barrier function by continuously binding H'. The second microfluidic device probes the permeability of the mucus barrier to nanoscale peptides, as a model for drug molecules and in vivo signaling molecules. The device enabled the creation of a mucus layer next to a flowing aqueous layer, mimicking the in vivo mucus layer and lumen of the gastrointestinal, respiratory, and female reproductive tracts. Peptides added to the aqueous flow diffused across the mucus barrier interface into the mucus layer. This device demonstrated that the mucus barrier provides selective permeability to nanoscale peptides based on electrostatic interactions, and suggest novel surface functionalization strategies for drug carriers to improve mucosal drug delivery. Taken together, this thesis provides new microfluidic tools to probe the selective permeability function of the mucus barrier. Using the microfluidic tools, we show new mechanistic understanding of this barrier.",
    "advisors": ["Jongyoon Han"],
    "text": "Microfluidic enabling technologies for measurement of the selective permeability of the mucus barrier Mucus is a biological hydrogel which lines the wet (non-keratinized) epithelia of the body. Mucus provides a gateway between the cells of the epithelium and the outside world, and is postulated to provide a selective filtering function which is critical to physiological functioning and has been implicated in diseases. Currently, much of the mechanisms and criteria of this selective filtering function is not well understood. In this thesis, we contribute novel microfluidic devices to characterize the selective permeability properties of the mucus barrier. Microfluidics provides the engineering ability to create channels with precise geometries, fluid flow capability, and allow chemical concentration gradients. Our devices mimic the physiological environment of the mucosa and enable improved measurements of the mucus layer selective permeability. The first microfluidic device mimics the acid barrier function of the stomach mucus layer. This device reproduces on-chip the secretion of mucus by the gastric mucosa into an acidic stomach lumen. We use this device to demonstrate that the secretion of mucins, the glycoprotein structural component of mucus, contributes significantly to the acid barrier function by continuously binding H'. The second microfluidic device probes the permeability of the mucus barrier to nanoscale peptides, as a model for drug molecules and in vivo signaling molecules. The device enabled the creation of a mucus layer next to a flowing aqueous layer, mimicking the in vivo mucus layer and lumen of the gastrointestinal, respiratory, and female reproductive tracts. Peptides added to the aqueous flow diffused across the mucus barrier interface into the mucus layer. This device demonstrated that the mucus barrier provides selective permeability to nanoscale peptides based on electrostatic interactions, and suggest novel surface functionalization strategies for drug carriers to improve mucosal drug delivery. Taken together, this thesis provides new microfluidic tools to probe the selective permeability function of the mucus barrier. Using the microfluidic tools, we show new mechanistic understanding of this barrier."
}, {
    "id": "oai:dspace.mit.edu:1721.1/8351",
    "title": "Estimating glottal voicing source characteristics by measuring and modeling the acceleration of the skin on the neck",
    "abstract": "In the clinical management of voice patients, quantifying vocal function is becoming increasingly important both for corroborating clinicians' subjective impressions during a voice evaluation and for assessing the effectiveness of surgery or voice therapy. Current devices for quantifying vocal function measure acoustic, aerodynamic or electric signals (i.e., sound pressure, airflow, or electroglottography) during short tasks such as reading. One technique that has shown potential for measuring vocal function but has been mostly used to quantify speech-related behaviors besides phonation is measuring the acceleration of the skin near the larynx. The acceleration of the skin on the neck between the cricoid cartilage and the sternal notch arises from the airflow pulses that result from vocal-fold vibration. At least two sets of structures play a role in transforming these airflow pulses into the measured acceleration: the subglottal system, and the tissues between the subglottal airspace and the accelerometer (e.g., tracheal cartilage, skin, etc.). Advantages of measuring acceleration over current techniques include 1) the structures that filter the glottal pulses vary less over time than the vocal tract and thus they may be adequately modeled as time-invariant, making signal processing potentially easier; 2) environmental acoustic noise has a minimal influence on the measured acceleration; and 3) the accelerometer's size and placement make it more comfortable and unobtrusive for extended recordings than current techniques. This thesis work investigated the potential of using the measured acceleration for quantifying vocal function.",
    "advisors": ["Robert E. Hillman", "Kenneth N. Stevens"],
    "text": "Estimating glottal voicing source characteristics by measuring and modeling the acceleration of the skin on the neck In the clinical management of voice patients, quantifying vocal function is becoming increasingly important both for corroborating clinicians' subjective impressions during a voice evaluation and for assessing the effectiveness of surgery or voice therapy. Current devices for quantifying vocal function measure acoustic, aerodynamic or electric signals (i.e., sound pressure, airflow, or electroglottography) during short tasks such as reading. One technique that has shown potential for measuring vocal function but has been mostly used to quantify speech-related behaviors besides phonation is measuring the acceleration of the skin near the larynx. The acceleration of the skin on the neck between the cricoid cartilage and the sternal notch arises from the airflow pulses that result from vocal-fold vibration. At least two sets of structures play a role in transforming these airflow pulses into the measured acceleration: the subglottal system, and the tissues between the subglottal airspace and the accelerometer (e.g., tracheal cartilage, skin, etc.). Advantages of measuring acceleration over current techniques include 1) the structures that filter the glottal pulses vary less over time than the vocal tract and thus they may be adequately modeled as time-invariant, making signal processing potentially easier; 2) environmental acoustic noise has a minimal influence on the measured acceleration; and 3) the accelerometer's size and placement make it more comfortable and unobtrusive for extended recordings than current techniques. This thesis work investigated the potential of using the measured acceleration for quantifying vocal function."
}, {
    "id": "oai:dspace.mit.edu:1721.1/83968",
    "title": "Micro and nanotechnology for cancer treatment",
    "abstract": "Cancer is responsible for over 7.6 million deaths worldwide; the majority of patients fail to respond to drugs or become resistant over time. In order to gain a better understanding of drug efficacy in patients, we developed three diagnostic technologies to address limitations in sample acquisition and improve the scale and sensitivity of current cancer diagnostic tools. In the first section, we describe a hybrid magnetic and size sorting microfluidic device that isolates rare circulating tumor cells from peripheral blood. The self-assembled magnetic sorter creates strong magnetic fields and effectively removes leukocytes tagged with magnetic nanoparticles. The size sorting region retains the remaining cells in single cell capture sites, while allowing small red blood cells to pass through 5pm gaps. The device achieves over 103 enrichment, up to 96% recovery of cancer cells and allows for on-chip molecular profiling. In the second section we use a magnetic nanoparticle decorated with small molecule drugs to assay target expression and drug binding in mock clinical samples of cancer cells spiked into whole blood. Specifically, we modify a PARP inhibitor (Olabarib) and conjugate it to a dextran coated iron oxide nanoparticle. We measure the presence of the drug nanosensor based on the change in T2 relaxation time using a miniaturized, handheld NMR sensor for point-of-care diagnosis. In the final section, we detail a photocleavable DNA barcoding method for understanding treatment response via multiplexed profiling of cancer cells. We validate our method with a 94 marker panel on different cell lines with varying treatments, showing high correlations to gold standard methods such as immunofluorescence and flow cytometry. Furthermore, we demonstrate single cell sensitivity, and identify a number of expected biomarkers in response to cell treatments. Finally, we demonstrate the potential of our method to help in clinical monitoring of patients by examining intra- and inter-patient heterogeneity, and by correlating pre and post-treatment tumor profiles to patient response. Together, we show how these technologies can help overcome clinical limitations and expedite advancements in cancer treatment.",
    "advisors": ["Ralph Weissleder"],
    "text": "Micro and nanotechnology for cancer treatment Cancer is responsible for over 7.6 million deaths worldwide; the majority of patients fail to respond to drugs or become resistant over time. In order to gain a better understanding of drug efficacy in patients, we developed three diagnostic technologies to address limitations in sample acquisition and improve the scale and sensitivity of current cancer diagnostic tools. In the first section, we describe a hybrid magnetic and size sorting microfluidic device that isolates rare circulating tumor cells from peripheral blood. The self-assembled magnetic sorter creates strong magnetic fields and effectively removes leukocytes tagged with magnetic nanoparticles. The size sorting region retains the remaining cells in single cell capture sites, while allowing small red blood cells to pass through 5pm gaps. The device achieves over 103 enrichment, up to 96% recovery of cancer cells and allows for on-chip molecular profiling. In the second section we use a magnetic nanoparticle decorated with small molecule drugs to assay target expression and drug binding in mock clinical samples of cancer cells spiked into whole blood. Specifically, we modify a PARP inhibitor (Olabarib) and conjugate it to a dextran coated iron oxide nanoparticle. We measure the presence of the drug nanosensor based on the change in T2 relaxation time using a miniaturized, handheld NMR sensor for point-of-care diagnosis. In the final section, we detail a photocleavable DNA barcoding method for understanding treatment response via multiplexed profiling of cancer cells. We validate our method with a 94 marker panel on different cell lines with varying treatments, showing high correlations to gold standard methods such as immunofluorescence and flow cytometry. Furthermore, we demonstrate single cell sensitivity, and identify a number of expected biomarkers in response to cell treatments. Finally, we demonstrate the potential of our method to help in clinical monitoring of patients by examining intra- and inter-patient heterogeneity, and by correlating pre and post-treatment tumor profiles to patient response. Together, we show how these technologies can help overcome clinical limitations and expedite advancements in cancer treatment."
}, {
    "id": "oai:dspace.mit.edu:1721.1/87505",
    "title": "Paxillin-dependent control of tumor angiogenesis",
    "abstract": "Angiogenesis- the growth of new capillaries from existing vessels- is required for tumor growth; however, tumor vessels exhibit abnormal structure and function, which impairs the targeted delivery of anti-cancer agents. While directional migration of capillary endothelial cells is critical for normal angiogenesis, the mechanism by which oriented capillary cell migration is controlled or how it is deregulated during tumorigenesis is unknown. Recently our lab reported that the focal adhesion protein, paxillin, is required for directional migration of fibroblasts. Endothelial cells also express paxillin and localize it in their focal adhesions. Thus, I set out to analyze whether paxillin influences directional migration of endothelial cells. When the expression of paxillin is knocked down in endothelial cells, this enhances their migration but decreases their directional persistence in vitro and in vivo in migration, angiogenesis and developmental assays. Having confirmed that paxillin plays a central role in controlling oriented capillary cell migration, I then studied the mechanism by which it contributes to normal microvessel network formation and tumor angiogenesis. I found that paxillin knockdown increases microvessel density but causes loss of sprout orientation. These characteristics resemble those of tumor vasculature, and, in fact, studies revealed that tumors inhibit paxillin expression in endothelial cells in vitro and in vivo by secreting soluble factors, such as the potent angiogenic factor VEGF. Mechanistically, paxillin knockdown decreases expression of the VEGF receptor neuropilin 2 (NRP2) but not VEGF receptor 2, and this is mediated by the transcription factor GATA2. Direct knockdown of NRP2 also increases endothelial cell migration and vessel density in vitro and in vivo and these effects are rescued by over expressing paxillin. In summary, these studies have led to the discovery of a new mechanism for control of directional endothelial cell migration during angiogenesis that is mediated by paxillin-NRP2 signaling. Importantly, this previously unknown mechanism is deregulated in tumor angiogenesis, which may contribute to the enhanced, disorganized microvasculature that is hallmark of cancer. These findings also revealed a new function for the focal adhesion protein, paxillin, as a mediator of tumor angiogenesis, and elucidated a novel mechanism for control of the expression of NRP2.",
    "advisors": ["Donald E. Ingber"],
    "text": "Paxillin-dependent control of tumor angiogenesis Angiogenesis- the growth of new capillaries from existing vessels- is required for tumor growth; however, tumor vessels exhibit abnormal structure and function, which impairs the targeted delivery of anti-cancer agents. While directional migration of capillary endothelial cells is critical for normal angiogenesis, the mechanism by which oriented capillary cell migration is controlled or how it is deregulated during tumorigenesis is unknown. Recently our lab reported that the focal adhesion protein, paxillin, is required for directional migration of fibroblasts. Endothelial cells also express paxillin and localize it in their focal adhesions. Thus, I set out to analyze whether paxillin influences directional migration of endothelial cells. When the expression of paxillin is knocked down in endothelial cells, this enhances their migration but decreases their directional persistence in vitro and in vivo in migration, angiogenesis and developmental assays. Having confirmed that paxillin plays a central role in controlling oriented capillary cell migration, I then studied the mechanism by which it contributes to normal microvessel network formation and tumor angiogenesis. I found that paxillin knockdown increases microvessel density but causes loss of sprout orientation. These characteristics resemble those of tumor vasculature, and, in fact, studies revealed that tumors inhibit paxillin expression in endothelial cells in vitro and in vivo by secreting soluble factors, such as the potent angiogenic factor VEGF. Mechanistically, paxillin knockdown decreases expression of the VEGF receptor neuropilin 2 (NRP2) but not VEGF receptor 2, and this is mediated by the transcription factor GATA2. Direct knockdown of NRP2 also increases endothelial cell migration and vessel density in vitro and in vivo and these effects are rescued by over expressing paxillin. In summary, these studies have led to the discovery of a new mechanism for control of directional endothelial cell migration during angiogenesis that is mediated by paxillin-NRP2 signaling. Importantly, this previously unknown mechanism is deregulated in tumor angiogenesis, which may contribute to the enhanced, disorganized microvasculature that is hallmark of cancer. These findings also revealed a new function for the focal adhesion protein, paxillin, as a mediator of tumor angiogenesis, and elucidated a novel mechanism for control of the expression of NRP2."
}, {
    "id": "oai:dspace.mit.edu:1721.1/79246",
    "title": "High-throughput methods for characterizing the immune repertoire",
    "abstract": "The adaptive immune system is one of the primary mediators in almost every major human disease, including infections, cancer, autoimmunity, and inflammation-based disorders. It fundamentally functions as a molecular classifier, and stores a memory of its previous exposures. However, until recently, methods to unlock this information or to exploit its power in the form of new therapeutic antibodies or affinity reagents have been limited by the use of traditional, low-throughput technologies. In this thesis, we leverage recent advances in high-throughput DNA sequencing technology to develop new methods to characterize and probe the immune repertoire in unprecedented detail. We use this technology to 1) characterize the rapid dynamics of the immune repertoire in response to influenza vaccination, 2) characterize elite neutralizing antibodies to HIV, to better understand the constraints for designing an HIV vaccine, and 3) develop new methodologies for discovering auto-antigens, and assaying large libraries of protein antigens in general. We hope that these projects will serve as stepping-stones towards filling the gap left by low-throughput methods in the development of antibody technologies.",
    "advisors": ["George M Church"],
    "text": "High-throughput methods for characterizing the immune repertoire The adaptive immune system is one of the primary mediators in almost every major human disease, including infections, cancer, autoimmunity, and inflammation-based disorders. It fundamentally functions as a molecular classifier, and stores a memory of its previous exposures. However, until recently, methods to unlock this information or to exploit its power in the form of new therapeutic antibodies or affinity reagents have been limited by the use of traditional, low-throughput technologies. In this thesis, we leverage recent advances in high-throughput DNA sequencing technology to develop new methods to characterize and probe the immune repertoire in unprecedented detail. We use this technology to 1) characterize the rapid dynamics of the immune repertoire in response to influenza vaccination, 2) characterize elite neutralizing antibodies to HIV, to better understand the constraints for designing an HIV vaccine, and 3) develop new methodologies for discovering auto-antigens, and assaying large libraries of protein antigens in general. We hope that these projects will serve as stepping-stones towards filling the gap left by low-throughput methods in the development of antibody technologies."
}, {
    "id": "oai:dspace.mit.edu:1721.1/70811",
    "title": "Understanding barriers to efficient nucleic acid delivery with bioresponsive block copolymers",
    "abstract": "The delivery of nucleic acids has the potential to revolutionize medicine by allowing previously untreatable diseases to be clinically addressed. Viral delivery systems have been held back by immunogenicity and toxicity concerns, but synthetic vectors have lagged in transfection efficiency. This thesis describes the rational design and systematic study of three classes of bioresponsive polymers for nucleic acid delivery. A central theme of the study was understanding how the structure of the polymers impacted each of the intracellular steps of delivery, rather than solely the end result. A powerful tool for efficiently quantifying endosomal escape was developed and applied to each of the material systems described. First, a linear-dendritic poly(amido amine) -poly(ethylene glycol) (PAMAM-PEG) block copolymer system previously developed in our lab was evaluated and its ability to overcome the sequential barriers of uptake, endosomal escape, and nuclear import were characterized. Next, a class of crosslinked linear polyethyleimine (xLPEI) hyperbranched polymers, which can contain disulfideresponsive linkages, were synthesized and investigated. It was demonstrated that free polymer in solution, not the presence of a functional bioresponsive domain, was responsible for the highly efficient and relatively nontoxic DNA delivery of this promising class of crosslinked polyamines. Finally, this analysis was applied to siRNA delivery by a library of amine-functionalized synthetic polypeptides. The pH-responsive secondary structure, micelle formation, and ester hydrolysis were studied prior to the discrete barrier-oriented analysis of the siRNA delivery potential of this library. It is hoped that the tools, materials, and systemic analysis of structure-function relationships in this thesis will enhance the process of discovery and development of clinically relevant gene carriers.",
    "advisors": ["Paula T. Hammond", "Robert S. Langer"],
    "text": "Understanding barriers to efficient nucleic acid delivery with bioresponsive block copolymers The delivery of nucleic acids has the potential to revolutionize medicine by allowing previously untreatable diseases to be clinically addressed. Viral delivery systems have been held back by immunogenicity and toxicity concerns, but synthetic vectors have lagged in transfection efficiency. This thesis describes the rational design and systematic study of three classes of bioresponsive polymers for nucleic acid delivery. A central theme of the study was understanding how the structure of the polymers impacted each of the intracellular steps of delivery, rather than solely the end result. A powerful tool for efficiently quantifying endosomal escape was developed and applied to each of the material systems described. First, a linear-dendritic poly(amido amine) -poly(ethylene glycol) (PAMAM-PEG) block copolymer system previously developed in our lab was evaluated and its ability to overcome the sequential barriers of uptake, endosomal escape, and nuclear import were characterized. Next, a class of crosslinked linear polyethyleimine (xLPEI) hyperbranched polymers, which can contain disulfideresponsive linkages, were synthesized and investigated. It was demonstrated that free polymer in solution, not the presence of a functional bioresponsive domain, was responsible for the highly efficient and relatively nontoxic DNA delivery of this promising class of crosslinked polyamines. Finally, this analysis was applied to siRNA delivery by a library of amine-functionalized synthetic polypeptides. The pH-responsive secondary structure, micelle formation, and ester hydrolysis were studied prior to the discrete barrier-oriented analysis of the siRNA delivery potential of this library. It is hoped that the tools, materials, and systemic analysis of structure-function relationships in this thesis will enhance the process of discovery and development of clinically relevant gene carriers."
}, {
    "id": "oai:dspace.mit.edu:1721.1/58399",
    "title": "Toward a drug delivery coating for intraocular lenses",
    "abstract": "Layer-by-layer assembly has become a quintessential tool for the creation of versatile, dynamic nanostructured materials able to dictate cellular behavior through exquisite surface functionality and delivery of bioactive agents. The primary aim of this work was to use layer-by-layer assembly to advance ophthalmic drug delivery modalities post cataract surgery to overcome the challenges of traditional postoperative therapy. Hydrolytically degradable multilayer films were used to create a multi-drug delivery coating for intraocular lenses (IOL). The establishment of a drug delivery coating for intraocular lenses required key advances in ultrathin film technology. This thesis focused on rational polymer design for tailored release, incorporation of hydrophobic small molecule therapeutics, and controlled multi-agent release. Fabrication rules and design tools necessary to create hydrolytically degradable polyelectrolyte multilayer films with preprogrammed advanced engineered release kinetics were investigated. A correlation between polycation hydrophobicity, as determined using octanol:water coefficients, allowed for the reliable prediction of release dynamics. A novel ultrathin system able to produce programmable zero order release kinetics of uncharged or hydrophobic small molecule therapeutics was developed. Charged cyclodextrin polymers were essential for the trapping of cyclodextrin-drug complexes in stable, surface eroding films capable of sustained drug release without altering therapeutic activity. In vitro investigation of cellular interactions with hydrolytically degradable multilayer films containing anti-inflammatory agents was conducted. These anti-inflammatory films controlled inflammation over physiologically relevant timescales and maintained the transparency and optical clarity of the IOL. Lastly, the first multilayer thin film system able to address the demands of both infection and inflammation, using small molecule pharmaceutics is described. The power, versatility, and utility of this multi-functional system were highlighted by the creation of functional drug coatings on intraocular lenses, bandage, and sutures. These combination devices effectively prevented bacterial growth while suppressing the production of inflammatory cytokines. Combined, these efforts surmounted key challenges toward the development of intraocular lenses able to prevent complications of cataract surgery and enhanced the fundamental understanding of layer-by-layer systems.",
    "advisors": ["Paula T. Hammond"],
    "text": "Toward a drug delivery coating for intraocular lenses Layer-by-layer assembly has become a quintessential tool for the creation of versatile, dynamic nanostructured materials able to dictate cellular behavior through exquisite surface functionality and delivery of bioactive agents. The primary aim of this work was to use layer-by-layer assembly to advance ophthalmic drug delivery modalities post cataract surgery to overcome the challenges of traditional postoperative therapy. Hydrolytically degradable multilayer films were used to create a multi-drug delivery coating for intraocular lenses (IOL). The establishment of a drug delivery coating for intraocular lenses required key advances in ultrathin film technology. This thesis focused on rational polymer design for tailored release, incorporation of hydrophobic small molecule therapeutics, and controlled multi-agent release. Fabrication rules and design tools necessary to create hydrolytically degradable polyelectrolyte multilayer films with preprogrammed advanced engineered release kinetics were investigated. A correlation between polycation hydrophobicity, as determined using octanol:water coefficients, allowed for the reliable prediction of release dynamics. A novel ultrathin system able to produce programmable zero order release kinetics of uncharged or hydrophobic small molecule therapeutics was developed. Charged cyclodextrin polymers were essential for the trapping of cyclodextrin-drug complexes in stable, surface eroding films capable of sustained drug release without altering therapeutic activity. In vitro investigation of cellular interactions with hydrolytically degradable multilayer films containing anti-inflammatory agents was conducted. These anti-inflammatory films controlled inflammation over physiologically relevant timescales and maintained the transparency and optical clarity of the IOL. Lastly, the first multilayer thin film system able to address the demands of both infection and inflammation, using small molecule pharmaceutics is described. The power, versatility, and utility of this multi-functional system were highlighted by the creation of functional drug coatings on intraocular lenses, bandage, and sutures. These combination devices effectively prevented bacterial growth while suppressing the production of inflammatory cytokines. Combined, these efforts surmounted key challenges toward the development of intraocular lenses able to prevent complications of cataract surgery and enhanced the fundamental understanding of layer-by-layer systems."
}, {
    "id": "oai:dspace.mit.edu:1721.1/54628",
    "title": "Prediction of parallel in-register amyloidogenic beta-structures In highly beta-rich protein sequences by pairwise propensity analysis",
    "abstract": "Amyloids and prion proteins are clinically and biologically important beta-structures, whose supersecondary structures are difficult to determine by standard experimental or computational means. In addition, significant conformational heterogeneity is known or suspected to exist in many amyloid fibrils. Recent work has indicated the utility of templates and pairwise probabilistic statistics in betastructure prediction. A new suite of programs, BETASCAN, STITCHER, and HELIXCAP, are presented to address the problem of amyloid structure prediction. BETASCAN calculates likelihood scores for potential beta-strands and strand-pairs based on correlations observed in parallel beta-sheets. The program then determines the strands and pairs with the greatest local likelihood for all of the sequence's potential beta-structures. BETASCAN suggests multiple alternate folding patterns and assigns relative ab initio probabilities based solely on amino acid sequence, probability tables, and pre-chosen parameters. STITCHER processes the output of BETASCAN and uses dynamic programming to 'stitch' structures from flexible abstract templates defined by constraints for amyloid-like all-beta structures. The 'stitched' structures are evaluated by a free-energy-based scoring algorithm incorporating BETASCAN scores, bonuses for favorable side-chain stacking, and penalties for linker entropy. The analyses of STITCHER structures emphasize the importance of side-chain stacking ladders in amyloid formation. HELIXCAP detects a class of end-caps, called beta-helix caps, which stabilize known beta-helix structures. These structures are known to stabilize globular beta-helix proteins and prevent their amyloidogenesis; their presence in a sequence is a powerful negative predictor of amyloid potential. Together, these algorithms permit detection and structural analysis of protein amyloidogenicity from sequence data, enhancing the experimental investigation of amyloids and prion proteins.",
    "advisors": ["Bonnie Berger", "Susan L. Lindquist"],
    "text": "Prediction of parallel in-register amyloidogenic beta-structures In highly beta-rich protein sequences by pairwise propensity analysis Amyloids and prion proteins are clinically and biologically important beta-structures, whose supersecondary structures are difficult to determine by standard experimental or computational means. In addition, significant conformational heterogeneity is known or suspected to exist in many amyloid fibrils. Recent work has indicated the utility of templates and pairwise probabilistic statistics in betastructure prediction. A new suite of programs, BETASCAN, STITCHER, and HELIXCAP, are presented to address the problem of amyloid structure prediction. BETASCAN calculates likelihood scores for potential beta-strands and strand-pairs based on correlations observed in parallel beta-sheets. The program then determines the strands and pairs with the greatest local likelihood for all of the sequence's potential beta-structures. BETASCAN suggests multiple alternate folding patterns and assigns relative ab initio probabilities based solely on amino acid sequence, probability tables, and pre-chosen parameters. STITCHER processes the output of BETASCAN and uses dynamic programming to 'stitch' structures from flexible abstract templates defined by constraints for amyloid-like all-beta structures. The 'stitched' structures are evaluated by a free-energy-based scoring algorithm incorporating BETASCAN scores, bonuses for favorable side-chain stacking, and penalties for linker entropy. The analyses of STITCHER structures emphasize the importance of side-chain stacking ladders in amyloid formation. HELIXCAP detects a class of end-caps, called beta-helix caps, which stabilize known beta-helix structures. These structures are known to stabilize globular beta-helix proteins and prevent their amyloidogenesis; their presence in a sequence is a powerful negative predictor of amyloid potential. Together, these algorithms permit detection and structural analysis of protein amyloidogenicity from sequence data, enhancing the experimental investigation of amyloids and prion proteins."
}, {
    "id": "oai:dspace.mit.edu:1721.1/43802",
    "title": "Statistical shape analysis of neuroanatomical structures based on spherical wavelet transformation",
    "abstract": "Evidence suggests that morphological changes of neuroanatomical structures may reflect abnormalities in neurodevelopment, or relate to a variety of disorders, such as schizophrenia and Alzheimer's disease (AD). Advances in high-resolution Magnetic Resonance Imaging (MRI) techniques allow us to study these alterations of brain structures in vivo. Previous work in studying the shape variations of brain structures has provided additional localized information compared with traditional volume-based study. However, challenges remain in finding an accurate shape presentation and conducting shape analysis with sound statistical principles. In this work, we develop methods for automatically extracting localized and multi-scale shape features and conducting statistical shape analysis of neuroanatomical structures obtained from MR images. We first develop a procedure to extract multi-scale shape features of brain structures using biorthogonal spherical wavelets. Using this wavelet-based shape representation, we build multi-scale shape models and study the localized cortical folding variations in a normal population using Principal Component Analysis (PCA). We then build a shape-based classification framework for detecting pathological changes of cortical surfaces using advanced classification methods, such as predictive Automatic Relevance Determination (pred-ARD), and demonstrate promising results in patient/control group comparison studies. Thirdly, we develop a nonlinear temporal model for studying the temporal order and regional difference of cortical folding development based on this shape representation. Furthermore, we develop a shape-guided segmentation method to improve the segmentation of sub-cortical structures, such as hippocampus, by using shape constraints obtained in the wavelet domain.",
    "advisors": ["Bruce Fischl"],
    "text": "Statistical shape analysis of neuroanatomical structures based on spherical wavelet transformation Evidence suggests that morphological changes of neuroanatomical structures may reflect abnormalities in neurodevelopment, or relate to a variety of disorders, such as schizophrenia and Alzheimer's disease (AD). Advances in high-resolution Magnetic Resonance Imaging (MRI) techniques allow us to study these alterations of brain structures in vivo. Previous work in studying the shape variations of brain structures has provided additional localized information compared with traditional volume-based study. However, challenges remain in finding an accurate shape presentation and conducting shape analysis with sound statistical principles. In this work, we develop methods for automatically extracting localized and multi-scale shape features and conducting statistical shape analysis of neuroanatomical structures obtained from MR images. We first develop a procedure to extract multi-scale shape features of brain structures using biorthogonal spherical wavelets. Using this wavelet-based shape representation, we build multi-scale shape models and study the localized cortical folding variations in a normal population using Principal Component Analysis (PCA). We then build a shape-based classification framework for detecting pathological changes of cortical surfaces using advanced classification methods, such as predictive Automatic Relevance Determination (pred-ARD), and demonstrate promising results in patient/control group comparison studies. Thirdly, we develop a nonlinear temporal model for studying the temporal order and regional difference of cortical folding development based on this shape representation. Furthermore, we develop a shape-guided segmentation method to improve the segmentation of sub-cortical structures, such as hippocampus, by using shape constraints obtained in the wavelet domain."
}, {
    "id": "oai:dspace.mit.edu:1721.1/63085",
    "title": "A unified model of electroporation and molecular transport",
    "abstract": "Biological membranes form transient, conductive pores in response to elevated transmembrane voltage, a phenomenon termed electroporation. These pores facilitate electrical and molecular transport across cell membranes that are normally impermeable. By applying pulsed electric fields to cells, electroporation can be used to deliver nucleic acids, drugs, and other molecules into cells, making it a powerful research tool. Because of its widely demonstrated utility for in vitro applications, researchers are increasingly investigating related in vivo clinical applications of electroporation, such as gene delivery, drug delivery, and tissue ablation. In this thesis, we describe a quantitative, mechanistic model of electroporation and concomitant molecular transport that can be used for guiding and interpreting electroporation experiments and applications. The model comprises coupled mathematical descriptions of electrical transport, electrodiffusive molecular transport, and pore dynamics. Where possible, each of these components is independently validated against experimental results in the literature. We determine the response of a discretized cell system to an applied electric pulse by assembling the discretized transport relations into a large system of nonlinear differential equations that is efficiently solved and analyzed with MATLAB. We validate the model by replicating in silico two sets of experiments in the literature that measure electroporation-mediated transport of fluorescent probes. The model predictions of molecular uptake are in excellent agreement with these experimental measurements, for which the applied electric pulses collectively span nearly three orders of magnitude in pulse duration (50 ts -20 ms) and an order of magnitude in pulse magnitude (0.3 -3 kV/cm). The advantages of our theoretical approach are the ability to (1) analyze in silico the same quantities that are measured by experimental studies in vitro, (2) simulate electroporation dynamics that are difficult to assess experimentally, and (3) quickly screen a wide array of electric pulse waveforms for particular applications. We believe that our approach will contribute to a greater understanding of the mechanisms of electroporation and provide an in silico platform for guiding new experiments and applications.",
    "advisors": ["James C. Weaver"],
    "text": "A unified model of electroporation and molecular transport Biological membranes form transient, conductive pores in response to elevated transmembrane voltage, a phenomenon termed electroporation. These pores facilitate electrical and molecular transport across cell membranes that are normally impermeable. By applying pulsed electric fields to cells, electroporation can be used to deliver nucleic acids, drugs, and other molecules into cells, making it a powerful research tool. Because of its widely demonstrated utility for in vitro applications, researchers are increasingly investigating related in vivo clinical applications of electroporation, such as gene delivery, drug delivery, and tissue ablation. In this thesis, we describe a quantitative, mechanistic model of electroporation and concomitant molecular transport that can be used for guiding and interpreting electroporation experiments and applications. The model comprises coupled mathematical descriptions of electrical transport, electrodiffusive molecular transport, and pore dynamics. Where possible, each of these components is independently validated against experimental results in the literature. We determine the response of a discretized cell system to an applied electric pulse by assembling the discretized transport relations into a large system of nonlinear differential equations that is efficiently solved and analyzed with MATLAB. We validate the model by replicating in silico two sets of experiments in the literature that measure electroporation-mediated transport of fluorescent probes. The model predictions of molecular uptake are in excellent agreement with these experimental measurements, for which the applied electric pulses collectively span nearly three orders of magnitude in pulse duration (50 ts -20 ms) and an order of magnitude in pulse magnitude (0.3 -3 kV/cm). The advantages of our theoretical approach are the ability to (1) analyze in silico the same quantities that are measured by experimental studies in vitro, (2) simulate electroporation dynamics that are difficult to assess experimentally, and (3) quickly screen a wide array of electric pulse waveforms for particular applications. We believe that our approach will contribute to a greater understanding of the mechanisms of electroporation and provide an in silico platform for guiding new experiments and applications."
}, {
    "id": "oai:dspace.mit.edu:1721.1/103501",
    "title": "Characterizing variation at short tandem repeats and their role in human genome regulation",
    "abstract": "A central goal in genomics is to understand the genetic variants that underlie molecular changes and lead to disease. Recent studies have identified thousands of genetic loci associated with human phenotypes. These have primarily analyzed point mutations, ignoring more complex types of variation. Here we focus on Short Tandem Repeats (STRs) as a model for complex variation. STRs are comprised of repeating motifs of 1-6bp that span over 1% of the human genome. The level of STR variation and its effect on phenotypes remains mostly uncharted, mainly due to the difficulty in accurately genotyping STRs on a large scale. To overcome bioinformatic challenges in STR genotyping, we developed lobSTR, an algorithm for profiling STRs from high throughput sequencing data. lobSTR employs a unique mapping strategy to rapidly align repetitive reads, and uses statistical learning techniques to account for STR-specific noise patterns. We applied lobSTR to generate the largest and highest quality STR catalog to date. This provided the first characterization of more than a million loci and gave novel insights into population-wide trends of STR variation. We used our catalog to conduct a genome-wide analysis of the contribution of STRs to gene expression in humans. This revealed that STRs explain 10-15% of the cis heritability of expression mediated by common variants and potentially play a role in various clinically relevant conditions. Overall these studies highlight the contribution of STRs to the genetic architecture of quantitative traits. We anticipate that integrating repetitive elements, specifically STRs, into genome-wide analyses will lead to the discovery of new genetic variants relevant to human conditions.",
    "advisors": ["Yaniv Erlich", "Mark J. Daly"],
    "text": "Characterizing variation at short tandem repeats and their role in human genome regulation A central goal in genomics is to understand the genetic variants that underlie molecular changes and lead to disease. Recent studies have identified thousands of genetic loci associated with human phenotypes. These have primarily analyzed point mutations, ignoring more complex types of variation. Here we focus on Short Tandem Repeats (STRs) as a model for complex variation. STRs are comprised of repeating motifs of 1-6bp that span over 1% of the human genome. The level of STR variation and its effect on phenotypes remains mostly uncharted, mainly due to the difficulty in accurately genotyping STRs on a large scale. To overcome bioinformatic challenges in STR genotyping, we developed lobSTR, an algorithm for profiling STRs from high throughput sequencing data. lobSTR employs a unique mapping strategy to rapidly align repetitive reads, and uses statistical learning techniques to account for STR-specific noise patterns. We applied lobSTR to generate the largest and highest quality STR catalog to date. This provided the first characterization of more than a million loci and gave novel insights into population-wide trends of STR variation. We used our catalog to conduct a genome-wide analysis of the contribution of STRs to gene expression in humans. This revealed that STRs explain 10-15% of the cis heritability of expression mediated by common variants and potentially play a role in various clinically relevant conditions. Overall these studies highlight the contribution of STRs to the genetic architecture of quantitative traits. We anticipate that integrating repetitive elements, specifically STRs, into genome-wide analyses will lead to the discovery of new genetic variants relevant to human conditions."
}, {
    "id": "oai:dspace.mit.edu:1721.1/34481",
    "title": "Neural representations of pitch : role of peripheral frequency selectivity",
    "abstract": "Investigating the neural mechanisms underlying the perception of the pitch of harmonic complex tones is of great importance for many reasons. Changes in pitch convey melody in music, and the superposition of different pitches is the basis for harmony. Pitch has an important role in speech, where it carries prosodic features and information about speaker identity. Pitch plays a major role in auditory scene analysis: differences in pitch are a major cue for sound source segregation, while frequency components that share a common fundamental frequency (FO) tend to be grouped into a single auditory object. In psychophysics, a positive correlation is commonly observed between the estimated \"resolvability\" of individual harmonics of complex tones, assumed to depend primarily on the frequency selectivity of the cochlea, and the strength of the corresponding pitch percepts. In this study, possible neural codes for the pitch of harmonic complex tones were investigated in the auditory nerve of anesthetized cats, with particular focus on their dependence on cochlear frequency selectivity, which was measured directly using both complex tones and band-reject noise. A \"rate-place\" representation of pitch, based on cues to peripherally-resolved harmonics in profiles of average discharge rate along tonotopically-arranged neurons, was compared to a \"temporal\" representation, based on periodicity cues in the distributions of interspike intervals of the entire auditory nerve.",
    "advisors": ["Bertrand Delgutte"],
    "text": "Neural representations of pitch : role of peripheral frequency selectivity Investigating the neural mechanisms underlying the perception of the pitch of harmonic complex tones is of great importance for many reasons. Changes in pitch convey melody in music, and the superposition of different pitches is the basis for harmony. Pitch has an important role in speech, where it carries prosodic features and information about speaker identity. Pitch plays a major role in auditory scene analysis: differences in pitch are a major cue for sound source segregation, while frequency components that share a common fundamental frequency (FO) tend to be grouped into a single auditory object. In psychophysics, a positive correlation is commonly observed between the estimated \"resolvability\" of individual harmonics of complex tones, assumed to depend primarily on the frequency selectivity of the cochlea, and the strength of the corresponding pitch percepts. In this study, possible neural codes for the pitch of harmonic complex tones were investigated in the auditory nerve of anesthetized cats, with particular focus on their dependence on cochlear frequency selectivity, which was measured directly using both complex tones and band-reject noise. A \"rate-place\" representation of pitch, based on cues to peripherally-resolved harmonics in profiles of average discharge rate along tonotopically-arranged neurons, was compared to a \"temporal\" representation, based on periodicity cues in the distributions of interspike intervals of the entire auditory nerve."
}, {
    "id": "oai:dspace.mit.edu:1721.1/28601",
    "title": "A shoe-integrated sensor system for wireless gait analysis and real-time therapeutic feedback",
    "abstract": "Clinical gait analysis currently involves either an expensive analysis in a motion laboratory, using highly accurate, if cumbersome, kinematic systems, or a qualitative analysis with a physician or physical therapist making visual observations. There is a need for a low cost device that falls in between these two methods, and can provide quantitative and repeatable results. In addition, continuous monitoring of gait would be useful for real-time physical rehabilitation. To free patients from the confines of a motion laboratory, this thesis has resulted in a wireless wearable system capable of measuring many parameters relevant to gait analysis. The extensive sensor suite includes three orthogonal accelerometers, and three orthogonal gyroscopes, four force sensors, two bi-directional bend sensors, two dynamic pressure sensors, as well as electric field height sensors. The \"GaitShoe\" was built to be worn on any shoes, without interfering with gait, and was designed to collect data unobtrusively, in any environment, and over long periods of time. Subject testing of the GaitShoe was carried out on ten healthy subjects with normal gait and five subjects with Parkinson's disease. The calibrated sensor outputs were analyzed, and compared to results obtained simultaneously from The Massachusetts General Hospital Biomotion Lab; the GaitShoe proved highly capable of detecting heel strike and toe off, as well as estimating orientation and position of the subject. A wide variety of features were developed from the calibrated sensor outputs, for use with standard pattern recognition techniques to classify the gait of the subject. The results of the classification demonstrated the ability of the GaitShoe to identify the subjects with",
    "advisors": ["Joseph A. Paradiso"],
    "text": "A shoe-integrated sensor system for wireless gait analysis and real-time therapeutic feedback Clinical gait analysis currently involves either an expensive analysis in a motion laboratory, using highly accurate, if cumbersome, kinematic systems, or a qualitative analysis with a physician or physical therapist making visual observations. There is a need for a low cost device that falls in between these two methods, and can provide quantitative and repeatable results. In addition, continuous monitoring of gait would be useful for real-time physical rehabilitation. To free patients from the confines of a motion laboratory, this thesis has resulted in a wireless wearable system capable of measuring many parameters relevant to gait analysis. The extensive sensor suite includes three orthogonal accelerometers, and three orthogonal gyroscopes, four force sensors, two bi-directional bend sensors, two dynamic pressure sensors, as well as electric field height sensors. The \"GaitShoe\" was built to be worn on any shoes, without interfering with gait, and was designed to collect data unobtrusively, in any environment, and over long periods of time. Subject testing of the GaitShoe was carried out on ten healthy subjects with normal gait and five subjects with Parkinson's disease. The calibrated sensor outputs were analyzed, and compared to results obtained simultaneously from The Massachusetts General Hospital Biomotion Lab; the GaitShoe proved highly capable of detecting heel strike and toe off, as well as estimating orientation and position of the subject. A wide variety of features were developed from the calibrated sensor outputs, for use with standard pattern recognition techniques to classify the gait of the subject. The results of the classification demonstrated the ability of the GaitShoe to identify the subjects with"
}, {
    "id": "oai:dspace.mit.edu:1721.1/38518",
    "title": "Electro-anatomical models of the cochlear implant",
    "abstract": "While cochlear implantation has become the standard care in treating patients with severe to profound sensorineural hearing loss, the variation in benefit (communicative ability) individual patients derive from implantation remains both large and, for the most part, unexplained. One explanation for this variation is the status of the implanted ear which, when examined histopathologically, also displays substantial variation due to both the pathogenesis of hearing loss (etiology, etc.) and pathological changes initiated by implantation. For instance, across-patient variation in electrode position and insertion depth is clearly present, as are differential amounts of residual spiral ganglion survival, fibrous tissue formation and electrode encapsulation, cochlear ossification, and idiosyncratic damage to adjacent cochlear structures. Because of the complex geometric electrical properties of the tissues found in the implanted ear, demonstrating the impact of pathological variability on neuronal excitation, and ultimately on behavioral performance, will likely require a detailed representation of the peripheral anatomy. Our approach has been to develop detailed, three-dimensional (3D) electro-anatomical models (EAMs) of the implanted ear capable of representing the aforementioned patient-specific types of pathological variation. In response to electric stimulation, these computational models predict an estimate of (1) the 3D electric field, (2) the cochleotopic pattern of neural activation, and (3) the electrically-evoked compound action potential (ECAP) recorded from intracochlear electrodes. This thesis focuses on three aims. First, two patient-specific EAMs are formulated from hundreds of digital images of the histologically-sectioned temporal bones of two patients, attempting to incorporate the detailed pathology of each. Second, model predictions are compared to relevant reports from the literature, data collected from a cohort of implanted research subjects, and, most importantly, to archival data collected during life from the same two patients used to derive our psychophysical threshold measures, and ECAP recordings) collectively show a promising correspondence between model-predicted and empirically-measured data. Third, by making incremental adjustments to the anatomical representation in the model, the impact of individual attributes are investigated, mechanisms that may degrade benefit suggested, and potential interventions explored.",
    "advisors": ["Donald K. Eddington"],
    "text": "Electro-anatomical models of the cochlear implant While cochlear implantation has become the standard care in treating patients with severe to profound sensorineural hearing loss, the variation in benefit (communicative ability) individual patients derive from implantation remains both large and, for the most part, unexplained. One explanation for this variation is the status of the implanted ear which, when examined histopathologically, also displays substantial variation due to both the pathogenesis of hearing loss (etiology, etc.) and pathological changes initiated by implantation. For instance, across-patient variation in electrode position and insertion depth is clearly present, as are differential amounts of residual spiral ganglion survival, fibrous tissue formation and electrode encapsulation, cochlear ossification, and idiosyncratic damage to adjacent cochlear structures. Because of the complex geometric electrical properties of the tissues found in the implanted ear, demonstrating the impact of pathological variability on neuronal excitation, and ultimately on behavioral performance, will likely require a detailed representation of the peripheral anatomy. Our approach has been to develop detailed, three-dimensional (3D) electro-anatomical models (EAMs) of the implanted ear capable of representing the aforementioned patient-specific types of pathological variation. In response to electric stimulation, these computational models predict an estimate of (1) the 3D electric field, (2) the cochleotopic pattern of neural activation, and (3) the electrically-evoked compound action potential (ECAP) recorded from intracochlear electrodes. This thesis focuses on three aims. First, two patient-specific EAMs are formulated from hundreds of digital images of the histologically-sectioned temporal bones of two patients, attempting to incorporate the detailed pathology of each. Second, model predictions are compared to relevant reports from the literature, data collected from a cohort of implanted research subjects, and, most importantly, to archival data collected during life from the same two patients used to derive our psychophysical threshold measures, and ECAP recordings) collectively show a promising correspondence between model-predicted and empirically-measured data. Third, by making incremental adjustments to the anatomical representation in the model, the impact of individual attributes are investigated, mechanisms that may degrade benefit suggested, and potential interventions explored."
}, {
    "id": "oai:dspace.mit.edu:1721.1/68457",
    "title": "A model of sinoatrial node cell regulation by the autonomic nervous system",
    "abstract": "The primary function of the heart is to pump blood at a sufficient rate to ensure perfusion of all the organs. This vital task is achieved in large part by controlling the rate of cardiac contractions, which are initiated by cells in the sinoatrial node, the \"pacemaker\" of the heart. The oscillation rate of these spontaneously active cells is tightly regulated by the sympathetic and parasympathetic branches of the autonomic nervous system. Our understanding of sinoatrial node cell function has been greatly advanced by experimental and modeling efforts that quantitatively describe the numerous ionic currents responsible for the cell's spontaneous depolarization and generation of the action potential. Several models have also explored the effect of sympathetic and parasympathetic activity on specific ion channels and have reproduced the classic slowing and acceleration phenomena. However, a complete model of this interaction does not exist: current models lack the ability to simulate simultaneous sympathetic and parasympathetic activation or to reproduce heart rate dynamics in response to time-varying autonomic inputs. We addressed this need by constructing a bottom-up model of sinoatrial node cell regulation by the autonomic nervous system, with a focus on reproducing the full range of heart rates observed under simultaneous sympathetic and parasympathetic nerve stimulation, as well as the dynamic heart rate response to steps in sympathetic or parasympathetic stimulation rate. In constructing our model, we consolidate a large body of experimental data in a consistent mathematical framework. The model comprises 57 nonlinear coupled ordinary differential equations based on first principles and the current mechanistic understanding of the component reactions, fits well all the experimental data used to build the model, and reproduces high-level features of the system that were not explicitly fit when building the model. The detailed nature of the model also allows numerous conclusions to be drawn about the mechanisms of heart rate control. A better understanding of these mechanisms in health and disease may enable the development of better diagnostics for cardiovascular disease and more targeted drug design. We also identified a number of limitations in the present model that can be refined through further experimental and numerical efforts.",
    "advisors": ["Richard J. Cohen"],
    "text": "A model of sinoatrial node cell regulation by the autonomic nervous system The primary function of the heart is to pump blood at a sufficient rate to ensure perfusion of all the organs. This vital task is achieved in large part by controlling the rate of cardiac contractions, which are initiated by cells in the sinoatrial node, the \"pacemaker\" of the heart. The oscillation rate of these spontaneously active cells is tightly regulated by the sympathetic and parasympathetic branches of the autonomic nervous system. Our understanding of sinoatrial node cell function has been greatly advanced by experimental and modeling efforts that quantitatively describe the numerous ionic currents responsible for the cell's spontaneous depolarization and generation of the action potential. Several models have also explored the effect of sympathetic and parasympathetic activity on specific ion channels and have reproduced the classic slowing and acceleration phenomena. However, a complete model of this interaction does not exist: current models lack the ability to simulate simultaneous sympathetic and parasympathetic activation or to reproduce heart rate dynamics in response to time-varying autonomic inputs. We addressed this need by constructing a bottom-up model of sinoatrial node cell regulation by the autonomic nervous system, with a focus on reproducing the full range of heart rates observed under simultaneous sympathetic and parasympathetic nerve stimulation, as well as the dynamic heart rate response to steps in sympathetic or parasympathetic stimulation rate. In constructing our model, we consolidate a large body of experimental data in a consistent mathematical framework. The model comprises 57 nonlinear coupled ordinary differential equations based on first principles and the current mechanistic understanding of the component reactions, fits well all the experimental data used to build the model, and reproduces high-level features of the system that were not explicitly fit when building the model. The detailed nature of the model also allows numerous conclusions to be drawn about the mechanisms of heart rate control. A better understanding of these mechanisms in health and disease may enable the development of better diagnostics for cardiovascular disease and more targeted drug design. We also identified a number of limitations in the present model that can be refined through further experimental and numerical efforts."
}, {
    "id": "oai:dspace.mit.edu:1721.1/70816",
    "title": "Alternative isoform regulation in myotonic dystrophy",
    "abstract": "Myotonic dystrophy (DM) is the most common form of adult onset muscular dystrophy, affecting more than 1 in 8000 individuals globally. The symptoms of DM are multi-systemic and include myotonia, severe muscle wasting, cardiac arrhythmias, cataracts, gastrointestinal dysfunction, and cognitive deficits. DM is caused by the expansion of CTG or CCTG repeat sequences expressed in noncoding portions of RNA, which sequester or activate RNA splicing factor proteins, leading to widespread deleterious changes in transcriptome isoform usage. We developed a method for studying transcriptomes, RNAseq, which provides a high resolution, digital inventory of gene and isoform expression. By applying RNAseq to human tissues and cell lines, we discovered that essentially 92-94% of all human genes are alternatively spliced, 86% of them with a minor isoform frequency 15% or more. We found that the majority of alternative splicing and alternative polyadenylation and cleavage events are tissue-regulated, and that patterns of these RNA processing events are strongly correlated across tissues, implicating protein factors that may regulate both types of events. We applied this method towards the goal of identifying transcriptome changes occurring in DM, focusing on the Muscleblind-like (MBNL) family of RNA binding proteins, which are functionally inactivated by CUG or CCUG repeats. Using RNAseq to profile tissues and cells depleted of MBNLs, we found that MBNL1 and MBNL2 co-regulate hundreds of redundant targets. MBNL1 UV cross-linking and immunoprecipitation, followed by sequencing (CLIPseq), was used to identify the in vivo transcriptome-wide binding locations of MBNL1, and facilitated the construction of a context-dependent RNA map for MBNL1 splicing regulation. Extensive 3' UTR binding of MBNL1 was found to localize mRNAs to membrane compartments of mouse myoblasts, suggesting a new global function for MBNLs, and additional mechanisms by which MBNL depletion can lead to DM symptoms.",
    "advisors": ["Christopher B. Burge", "David E. Housman"],
    "text": "Alternative isoform regulation in myotonic dystrophy Myotonic dystrophy (DM) is the most common form of adult onset muscular dystrophy, affecting more than 1 in 8000 individuals globally. The symptoms of DM are multi-systemic and include myotonia, severe muscle wasting, cardiac arrhythmias, cataracts, gastrointestinal dysfunction, and cognitive deficits. DM is caused by the expansion of CTG or CCTG repeat sequences expressed in noncoding portions of RNA, which sequester or activate RNA splicing factor proteins, leading to widespread deleterious changes in transcriptome isoform usage. We developed a method for studying transcriptomes, RNAseq, which provides a high resolution, digital inventory of gene and isoform expression. By applying RNAseq to human tissues and cell lines, we discovered that essentially 92-94% of all human genes are alternatively spliced, 86% of them with a minor isoform frequency 15% or more. We found that the majority of alternative splicing and alternative polyadenylation and cleavage events are tissue-regulated, and that patterns of these RNA processing events are strongly correlated across tissues, implicating protein factors that may regulate both types of events. We applied this method towards the goal of identifying transcriptome changes occurring in DM, focusing on the Muscleblind-like (MBNL) family of RNA binding proteins, which are functionally inactivated by CUG or CCUG repeats. Using RNAseq to profile tissues and cells depleted of MBNLs, we found that MBNL1 and MBNL2 co-regulate hundreds of redundant targets. MBNL1 UV cross-linking and immunoprecipitation, followed by sequencing (CLIPseq), was used to identify the in vivo transcriptome-wide binding locations of MBNL1, and facilitated the construction of a context-dependent RNA map for MBNL1 splicing regulation. Extensive 3' UTR binding of MBNL1 was found to localize mRNAs to membrane compartments of mouse myoblasts, suggesting a new global function for MBNLs, and additional mechanisms by which MBNL depletion can lead to DM symptoms."
}, {
    "id": "oai:dspace.mit.edu:1721.1/113789",
    "title": "Acoustic features of impaired articulation due to amyotrophic lateral sclerosis",
    "abstract": "Progressive bulbar motor deterioration resulting from amyotrophic lateral sclerosis (ALS) leads to speech impairment. Despite the devastating consequences of speech impairment to life quality, few options are available to objectively assess speech motor involvement. The overarching goal of this research was to derive objective measures of speech acoustics that can be used to support clinical decision making. To achieve this goal, we obtained 121 speech samples from 33 patients with ALS who repeated the phrase \"Buy Bobby a puppy\" five times in succession. In total, 342 acoustic features were semi-automatically extracted from each speech recording. Pearson correlations were computed between each feature and three metrics of overall speech severity: sentence intelligibility, speaking rate, and communication efficiency. The findings were grounded within a physiologic framework where acoustic features were grouped into one of three domains that when combined, were hypothesized to broadly characterize articulatory performance: articulatory specification, articulatory coupling, and articulatory consistency. To obtain the most accurate prediction of ALS with the features we extracted, we compared two machine learning algorithms: linear regression and random forest. In shuffle-split cross-validation, the strongest mean Pearson correlations we obtained between actual and predicted intelligibility, speaking rate, and communication efficiency were 0.67, 0.74, and 0.77, respectively (SD=0.077, 0.050, and 0.059, respectively). Of the three domains, the specificity features were the most strongly associated with intelligibility impairments (mean r=0.68), and coupling was the most strongly associated with slower speaking rate (mean r=0.73). Specificity and coupling yielded similar performances in communication efficiency prediction. Other contributions of this thesis are that it is the first to implement a framework of dysarthric speech in terms of three domains: specification, coupling, and consistency; the first to validate automated formant tracking in dysarthric speech; and the first to perform an in-depth investigation into physiologically-inspired acoustic features that describe articulatory impairments of patients with ALS. Novel findings include the presence of abnormal formant coupling patterns, which may suggest greater tonguejaw coupling, in patients with more severe dysarthria due to ALS. Areas of future research involve further feature discovery, improved analysis methods, and a deeper understanding of relations to articulatory kinematics.",
    "advisors": ["Thomas F. Quatieri", "Jordan R. Green"],
    "text": "Acoustic features of impaired articulation due to amyotrophic lateral sclerosis Progressive bulbar motor deterioration resulting from amyotrophic lateral sclerosis (ALS) leads to speech impairment. Despite the devastating consequences of speech impairment to life quality, few options are available to objectively assess speech motor involvement. The overarching goal of this research was to derive objective measures of speech acoustics that can be used to support clinical decision making. To achieve this goal, we obtained 121 speech samples from 33 patients with ALS who repeated the phrase \"Buy Bobby a puppy\" five times in succession. In total, 342 acoustic features were semi-automatically extracted from each speech recording. Pearson correlations were computed between each feature and three metrics of overall speech severity: sentence intelligibility, speaking rate, and communication efficiency. The findings were grounded within a physiologic framework where acoustic features were grouped into one of three domains that when combined, were hypothesized to broadly characterize articulatory performance: articulatory specification, articulatory coupling, and articulatory consistency. To obtain the most accurate prediction of ALS with the features we extracted, we compared two machine learning algorithms: linear regression and random forest. In shuffle-split cross-validation, the strongest mean Pearson correlations we obtained between actual and predicted intelligibility, speaking rate, and communication efficiency were 0.67, 0.74, and 0.77, respectively (SD=0.077, 0.050, and 0.059, respectively). Of the three domains, the specificity features were the most strongly associated with intelligibility impairments (mean r=0.68), and coupling was the most strongly associated with slower speaking rate (mean r=0.73). Specificity and coupling yielded similar performances in communication efficiency prediction. Other contributions of this thesis are that it is the first to implement a framework of dysarthric speech in terms of three domains: specification, coupling, and consistency; the first to validate automated formant tracking in dysarthric speech; and the first to perform an in-depth investigation into physiologically-inspired acoustic features that describe articulatory impairments of patients with ALS. Novel findings include the presence of abnormal formant coupling patterns, which may suggest greater tonguejaw coupling, in patients with more severe dysarthria due to ALS. Areas of future research involve further feature discovery, improved analysis methods, and a deeper understanding of relations to articulatory kinematics."
}, {
    "id": "oai:dspace.mit.edu:1721.1/62518",
    "title": "Responses from electric stimulation of cochlear nucleus",
    "abstract": "Cochlear nucleus (CN), the exclusive destination of the auditory nerve, is the gateway for all central processing of auditory information. The CN comprises three major subdivisions: anteroventral, posteroventral and dorsal (AVCN, PVCN and DCN, respectively), each of which contains anatomically and physiologically distinct neurons projecting onto different targets. This research used focal electric stimulation of small, confined parts of the CN in anesthetized guinea pigs to resolve the roles of the CN divisions, in two contexts. Part i explored the effect of stimulation on the gross neural potential (electrically evoked auditory brainstem response, EABR). In AVCN and PVCN away from the 8th nerve fibers entering the brainstem, stimulation consistently evoked waveforms comprising 3 waves, suggesting a diffuse distribution of cellular generator of the EABR. On the other hand, in vestibular structures (vestibular nerve root and Scarpa's ganglion), the characteristic waveform comprised only two waves. Stimulation of multiple neural structures, as seen with higher stimulus levels or stimulation in auditory nerve root area generally produced more complex and variable waveforms. Part 2 explored the effects of stimulation on the activation of one type of auditory reflex, medial olivocochlear (MOC) reflex. The reflex was monitored through its effects on distortion product otoacoustic emission (DPOAE). The MOC reflex was activated bilaterally by stimulating PVCN or AVCN shell, but not AVCN core. These results suggest that there are two groups of MOC interneurons in specific parts of CN.",
    "advisors": ["M. Christian Brown"],
    "text": "Responses from electric stimulation of cochlear nucleus Cochlear nucleus (CN), the exclusive destination of the auditory nerve, is the gateway for all central processing of auditory information. The CN comprises three major subdivisions: anteroventral, posteroventral and dorsal (AVCN, PVCN and DCN, respectively), each of which contains anatomically and physiologically distinct neurons projecting onto different targets. This research used focal electric stimulation of small, confined parts of the CN in anesthetized guinea pigs to resolve the roles of the CN divisions, in two contexts. Part i explored the effect of stimulation on the gross neural potential (electrically evoked auditory brainstem response, EABR). In AVCN and PVCN away from the 8th nerve fibers entering the brainstem, stimulation consistently evoked waveforms comprising 3 waves, suggesting a diffuse distribution of cellular generator of the EABR. On the other hand, in vestibular structures (vestibular nerve root and Scarpa's ganglion), the characteristic waveform comprised only two waves. Stimulation of multiple neural structures, as seen with higher stimulus levels or stimulation in auditory nerve root area generally produced more complex and variable waveforms. Part 2 explored the effects of stimulation on the activation of one type of auditory reflex, medial olivocochlear (MOC) reflex. The reflex was monitored through its effects on distortion product otoacoustic emission (DPOAE). The MOC reflex was activated bilaterally by stimulating PVCN or AVCN shell, but not AVCN core. These results suggest that there are two groups of MOC interneurons in specific parts of CN."
}, {
    "id": "oai:dspace.mit.edu:1721.1/113788",
    "title": "Oxygen, the invisible orchestrator of metabolism and disease : a focus on mitochondrial And peroxisomal dysfunction",
    "abstract": "Variations in atmospheric oxygen levels can be traced over evolutionary time and track closely with the development of multicellular life, speciation events, appearance of placental mammals and the creation of a cardio-respiratory system. As the final electron acceptor for aerobic ATP production, oxygen allows energy-intensive metabolic pathways to exist. Furthermore, oxygen is the most utilized substrate for known biochemical reactions, surpassing even ATP and NAD+. As a result, variations in oxygen levels have far-reaching consequences on human physiology and health. Mitochondrial disorders are the most common inborn errors of metabolism, affecting approximately 1 in 5000 live births. Patients can present in infancy or adulthood with symptoms affecting multiple organ systems including blindness, deafness, muscle weakness, developmental delay and severe neurological impairment. Unfortunately, there are currently no proven therapies for mitochondrial disorders. My thesis work has focused on combining systems biology, animal physiology and cellular metabolism approaches to develop new therapies for these disorders. More specifically, I have identified hypoxic breathing, equivalent to living at 4500m altitude, as protective in the setting of severe mitochondrial disease. First, I performed a genetic screen and found paradoxically, that hypoxic breathing and hypoxia responses are protective in mitochondrial disease. I then characterized the physiology and preclinical regimens of hypoxia therapy, laying the groundwork for translation to human patients. Fascinated by such a vital role for oxygen in human disease, I went on to define adaptive pathways in varying oxygen tensions. This work highlights the differential reliance on entire organelles at extreme oxygen levels. And finally, I studied the metabolic and proteomic consequences of defects in peroxisome metabolism and disease.",
    "advisors": ["Vamsi K. Mootha"],
    "text": "Oxygen, the invisible orchestrator of metabolism and disease : a focus on mitochondrial And peroxisomal dysfunction Variations in atmospheric oxygen levels can be traced over evolutionary time and track closely with the development of multicellular life, speciation events, appearance of placental mammals and the creation of a cardio-respiratory system. As the final electron acceptor for aerobic ATP production, oxygen allows energy-intensive metabolic pathways to exist. Furthermore, oxygen is the most utilized substrate for known biochemical reactions, surpassing even ATP and NAD+. As a result, variations in oxygen levels have far-reaching consequences on human physiology and health. Mitochondrial disorders are the most common inborn errors of metabolism, affecting approximately 1 in 5000 live births. Patients can present in infancy or adulthood with symptoms affecting multiple organ systems including blindness, deafness, muscle weakness, developmental delay and severe neurological impairment. Unfortunately, there are currently no proven therapies for mitochondrial disorders. My thesis work has focused on combining systems biology, animal physiology and cellular metabolism approaches to develop new therapies for these disorders. More specifically, I have identified hypoxic breathing, equivalent to living at 4500m altitude, as protective in the setting of severe mitochondrial disease. First, I performed a genetic screen and found paradoxically, that hypoxic breathing and hypoxia responses are protective in mitochondrial disease. I then characterized the physiology and preclinical regimens of hypoxia therapy, laying the groundwork for translation to human patients. Fascinated by such a vital role for oxygen in human disease, I went on to define adaptive pathways in varying oxygen tensions. This work highlights the differential reliance on entire organelles at extreme oxygen levels. And finally, I studied the metabolic and proteomic consequences of defects in peroxisome metabolism and disease."
}, {
    "id": "oai:dspace.mit.edu:1721.1/33680",
    "title": "Exploring genomic medicine using integrative biology",
    "abstract": "Instead of focusing on the cell, or the genotype, or on any single measurement modality, using integrative biology allows us to think holistically and horizontally. A disease like diabetes can lead to myocardial infarction, nephropathy, and neuropathy; to study diabetes in genomic medicine would require reasoning from a disease to all its various complications to the genome and back. I am studying the process of intersecting nearly-comprehensive data sets in molecular biology, across three representative modalities (microarrays, RNAi and quantitative trait loci) out of the more than 30 available today. This is difficult because the semantics and context of each experiment performed becomes more important, necessitating a detailed knowledge about the biological domain. I addressed this problem by using all public microarray data from NIH, unifying 50 million expression measurements with standard gene identifiers and representing the experimental context of each using the Unified Medical Language System, a vocabulary of over 1 million concepts. I created an automated system to join data sets related by experimental context.",
    "advisors": ["Isaac Kohane"],
    "text": "Exploring genomic medicine using integrative biology Instead of focusing on the cell, or the genotype, or on any single measurement modality, using integrative biology allows us to think holistically and horizontally. A disease like diabetes can lead to myocardial infarction, nephropathy, and neuropathy; to study diabetes in genomic medicine would require reasoning from a disease to all its various complications to the genome and back. I am studying the process of intersecting nearly-comprehensive data sets in molecular biology, across three representative modalities (microarrays, RNAi and quantitative trait loci) out of the more than 30 available today. This is difficult because the semantics and context of each experiment performed becomes more important, necessitating a detailed knowledge about the biological domain. I addressed this problem by using all public microarray data from NIH, unifying 50 million expression measurements with standard gene identifiers and representing the experimental context of each using the Unified Medical Language System, a vocabulary of over 1 million concepts. I created an automated system to join data sets related by experimental context."
}, {
    "id": "oai:dspace.mit.edu:1721.1/36166",
    "title": "Binaural interactions in the auditory midbrain with bilateral electric stimulation of the cochlea",
    "abstract": "Bilateral cochlear implantation seeks to restore the advantages of binaural hearing to the profoundly deaf by giving them access to binaural cues normally important for accurate sound localization and speech reception in noise. This thesis characterizes binaural interactions in auditory neurons using a cat model of bilateral cochlear implants. Single neuron responses in the inferior colliculus (IC), the main nucleus of the auditory midbrain, were studied using electric stimulation of bilaterally implanted intracochlear electrode arrays. Neural tuning to interaural timing difference (ITD) was emphasized since it is an important binaural cue and is well represented in IC neural responses. Stimulation parameters were explored in an effort to find stimuli that might result in the best ITD sensitivity for clinical use. The majority of IC neurons were found to be sensitive to ITD with low-rate constant-amplitude pulse trains. Electric ITD tuning was often as sharp as that with acoustic stimulation in normal-hearing animals, but many neurons had dynamic ranges of ITD sensitivity limited to a few decibels. Consistent with behavioral results in bilaterally implanted humans, neural ITD discrimination thresholds degraded with increasing pulse rates above 100 pulses per second (pps).",
    "advisors": ["Bertrand Delgutte"],
    "text": "Binaural interactions in the auditory midbrain with bilateral electric stimulation of the cochlea Bilateral cochlear implantation seeks to restore the advantages of binaural hearing to the profoundly deaf by giving them access to binaural cues normally important for accurate sound localization and speech reception in noise. This thesis characterizes binaural interactions in auditory neurons using a cat model of bilateral cochlear implants. Single neuron responses in the inferior colliculus (IC), the main nucleus of the auditory midbrain, were studied using electric stimulation of bilaterally implanted intracochlear electrode arrays. Neural tuning to interaural timing difference (ITD) was emphasized since it is an important binaural cue and is well represented in IC neural responses. Stimulation parameters were explored in an effort to find stimuli that might result in the best ITD sensitivity for clinical use. The majority of IC neurons were found to be sensitive to ITD with low-rate constant-amplitude pulse trains. Electric ITD tuning was often as sharp as that with acoustic stimulation in normal-hearing animals, but many neurons had dynamic ranges of ITD sensitivity limited to a few decibels. Consistent with behavioral results in bilaterally implanted humans, neural ITD discrimination thresholds degraded with increasing pulse rates above 100 pulses per second (pps)."
}, {
    "id": "oai:dspace.mit.edu:1721.1/87501",
    "title": "An intraperitoneal implantable drug delivery device for the treatment of ovarian cancer",
    "abstract": "Ovarian cancer is the fifth leading cause of cancer-related deaths in women and the deadliest gynecologic cancer. The current standard treatment for advanced ovarian cancer includes a minimally invasive cytoreduction surgery, followed by intravenous (IV) or intraperitoneal (IP) chemotherapy with cisplatin and taxol. Clinical trials showed that the IP cisplatin treatment regimen was able to prolong overall survival by 16 months but only 42% of subjects completed all cycles of the IP therapy. The primary reason for the early termination of the IP treatment is catheter-related complications. The implantation of the catheter is also a complex procedure that can only be performed at premier centers by trained personnel. An alternative for IP administration that eliminates catheter-related complications and simplifies IP drug administration would therefore allow more patients to enjoy the benefits of IP therapy. A drug delivery device for use in a mouse model was developed as a tool to prove that maintaining a low constant cisplatin concentration in the peritoneal cavity and serum would improve the treatment outcome and reduce drug-related toxicity in ovarian cancer, compared to periodic IP bolus drug infusion. The device demonstrated highly linear and easily tunable in vitro release and exhibited excellent in vitro-in vivo correlation. Investigations of the device pharmacokinetics in vivo proved that the device was able to maintain a low and constant cisplatin concentration both locally in the peritoneal cavity and in the serum over up to six weeks. In vitro cytotoxicity of continuous cisplatin dosing with various human ovarian cancer cells lines was demonstrated. An in vivo xenograft SKOV-3 tumor model was established and optimized to reflect the distribution of ovarian cancer metastases in humans. The device achieved effective tumor growth retardation without systemic toxicity. An IP bolus injection scheme with a similar area-under-curve (AUC), however, caused severe bone marrow depletion. The results verified that the treatment efficacy correlates with the AUC but not the peak concentration, Cma. These promising preclinical results highlight the potential of this new therapeutic regimen to change the course of ovarian cancer care and warrant the need for designing a human device before proceeding to human trials.",
    "advisors": ["Michael J. Cima"],
    "text": "An intraperitoneal implantable drug delivery device for the treatment of ovarian cancer Ovarian cancer is the fifth leading cause of cancer-related deaths in women and the deadliest gynecologic cancer. The current standard treatment for advanced ovarian cancer includes a minimally invasive cytoreduction surgery, followed by intravenous (IV) or intraperitoneal (IP) chemotherapy with cisplatin and taxol. Clinical trials showed that the IP cisplatin treatment regimen was able to prolong overall survival by 16 months but only 42% of subjects completed all cycles of the IP therapy. The primary reason for the early termination of the IP treatment is catheter-related complications. The implantation of the catheter is also a complex procedure that can only be performed at premier centers by trained personnel. An alternative for IP administration that eliminates catheter-related complications and simplifies IP drug administration would therefore allow more patients to enjoy the benefits of IP therapy. A drug delivery device for use in a mouse model was developed as a tool to prove that maintaining a low constant cisplatin concentration in the peritoneal cavity and serum would improve the treatment outcome and reduce drug-related toxicity in ovarian cancer, compared to periodic IP bolus drug infusion. The device demonstrated highly linear and easily tunable in vitro release and exhibited excellent in vitro-in vivo correlation. Investigations of the device pharmacokinetics in vivo proved that the device was able to maintain a low and constant cisplatin concentration both locally in the peritoneal cavity and in the serum over up to six weeks. In vitro cytotoxicity of continuous cisplatin dosing with various human ovarian cancer cells lines was demonstrated. An in vivo xenograft SKOV-3 tumor model was established and optimized to reflect the distribution of ovarian cancer metastases in humans. The device achieved effective tumor growth retardation without systemic toxicity. An IP bolus injection scheme with a similar area-under-curve (AUC), however, caused severe bone marrow depletion. The results verified that the treatment efficacy correlates with the AUC but not the peak concentration, Cma. These promising preclinical results highlight the potential of this new therapeutic regimen to change the course of ovarian cancer care and warrant the need for designing a human device before proceeding to human trials."
}, {
    "id": "oai:dspace.mit.edu:1721.1/70812",
    "title": "Online control of articulation based on auditory feedback in normal Speech and stuttering : behavioral and modeling studies",
    "abstract": "Articulation of multisyllabic speech requires a high degree of accuracy in controlling the spatial (positional) and the temporal parameters of articulatory movements. In stuttering, a disorder of speech fluency, failures to meet these control requirements occur frequently, leading to dysfluencies such as sound repetitions and prolongations. Currently, little is known about the sensorimotor mechanisms underlying the control of multisyllabic articulation and how they break down in stuttering. This dissertation is focused on the interaction between multisyllabic articulation and auditory feedback (AF), the perception of one's own speech sounds during speech production, which has been shown previously to play important roles in quasi-static articulations as well as in the mechanisms of stuttering. To investigate this topic empirically, we developed a digital signal processing platform for introducing flexible online perturbations of time-varying formants in speakers' AF during speech production. This platform was in a series of perturbation experiments, in which we aimed separately at elucidating the role of AF in controlling the spatial and temporal parameters of multisyllabic articulation. Under these perturbations of AF, normal subjects showed small but significant and specific online adjustments in the spatial and temporal parameters of articulation, which provided first evidence for a role of AF in the online fine-tuning of articulatory trajectories. To model and explain these findings, we designed and tested sqDIVA, a computational model for the sensory feedback-based control of speech movement timing. Test results indicated that this new model accurately accounted for the spatiotemporal compensation patterns observed in the perturbation experiments. In addition, we investigated empirically how the AF-based online speech motor control differed between people who stutter (PWS) and normal speakers. The PWS group showed compensatory responses significantly smaller in magnitude and slower in onset compared to the control subjects' responses. This under-compensation to AF perturbation was observed for both quasi-static vowels and multisyllabic speech, and for both the spatial and temporal control of articulation. This abnormal sensorimotor performance supports the hypothesis that stuttering involves deficits in the rapid internal transformations between the auditory and motor domains, with important implications for the neural basis of this disorder.",
    "advisors": ["Frank H. Guenther", "Joseph S. Perkell"],
    "text": "Online control of articulation based on auditory feedback in normal Speech and stuttering : behavioral and modeling studies Articulation of multisyllabic speech requires a high degree of accuracy in controlling the spatial (positional) and the temporal parameters of articulatory movements. In stuttering, a disorder of speech fluency, failures to meet these control requirements occur frequently, leading to dysfluencies such as sound repetitions and prolongations. Currently, little is known about the sensorimotor mechanisms underlying the control of multisyllabic articulation and how they break down in stuttering. This dissertation is focused on the interaction between multisyllabic articulation and auditory feedback (AF), the perception of one's own speech sounds during speech production, which has been shown previously to play important roles in quasi-static articulations as well as in the mechanisms of stuttering. To investigate this topic empirically, we developed a digital signal processing platform for introducing flexible online perturbations of time-varying formants in speakers' AF during speech production. This platform was in a series of perturbation experiments, in which we aimed separately at elucidating the role of AF in controlling the spatial and temporal parameters of multisyllabic articulation. Under these perturbations of AF, normal subjects showed small but significant and specific online adjustments in the spatial and temporal parameters of articulation, which provided first evidence for a role of AF in the online fine-tuning of articulatory trajectories. To model and explain these findings, we designed and tested sqDIVA, a computational model for the sensory feedback-based control of speech movement timing. Test results indicated that this new model accurately accounted for the spatiotemporal compensation patterns observed in the perturbation experiments. In addition, we investigated empirically how the AF-based online speech motor control differed between people who stutter (PWS) and normal speakers. The PWS group showed compensatory responses significantly smaller in magnitude and slower in onset compared to the control subjects' responses. This under-compensation to AF perturbation was observed for both quasi-static vowels and multisyllabic speech, and for both the spatial and temporal control of articulation. This abnormal sensorimotor performance supports the hypothesis that stuttering involves deficits in the rapid internal transformations between the auditory and motor domains, with important implications for the neural basis of this disorder."
}, {
    "id": "oai:dspace.mit.edu:1721.1/57543",
    "title": "Impact of human vocal fold vibratory asymmetries on acoustic characteristics of sustained vowel phonation",
    "abstract": "Clinical voice specialists make critical diagnostic, medical, therapeutic, and surgical decisions by coupling visual observations of vocal fold tissue motion with auditory-perceptual assessments of voice quality. The details of the relationship between vocal fold tissue motion and the voice produced are not fully understood, and there is recent evidence that the diagnostic significance of asymmetries during vocal fold vibration may be over-interpreted during clinical voice assessment. An automated system based on high-speed videoendoscopy recordings was developed to objectively quantify vocal fold vibratory asymmetry with initial validation from manual markings and visualperceptual judgments. Efficient estimation of these measures was possible due to recent technological advances in high-speed imaging of the larynx that enabled the capture and processing of high-resolution video (up to 10,000 images per second) of rapid vocal fold vibrations (100-1000 times per second). Synchronized recordings of the acoustic voice signal were made to explore physiological-acoustic relationships that were not possible using clinical stroboscopic imaging systems. In an initial study of asymmetric vibration in 14 patients treated for laryngeal cancer, perturbations in the voice signal were most associated with asymmetry that changed across vibratory cycles, while the overall level of asymmetry did not contribute to degradations in voice quality measures.",
    "advisors": ["Robert E. Hillman", "Thomas F. Quatieri"],
    "text": "Impact of human vocal fold vibratory asymmetries on acoustic characteristics of sustained vowel phonation Clinical voice specialists make critical diagnostic, medical, therapeutic, and surgical decisions by coupling visual observations of vocal fold tissue motion with auditory-perceptual assessments of voice quality. The details of the relationship between vocal fold tissue motion and the voice produced are not fully understood, and there is recent evidence that the diagnostic significance of asymmetries during vocal fold vibration may be over-interpreted during clinical voice assessment. An automated system based on high-speed videoendoscopy recordings was developed to objectively quantify vocal fold vibratory asymmetry with initial validation from manual markings and visualperceptual judgments. Efficient estimation of these measures was possible due to recent technological advances in high-speed imaging of the larynx that enabled the capture and processing of high-resolution video (up to 10,000 images per second) of rapid vocal fold vibrations (100-1000 times per second). Synchronized recordings of the acoustic voice signal were made to explore physiological-acoustic relationships that were not possible using clinical stroboscopic imaging systems. In an initial study of asymmetric vibration in 14 patients treated for laryngeal cancer, perturbations in the voice signal were most associated with asymmetry that changed across vibratory cycles, while the overall level of asymmetry did not contribute to degradations in voice quality measures."
}, {
    "id": "oai:dspace.mit.edu:1721.1/45624",
    "title": "Privacy and identifiability in clinical research, personalized medicine, and public health surveillance",
    "abstract": "Electronic transmission of protected health information has become pervasive in research, clinical, and public health investigations, posing substantial risk to patient privacy. From clinical genetic screenings to publication of data in research studies, these activities have the potential to disclose identity, medical conditions, and hereditary data. To enable an era of personalized medicine, many research studies are attempting to correlate individual clinical outcomes with genomic data, leading to thousands of new investigations. Critical to the success of many of these studies is research participation by individuals who are willing to share their genotypic and clinical data with investigators, necessitating methods and policies that preserve privacy with such disclosures. We explore quantitative models that allow research participants, patients and investigators to fully understand these complex privacy risks when disclosing medical data. This modeling will improve the informed consent and risk assessment process, for both demographic and medical data, each with distinct domain-specific scenarios. We first discuss the disclosure risk for genomic data, investigating both the risk of re-identification for SNPs and mutations, as well as the disclosure impact on family members. Next, the deidentification and anonymization of geospatial datasets containing information about patient home addresses will be examined, using mathematical skewing algorithms as well as a linear programming approach. Finally, we consider the re-identification potential of geospatial data, commonly shared in both textual form and in printed maps in journals and public health practice. We also explore methods to quantify the anonymity afforded when using these anonymization techniques.",
    "advisors": ["Peter Szolovits"],
    "text": "Privacy and identifiability in clinical research, personalized medicine, and public health surveillance Electronic transmission of protected health information has become pervasive in research, clinical, and public health investigations, posing substantial risk to patient privacy. From clinical genetic screenings to publication of data in research studies, these activities have the potential to disclose identity, medical conditions, and hereditary data. To enable an era of personalized medicine, many research studies are attempting to correlate individual clinical outcomes with genomic data, leading to thousands of new investigations. Critical to the success of many of these studies is research participation by individuals who are willing to share their genotypic and clinical data with investigators, necessitating methods and policies that preserve privacy with such disclosures. We explore quantitative models that allow research participants, patients and investigators to fully understand these complex privacy risks when disclosing medical data. This modeling will improve the informed consent and risk assessment process, for both demographic and medical data, each with distinct domain-specific scenarios. We first discuss the disclosure risk for genomic data, investigating both the risk of re-identification for SNPs and mutations, as well as the disclosure impact on family members. Next, the deidentification and anonymization of geospatial datasets containing information about patient home addresses will be examined, using mathematical skewing algorithms as well as a linear programming approach. Finally, we consider the re-identification potential of geospatial data, commonly shared in both textual form and in printed maps in journals and public health practice. We also explore methods to quantify the anonymity afforded when using these anonymization techniques."
}, {
    "id": "oai:dspace.mit.edu:1721.1/95864",
    "title": "Characterization of cochlear transcription, translation and energy extraction in aging and noise-induced pathology",
    "abstract": "Success in otologic practice is currently limited by the diagnostic tools and treatment options available to address an individual's specific presentation of hearing loss. This limitation results from insufficient characterization of the inner ear's biochemical environment as well as physical hurdles associated with accessing inner ear tissues. The encapsulation of the hearing organ within a bony shell and delicate nature of its tissues make standard tissue biopsy techniques impossible and leave many imaging methods impractical. This thesis sought to approach these clinical limitations in two ways: (1) performing novel transcriptional and translational characterizations of inner ear tissues and (2) development of a novel technique to access and communicate diagnostic information from within the inner ear. The first part of this thesis employs whole transcriptome shotgun sequencing to study murine inner ear transcriptional activity in young, healthy animals as well as changes associated with organ aging and noise-induced auditory neuropathy, an important mechanism of hearing impairment in humans. Knowledge of the inner ear's transcriptional behavior (Part I) is coupled with novel translational insights provided by high-throughput tandem mass-spectrometry (Part III) studies of human inner ear fluids obtained from healthy and pathologic populations. These studies illuminate homeostatic mechanisms employed by the highly specialized inner ear tissues, providing a critical knowledge-base for inner ear scientists and pharmacologists, and identify important expression-level changes which occur during the onset and progression of inner ear pathologies. While these high-throughput studies offer the powerful ability to gain a wealth of knowledge into which genes are active within the inner ear, functional assessment of the specific role these genes play must be assessed in a more focused manner. Phenotypic characterization of mice with specific genetic mutations (Part II) has been performed to provide critical insight into the specific role Fgf23 plays in development and maintenance of the auditory system. The second arm of this thesis seeks to provide clinical practicality to the above work by developing a method to safely access the inner ear environment to gather and communicate diagnostic information (Part IV). A guinea pig model was utilized to develop an approach to insert microelectrodes into the fluid spaces of the inner ear in order to harness and monitor the natural electrochemical gradient of the organ. The useful energy extracted from this \"biological battery\" was used to power a combined microchip/radio transmitter capable of performing voltage-sensing operations within endolymph and wirelessly relaying this information to an external receiver. This study was the first to utilize a mammalian electrochemical potential to power an electronic device. By performing this task while preserving the integrity of the hearing organ this work provides the first, critical proof-of-concept demonstration toward clinically-applicable sensing and therapeutic devices powered by the inner ear. Further refinement of this technique into a long-term, fully-implantable device will enable previously impossible longitudinal studies of organ behavior in awake, behaving subjects and the incorporation of sensing modalities into current inner ear prostheses to monitor biochemical changes and maximize patient benefits.",
    "advisors": ["Konstantina M. Stankovi"],
    "text": "Characterization of cochlear transcription, translation and energy extraction in aging and noise-induced pathology Success in otologic practice is currently limited by the diagnostic tools and treatment options available to address an individual's specific presentation of hearing loss. This limitation results from insufficient characterization of the inner ear's biochemical environment as well as physical hurdles associated with accessing inner ear tissues. The encapsulation of the hearing organ within a bony shell and delicate nature of its tissues make standard tissue biopsy techniques impossible and leave many imaging methods impractical. This thesis sought to approach these clinical limitations in two ways: (1) performing novel transcriptional and translational characterizations of inner ear tissues and (2) development of a novel technique to access and communicate diagnostic information from within the inner ear. The first part of this thesis employs whole transcriptome shotgun sequencing to study murine inner ear transcriptional activity in young, healthy animals as well as changes associated with organ aging and noise-induced auditory neuropathy, an important mechanism of hearing impairment in humans. Knowledge of the inner ear's transcriptional behavior (Part I) is coupled with novel translational insights provided by high-throughput tandem mass-spectrometry (Part III) studies of human inner ear fluids obtained from healthy and pathologic populations. These studies illuminate homeostatic mechanisms employed by the highly specialized inner ear tissues, providing a critical knowledge-base for inner ear scientists and pharmacologists, and identify important expression-level changes which occur during the onset and progression of inner ear pathologies. While these high-throughput studies offer the powerful ability to gain a wealth of knowledge into which genes are active within the inner ear, functional assessment of the specific role these genes play must be assessed in a more focused manner. Phenotypic characterization of mice with specific genetic mutations (Part II) has been performed to provide critical insight into the specific role Fgf23 plays in development and maintenance of the auditory system. The second arm of this thesis seeks to provide clinical practicality to the above work by developing a method to safely access the inner ear environment to gather and communicate diagnostic information (Part IV). A guinea pig model was utilized to develop an approach to insert microelectrodes into the fluid spaces of the inner ear in order to harness and monitor the natural electrochemical gradient of the organ. The useful energy extracted from this \"biological battery\" was used to power a combined microchip/radio transmitter capable of performing voltage-sensing operations within endolymph and wirelessly relaying this information to an external receiver. This study was the first to utilize a mammalian electrochemical potential to power an electronic device. By performing this task while preserving the integrity of the hearing organ this work provides the first, critical proof-of-concept demonstration toward clinically-applicable sensing and therapeutic devices powered by the inner ear. Further refinement of this technique into a long-term, fully-implantable device will enable previously impossible longitudinal studies of organ behavior in awake, behaving subjects and the incorporation of sensing modalities into current inner ear prostheses to monitor biochemical changes and maximize patient benefits."
}, {
    "id": "oai:dspace.mit.edu:1721.1/65519",
    "title": "A general method for studying autocrine signaling and its impact on cancer cell growth",
    "abstract": "Autocrine signaling plays essential roles in providing self-sustaining growth signals to cancer cells. Since the introduction of the autocrine hypothesis in 1980s, the contribution of autocrine signaling in cancer medicine has been limited to cancer tissues with adequately characterized mitogenic pathways. Its closed-loop nature and complex interplay with other environmental cues prevents the experimental study of unknown autocrine loops, requiring specific perturbing agents to inhibit the underlying ligand/receptor interactions. Recent studies reported the ability of drug-resistant cancer cells to acquire mitogenic signals from previously neglected autocrine loops, causing tumor recurrence. Methods that can evaluate autocrine-loop dependency in more diverse cancer tissues will help identify other means that autocrine signaling employs to maintain cancer growth. This thesis presents the use of cell-patterning methods as a tool for modulating intrinsically generated diffusive signaling cues. Such technology enables the investigation of autocrine loops without the need for specific therapeutics or prior knowledge of underlying ligand/receptor pairs. To achieve this goal, the first aim of this thesis is to determine characteristics of autocrine signaling that pertain to modulation of intercellular spacing, using existing investigation methods. In addition to demonstrating the limitation of conventional methods in examining unknown autocrine loops, we showed that changes of intercellular spacing in randomly plated culture cannot specifically modulate autocrine activity, due to the concurrent changes of other environmental cues. The second aim of this thesis is to establish engineering tools for 1) ensuring modification of only autocrine loops with the modulated cell arrangement and 2) providing prediction of autocrine activity changes with varying intercellular spacing. We illustrated cell-patteming approaches for introducing spatial regularity to standard cell culture. We then developed a stochastic model to predict changes of ligand/receptor binding with varying cell arrangement designs. We determined the spatial requirement for autocrine activity to transition from the isolated to the communicative mode. The model also helps determine cell-patterning designs that can potentially maintain uniform impacts of non-diffusive signaling cues while enabling specific modulation of autocrine signaling. In the last aim of this thesis, we evaluated the ability of regularly-shaped cell arrays to demonstrate the impact of autocrine signaling in supporting cancer growth. In comparison to randomly-plated culture, the cellpatterning platform exhibited growth change with altering intercellular spacing that better corresponds with the predicted and measured changes of autocrine ligand capture. With increasing global cell density, we also showed that regularly-shaped cell arrays acquire more uniform distribution of local cell density, while the randomly-plated cells exhibit distinct changes of local cell density. We present in this thesis the first method for the modulation of combined autocrine activity while ensuring minimal concurrent alteration of non-diffusive cues without the need of specific perturbing agents.",
    "advisors": ["Joel Voldman"],
    "text": "A general method for studying autocrine signaling and its impact on cancer cell growth Autocrine signaling plays essential roles in providing self-sustaining growth signals to cancer cells. Since the introduction of the autocrine hypothesis in 1980s, the contribution of autocrine signaling in cancer medicine has been limited to cancer tissues with adequately characterized mitogenic pathways. Its closed-loop nature and complex interplay with other environmental cues prevents the experimental study of unknown autocrine loops, requiring specific perturbing agents to inhibit the underlying ligand/receptor interactions. Recent studies reported the ability of drug-resistant cancer cells to acquire mitogenic signals from previously neglected autocrine loops, causing tumor recurrence. Methods that can evaluate autocrine-loop dependency in more diverse cancer tissues will help identify other means that autocrine signaling employs to maintain cancer growth. This thesis presents the use of cell-patterning methods as a tool for modulating intrinsically generated diffusive signaling cues. Such technology enables the investigation of autocrine loops without the need for specific therapeutics or prior knowledge of underlying ligand/receptor pairs. To achieve this goal, the first aim of this thesis is to determine characteristics of autocrine signaling that pertain to modulation of intercellular spacing, using existing investigation methods. In addition to demonstrating the limitation of conventional methods in examining unknown autocrine loops, we showed that changes of intercellular spacing in randomly plated culture cannot specifically modulate autocrine activity, due to the concurrent changes of other environmental cues. The second aim of this thesis is to establish engineering tools for 1) ensuring modification of only autocrine loops with the modulated cell arrangement and 2) providing prediction of autocrine activity changes with varying intercellular spacing. We illustrated cell-patteming approaches for introducing spatial regularity to standard cell culture. We then developed a stochastic model to predict changes of ligand/receptor binding with varying cell arrangement designs. We determined the spatial requirement for autocrine activity to transition from the isolated to the communicative mode. The model also helps determine cell-patterning designs that can potentially maintain uniform impacts of non-diffusive signaling cues while enabling specific modulation of autocrine signaling. In the last aim of this thesis, we evaluated the ability of regularly-shaped cell arrays to demonstrate the impact of autocrine signaling in supporting cancer growth. In comparison to randomly-plated culture, the cellpatterning platform exhibited growth change with altering intercellular spacing that better corresponds with the predicted and measured changes of autocrine ligand capture. With increasing global cell density, we also showed that regularly-shaped cell arrays acquire more uniform distribution of local cell density, while the randomly-plated cells exhibit distinct changes of local cell density. We present in this thesis the first method for the modulation of combined autocrine activity while ensuring minimal concurrent alteration of non-diffusive cues without the need of specific perturbing agents."
}, {
    "id": "oai:dspace.mit.edu:1721.1/57880",
    "title": "The grapefruit flavonoid naringenin as a Hepatitis C virus therapy : efficacy, mechanism and delivery",
    "abstract": "Hepatitis C virus (HCV) infection accounts for approximately 40% of chronic liver disease in the United States and results in an estimated 8,000-10,000 deaths annually. Simulations suggest that in the next decade morbidity and mortality associated with HCV infections will result in approximately 200,000 deaths and direct medical expenditures of over $10 billion. Furthermore, recent WHO estimates of worldwide prevalence suggest that up to 2% of the world population is infected with HCV, representing between 120 and 200 million people. For reasons that are still poorly understood, the current standard of care is effective only in a subset of patients, and depends on both patient-related and disease-related characteristics. Sustained virological response (SVR) - HCV RNA in patient plasma drops below detectable levels at week 24 following completion of treatment, which is thought to be indicative of curing the disease - is attainable in only ~50% of patients. In recent years, HCV production has been shown to be inextricably linked to lipid metabolism and to the secretion of very low density lipoproteins (VLDL) from hepatocytes. This suggests that by modulating lipid metabolism in the cell, viral production may be reduced in a clinically relevant manner.",
    "advisors": ["Martin L. Yarmush", "Yaakov Nahmias"],
    "text": "The grapefruit flavonoid naringenin as a Hepatitis C virus therapy : efficacy, mechanism and delivery Hepatitis C virus (HCV) infection accounts for approximately 40% of chronic liver disease in the United States and results in an estimated 8,000-10,000 deaths annually. Simulations suggest that in the next decade morbidity and mortality associated with HCV infections will result in approximately 200,000 deaths and direct medical expenditures of over $10 billion. Furthermore, recent WHO estimates of worldwide prevalence suggest that up to 2% of the world population is infected with HCV, representing between 120 and 200 million people. For reasons that are still poorly understood, the current standard of care is effective only in a subset of patients, and depends on both patient-related and disease-related characteristics. Sustained virological response (SVR) - HCV RNA in patient plasma drops below detectable levels at week 24 following completion of treatment, which is thought to be indicative of curing the disease - is attainable in only ~50% of patients. In recent years, HCV production has been shown to be inextricably linked to lipid metabolism and to the secretion of very low density lipoproteins (VLDL) from hepatocytes. This suggests that by modulating lipid metabolism in the cell, viral production may be reduced in a clinically relevant manner."
}, {
    "id": "oai:dspace.mit.edu:1721.1/117896",
    "title": "Evaluation of the Mark III spacesuit : an experimental and computational modeling approach",
    "abstract": "Spacesuit Assemblies (SSAs) provide life support for human operators performing extravehicular activities (EVAs). The overall goal of this research was to investigate three research questions to address gaps in the field of spacesuit assembly (SSA) evaluations: [1] What are the mobility and agility limitations causing operators to experience performance decrements when wearing a SSA?; [2] What is causing operators to experience increased joint torques?; and [3] How does the distributed weight of an SSA, transferred to the operator, affect performance? This research leveraged both experimental and computational modeling capabilities to evaluate SSAs with a human-centered focus, in ways previously unachievable. The space suit evaluated for this research was NASA's Mark III (MkIII) Planetary Technology Demonstrator SSA, built to test the next generation in planetary exploration capabilities, improving upon Apollo era technology. The hip brief assembly (HBA) is built with three nested bearings, each with a single rotational degree of freedom that together provide the range of motion, walking efficiency, and kneeling capabilities. An initial investigation, combining a pilot study and supporting modeling, revealed limitations in the current human-SSA system that may impair the operator's mobility/stability and agility. Limitations identified and investigated in this thesis include SSA degrees of freedom (DOFs), the SSA range of motion (ROM) envelope, the bearing resistances, the SSA component's inertial effects, the SSA mass load transfer dynamics, and suit fit. The SSA architecture was modeled as part of the thesis, creating a tool that was useful in the investigation of the human-suit system. The model relied on SSA component geometries and inherent mass/inertia and bearing resistance characteristics to output joint dynamics, rather than requiring those dynamics as an input (which would require extensive experimental setups). The model was used to isolate components that contribute to the measured operator performance degradations and to quantify the extent of their contributions. These investigations lead to suggestions for design requirements and evaluation techniques that can guide future SSA development and evaluations.",
    "advisors": ["Leia Stirling"],
    "text": "Evaluation of the Mark III spacesuit : an experimental and computational modeling approach Spacesuit Assemblies (SSAs) provide life support for human operators performing extravehicular activities (EVAs). The overall goal of this research was to investigate three research questions to address gaps in the field of spacesuit assembly (SSA) evaluations: [1] What are the mobility and agility limitations causing operators to experience performance decrements when wearing a SSA?; [2] What is causing operators to experience increased joint torques?; and [3] How does the distributed weight of an SSA, transferred to the operator, affect performance? This research leveraged both experimental and computational modeling capabilities to evaluate SSAs with a human-centered focus, in ways previously unachievable. The space suit evaluated for this research was NASA's Mark III (MkIII) Planetary Technology Demonstrator SSA, built to test the next generation in planetary exploration capabilities, improving upon Apollo era technology. The hip brief assembly (HBA) is built with three nested bearings, each with a single rotational degree of freedom that together provide the range of motion, walking efficiency, and kneeling capabilities. An initial investigation, combining a pilot study and supporting modeling, revealed limitations in the current human-SSA system that may impair the operator's mobility/stability and agility. Limitations identified and investigated in this thesis include SSA degrees of freedom (DOFs), the SSA range of motion (ROM) envelope, the bearing resistances, the SSA component's inertial effects, the SSA mass load transfer dynamics, and suit fit. The SSA architecture was modeled as part of the thesis, creating a tool that was useful in the investigation of the human-suit system. The model relied on SSA component geometries and inherent mass/inertia and bearing resistance characteristics to output joint dynamics, rather than requiring those dynamics as an input (which would require extensive experimental setups). The model was used to isolate components that contribute to the measured operator performance degradations and to quantify the extent of their contributions. These investigations lead to suggestions for design requirements and evaluation techniques that can guide future SSA development and evaluations."
}, {
    "id": "oai:dspace.mit.edu:1721.1/72917",
    "title": "Platforms for exploring host-pathogen interactions in hepatitis C virus infection",
    "abstract": "Afflicting almost 200 million worldwide, hepatitis C virus (HCV) mounts a chronic infection of liver hepatocytes that causes substantial morbidity and mortality. An understanding of host-virus interactions will drive the development of therapeutics, but research is restrained by available experimental tools. Due to the cost and unreliability of existing humanized mouse and primate in vivo models, HCV research is almost exclusively performed using in vitro platforms which suffer from three major limitations. First, challenges in primary hepatocyte culture and the general non-permissiveness of liver cell lines have necessitated the use of a uniquely permissive hepatoma line derived from a single donor, questioning the generalizability of findings to the broader population. Second, this cell line deviates appreciably from native liver in functions central to HCV infection, including innate immune signaling, polarization, and proliferation. Third, infection is typically studied using bulk assays with suboptimal specificity, sensitivity, and content. Here, we describe three technologies for overcoming these limitations in the study of host-virus interactions. We demonstrate their utility in exploring innate immune signaling, a clinically significant component of HCV pathogenesis. Section I describes an in vitro platform for investigating inter-host variations in the natural history of infection and treatment response. We show that directed differentiation of induced pluripotent stem cells (iPSCs) yields patient-specific liver tissue that is permissive to HCV and responds to infection with a robust innate immune response, opening the door to \"personalizing\" the study and treatment of infection. In Section II, we demonstrate that tissue-engineered, micropatterned co-cultures (MPCCs) of primary hepatocytes and supportive stroma are permissive to HCV, enabling investigations in a more natural host. We then show that innate immune signaling curtails infection in this model, and that its inhibition enhances infection 2-3 orders of magnitude. Lastly, we use MPCCs to uncover a novel liver immunoregulatory mechanism whereby innate immune surveillance is depressed, permitting efficient replication of hepatotropic pathogens. Finally, Section III details a high-content imaging assay that enables visualization and enumeration of single viral genomes in individual cells. We demonstrate that single-cell, multiplexed quantification of viral genomes and host gene transcripts can be used to dissect host-virus interactions, yielding an unexpected positive correlation between stage of infection and response to an innate immune cytokine. The solutions described here will enable the pursuit of previously intractable research questions for HCV and other viruses, accelerating progress towards the development of antivirals and vaccines. Further, the insights gained regarding the interplay between HCV and innate immunity have important clinical ramifications, including a novel therapeutic strategy.",
    "advisors": ["Sangeeta N. Bhatia"],
    "text": "Platforms for exploring host-pathogen interactions in hepatitis C virus infection Afflicting almost 200 million worldwide, hepatitis C virus (HCV) mounts a chronic infection of liver hepatocytes that causes substantial morbidity and mortality. An understanding of host-virus interactions will drive the development of therapeutics, but research is restrained by available experimental tools. Due to the cost and unreliability of existing humanized mouse and primate in vivo models, HCV research is almost exclusively performed using in vitro platforms which suffer from three major limitations. First, challenges in primary hepatocyte culture and the general non-permissiveness of liver cell lines have necessitated the use of a uniquely permissive hepatoma line derived from a single donor, questioning the generalizability of findings to the broader population. Second, this cell line deviates appreciably from native liver in functions central to HCV infection, including innate immune signaling, polarization, and proliferation. Third, infection is typically studied using bulk assays with suboptimal specificity, sensitivity, and content. Here, we describe three technologies for overcoming these limitations in the study of host-virus interactions. We demonstrate their utility in exploring innate immune signaling, a clinically significant component of HCV pathogenesis. Section I describes an in vitro platform for investigating inter-host variations in the natural history of infection and treatment response. We show that directed differentiation of induced pluripotent stem cells (iPSCs) yields patient-specific liver tissue that is permissive to HCV and responds to infection with a robust innate immune response, opening the door to \"personalizing\" the study and treatment of infection. In Section II, we demonstrate that tissue-engineered, micropatterned co-cultures (MPCCs) of primary hepatocytes and supportive stroma are permissive to HCV, enabling investigations in a more natural host. We then show that innate immune signaling curtails infection in this model, and that its inhibition enhances infection 2-3 orders of magnitude. Lastly, we use MPCCs to uncover a novel liver immunoregulatory mechanism whereby innate immune surveillance is depressed, permitting efficient replication of hepatotropic pathogens. Finally, Section III details a high-content imaging assay that enables visualization and enumeration of single viral genomes in individual cells. We demonstrate that single-cell, multiplexed quantification of viral genomes and host gene transcripts can be used to dissect host-virus interactions, yielding an unexpected positive correlation between stage of infection and response to an innate immune cytokine. The solutions described here will enable the pursuit of previously intractable research questions for HCV and other viruses, accelerating progress towards the development of antivirals and vaccines. Further, the insights gained regarding the interplay between HCV and innate immunity have important clinical ramifications, including a novel therapeutic strategy."
}, {
    "id": "oai:dspace.mit.edu:1721.1/33841",
    "title": "The role of HIF-1 alpha in the localization of embryonic stem cells with respect to hypoxia within teratomas",
    "abstract": "In embryonic stem (ES) cell tumors, the hypoxia-inducible transcription factor, HIF- 1[alpha], has been shown to be a tumor suppressor, and HIF-1[alpha]-expressing cells have been shown to localize preferentially in vivo to regions near tumor vasculature. These differences were proposed to be due to increased hypoxia-induced apoptosis and growth arrest of HIF-1[alpha]-expressing ES cells. This thesis presents a careful investigation into the localization of ES cells in vitro and in vivo with respect to hypoxia. A sandwich culture system was utilized in which controlled gradients of oxygen and nutrients are developed in the vicinity of the tumor cells. A diffusion-consumption model was utilized to predict the oxygen and glucose concentration profiles within the system. Oxygen and glucose consumption rates were measured and used as inputs into the model, and the concentration profiles were found to depend on a single experimental parameter, the cell density within the system. The optimum cell density was found in which stable, measurable oxygen gradients develop over 2-3 mm. The model demonstrated excellent agreement between the predicted oxygen concentration profiles and experimentally determined oxygen gradients. In vitro, there was no difference in localization with respect to hypoxia between tumor cells expressing or lacking HIF-1[alpha].",
    "advisors": ["Rakesh K. Jain"],
    "text": "The role of HIF-1 alpha in the localization of embryonic stem cells with respect to hypoxia within teratomas In embryonic stem (ES) cell tumors, the hypoxia-inducible transcription factor, HIF- 1[alpha], has been shown to be a tumor suppressor, and HIF-1[alpha]-expressing cells have been shown to localize preferentially in vivo to regions near tumor vasculature. These differences were proposed to be due to increased hypoxia-induced apoptosis and growth arrest of HIF-1[alpha]-expressing ES cells. This thesis presents a careful investigation into the localization of ES cells in vitro and in vivo with respect to hypoxia. A sandwich culture system was utilized in which controlled gradients of oxygen and nutrients are developed in the vicinity of the tumor cells. A diffusion-consumption model was utilized to predict the oxygen and glucose concentration profiles within the system. Oxygen and glucose consumption rates were measured and used as inputs into the model, and the concentration profiles were found to depend on a single experimental parameter, the cell density within the system. The optimum cell density was found in which stable, measurable oxygen gradients develop over 2-3 mm. The model demonstrated excellent agreement between the predicted oxygen concentration profiles and experimentally determined oxygen gradients. In vitro, there was no difference in localization with respect to hypoxia between tumor cells expressing or lacking HIF-1[alpha]."
}, {
    "id": "oai:dspace.mit.edu:1721.1/84409",
    "title": "Biophysical modeling of hemodynamic-based neuroimaging techniques",
    "abstract": "Two different hemodynamic-based neuroimaging techniques were studied in this work. Near-Infrared Spectroscopy (NIRS) is a promising technique to measure cerebral hemodynamics in a clinical setting due to its potential for continuous monitoring. However, the presence of strong systemic interference in the signal significantly limits our ability to recover the hemodynamic response without averaging tens of trials. Developing a new methodology to clean the NIRS signal from systemic interference and isolate the cortical signal would therefore significantly increase our ability to recover the hemodynamic response opening the door for clinical NIRS studies such as epilepsy. Toward this goal, a new method based on multi-distance measurements and state-space modeling was developed and further optimized to remove systemic physiological oscillations contaminating the NIRS signal. Furthermore, the cortical and pial contributions to the NIRS signal were quantified using a new multimodal regression analysis. Functional Magnetic Resonance Imaging (fMRI) based on the Blood Oxygenation Level Dependent (BOLD) response has become the method of choice for exploring brain function, and yet the physiological basis of this technique is still poorly understood. Despite the effort, a detailed and validated model relating the signal measured to the physiological changes occurring in the cortical tissue is still lacking. Modeling the BOLD signal is challenging because of the difficulty to take into account the complex morphology of the cortical microvasculature, the distribution of oxygen in those microvessels and its dynamics during neuronal activation. Here, we overcome this difficulty by performing Monte Carlo simulations over real microvascular networks and oxygen distributions measured in vivo on rodents, at rest and during forepaw stimulation, using two-photon microscopy. Our model reveals for the first time the specific contribution of individual vascular compartment to the BOLD signal, for different field strengths and different cortical orientations. Our model makes a new prediction: the amplitude of the BOLD signal produced by a given physiological change during neuronal activation depends on the spatial orientation of the cortical region in the MRI scanner. This occurs because veins are preferentially oriented either perpendicular or parallel to the cortical surface in the gray matter.",
    "advisors": ["David A. Boas"],
    "text": "Biophysical modeling of hemodynamic-based neuroimaging techniques Two different hemodynamic-based neuroimaging techniques were studied in this work. Near-Infrared Spectroscopy (NIRS) is a promising technique to measure cerebral hemodynamics in a clinical setting due to its potential for continuous monitoring. However, the presence of strong systemic interference in the signal significantly limits our ability to recover the hemodynamic response without averaging tens of trials. Developing a new methodology to clean the NIRS signal from systemic interference and isolate the cortical signal would therefore significantly increase our ability to recover the hemodynamic response opening the door for clinical NIRS studies such as epilepsy. Toward this goal, a new method based on multi-distance measurements and state-space modeling was developed and further optimized to remove systemic physiological oscillations contaminating the NIRS signal. Furthermore, the cortical and pial contributions to the NIRS signal were quantified using a new multimodal regression analysis. Functional Magnetic Resonance Imaging (fMRI) based on the Blood Oxygenation Level Dependent (BOLD) response has become the method of choice for exploring brain function, and yet the physiological basis of this technique is still poorly understood. Despite the effort, a detailed and validated model relating the signal measured to the physiological changes occurring in the cortical tissue is still lacking. Modeling the BOLD signal is challenging because of the difficulty to take into account the complex morphology of the cortical microvasculature, the distribution of oxygen in those microvessels and its dynamics during neuronal activation. Here, we overcome this difficulty by performing Monte Carlo simulations over real microvascular networks and oxygen distributions measured in vivo on rodents, at rest and during forepaw stimulation, using two-photon microscopy. Our model reveals for the first time the specific contribution of individual vascular compartment to the BOLD signal, for different field strengths and different cortical orientations. Our model makes a new prediction: the amplitude of the BOLD signal produced by a given physiological change during neuronal activation depends on the spatial orientation of the cortical region in the MRI scanner. This occurs because veins are preferentially oriented either perpendicular or parallel to the cortical surface in the gray matter."
}, {
    "id": "oai:dspace.mit.edu:1721.1/79245",
    "title": "Noise-induced cochlear neuronal degeneration and its role in hyperacusis -- and tinnitus-like behavior",
    "abstract": "Perceptual abnormalities such as hyperacusis and tinnitus often occur following acoustic overexposure. Although such exposure can also result in permanent threshold elevation, some individuals with noise-induced hyperacusis or tinnitus show clinically normal thresholds. Recent work in animals has shown that noise exposure can cause permanent degeneration of the cochlear nerve despite complete threshold recovery and lack of hair cell damage (Kujawa and Liberman, J Neurosci 29:14077-14085, 2009). Here we ask whether this noise-induced primary neuronal degeneration results in abnormal auditory behavior, indexed by the acoustic startle response and prepulse inhibition (PPI) of startle. Responses to tones and to broadband noise were measured in mice exposed either to a neuropathic exposure causing primary neuronal degeneration, or to a lower intensity, nonneuropathic noise, and in unexposed controls. Mice with cochlear neuronal loss displayed hyper-responsivity to sound, as evidenced by lower startle thresholds and enhanced PPI, while exposed mice without neuronal loss showed control-like responses. Gap PPI tests, often used to assess tinnitus, revealed spectrally restricted, as well as broadband, gap-detection deficits in mice with primary neuronal degeneration, but not in exposed mice without neuropathy. Crossmodal PPI tests and behavioral assays of anxiety revealed no significant differences among groups, suggesting that the changes in startle-based auditory behavior reflect a neuropathyrelated alteration specifically of auditory neural pathways. Despite significantly reduced cochlear nerve response, seen as reduced wave 1 of the auditory brainstem response, later peaks were unchanged or enhanced, suggesting neural hyperactivity in the auditory brainstem that could underlie the abnormal behavior on the startle tests. Taken together, the results suggest a role for cochlear primary neuronal degeneration in central neural excitability and, by extension, in the generation of tinnitus and hyperacusis.",
    "advisors": ["M. Charles Liberman"],
    "text": "Noise-induced cochlear neuronal degeneration and its role in hyperacusis -- and tinnitus-like behavior Perceptual abnormalities such as hyperacusis and tinnitus often occur following acoustic overexposure. Although such exposure can also result in permanent threshold elevation, some individuals with noise-induced hyperacusis or tinnitus show clinically normal thresholds. Recent work in animals has shown that noise exposure can cause permanent degeneration of the cochlear nerve despite complete threshold recovery and lack of hair cell damage (Kujawa and Liberman, J Neurosci 29:14077-14085, 2009). Here we ask whether this noise-induced primary neuronal degeneration results in abnormal auditory behavior, indexed by the acoustic startle response and prepulse inhibition (PPI) of startle. Responses to tones and to broadband noise were measured in mice exposed either to a neuropathic exposure causing primary neuronal degeneration, or to a lower intensity, nonneuropathic noise, and in unexposed controls. Mice with cochlear neuronal loss displayed hyper-responsivity to sound, as evidenced by lower startle thresholds and enhanced PPI, while exposed mice without neuronal loss showed control-like responses. Gap PPI tests, often used to assess tinnitus, revealed spectrally restricted, as well as broadband, gap-detection deficits in mice with primary neuronal degeneration, but not in exposed mice without neuropathy. Crossmodal PPI tests and behavioral assays of anxiety revealed no significant differences among groups, suggesting that the changes in startle-based auditory behavior reflect a neuropathyrelated alteration specifically of auditory neural pathways. Despite significantly reduced cochlear nerve response, seen as reduced wave 1 of the auditory brainstem response, later peaks were unchanged or enhanced, suggesting neural hyperactivity in the auditory brainstem that could underlie the abnormal behavior on the startle tests. Taken together, the results suggest a role for cochlear primary neuronal degeneration in central neural excitability and, by extension, in the generation of tinnitus and hyperacusis."
}, {
    "id": "oai:dspace.mit.edu:1721.1/39575",
    "title": "Phonological and semantic influences on auditory word perception in children with and without reading impairments using magnetoencephalography (MEG) and electroencephalography (EEG)",
    "abstract": "Children with dyslexia struggle with learning to read despite adequate intelligence, motivation, and schooling. Over the years, there has been a growing consensus about the role of phonological processing in reading disability. Poor readers typically do worse than their normal reading peers on tasks that require phonological processing which has been linked, directly or indirectly, to their speech perception abilities. The work in this thesis combined behavioral, MEG, and EEG methods to examine how normal and reading-impaired children, 7-13 years of age, perceive speech under varying degrees of phonological contrast (1 vs. 3 phonetic features). In a series of auditory word perception experiments, good and poor readers were found to do worse in accuracy and/or reaction times in phonologically similar (i.e., 1-feature contrast) than phonologically dissimilar (i.e., 2 or 3-feature contrast) conditions. Despite the similar behavioral performance and EEG responses for the two groups, a region of interest (ROI) based MEG approach revealed differences in the brain activation of the two groups in superior temporal regions at 140 to 300 ms.",
    "advisors": ["Maria Mody"],
    "text": "Phonological and semantic influences on auditory word perception in children with and without reading impairments using magnetoencephalography (MEG) and electroencephalography (EEG) Children with dyslexia struggle with learning to read despite adequate intelligence, motivation, and schooling. Over the years, there has been a growing consensus about the role of phonological processing in reading disability. Poor readers typically do worse than their normal reading peers on tasks that require phonological processing which has been linked, directly or indirectly, to their speech perception abilities. The work in this thesis combined behavioral, MEG, and EEG methods to examine how normal and reading-impaired children, 7-13 years of age, perceive speech under varying degrees of phonological contrast (1 vs. 3 phonetic features). In a series of auditory word perception experiments, good and poor readers were found to do worse in accuracy and/or reaction times in phonologically similar (i.e., 1-feature contrast) than phonologically dissimilar (i.e., 2 or 3-feature contrast) conditions. Despite the similar behavioral performance and EEG responses for the two groups, a region of interest (ROI) based MEG approach revealed differences in the brain activation of the two groups in superior temporal regions at 140 to 300 ms."
}, {
    "id": "oai:dspace.mit.edu:1721.1/104610",
    "title": "Design of an intraperitoneal drug-release device for advanced ovarian cancer therapy",
    "abstract": "More than 14,000 women in the United States die from ovarian cancer each year. The standard of care is tumor-debulking surgery followed by adjuvant chemotherapy. Combination intraperitoneal (IP) and intravenous (IV) chemotherapy has been shown to lengthen survival over IV therapy alone. Large-volume infusions, drug-associated toxicity, and catheter-associated complications, however, increase morbidity and limit patient adherence, often resulting in discontinuation of IP therapy. The technical skill required for catheter implantation and IP chemotherapy administration has also limited its clinical adoption. The proposed solution is an implantable IP device capable of localized drug delivery that maintains the efficacy of the standard of care and overcomes current clinical challenges. A reservoir-based device was developed to release cisplatin at a constant rate. In vivo studies demonstrated that continuous dosing reduces tumor burden to the same extent as weekly IP injections. The implanted device induced significantly less systemic toxicity compared to IP injections, despite administration of higher cumulative doses. A subsequent in vitro study revealed that greater tumor shrinkage following continuous cisplatin exposure was achieved with smaller tumor nodules. These results support that an implanted device would be maximally effective against microscopic residual disease. In vitro results also illustrated that a human-scale device fabricated from orifice-lined silicone can be designed to release cisplatin continuously at the desired rate. The promising preclinical results in this thesis highlight the potential for this novel IP dosing regimen to improve the treatment of late-stage ovarian cancer and set the stage for development of the proposed human device.",
    "advisors": ["Michael J. Cima"],
    "text": "Design of an intraperitoneal drug-release device for advanced ovarian cancer therapy More than 14,000 women in the United States die from ovarian cancer each year. The standard of care is tumor-debulking surgery followed by adjuvant chemotherapy. Combination intraperitoneal (IP) and intravenous (IV) chemotherapy has been shown to lengthen survival over IV therapy alone. Large-volume infusions, drug-associated toxicity, and catheter-associated complications, however, increase morbidity and limit patient adherence, often resulting in discontinuation of IP therapy. The technical skill required for catheter implantation and IP chemotherapy administration has also limited its clinical adoption. The proposed solution is an implantable IP device capable of localized drug delivery that maintains the efficacy of the standard of care and overcomes current clinical challenges. A reservoir-based device was developed to release cisplatin at a constant rate. In vivo studies demonstrated that continuous dosing reduces tumor burden to the same extent as weekly IP injections. The implanted device induced significantly less systemic toxicity compared to IP injections, despite administration of higher cumulative doses. A subsequent in vitro study revealed that greater tumor shrinkage following continuous cisplatin exposure was achieved with smaller tumor nodules. These results support that an implanted device would be maximally effective against microscopic residual disease. In vitro results also illustrated that a human-scale device fabricated from orifice-lined silicone can be designed to release cisplatin continuously at the desired rate. The promising preclinical results in this thesis highlight the potential for this novel IP dosing regimen to improve the treatment of late-stage ovarian cancer and set the stage for development of the proposed human device."
}, {
    "id": "oai:dspace.mit.edu:1721.1/30268",
    "title": "Noise reduction algorithms and performance metrics for improving speech reception in noise by cochlear-implant users",
    "abstract": "This thesis addresses the design and evaluation of algorithms to improve speech reception for cochlear-implant (CI) users in adverse listening environments. We develop and assess performance metrics for use in the algorithm design process; such metrics make algorithm evaluation efficient, consistent, and subject independent. One promising performance metric is the Speech Transmission Index (STI), which is well correlated with speech reception by normal-hearing listeners for additive noise and reverberation. We expect the STI will effectively predict speech reception by CI users since typical CI sound-processing strategies, like the STI, rely on the envelope signals in frequency bands spanning the speech spectrum. However, STI-based metrics have proven unsatisfactory for assessing the effects of nonlinear operations on the intelligibility of processed speech. In this work we consider modifications to the STI that account for nonlinear operations commonly found in CI sound-processing and noise reduction algorithms. We consider a number of existing speech-based STI metrics and propose novel metrics applicable to nonlinear operations. A preliminary evaluation results in the selection of three candidate metrics for extensive evaluation. In four central experiments, we consider the effects of acoustic degradation, N-of-M processing, spectral subtraction, and binaural noise reduction on the intelligibility of CI-processed speech. We assess the ability of the candidate metrics to predict speech reception scores.",
    "advisors": ["Julie E. Greenberg"],
    "text": "Noise reduction algorithms and performance metrics for improving speech reception in noise by cochlear-implant users This thesis addresses the design and evaluation of algorithms to improve speech reception for cochlear-implant (CI) users in adverse listening environments. We develop and assess performance metrics for use in the algorithm design process; such metrics make algorithm evaluation efficient, consistent, and subject independent. One promising performance metric is the Speech Transmission Index (STI), which is well correlated with speech reception by normal-hearing listeners for additive noise and reverberation. We expect the STI will effectively predict speech reception by CI users since typical CI sound-processing strategies, like the STI, rely on the envelope signals in frequency bands spanning the speech spectrum. However, STI-based metrics have proven unsatisfactory for assessing the effects of nonlinear operations on the intelligibility of processed speech. In this work we consider modifications to the STI that account for nonlinear operations commonly found in CI sound-processing and noise reduction algorithms. We consider a number of existing speech-based STI metrics and propose novel metrics applicable to nonlinear operations. A preliminary evaluation results in the selection of three candidate metrics for extensive evaluation. In four central experiments, we consider the effects of acoustic degradation, N-of-M processing, spectral subtraction, and binaural noise reduction on the intelligibility of CI-processed speech. We assess the ability of the candidate metrics to predict speech reception scores."
}, {
    "id": "oai:dspace.mit.edu:1721.1/72915",
    "title": "The human molecular clock and mutation process : a characterization using microsatellite DNA",
    "abstract": "In the past decade, thousands of human genomes have been catalogued, either by whole-genome sequencing or by targeted genotyping. The variability between human genomes encodes invaluable information about human traits and genetic diseases, as well as human migration patterns and population interactions. A key challenge is to understand and characterize the evolution of the variability between human genomes. In this thesis, I focus on studying human evolution through the use of microsatellites, which are simple repetitive sections of DNA of typically 1-6bp motifs (e.g. CACACACACA) that are highly polymorphic and highly mutable. The first aim is to establish that microsatellites are useful as reliable molecular clocks, such that its evolution highly correlates to time, especially when applied to the time range appropriate for human history. Using existing models of microsatellites, we examine microsatellite data from populations around the world to demonstrate that microsatellites are accurate molecular clocks for coalescent times of at least two million years. These results raise the prospect of using microsatellite data sets to determine parameters of population history. In order to calibrate genetic distances into time, the mutation rate must be known. This leads to the second aim, which is to directly measure the microsatellite mutation rate from largescale pedigree genetics data and provide a precision that is unprecedented. To do so, we use data from over 95,000 individuals in Icelandic pedigrees, genotyped in over 3000 microsatellite loci. Using trio and extended-family based approaches, we discover 2058 denovo mutations. In addition, we also attempt to capture many features that are covariates with the mutation rate, such as parental gender and age. The third aim takes our empirical observations of the microsatellite mutation process to build a new model of microsatellite evolution. This model improves upon the standard random walk model with features we have captured from aim 2. We use a Bayesian coalescent approach to provide a model that estimates the sequence mutation rate, European genetic divergence times, and human-chimpanzee speciation time.",
    "advisors": ["David Reich", "Nick Patterson"],
    "text": "The human molecular clock and mutation process : a characterization using microsatellite DNA In the past decade, thousands of human genomes have been catalogued, either by whole-genome sequencing or by targeted genotyping. The variability between human genomes encodes invaluable information about human traits and genetic diseases, as well as human migration patterns and population interactions. A key challenge is to understand and characterize the evolution of the variability between human genomes. In this thesis, I focus on studying human evolution through the use of microsatellites, which are simple repetitive sections of DNA of typically 1-6bp motifs (e.g. CACACACACA) that are highly polymorphic and highly mutable. The first aim is to establish that microsatellites are useful as reliable molecular clocks, such that its evolution highly correlates to time, especially when applied to the time range appropriate for human history. Using existing models of microsatellites, we examine microsatellite data from populations around the world to demonstrate that microsatellites are accurate molecular clocks for coalescent times of at least two million years. These results raise the prospect of using microsatellite data sets to determine parameters of population history. In order to calibrate genetic distances into time, the mutation rate must be known. This leads to the second aim, which is to directly measure the microsatellite mutation rate from largescale pedigree genetics data and provide a precision that is unprecedented. To do so, we use data from over 95,000 individuals in Icelandic pedigrees, genotyped in over 3000 microsatellite loci. Using trio and extended-family based approaches, we discover 2058 denovo mutations. In addition, we also attempt to capture many features that are covariates with the mutation rate, such as parental gender and age. The third aim takes our empirical observations of the microsatellite mutation process to build a new model of microsatellite evolution. This model improves upon the standard random walk model with features we have captured from aim 2. We use a Bayesian coalescent approach to provide a model that estimates the sequence mutation rate, European genetic divergence times, and human-chimpanzee speciation time."
}, {
    "id": "oai:dspace.mit.edu:1721.1/8071",
    "title": "The economics of HIV testing in Africa",
    "abstract": "This thesis examines the problem of resource allocation in Africa to combat the HIV/AIDS epidemic. It focuses on the use of one specific technology, the anti-HIV antibody test. After describing the characteristics of the epidemic, the special problems that accompany the allocation of health resources in Africa are explored. A short description of the biological and technical aspects of HIV testing is followed by three case studies which examine different uses of the technology. (1) A model of the use of HIV testing to screen blood donors is demonstrated in several hypothetical situations to evaluate under which circumstances HIV screening is cost-effective and which is the most cost-effective of a number of testing systems. (2) Use of the HIV test is considered from a cost-effectiveness perspective for the purpose of helping to confirm the diagnosis of HIV-related disease. Possible benefits of testing (including more rapid initiation of appropriate treatment, avoidance of the cost and iatrogenic complications of inappropriate treatment, and more efficient rationing of health care resources) are compared to possible costs (including monetary costs, emotional costs, and costs associated with false test results). A detailed protocol is presented of a prospective study to evaluate the appropriate use of the HIV test in the inpatient hospital setting. (3) Serologic surveys, including procurement of samples and testing for HIV, comprise the bulk of any program to monitor and characterize the epidemiology of the HIV/AIDS epidemic.",
    "advisors": ["Jeffrey E. Harris"],
    "text": "The economics of HIV testing in Africa This thesis examines the problem of resource allocation in Africa to combat the HIV/AIDS epidemic. It focuses on the use of one specific technology, the anti-HIV antibody test. After describing the characteristics of the epidemic, the special problems that accompany the allocation of health resources in Africa are explored. A short description of the biological and technical aspects of HIV testing is followed by three case studies which examine different uses of the technology. (1) A model of the use of HIV testing to screen blood donors is demonstrated in several hypothetical situations to evaluate under which circumstances HIV screening is cost-effective and which is the most cost-effective of a number of testing systems. (2) Use of the HIV test is considered from a cost-effectiveness perspective for the purpose of helping to confirm the diagnosis of HIV-related disease. Possible benefits of testing (including more rapid initiation of appropriate treatment, avoidance of the cost and iatrogenic complications of inappropriate treatment, and more efficient rationing of health care resources) are compared to possible costs (including monetary costs, emotional costs, and costs associated with false test results). A detailed protocol is presented of a prospective study to evaluate the appropriate use of the HIV test in the inpatient hospital setting. (3) Serologic surveys, including procurement of samples and testing for HIV, comprise the bulk of any program to monitor and characterize the epidemiology of the HIV/AIDS epidemic."
}, {
    "id": "oai:dspace.mit.edu:1721.1/43808",
    "title": "Advanced brachytherapy dosimetric considerations",
    "abstract": "The practice of brachytherapy and brachytherapy dosimetry was investigated with emphasis on evaluations of dose distributions and shielding considerations for both photon- and neutron-emitting radionuclides. Monte Carlo simulation methods were employed to calculate dose distributions for virtual and commercial brachytherapy sources. Radionuclides studied were 103Pd, 1251, 131Cs, 137Cs, 169b, 192Ir, and 252Cf. 252Cf sources also emit neutrons from spontaneous fission. The brachytherapy dosimetry protocol recommended by the American Association of Physicists in Medicine was followed and evaluated for conditions of partial scatter (non-infinite media) and material inhomogeneities, both commonly encountered in brachytherapy treatment. Furthermore, energy-dependent characteristics of dosimetry parameters were evaluated and reference calculations performed for virtual photon and neutron sources. These findings were applied to three clinical brachytherapy cases: eye plaques using 103Pd, 125I, and 131Cs; high-dose rate 252Cf treatment; and, 2 Cf plaques for superficial lesions. For eye plaques, material heterogeneities were significant for each radionuclide with dose reduction at 5 mm of 18%, 11%, and 10% for P03pd, 125I, and 131Cs, respectively. For a proposed highdose rate 252Cf source (5mm length), relative brachytherapy dosimetry parameters were found to be similar to those obtained for a low-dose rate Applicator Tube-type source (15 mm length). Considering 252Cf plaque brachytherapy when partial scatter conditions were accounted for, central axis equivalent dose rate decreased by 11  1% and 7  2% for depths of 4 to 50 mm, respectively.",
    "advisors": ["Mark J. Rivard"],
    "text": "Advanced brachytherapy dosimetric considerations The practice of brachytherapy and brachytherapy dosimetry was investigated with emphasis on evaluations of dose distributions and shielding considerations for both photon- and neutron-emitting radionuclides. Monte Carlo simulation methods were employed to calculate dose distributions for virtual and commercial brachytherapy sources. Radionuclides studied were 103Pd, 1251, 131Cs, 137Cs, 169b, 192Ir, and 252Cf. 252Cf sources also emit neutrons from spontaneous fission. The brachytherapy dosimetry protocol recommended by the American Association of Physicists in Medicine was followed and evaluated for conditions of partial scatter (non-infinite media) and material inhomogeneities, both commonly encountered in brachytherapy treatment. Furthermore, energy-dependent characteristics of dosimetry parameters were evaluated and reference calculations performed for virtual photon and neutron sources. These findings were applied to three clinical brachytherapy cases: eye plaques using 103Pd, 125I, and 131Cs; high-dose rate 252Cf treatment; and, 2 Cf plaques for superficial lesions. For eye plaques, material heterogeneities were significant for each radionuclide with dose reduction at 5 mm of 18%, 11%, and 10% for P03pd, 125I, and 131Cs, respectively. For a proposed highdose rate 252Cf source (5mm length), relative brachytherapy dosimetry parameters were found to be similar to those obtained for a low-dose rate Applicator Tube-type source (15 mm length). Considering 252Cf plaque brachytherapy when partial scatter conditions were accounted for, central axis equivalent dose rate decreased by 11  1% and 7  2% for depths of 4 to 50 mm, respectively."
}, {
    "id": "oai:dspace.mit.edu:1721.1/32855",
    "title": "A new model for electric force microscopy and its application for electrostatically generated phase difference in tapping mode AFM",
    "abstract": "The harmonic force balance method was used to model and simulate electric force microscopy (EFM) and electrostatically generated phase difference in tapping mode AFM (EPTA) measurements. Simulations show that the harmonic force balance approach matches and explains EFM and EPTA experimental results well. Simulations also show that the model depended on both geometric and materials parameters. The harmonic force balance model was subsequently used to directly simulate a previously performed EPTA experiment. Data obtained from the model showed a remarkable similarity to the experimentally obtained data, thus validating the use of the harmonic force balance model to simulate EPTA data.",
    "advisors": ["Francisco Stellacci"],
    "text": "A new model for electric force microscopy and its application for electrostatically generated phase difference in tapping mode AFM The harmonic force balance method was used to model and simulate electric force microscopy (EFM) and electrostatically generated phase difference in tapping mode AFM (EPTA) measurements. Simulations show that the harmonic force balance approach matches and explains EFM and EPTA experimental results well. Simulations also show that the model depended on both geometric and materials parameters. The harmonic force balance model was subsequently used to directly simulate a previously performed EPTA experiment. Data obtained from the model showed a remarkable similarity to the experimentally obtained data, thus validating the use of the harmonic force balance model to simulate EPTA data."
}, {
    "id": "oai:dspace.mit.edu:1721.1/76122",
    "title": "Microstructure and mechanical properties of bamboo in compression",
    "abstract": "Bamboo has received much interest recently as a construction material due to its strength, rapid growth, and abundance in developing nations such as China, India, and Brazil. The main obstacle to the widespread use of bamboo as a structural material is the lack of adequate information on the mechanical properties of bamboo. In this work, the microstructure and mechanical properties of Phyllostachis dulcis bamboo are studied to help produce a model for the mechanical properties of bamboo. Specifically, a linear relationship is established between the density of bamboo samples, which is known to vary radially, and their strength in compression. Nanoindentation of vascular bundles in various positions in bamboo samples revealed that the Young's modulus and hardness of the bundles vary in the radial direction but not around the circumference. The compressive strength of bamboo samples was found to vary from 40 to 95 MPa, while nanoindentation results show the Young's modulus of vascular bundles ranges from 15 to 18 GPa and the hardness ranges from 380 to 530 MPa.",
    "advisors": ["Lorna J. Gibson"],
    "text": "Microstructure and mechanical properties of bamboo in compression Bamboo has received much interest recently as a construction material due to its strength, rapid growth, and abundance in developing nations such as China, India, and Brazil. The main obstacle to the widespread use of bamboo as a structural material is the lack of adequate information on the mechanical properties of bamboo. In this work, the microstructure and mechanical properties of Phyllostachis dulcis bamboo are studied to help produce a model for the mechanical properties of bamboo. Specifically, a linear relationship is established between the density of bamboo samples, which is known to vary radially, and their strength in compression. Nanoindentation of vascular bundles in various positions in bamboo samples revealed that the Young's modulus and hardness of the bundles vary in the radial direction but not around the circumference. The compressive strength of bamboo samples was found to vary from 40 to 95 MPa, while nanoindentation results show the Young's modulus of vascular bundles ranges from 15 to 18 GPa and the hardness ranges from 380 to 530 MPa."
}, {
    "id": "oai:dspace.mit.edu:1721.1/81143",
    "title": "Dye-doped polymer nanoparticles for flexible, bulk luminescent solar concentrators",
    "abstract": "Bulk luminescent solar concentrators (LSC) cannot make use of Forster resonance energy transfer (FRET) due to necessarily low dye concentrations. In this thesis, we attempt to present a poly-vinylalcohol (PVA) waveguide containing dye-aggregate polystyrene nanospheres that enable FRET at concentrations below that required for the bulk LSC due to dye confinement. In the aqueous state, the maximum achieved energy transfer efficiency of the dye-doped nanoparticles was found to be 8 7% for lwt%/lwt% doping of Coumarin 1 (C1) and Coumarin 6 (C6). In the solid state, however, energy transfer is lost, reducing to 32.8% and 20.1% respectively for the C1(lwt%)/C6(lwt%) and C1(0.5wt%)/C6(lwt/ ) iterations, respectively. Presumably, the dyes leach out of the polystyrene nanospheres and into the PVA waveguide upon water evaporation during drop casting.",
    "advisors": ["Marc Baldo"],
    "text": "Dye-doped polymer nanoparticles for flexible, bulk luminescent solar concentrators Bulk luminescent solar concentrators (LSC) cannot make use of Forster resonance energy transfer (FRET) due to necessarily low dye concentrations. In this thesis, we attempt to present a poly-vinylalcohol (PVA) waveguide containing dye-aggregate polystyrene nanospheres that enable FRET at concentrations below that required for the bulk LSC due to dye confinement. In the aqueous state, the maximum achieved energy transfer efficiency of the dye-doped nanoparticles was found to be 8 7% for lwt%/lwt% doping of Coumarin 1 (C1) and Coumarin 6 (C6). In the solid state, however, energy transfer is lost, reducing to 32.8% and 20.1% respectively for the C1(lwt%)/C6(lwt%) and C1(0.5wt%)/C6(lwt/ ) iterations, respectively. Presumably, the dyes leach out of the polystyrene nanospheres and into the PVA waveguide upon water evaporation during drop casting."
}, {
    "id": "oai:dspace.mit.edu:1721.1/119066",
    "title": "Sintering of small particles",
    "abstract": "An atomistic approach to modeling the sintering of nanocrystalline alloys has been developed. It has been shown that there exist alloys that exhibit both nanostructured stability and undergo an accelerated sintering process [1], [2]. However, the widespread adoption of such alloys has been limited by a lack of understanding of the processing kinetics that lead to the accelerated sintering phenomena. To better understand the role of surface diffusion, and the effect that system enthalpies of mixing have on inter-particle neck formation, a 3D kinetic monte carlo (KMC) model was proposed to study these phenomena. The results of these simulations demonstrate that positive enthalpy of mixing highlighted as a necessary criterion for nanocrystalline stability in [1], also leads to the fast diffusing elements ability to form the interparticle neck. The condition of lower temperature neck formation by fast diffusing alloy elements is hypothesized to be the mechanism behind which accelerated sintering occurs. The findings in this paper demonstrate that positive enthalpy of mixing alloys can be designed to sinter at lower temperatures and shorter cycle durations if they have adequate solute present on the surface of the particle.",
    "advisors": ["Christopher A. Schuh"],
    "text": "Sintering of small particles An atomistic approach to modeling the sintering of nanocrystalline alloys has been developed. It has been shown that there exist alloys that exhibit both nanostructured stability and undergo an accelerated sintering process [1], [2]. However, the widespread adoption of such alloys has been limited by a lack of understanding of the processing kinetics that lead to the accelerated sintering phenomena. To better understand the role of surface diffusion, and the effect that system enthalpies of mixing have on inter-particle neck formation, a 3D kinetic monte carlo (KMC) model was proposed to study these phenomena. The results of these simulations demonstrate that positive enthalpy of mixing highlighted as a necessary criterion for nanocrystalline stability in [1], also leads to the fast diffusing elements ability to form the interparticle neck. The condition of lower temperature neck formation by fast diffusing alloy elements is hypothesized to be the mechanism behind which accelerated sintering occurs. The findings in this paper demonstrate that positive enthalpy of mixing alloys can be designed to sinter at lower temperatures and shorter cycle durations if they have adequate solute present on the surface of the particle."
}, {
    "id": "oai:dspace.mit.edu:1721.1/98644",
    "title": "Decreasing water absorption in and environmental analysis of alkali activated bricks",
    "abstract": "Alkali activated bricks offer an alternative to traditional clay fired bricks for use in construction in the developing world. Previous work in this lab focused on creating a robust mix formulation to create these bricks, but they faced high water absorption and were not optimized under pressure molding conditions. The motivation for the work on alkali-activated bricks is based on the claim that they have a lower environmental burden, but this claim has not yet been verified for this formulation. Thus, this thesis focused on the effects of controlled testing of formation pressure and particle size distribution on brick performance and understanding the relative environmental impacts of clay fired bricks and alkali activated bricks. It was found that water absorption and compressive strength have a strong dependence on forming pressure, with 3-day compressive strengths ranging from 7MPa to 27MPa and water absorption from 35% to as high as 60% as forming pressure increased from 5 to 35Mpa. Sieving of the ash used in the bricks to control for particle size distribution had a minimal effect on performance, but the similarity is attributed to the fact that packing density within the selected particle size ranges were similar. Further testing on controlled mixing of particle sizes is needed to see if better performance can be obtained. Life cycle assessment results verify the claim that the bricks perform better from an environmental perspective, but also show the dependence of that performance on variables such as lime content or kiln efficiency.",
    "advisors": ["Elsa Olivetti"],
    "text": "Decreasing water absorption in and environmental analysis of alkali activated bricks Alkali activated bricks offer an alternative to traditional clay fired bricks for use in construction in the developing world. Previous work in this lab focused on creating a robust mix formulation to create these bricks, but they faced high water absorption and were not optimized under pressure molding conditions. The motivation for the work on alkali-activated bricks is based on the claim that they have a lower environmental burden, but this claim has not yet been verified for this formulation. Thus, this thesis focused on the effects of controlled testing of formation pressure and particle size distribution on brick performance and understanding the relative environmental impacts of clay fired bricks and alkali activated bricks. It was found that water absorption and compressive strength have a strong dependence on forming pressure, with 3-day compressive strengths ranging from 7MPa to 27MPa and water absorption from 35% to as high as 60% as forming pressure increased from 5 to 35Mpa. Sieving of the ash used in the bricks to control for particle size distribution had a minimal effect on performance, but the similarity is attributed to the fact that packing density within the selected particle size ranges were similar. Further testing on controlled mixing of particle sizes is needed to see if better performance can be obtained. Life cycle assessment results verify the claim that the bricks perform better from an environmental perspective, but also show the dependence of that performance on variables such as lime content or kiln efficiency."
}, {
    "id": "oai:dspace.mit.edu:1721.1/89984",
    "title": "Process variables controlling consistency of carbon nanotube forest growth",
    "abstract": "Aligned arrays of carbon nanotubes (A-CNTs), called CNT forests, are the precursor for controlled-morphology macroscopic nanocomposites and nanoengineered composites due to theirscale-dependent, tunable physicall properties. Applications include polymer and ceramic matrix nanocomposites (PNCs and CMNCs), nanostiching as laminate interply reinforcement, as well as in supercapacitors, MEMS devices and electrodes for ion actuators and sensors. A key component of manufacturing materials comprised of A-CNTs is controlling the morphology and geometry of the CNT forest. Current laboratory findings show significant variability in CNT forest growth characteristics, and an experimental study was conducted to better understand and control for the observd process variations. An exploratory investigation of growth parameters allowed for a local optimization of growth temperature and hydrocarbon flow rates, as well as an acceptable range of sample placement in the CVD furnace to achieve ~1mm tall CNT forests. Results from this investigation led to the conclusion that the significant inconsistencies between consecutive growths must be due to factors out of direct control, mainly humidity. A new system is being developed to better control for and monitor water in the furnace. A second investigation focused on post growth cool down effects, and the possible shortening (deforestation) of CNTs at high temperatures without a renewing source of the carbon precursor. Deforestation conditions did not lead to CNT shortening.",
    "advisors": ["Brian L. Wardle", "Carl V. Thompson"],
    "text": "Process variables controlling consistency of carbon nanotube forest growth Aligned arrays of carbon nanotubes (A-CNTs), called CNT forests, are the precursor for controlled-morphology macroscopic nanocomposites and nanoengineered composites due to theirscale-dependent, tunable physicall properties. Applications include polymer and ceramic matrix nanocomposites (PNCs and CMNCs), nanostiching as laminate interply reinforcement, as well as in supercapacitors, MEMS devices and electrodes for ion actuators and sensors. A key component of manufacturing materials comprised of A-CNTs is controlling the morphology and geometry of the CNT forest. Current laboratory findings show significant variability in CNT forest growth characteristics, and an experimental study was conducted to better understand and control for the observd process variations. An exploratory investigation of growth parameters allowed for a local optimization of growth temperature and hydrocarbon flow rates, as well as an acceptable range of sample placement in the CVD furnace to achieve ~1mm tall CNT forests. Results from this investigation led to the conclusion that the significant inconsistencies between consecutive growths must be due to factors out of direct control, mainly humidity. A new system is being developed to better control for and monitor water in the furnace. A second investigation focused on post growth cool down effects, and the possible shortening (deforestation) of CNTs at high temperatures without a renewing source of the carbon precursor. Deforestation conditions did not lead to CNT shortening."
}, {
    "id": "oai:dspace.mit.edu:1721.1/58451",
    "title": "An exploration of automotive platinum demand and its impacts on the platinum market",
    "abstract": "The platinum market is a material market of increasing interest, as platinum demand has grown faster than supply in recent years. As a result, the price of platinum has increased, causing end-user firms to experience material scarcity through the presence of these high prices. A significant driver of this demand growth for the last several decades is demand automotive sector, which is responsible for almost 60% of total primary platinum demand, due to the use of platinum in three way catalysts. Platinum is one of the materials utilized to catalyze reactions that prevent vehicle emissions from entering the atmosphere, which can have a severe impact on air quality. Two factors will likely contribute to the future growth of automotive platinum demand: the trend in increased use of platinum per vehicle, and expected growth in the number of automobiles produced and sold around the world. While the automotive market is relatively saturated in developed economies, automotive sales growth potential is particularly high in developing areas, such as BRIC countries. It follows that future growth in automotive platinum demand is likely to be significant. As such, the study aims to characterize the drivers of automotive platinum demand and to establish how this demand sector impacts the platinum market as a whole. This characterization is achieved through regression analysis and by utilizing a platinum market simulation model.",
    "advisors": ["Randolph E. Kirchain Jr"],
    "text": "An exploration of automotive platinum demand and its impacts on the platinum market The platinum market is a material market of increasing interest, as platinum demand has grown faster than supply in recent years. As a result, the price of platinum has increased, causing end-user firms to experience material scarcity through the presence of these high prices. A significant driver of this demand growth for the last several decades is demand automotive sector, which is responsible for almost 60% of total primary platinum demand, due to the use of platinum in three way catalysts. Platinum is one of the materials utilized to catalyze reactions that prevent vehicle emissions from entering the atmosphere, which can have a severe impact on air quality. Two factors will likely contribute to the future growth of automotive platinum demand: the trend in increased use of platinum per vehicle, and expected growth in the number of automobiles produced and sold around the world. While the automotive market is relatively saturated in developed economies, automotive sales growth potential is particularly high in developing areas, such as BRIC countries. It follows that future growth in automotive platinum demand is likely to be significant. As such, the study aims to characterize the drivers of automotive platinum demand and to establish how this demand sector impacts the platinum market as a whole. This characterization is achieved through regression analysis and by utilizing a platinum market simulation model."
}, {
    "id": "oai:dspace.mit.edu:1721.1/98002",
    "title": "Targeted magnetic nanoparticles for remote manipulation of protein aggregation",
    "abstract": "Local heat delivered by magnetic nanoparticles (MNPs) selectively attached to their target proteins can be used to manipulate and break up toxic or obstructive aggregates. We applied this magnetic hyperthermia treatment to the amyloid beta (A[beta]) peptide, which unnaturally folds and self-assembles forming amyloid fibrils and insoluble plaques characteristic of amyloidgenic diseases such as Alzheimer's disease. We demonstrate remote disaggregation of A[beta] aggregates using heat dissipated by ferrite MNPs in the presence of an alternating magnetic field (AMF). Specific targeting was achieved by MNP functionalization with a targeting peptide sequence that binds a hydrophobic domain of A[beta]. AMF parameters and MNP composition and size were tailored to maximize hysteretic power losses. Transmission electron microscopy image analysis and thioflavin T fluorescence spectroscopy were used to characterize the morphology and size distribution of aggregates before and after AMF stimulus. We found that the AMF stimulus is effective at destabilizing A[beta] deposits and causing a reduction in aggregate size. This targeting scheme has potential as a therapy for amyloidosis and as a minimally invasive tool for analyzing and controlling protein aggregation.",
    "advisors": ["Polina Anikeeva"],
    "text": "Targeted magnetic nanoparticles for remote manipulation of protein aggregation Local heat delivered by magnetic nanoparticles (MNPs) selectively attached to their target proteins can be used to manipulate and break up toxic or obstructive aggregates. We applied this magnetic hyperthermia treatment to the amyloid beta (A[beta]) peptide, which unnaturally folds and self-assembles forming amyloid fibrils and insoluble plaques characteristic of amyloidgenic diseases such as Alzheimer's disease. We demonstrate remote disaggregation of A[beta] aggregates using heat dissipated by ferrite MNPs in the presence of an alternating magnetic field (AMF). Specific targeting was achieved by MNP functionalization with a targeting peptide sequence that binds a hydrophobic domain of A[beta]. AMF parameters and MNP composition and size were tailored to maximize hysteretic power losses. Transmission electron microscopy image analysis and thioflavin T fluorescence spectroscopy were used to characterize the morphology and size distribution of aggregates before and after AMF stimulus. We found that the AMF stimulus is effective at destabilizing A[beta] deposits and causing a reduction in aggregate size. This targeting scheme has potential as a therapy for amyloidosis and as a minimally invasive tool for analyzing and controlling protein aggregation."
}, {
    "id": "oai:dspace.mit.edu:1721.1/44816",
    "title": "Effects of catalyst pretreatment for carbon nanotube growth",
    "abstract": "The effects of pretreatment of iron catalyst for carbon nanotube (CNT) growth was studied. CNTs were grown on Fe/A1203 (1/10 nm) thin-film catalyst deposited on silicon substrates via exposure to C2H4 in a thermal chemical vapor deposition (CVD) furnace. During CVD, the sample was exposed to a carrier gas (Argon) for the 35-minute temperature ramp, and 15-minute anneal, then to a mix of carrier gas and ethylene for a 15-minute growth stage. Experiments were performed varying the amount of oxygen contaminant in the carrier gas, and the time of hydrogen introduction. Samples were characterized via atomic force microscopy (AFM), scanning electron microscopy (SEM) and transmission electron microscopy (TEM). It was found that the later hydrogen was introduced, the higher the catalyst density and the taller the CNT carpet. The catalyst efficiency was also shown to increase with later hydrogen introduction. No clear trend was observed between the amount of oxygen in the carrier gas and the height of CNT growth. Data points to the model of catalyst coarsening being crucial to the nucleation and growth of CNTs and the parameters of CNTs grown. Variations in trends are discussed.",
    "advisors": ["Carl V. Thompson"],
    "text": "Effects of catalyst pretreatment for carbon nanotube growth The effects of pretreatment of iron catalyst for carbon nanotube (CNT) growth was studied. CNTs were grown on Fe/A1203 (1/10 nm) thin-film catalyst deposited on silicon substrates via exposure to C2H4 in a thermal chemical vapor deposition (CVD) furnace. During CVD, the sample was exposed to a carrier gas (Argon) for the 35-minute temperature ramp, and 15-minute anneal, then to a mix of carrier gas and ethylene for a 15-minute growth stage. Experiments were performed varying the amount of oxygen contaminant in the carrier gas, and the time of hydrogen introduction. Samples were characterized via atomic force microscopy (AFM), scanning electron microscopy (SEM) and transmission electron microscopy (TEM). It was found that the later hydrogen was introduced, the higher the catalyst density and the taller the CNT carpet. The catalyst efficiency was also shown to increase with later hydrogen introduction. No clear trend was observed between the amount of oxygen in the carrier gas and the height of CNT growth. Data points to the model of catalyst coarsening being crucial to the nucleation and growth of CNTs and the parameters of CNTs grown. Variations in trends are discussed."
}, {
    "id": "oai:dspace.mit.edu:1721.1/35064",
    "title": "The role of hydrogen in the growth of carbon nanotubes : a study of the catalyst state and morphology",
    "abstract": "The role of hydrogen in chemical vapor decomposition (CVD) of C2H4 for growth of carbon nanotubes (CNTs) was investigated. Fe/A1203 (1/10 nm) catalyst layers were used for growth on Si substrates and the times at which H2 was introduced during the 40 minute temperature ramp, 15 minute annealing (without C2H4), and 15 minute growth (during which C2H4 was flowing) stages was varied. When H2 was introduced before heating, CNTs grew to a length of [approx.] 0.3 mm. However, CNT growth was severely suppressed when H2 was introduced at different points during temperature ramp. Recovery of CNT growth was observed when H2 was introduced during the annealing and growth stages. Under optimum conditions, an [approx.] 1 mm thick carpet of CNTs could be obtained. The chemical state and morphology of the catalysts as a function of the time of H2 introduction were examined using XPS and AFM, respectively. We found that the as-deposited state of Fe was an iron oxide, due to reaction with 02 in the atmosphere, and that the H2 reduced the iron oxide to different oxidation states, depending on the time of H2 introduction. AFM inspection showed that surface roughness could also be correlated with areas of vertical CNT growth.",
    "advisors": ["Carl V. Thompson"],
    "text": "The role of hydrogen in the growth of carbon nanotubes : a study of the catalyst state and morphology The role of hydrogen in chemical vapor decomposition (CVD) of C2H4 for growth of carbon nanotubes (CNTs) was investigated. Fe/A1203 (1/10 nm) catalyst layers were used for growth on Si substrates and the times at which H2 was introduced during the 40 minute temperature ramp, 15 minute annealing (without C2H4), and 15 minute growth (during which C2H4 was flowing) stages was varied. When H2 was introduced before heating, CNTs grew to a length of [approx.] 0.3 mm. However, CNT growth was severely suppressed when H2 was introduced at different points during temperature ramp. Recovery of CNT growth was observed when H2 was introduced during the annealing and growth stages. Under optimum conditions, an [approx.] 1 mm thick carpet of CNTs could be obtained. The chemical state and morphology of the catalysts as a function of the time of H2 introduction were examined using XPS and AFM, respectively. We found that the as-deposited state of Fe was an iron oxide, due to reaction with 02 in the atmosphere, and that the H2 reduced the iron oxide to different oxidation states, depending on the time of H2 introduction. AFM inspection showed that surface roughness could also be correlated with areas of vertical CNT growth."
}, {
    "id": "oai:dspace.mit.edu:1721.1/43209",
    "title": "Inhibition of biofilm growth on highly polycationic polyelectrolyte multilayers",
    "abstract": "The epithelial cell adhesion molecule E-cadherin is often down regulated during carcinoma progression and metastatic spread of tumors. However, the precise mechanism and molecular basis of metastasis promotion by E-cadherin loss is not completely understood. To investigate its role in metastasis, I utilized two distinct methods of E-cadherin inhibition that distinguish between E-cadherin's cell-cell adhesion and intracellular signaling functions. While the disruption of cell-cell contacts alone does not enable metastasis in vivo, the loss of E-cadherin protein does, through induction of an epithelial-to-mesenchymal transition (EMT), invasiveness and anoikis-resistance. E-cadherin binding partner f3-catenin is necessary but not sufficient for these phenotypes. In addition, gene expression analysis shows that E-cadherin loss results in the induction of multiple transcription factors, at least one of which, Twist, is necessary for E-cadherin loss-induced metastasis. These findings indicate that E-cadherin loss in tumors contributes to metastatic dissemination by inducing wide-ranging transcriptional and functional changes. In addition to promoting metastasis, loss of E-cadherin and the accompanying EMT renders cells resistant to conventional chemotherapeutic drugs. As the cells that have undergone an EMT represent the pool of cancer cells most competent to metastasize and lead to tumor recurrence, it is of vital importance to find therapies that effectively target such cells. Paired cell lines that differ in their differentiation state were utilized to discover compounds with selective toxicity against cells that have undergone an EMT. High-throughput screening of small molecule libraries resulted in a number of compounds that specifically affect the viability of cells that have undergone an EMT while having minimal cytotoxic effects on control epithelial cells. These studies establish a proof-of-principle for discovering compounds that target highly metastatic and otherwise chemotherapy resistant cancer cells.",
    "advisors": ["Michael F. Rubner"],
    "text": "Inhibition of biofilm growth on highly polycationic polyelectrolyte multilayers The epithelial cell adhesion molecule E-cadherin is often down regulated during carcinoma progression and metastatic spread of tumors. However, the precise mechanism and molecular basis of metastasis promotion by E-cadherin loss is not completely understood. To investigate its role in metastasis, I utilized two distinct methods of E-cadherin inhibition that distinguish between E-cadherin's cell-cell adhesion and intracellular signaling functions. While the disruption of cell-cell contacts alone does not enable metastasis in vivo, the loss of E-cadherin protein does, through induction of an epithelial-to-mesenchymal transition (EMT), invasiveness and anoikis-resistance. E-cadherin binding partner f3-catenin is necessary but not sufficient for these phenotypes. In addition, gene expression analysis shows that E-cadherin loss results in the induction of multiple transcription factors, at least one of which, Twist, is necessary for E-cadherin loss-induced metastasis. These findings indicate that E-cadherin loss in tumors contributes to metastatic dissemination by inducing wide-ranging transcriptional and functional changes. In addition to promoting metastasis, loss of E-cadherin and the accompanying EMT renders cells resistant to conventional chemotherapeutic drugs. As the cells that have undergone an EMT represent the pool of cancer cells most competent to metastasize and lead to tumor recurrence, it is of vital importance to find therapies that effectively target such cells. Paired cell lines that differ in their differentiation state were utilized to discover compounds with selective toxicity against cells that have undergone an EMT. High-throughput screening of small molecule libraries resulted in a number of compounds that specifically affect the viability of cells that have undergone an EMT while having minimal cytotoxic effects on control epithelial cells. These studies establish a proof-of-principle for discovering compounds that target highly metastatic and otherwise chemotherapy resistant cancer cells."
}, {
    "id": "oai:dspace.mit.edu:1721.1/110960",
    "title": "Advanced electrochemical characterization of copper deposition",
    "abstract": "The electrodeposition of copper metal in a concentrated sulfuric acid solution is reported to occur through a four-step mechanism: (I) the dehydration of Cu2+ (H2O)6, (II) the reduction of Cu2+ to cu+, (III) the dehydration cu+ (H2O)6-x, (IV) the reduction of Cu+ to copper metal. The dehydration steps have been found to be responsible for the pH-dependence of the electrodeposition reaction. It is also reported, although not well understood, that the presence of Fe2+ ions affects the reaction kinetics. In this work, the kinetics of copper electrodeposition were studied using alternating current cyclic voltammetry. The reaction was studied at a copper rotating disk electrode with varying concentrations of Cu2+ and Fe2+ . At sufficiently low pH, and a sufficiently high concentration of Fe2+ , the deposition kinetics may be slowed enough to separately observe the two electron transfer steps involved in copper reduction. It was found that Fe2+ ions affect the electrodeposition kinetic by slowing down reaction kinetics, particularly the second electron transfer reaction.",
    "advisors": ["Antoine Allanore"],
    "text": "Advanced electrochemical characterization of copper deposition The electrodeposition of copper metal in a concentrated sulfuric acid solution is reported to occur through a four-step mechanism: (I) the dehydration of Cu2+ (H2O)6, (II) the reduction of Cu2+ to cu+, (III) the dehydration cu+ (H2O)6-x, (IV) the reduction of Cu+ to copper metal. The dehydration steps have been found to be responsible for the pH-dependence of the electrodeposition reaction. It is also reported, although not well understood, that the presence of Fe2+ ions affects the reaction kinetics. In this work, the kinetics of copper electrodeposition were studied using alternating current cyclic voltammetry. The reaction was studied at a copper rotating disk electrode with varying concentrations of Cu2+ and Fe2+ . At sufficiently low pH, and a sufficiently high concentration of Fe2+ , the deposition kinetics may be slowed enough to separately observe the two electron transfer steps involved in copper reduction. It was found that Fe2+ ions affect the electrodeposition kinetic by slowing down reaction kinetics, particularly the second electron transfer reaction."
}, {
    "id": "oai:dspace.mit.edu:1721.1/42994",
    "title": "Reverse logistics and large-scale material recovery from electronics waste",
    "abstract": "Waste consolidation is a crucial step in the development of cost-effective, nation-wide material reclamation networks. This thesis project investigates typical and conformational tendencies of a hypothetical end-of-life electronics recycling system based in the United States. Optimal waste processor configurations, along with cost drivers and sensitivities are identified using a simple reverse logistics linear programming model. The experimental procedure entails varying the model scenario based on: type of material being recycled, the properties of current recycling and consolidation practices, and an extrapolation of current trends into the future. The transition from a decentralized to a centralized recycling network is shown to be dependent on the balance between transportation costs and facility costs, with the latter being a much more important cost consideration than the former. Additionally, this project sets the stage for a great deal of future work to ensure the profitability of domestic e-waste recycling systems.",
    "advisors": ["Randolph E. Kirchain, Jr"],
    "text": "Reverse logistics and large-scale material recovery from electronics waste Waste consolidation is a crucial step in the development of cost-effective, nation-wide material reclamation networks. This thesis project investigates typical and conformational tendencies of a hypothetical end-of-life electronics recycling system based in the United States. Optimal waste processor configurations, along with cost drivers and sensitivities are identified using a simple reverse logistics linear programming model. The experimental procedure entails varying the model scenario based on: type of material being recycled, the properties of current recycling and consolidation practices, and an extrapolation of current trends into the future. The transition from a decentralized to a centralized recycling network is shown to be dependent on the balance between transportation costs and facility costs, with the latter being a much more important cost consideration than the former. Additionally, this project sets the stage for a great deal of future work to ensure the profitability of domestic e-waste recycling systems."
}, {
    "id": "oai:dspace.mit.edu:1721.1/35060",
    "title": "Microfluidic emulsion characterization for the development of armored droplet arrays",
    "abstract": "An experimental study was performed to determine the best method for using a flow-focusing device to produce monodisperse water droplets in a polymer flow with sufficient spacing to polymerize a protective shell around the droplets using continuous flow lithography. Contact angle measurements and surface tension measurements were used to determine how wettable the polymer is with respect to water and PDMS. Polymerization reaction kinetics tests were used to determine a suitable polymer for the system. The droplet size and spacing for different flow-focusing devices with different dimensions were characterized to determine the best dimensions. Finally, characterization tests for various polymer and water flow rates were performed to examine the droplet size, spacing, velocity and frequency of production, as well as the fluctuations and instabilities in the system. From these characterization tests it was determined that the best flow systems for armoring droplets arise when the water flow rate is greater than 0.05pL/min, the polymer flow rate is between 0.4 and 1.2pL/min and the flow-rate ration of water to polymer is less than 1:10.",
    "advisors": ["Darrell Irvine", "Patrick Doyle"],
    "text": "Microfluidic emulsion characterization for the development of armored droplet arrays An experimental study was performed to determine the best method for using a flow-focusing device to produce monodisperse water droplets in a polymer flow with sufficient spacing to polymerize a protective shell around the droplets using continuous flow lithography. Contact angle measurements and surface tension measurements were used to determine how wettable the polymer is with respect to water and PDMS. Polymerization reaction kinetics tests were used to determine a suitable polymer for the system. The droplet size and spacing for different flow-focusing devices with different dimensions were characterized to determine the best dimensions. Finally, characterization tests for various polymer and water flow rates were performed to examine the droplet size, spacing, velocity and frequency of production, as well as the fluctuations and instabilities in the system. From these characterization tests it was determined that the best flow systems for armoring droplets arise when the water flow rate is greater than 0.05pL/min, the polymer flow rate is between 0.4 and 1.2pL/min and the flow-rate ration of water to polymer is less than 1:10."
}, {
    "id": "oai:dspace.mit.edu:1721.1/35061",
    "title": "Virus-enabled synthesis of titanium oxide nanowires",
    "abstract": "Bio-assisted materials fabrication methods allow for the production of high technology materials and devices at lower costs and with less environmental impact. To expand the biological toolkit for synthesizing materials, we demonstrated titanium oxide nanowire synthesis with use of engineered M13 virus at room temperature. In this virus-enabled synthesis process, negatively-charged titanium fluoro complexes nucleate at positive amine sites on the virus, and a subsequent anion-scavenging reaction drives the synthesis of titanium oxide on the virus. TEM imagery provided visual validation of the nanowire formation, and XRD analysis identified the crystalline structure as anatase.",
    "advisors": ["Angela M. Belcher"],
    "text": "Virus-enabled synthesis of titanium oxide nanowires Bio-assisted materials fabrication methods allow for the production of high technology materials and devices at lower costs and with less environmental impact. To expand the biological toolkit for synthesizing materials, we demonstrated titanium oxide nanowire synthesis with use of engineered M13 virus at room temperature. In this virus-enabled synthesis process, negatively-charged titanium fluoro complexes nucleate at positive amine sites on the virus, and a subsequent anion-scavenging reaction drives the synthesis of titanium oxide on the virus. TEM imagery provided visual validation of the nanowire formation, and XRD analysis identified the crystalline structure as anatase."
}, {
    "id": "oai:dspace.mit.edu:1721.1/76147",
    "title": "High yield production of inorganic graphene-like materials (MoS, WS, BN) through liquid exfoliation testing key parameters",
    "abstract": "Inorganic graphene-like materials such as molybdenum disulfide (MoS), tungsten sulfide (WS), and boron nitride (BN) are known to have electronic properties. When exfoliated into layers and casted onto carbon nanofilms, they can become potentially cheap and efficient electronic materials for magnetic sensing and energy storage devices. The goal of this experiment is to use a general liquid-phase method to exfoliate and optimize a number of parameters that can yield the highest concentration of layered quantities of MoS, WS, and BN. The key parameters optmized were material concentration, surfactant concentration, sonication method and duration, and centrifuge speed. Therefore, different concentrations of the three materials were mixed with different concentrations of the surfactant, sodium cholate hydrate (CHNaO  xHO), to make suspensions. These suspensions were then sonicated and centrifuged at different durations and speeds, respectively. Absorption was measured for all of the suspensions using ultraviolet-visible spectrometer to determine what parameters yielded the highest concentration of the three materials since a high UV absorption generally equated to a high yield of the layered materials. The final optimal parameters that yielded the highest concentration of each material were: 3 mg/ml material concentration, 3 mg/ml surfactant concentration, 30-minute continuous tip sonication method, and 1-hr 500 RPM centrifugation. Droplets of these optimal suspensions were then casted onto carbon nanofilms, and transmission electron microscopy (TEM) was performed on the films to confirm the layered, flaked characteristics and the hexagonal structures of MoS, WS, and BN.",
    "advisors": ["Linn W. Hobbs"],
    "text": "High yield production of inorganic graphene-like materials (MoS, WS, BN) through liquid exfoliation testing key parameters Inorganic graphene-like materials such as molybdenum disulfide (MoS), tungsten sulfide (WS), and boron nitride (BN) are known to have electronic properties. When exfoliated into layers and casted onto carbon nanofilms, they can become potentially cheap and efficient electronic materials for magnetic sensing and energy storage devices. The goal of this experiment is to use a general liquid-phase method to exfoliate and optimize a number of parameters that can yield the highest concentration of layered quantities of MoS, WS, and BN. The key parameters optmized were material concentration, surfactant concentration, sonication method and duration, and centrifuge speed. Therefore, different concentrations of the three materials were mixed with different concentrations of the surfactant, sodium cholate hydrate (CHNaO  xHO), to make suspensions. These suspensions were then sonicated and centrifuged at different durations and speeds, respectively. Absorption was measured for all of the suspensions using ultraviolet-visible spectrometer to determine what parameters yielded the highest concentration of the three materials since a high UV absorption generally equated to a high yield of the layered materials. The final optimal parameters that yielded the highest concentration of each material were: 3 mg/ml material concentration, 3 mg/ml surfactant concentration, 30-minute continuous tip sonication method, and 1-hr 500 RPM centrifugation. Droplets of these optimal suspensions were then casted onto carbon nanofilms, and transmission electron microscopy (TEM) was performed on the films to confirm the layered, flaked characteristics and the hexagonal structures of MoS, WS, and BN."
}, {
    "id": "oai:dspace.mit.edu:1721.1/96453",
    "title": "Cu-based shape memory microwires : towards complex structures",
    "abstract": "Shape memory alloys are a distinctive type of material that exhibits the fascinating properties of the shape memory effect and superelasticity. Shape memory properties are characterized by the diffusionless phase transformation between austenite and martensite that can be thermally or stress induced. Cu-based shape memory alloys provide an exciting area of research due to lower costs and higher working temperatures compared to Ni-Ti alloys prevalent in industry today. This work investigates the shape memory properties of oligocrystalline Cu-Al-Ni and Cu- Al-Mn-Ni microwires produced using a melt spinner. The melt spinner yielded continuous wires in quantities useful for the creation of complex structures. The composition of the wires is observed to change throughout processing of alloys and wires. Electropolishing rates were determined for improving surface texture and size constraint.",
    "advisors": ["Christopher A. Schuh"],
    "text": "Cu-based shape memory microwires : towards complex structures Shape memory alloys are a distinctive type of material that exhibits the fascinating properties of the shape memory effect and superelasticity. Shape memory properties are characterized by the diffusionless phase transformation between austenite and martensite that can be thermally or stress induced. Cu-based shape memory alloys provide an exciting area of research due to lower costs and higher working temperatures compared to Ni-Ti alloys prevalent in industry today. This work investigates the shape memory properties of oligocrystalline Cu-Al-Ni and Cu- Al-Mn-Ni microwires produced using a melt spinner. The melt spinner yielded continuous wires in quantities useful for the creation of complex structures. The composition of the wires is observed to change throughout processing of alloys and wires. Electropolishing rates were determined for improving surface texture and size constraint."
}, {
    "id": "oai:dspace.mit.edu:1721.1/75850",
    "title": "Mechanical characterization of jammable granular systems",
    "abstract": "The mode by which a granular material can transition between fluid-like and solid-like states has been often referred to as jamming. The use of this property (via vacuum pressure) for engineering applications has only recently been explored. Several possible applications are presented. However, thorough characterization of mechanical properties and material selection for jammed systems has not been reported. Glass beads of differing size distributions, silica blasting media, sand, and ground coffee were tested under different vacuum pressures in a procedure similar to an unconsolidated-undrained triaxial compression test for soils. Coffee was found to have the highest strength to weight ratio. Literature predictions of the trend between applied pressure and effective Young' modulus was also investigated.",
    "advisors": ["Neri Oxman"],
    "text": "Mechanical characterization of jammable granular systems The mode by which a granular material can transition between fluid-like and solid-like states has been often referred to as jamming. The use of this property (via vacuum pressure) for engineering applications has only recently been explored. Several possible applications are presented. However, thorough characterization of mechanical properties and material selection for jammed systems has not been reported. Glass beads of differing size distributions, silica blasting media, sand, and ground coffee were tested under different vacuum pressures in a procedure similar to an unconsolidated-undrained triaxial compression test for soils. Coffee was found to have the highest strength to weight ratio. Literature predictions of the trend between applied pressure and effective Young' modulus was also investigated."
}, {
    "id": "oai:dspace.mit.edu:1721.1/119027",
    "title": "Geometric and optical transformations of supramolecular host-guest amphiphiles",
    "abstract": "Molecular self-assembly has been an area of recent interest due to its application in a variety of important contexts including drug delivery, regenerative medicine, energy applications, and others. Simultaneously, host-guest chemistry provides a robust and powerful mechanism for inducing switching on the molecular level. In this research, we demonstrate a new platform that combines molecular self-assembly of an amphiphilic chromophore guest molecule with its host molecule counterpart, CB[8] in water. We find that upon addition of CB[8] to a solution of the amphiphilic guest molecule, host-guest complexation occurs and a transition in the morphology of the observed self-assembled nanostructures occurs. Here we present the synthetic route to our amphiphilic guest molecule, in addition to the nanostructural characterization of the supramolecular nanostructures and the host-guest nanostructure by TEM, UV-Vis, and fluorescence spectra.",
    "advisors": ["Julia Ortony"],
    "text": "Geometric and optical transformations of supramolecular host-guest amphiphiles Molecular self-assembly has been an area of recent interest due to its application in a variety of important contexts including drug delivery, regenerative medicine, energy applications, and others. Simultaneously, host-guest chemistry provides a robust and powerful mechanism for inducing switching on the molecular level. In this research, we demonstrate a new platform that combines molecular self-assembly of an amphiphilic chromophore guest molecule with its host molecule counterpart, CB[8] in water. We find that upon addition of CB[8] to a solution of the amphiphilic guest molecule, host-guest complexation occurs and a transition in the morphology of the observed self-assembled nanostructures occurs. Here we present the synthetic route to our amphiphilic guest molecule, in addition to the nanostructural characterization of the supramolecular nanostructures and the host-guest nanostructure by TEM, UV-Vis, and fluorescence spectra."
}, {
    "id": "oai:dspace.mit.edu:1721.1/43212",
    "title": "Study of the effect of mechanical stiffness substrata, assembled with polyelectrolyte multilayer thin films, on biofilm forming staphylococcus epidermidis' initial adhesion mechanism",
    "abstract": "Polyelectrolyte multilayer thin films are polymer films assembled through a layer-by-layer sequential addition of oppositely charged polymers. The layer-by-layer film assembly technique allows for properties such as film thickness, chemical functionality, and elastic moduli to be easily altered by changing the pH in solution, or the number of bilayers added. This thesis examined the use of polyelectrolyte multilayer films, assembled with poly(allylamine hydrochloride) (PAH) and poly(acrylic acid) (PAA), to alter substrata mechanical stiffness, which was used to explore the response of biofilm forming staphylococci epidermidis. The formation of biofilms on medical device surfaces is currently responsible for a significant amount of infections acquired in hospitals. Currently mechanisms responsible for the initial adhesion of bacteria are not completely understood. Previous work completed in the Rubner and Van Vliet labs at MIT suggests a mechanoselective adhesion mechanism in prokaryotes. The existence of a positive correlation between mechanical stiffness and bacterial adhesion, independent of surface roughness or charge density, has already been shown in a non-biofilm forming strain of bacteria. This thesis focused on exploring the role mechanical stiffness substrata has on biofilm forming bacterial adhesion by conducting bacterial assay experiments on polyelectrolyte multilayer films. The results showed no positive correlation between mechanical stiffness and cell adhesion with biofilm forming staphylococcus epidermidis. Also, even under an applied shear force the amount of bacteria adhered on the surface was not affected. In all cases tested, the biofilm forming strain of bacteria was able to adhere and grow successfully.",
    "advisors": ["Michael F. Rubner"],
    "text": "Study of the effect of mechanical stiffness substrata, assembled with polyelectrolyte multilayer thin films, on biofilm forming staphylococcus epidermidis' initial adhesion mechanism Polyelectrolyte multilayer thin films are polymer films assembled through a layer-by-layer sequential addition of oppositely charged polymers. The layer-by-layer film assembly technique allows for properties such as film thickness, chemical functionality, and elastic moduli to be easily altered by changing the pH in solution, or the number of bilayers added. This thesis examined the use of polyelectrolyte multilayer films, assembled with poly(allylamine hydrochloride) (PAH) and poly(acrylic acid) (PAA), to alter substrata mechanical stiffness, which was used to explore the response of biofilm forming staphylococci epidermidis. The formation of biofilms on medical device surfaces is currently responsible for a significant amount of infections acquired in hospitals. Currently mechanisms responsible for the initial adhesion of bacteria are not completely understood. Previous work completed in the Rubner and Van Vliet labs at MIT suggests a mechanoselective adhesion mechanism in prokaryotes. The existence of a positive correlation between mechanical stiffness and bacterial adhesion, independent of surface roughness or charge density, has already been shown in a non-biofilm forming strain of bacteria. This thesis focused on exploring the role mechanical stiffness substrata has on biofilm forming bacterial adhesion by conducting bacterial assay experiments on polyelectrolyte multilayer films. The results showed no positive correlation between mechanical stiffness and cell adhesion with biofilm forming staphylococcus epidermidis. Also, even under an applied shear force the amount of bacteria adhered on the surface was not affected. In all cases tested, the biofilm forming strain of bacteria was able to adhere and grow successfully."
}, {
    "id": "oai:dspace.mit.edu:1721.1/32850",
    "title": "Poly(ethylene glycol) hydrogel microspheres as a controlled release device",
    "abstract": "Vaccines for infections such as measles, polio, or chicken pox contain live attenuated viruses, which can sometimes lead to infection. Our objective is to develop an improved strategy for vaccines that induces patent immune responses against persistent viral infections. Three processes must occur to successfully produce immunity; the first is the attraction of immature Dendritic Cells (DCs), loading them with particular antigens, and then maturing the DCs. This project focuses on DC attraction to an immunization site by fabricating crosslinked polyethylene glycol hydrogel microspheres that encapsulate a chemoattractant. This study was performed to determine whether the diffusion of the chemoattractant could be controlled by varying the amount of crosslinker and by incorporating ionic groups in the polymer matrix. It was found that the crosslinker amounts successfully altered the release profiles of the protein. The ionic groups incorporated in the polymer matrix effectively altered the diffusion of both positively and negatively charged protein diffusion.",
    "advisors": ["Darrell J. Irvine"],
    "text": "Poly(ethylene glycol) hydrogel microspheres as a controlled release device Vaccines for infections such as measles, polio, or chicken pox contain live attenuated viruses, which can sometimes lead to infection. Our objective is to develop an improved strategy for vaccines that induces patent immune responses against persistent viral infections. Three processes must occur to successfully produce immunity; the first is the attraction of immature Dendritic Cells (DCs), loading them with particular antigens, and then maturing the DCs. This project focuses on DC attraction to an immunization site by fabricating crosslinked polyethylene glycol hydrogel microspheres that encapsulate a chemoattractant. This study was performed to determine whether the diffusion of the chemoattractant could be controlled by varying the amount of crosslinker and by incorporating ionic groups in the polymer matrix. It was found that the crosslinker amounts successfully altered the release profiles of the protein. The ionic groups incorporated in the polymer matrix effectively altered the diffusion of both positively and negatively charged protein diffusion."
}, {
    "id": "oai:dspace.mit.edu:1721.1/43214",
    "title": "Control of the self-assembly of alkanethiol-coated gold nanoparticles in the solid state",
    "abstract": "A study of the behavior of nanoparticles in the presence of solvent vapors is presented. Millimeter-scale films of gold nanoparticles, one nanometer thick, are treated with solvent vapors at various temperatures and the behavior of the nanoparticles is tracked over time using transmission electron microscopy. The ultimate goal of this processing is to repair defects such as grains, dislocations, and vacancies in the original superlattice. Additionally, Langmuir-Schaeffer films of gold nanoparticles on water surfaces are subjected to thermal and ultrasonic treatment in an attempt to correct defects in the films, which are then transferred to solid substrates for observation. Unfortunately, none of these approaches is able to reduce the defect concentration in a lattice, although thermal treatment and sonication of Langmuir-Schaeffer nanoparticle films are found to provide a controllable approach to depositing exact double layers of nanoparticles.",
    "advisors": ["Francesco Stellacci"],
    "text": "Control of the self-assembly of alkanethiol-coated gold nanoparticles in the solid state A study of the behavior of nanoparticles in the presence of solvent vapors is presented. Millimeter-scale films of gold nanoparticles, one nanometer thick, are treated with solvent vapors at various temperatures and the behavior of the nanoparticles is tracked over time using transmission electron microscopy. The ultimate goal of this processing is to repair defects such as grains, dislocations, and vacancies in the original superlattice. Additionally, Langmuir-Schaeffer films of gold nanoparticles on water surfaces are subjected to thermal and ultrasonic treatment in an attempt to correct defects in the films, which are then transferred to solid substrates for observation. Unfortunately, none of these approaches is able to reduce the defect concentration in a lattice, although thermal treatment and sonication of Langmuir-Schaeffer nanoparticle films are found to provide a controllable approach to depositing exact double layers of nanoparticles."
}, {
    "id": "oai:dspace.mit.edu:1721.1/32849",
    "title": "Anomalous solubility behavior of mixed monolayer protected metal nanoparticles",
    "abstract": "The solubility of mixed monolayer protected gold nanoparticles was studied. Monolayer protected metal nanoparticles are attractive materials because of the optical and electronic properties of their metal cores and because of the surface properties of their ligand coating. Recently, it was discovered that a mixture of ligands phase separate into ordered domains of single nanometer or subnanometer width on the surface of metal nanoparticles. The morphology and length of the ligand domains (which take the form of ripples on the particle surface) has given these nanoparticles novel properties. Because monolayer protected nanoparticles can be dissolved and dried many times, they can be handled and processed in ways not available to other nanomaterials. Understanding the solubility of mixed monolayer protected metal nanoparticles could help in implementing their unique new properties. This study demonstrates that the solubility of these particles in organic solvents cannot be explained only in terms of the composition of the ligand shell. Instead, solubility is also closely linked to morphology of the ligand shell via relationships between the size of the solvent molecule and the size of the features in the morphology.",
    "advisors": ["Francesco Stellacci"],
    "text": "Anomalous solubility behavior of mixed monolayer protected metal nanoparticles The solubility of mixed monolayer protected gold nanoparticles was studied. Monolayer protected metal nanoparticles are attractive materials because of the optical and electronic properties of their metal cores and because of the surface properties of their ligand coating. Recently, it was discovered that a mixture of ligands phase separate into ordered domains of single nanometer or subnanometer width on the surface of metal nanoparticles. The morphology and length of the ligand domains (which take the form of ripples on the particle surface) has given these nanoparticles novel properties. Because monolayer protected nanoparticles can be dissolved and dried many times, they can be handled and processed in ways not available to other nanomaterials. Understanding the solubility of mixed monolayer protected metal nanoparticles could help in implementing their unique new properties. This study demonstrates that the solubility of these particles in organic solvents cannot be explained only in terms of the composition of the ligand shell. Instead, solubility is also closely linked to morphology of the ligand shell via relationships between the size of the solvent molecule and the size of the features in the morphology."
}, {
    "id": "oai:dspace.mit.edu:1721.1/111357",
    "title": "TCAQ-based polymer for electrochemically mediated separations",
    "abstract": "Redox-mediated separation systems offer the potential to efficiently desalinate water and to purify contaminated waste streams, among other health and environmental applications. A TCAQ-based polymer, unique for its two-electron redox reaction, was synthesized for use in redox-mediated separation systems, and its performance was quantified in low-concentration aqueous salt solutions. The polyvinyl ferrocene (PVF)//PTCAQ system displayed an ion adsorption capacity much higher than previously reported literature values for capacitive or redox deionization systems.",
    "advisors": ["T. Alan Hatton", "Elsa Olivetti"],
    "text": "TCAQ-based polymer for electrochemically mediated separations Redox-mediated separation systems offer the potential to efficiently desalinate water and to purify contaminated waste streams, among other health and environmental applications. A TCAQ-based polymer, unique for its two-electron redox reaction, was synthesized for use in redox-mediated separation systems, and its performance was quantified in low-concentration aqueous salt solutions. The polyvinyl ferrocene (PVF)//PTCAQ system displayed an ion adsorption capacity much higher than previously reported literature values for capacitive or redox deionization systems."
}, {
    "id": "oai:dspace.mit.edu:1721.1/44817",
    "title": "Temperature effects on the electronic conductivity of single-walled carbon nanotubes",
    "abstract": "The room-temperature electronic conductivity and temperature dependence of conductivity were measured for samples of carbon nanotubes of three types: pristine; functionalized with a nitrobenzene covalent functionalization, which are expected to display poor electronic conductivity; and functionalized with a carbene covalent functionalization, which are expected to display pristine-like conductivity. Measurements were taken via four-point probe measurement across palladium contacts on a silicon surface coated with a distribution of nanotubes. Room-temperature measurements indicate that carbene-functionalized tubes do exhibit significantly greater conductivity than nitrobenzene-functionalized tubes, but also significantly less than pristine tubes. Statistically different distributions of resistances observed in similarly prepared samples indicate that this measurement technique is strongly affected by uncontrollable and so far uncharacterizable parameters of the employed sample preparation technique. Measurements at varying temperature indicate the expected linear relationship of resistance and temperature is dominated in pristine and carbene samples by a effect, possibly contamination-related, which significantly and permanently increases the resistance of samples after cycling to high temperatures, and which occurs repeatedly with additional cycling. Carbene-functionalized samples were observed to exhibit similar temperature behavior to pristine samples, while nitrobenzene-functionalized samples displayed erratic, unpredictable behavior.",
    "advisors": ["Francesco Stellacci"],
    "text": "Temperature effects on the electronic conductivity of single-walled carbon nanotubes The room-temperature electronic conductivity and temperature dependence of conductivity were measured for samples of carbon nanotubes of three types: pristine; functionalized with a nitrobenzene covalent functionalization, which are expected to display poor electronic conductivity; and functionalized with a carbene covalent functionalization, which are expected to display pristine-like conductivity. Measurements were taken via four-point probe measurement across palladium contacts on a silicon surface coated with a distribution of nanotubes. Room-temperature measurements indicate that carbene-functionalized tubes do exhibit significantly greater conductivity than nitrobenzene-functionalized tubes, but also significantly less than pristine tubes. Statistically different distributions of resistances observed in similarly prepared samples indicate that this measurement technique is strongly affected by uncontrollable and so far uncharacterizable parameters of the employed sample preparation technique. Measurements at varying temperature indicate the expected linear relationship of resistance and temperature is dominated in pristine and carbene samples by a effect, possibly contamination-related, which significantly and permanently increases the resistance of samples after cycling to high temperatures, and which occurs repeatedly with additional cycling. Carbene-functionalized samples were observed to exhibit similar temperature behavior to pristine samples, while nitrobenzene-functionalized samples displayed erratic, unpredictable behavior."
}, {
    "id": "oai:dspace.mit.edu:1721.1/76126",
    "title": "Assessment of the accuracy of DFT (Density Functional Theory) for the photochromic behavior of dihydroazulene (DHA)",
    "abstract": "Efficient utilization of the sun as a renewable and clean energy source is one of the greatest goals and challenges of this century due to the increasing demand for energy and its environmental impact. Photoactive molecules that can store the sun's energy in the form of chemical bonds have been of interest to harness the sun's energy since the 1970s. However, all of the photoactive systems studied have problems with degradation making them impractical. Recently, the Grossman Group used computation to show that nanotemplating of the azobenzene photoactivesystem improves problems with degradation. We believe that this could be a platform technology for other photoactive systems like azobenzene. We would like to use high throughput screenings to identify other promising photoactive molecules. We would like to use Density Functional Theory (DFT) calculations for these studies, since DFT is the least computationally intense Quantum Mechanical model used on large chemical systems. For photosystems like azobenzene, nombomadiene, and diruthenium fulvalene, DFT predictions have been found to match well with experimental predictions, suggesting that DFT can be used to confidently predict properties of these fuels. However, for dihydroazulene(DHA) photoactive predictions using different DFT functionals do not match with each other and experiment. Our analysis suggests that lack of error cancelation due to a drastic change in the conjugation in DHA as compared to VHF might account for the variation in predictions based on different DFT functionals. It was also found that the DFT functional, [psi]B97X-D, makes similar predictions as the more computationally intense post Hartree-Fock methods by including couple cluster terms that better capture weak interactions.",
    "advisors": ["Jeffrey C. Grossman"],
    "text": "Assessment of the accuracy of DFT (Density Functional Theory) for the photochromic behavior of dihydroazulene (DHA) Efficient utilization of the sun as a renewable and clean energy source is one of the greatest goals and challenges of this century due to the increasing demand for energy and its environmental impact. Photoactive molecules that can store the sun's energy in the form of chemical bonds have been of interest to harness the sun's energy since the 1970s. However, all of the photoactive systems studied have problems with degradation making them impractical. Recently, the Grossman Group used computation to show that nanotemplating of the azobenzene photoactivesystem improves problems with degradation. We believe that this could be a platform technology for other photoactive systems like azobenzene. We would like to use high throughput screenings to identify other promising photoactive molecules. We would like to use Density Functional Theory (DFT) calculations for these studies, since DFT is the least computationally intense Quantum Mechanical model used on large chemical systems. For photosystems like azobenzene, nombomadiene, and diruthenium fulvalene, DFT predictions have been found to match well with experimental predictions, suggesting that DFT can be used to confidently predict properties of these fuels. However, for dihydroazulene(DHA) photoactive predictions using different DFT functionals do not match with each other and experiment. Our analysis suggests that lack of error cancelation due to a drastic change in the conjugation in DHA as compared to VHF might account for the variation in predictions based on different DFT functionals. It was also found that the DFT functional, [psi]B97X-D, makes similar predictions as the more computationally intense post Hartree-Fock methods by including couple cluster terms that better capture weak interactions."
}, {
    "id": "oai:dspace.mit.edu:1721.1/32848",
    "title": "Creating selective directional interactions with defects caused by subnanometre-ordered ligand domains on the surface of colloidal metal nanoparticles for the purpose of directed self-assembly",
    "abstract": "Introduction: The ability to utilize directional, specific bonds are a fundamental property of atoms which has allowed us to predictably create molecules of consistent geometry and composition for centuries. One fundamental difference between a true atom and a nanoparticle is that to date, nanoparticles do not possess this property.",
    "advisors": ["Francesco Stellacci"],
    "text": "Creating selective directional interactions with defects caused by subnanometre-ordered ligand domains on the surface of colloidal metal nanoparticles for the purpose of directed self-assembly Introduction: The ability to utilize directional, specific bonds are a fundamental property of atoms which has allowed us to predictably create molecules of consistent geometry and composition for centuries. One fundamental difference between a true atom and a nanoparticle is that to date, nanoparticles do not possess this property."
}, {
    "id": "oai:dspace.mit.edu:1721.1/89965",
    "title": "A more comprehensive life cycle cost analysis of pavement materials alternatives",
    "abstract": "Life Cycle Cost Analysis (LCCA) is a commonly used tool in analyzing the economic viability of highway construction investments. The initial and life-cycle materials costs associated with highway construction involve a high level of uncertainty and therefore warrant extensive and dynamic cost analysis. These uncertainties derive from extensive materials usage costs. Despite the advantages of implementing a probabilistic approach to cost analysis, many state departments of transportation (DOTs) continue to employ a deterministic model, thereby misjudging, and often altogether neglecting the underlying uncertainty and risks. The goals of this paper are twofold: first, to validate forecasting as a viable method to predict future materials' prices, and second, to explore economies of scale as a potential driver of uncertainty. The paper will then apply these results to a case study methodology, looking at a comparative LCCA of two materials alternative, asphalt vs. concrete pavement designs for two states: Florida and Colorado. Endeavoring in this light, the author has characterized uncertainty in a way that will be comprehensible by practitioners. This research has successfully validated out-of-sample forecasting as a superior method of forecasting materials prices, characterized uncertainty related to project quantity, and delivered results using a relatable case study approach.",
    "advisors": ["Joel P. Clark"],
    "text": "A more comprehensive life cycle cost analysis of pavement materials alternatives Life Cycle Cost Analysis (LCCA) is a commonly used tool in analyzing the economic viability of highway construction investments. The initial and life-cycle materials costs associated with highway construction involve a high level of uncertainty and therefore warrant extensive and dynamic cost analysis. These uncertainties derive from extensive materials usage costs. Despite the advantages of implementing a probabilistic approach to cost analysis, many state departments of transportation (DOTs) continue to employ a deterministic model, thereby misjudging, and often altogether neglecting the underlying uncertainty and risks. The goals of this paper are twofold: first, to validate forecasting as a viable method to predict future materials' prices, and second, to explore economies of scale as a potential driver of uncertainty. The paper will then apply these results to a case study methodology, looking at a comparative LCCA of two materials alternative, asphalt vs. concrete pavement designs for two states: Florida and Colorado. Endeavoring in this light, the author has characterized uncertainty in a way that will be comprehensible by practitioners. This research has successfully validated out-of-sample forecasting as a superior method of forecasting materials prices, characterized uncertainty related to project quantity, and delivered results using a relatable case study approach."
}, {
    "id": "oai:dspace.mit.edu:1721.1/44813",
    "title": "Structural, magnetic, and optical properties of orthoferrite thin films",
    "abstract": "Pulsed laser deposition was used to create thin films of Ce-Fe-O and Y-Fe-O systems. Deposition temperature and ambient oxygen pressure were varied systematically between samples to determine which deposition conditions were most favorable to the formation of cerium/yttrium orthoferrite. The structure and composition of each film were then determined using X-ray diffraction and wavelength dispersive spectroscopy respectively. In addition, the magnetic and optical properties of the yttrium films were characterized to determine the suitability of these materials as Faraday isolators at A=1550 nm. Results show that orthoferrite crystal structures in these systems are not stable in the temperature and oxygen ranges tested. It was also found that increasing oxygen pressure caused exponential decay in the deposition rate. Most films were amorphous, exhibiting a paramagnetic M-H plot and a Verdet coefficient between 0.37 and 0.89 deg cm-1 Gauss-1",
    "advisors": ["Caroline Ross"],
    "text": "Structural, magnetic, and optical properties of orthoferrite thin films Pulsed laser deposition was used to create thin films of Ce-Fe-O and Y-Fe-O systems. Deposition temperature and ambient oxygen pressure were varied systematically between samples to determine which deposition conditions were most favorable to the formation of cerium/yttrium orthoferrite. The structure and composition of each film were then determined using X-ray diffraction and wavelength dispersive spectroscopy respectively. In addition, the magnetic and optical properties of the yttrium films were characterized to determine the suitability of these materials as Faraday isolators at A=1550 nm. Results show that orthoferrite crystal structures in these systems are not stable in the temperature and oxygen ranges tested. It was also found that increasing oxygen pressure caused exponential decay in the deposition rate. Most films were amorphous, exhibiting a paramagnetic M-H plot and a Verdet coefficient between 0.37 and 0.89 deg cm-1 Gauss-1"
}, {
    "id": "oai:dspace.mit.edu:1721.1/58274",
    "title": "Fracture of the interlayer junction of the shell from a deep-sea hydrothermal vent gastropod",
    "abstract": "There is considerable amount of interest in the hierarchical nanomechanical processes that contribute to property amplification of biomaterials. An investigation of these processes and the quantification of the mechanical properties and structure of a biomaterial multilayer is determined. The multilayer was composed of an inner, aragonite-like layer and a middle, compliant layer with a gradient layer between the two exhibiting a non-uniform composition and structure. It was found that the hardness of the middle, compliant layer was 0.1860.007 GPa, while the inner, aragonite-like had a hardness of 2.10.22 GPa. The hardness was found to be 1.660.44 GPa within the gradient layer. The indentation toughness of the inner layer was found to be 0.307+0.097 MPa*m1/2 . It was also found that cracks propagated along the grain boundaries within the inner and gradient layers. Crack growth was thus driven by the separation of the grains. The formation of multiple cracks ahead of the crack tip suggested the formation of bands analogous to dilatation bands observed in nacre under certain stress-states. Thus, the mechanisms behind grain separation, the micro-architecture of the anisotropic aragonite grains and other constituents, and the gradual compositional change observed in the tougher gradient layer all acted as toughening mechanisms and contributed to overall property amplification of the shell.",
    "advisors": ["Christine Ortiz"],
    "text": "Fracture of the interlayer junction of the shell from a deep-sea hydrothermal vent gastropod There is considerable amount of interest in the hierarchical nanomechanical processes that contribute to property amplification of biomaterials. An investigation of these processes and the quantification of the mechanical properties and structure of a biomaterial multilayer is determined. The multilayer was composed of an inner, aragonite-like layer and a middle, compliant layer with a gradient layer between the two exhibiting a non-uniform composition and structure. It was found that the hardness of the middle, compliant layer was 0.1860.007 GPa, while the inner, aragonite-like had a hardness of 2.10.22 GPa. The hardness was found to be 1.660.44 GPa within the gradient layer. The indentation toughness of the inner layer was found to be 0.307+0.097 MPa*m1/2 . It was also found that cracks propagated along the grain boundaries within the inner and gradient layers. Crack growth was thus driven by the separation of the grains. The formation of multiple cracks ahead of the crack tip suggested the formation of bands analogous to dilatation bands observed in nacre under certain stress-states. Thus, the mechanisms behind grain separation, the micro-architecture of the anisotropic aragonite grains and other constituents, and the gradual compositional change observed in the tougher gradient layer all acted as toughening mechanisms and contributed to overall property amplification of the shell."
}, {
    "id": "oai:dspace.mit.edu:1721.1/42998",
    "title": "Economic and environmental evaluation of end-of-life aerospace aluminum options using optimization methods",
    "abstract": "The benefits of recycling have long been understood and the conspicuous energy savings of secondary aluminum production have caused aluminum recycling to increase. Obsolete aircraft are a valuable source of aluminum scrap and recent efforts to fortify the aerospace aluminum recycling infrastructure have drawn attention to the potential of sophisticated sorting methods to maximize the economic gain of using aerospace scrap in secondary production. The aim of this research was to use linear optimization to assess the economic viability of sorting technologies for enabling wrought products in general and aerospace alloys in particular to be recycled back to high value applications. A chance-constrained model was used to select the alloys that consumed the largest quantity of aerospace alloys in their production, thereby establishing a strategic portfolio of finished goods. Ten of the fifteen alloys in the portfolio were of the 2xxx and 7xxx alloy series that are standard in the production of aerospace components. An aerospace end-of-life case study was performed in which cases varied by their input scrap streams, each having a compositional uncertainty associated with the different degrees of sorting that methods currently in use and technologies in development can achieve. The chance-constrained model calculated the production cost for each case and determined that when aerospace components were identified to the precision of individual alloys, the production cost was 20.87% lower than the cost for primary production. Using automatically sorted scrap input yielded a production cost that was 5.34% lower than the cost of primary production.",
    "advisors": ["Randolph E. Kirchain, Jr"],
    "text": "Economic and environmental evaluation of end-of-life aerospace aluminum options using optimization methods The benefits of recycling have long been understood and the conspicuous energy savings of secondary aluminum production have caused aluminum recycling to increase. Obsolete aircraft are a valuable source of aluminum scrap and recent efforts to fortify the aerospace aluminum recycling infrastructure have drawn attention to the potential of sophisticated sorting methods to maximize the economic gain of using aerospace scrap in secondary production. The aim of this research was to use linear optimization to assess the economic viability of sorting technologies for enabling wrought products in general and aerospace alloys in particular to be recycled back to high value applications. A chance-constrained model was used to select the alloys that consumed the largest quantity of aerospace alloys in their production, thereby establishing a strategic portfolio of finished goods. Ten of the fifteen alloys in the portfolio were of the 2xxx and 7xxx alloy series that are standard in the production of aerospace components. An aerospace end-of-life case study was performed in which cases varied by their input scrap streams, each having a compositional uncertainty associated with the different degrees of sorting that methods currently in use and technologies in development can achieve. The chance-constrained model calculated the production cost for each case and determined that when aerospace components were identified to the precision of individual alloys, the production cost was 20.87% lower than the cost for primary production. Using automatically sorted scrap input yielded a production cost that was 5.34% lower than the cost of primary production."
}, {
    "id": "oai:dspace.mit.edu:1721.1/32851",
    "title": "Evolution of microstructure and crystalline texture in aluminum sheet metal subjected to high strain rate biaxial deformation",
    "abstract": "Electrohydraulic forming was used to biaxially stretch commercial Aluminum 5052 sheet metal workpieces at a high strain rate. Annealed and unannealed workpieces were formed. Specimens were taken from unformed metal and from the formed workpieces. Microstructures were examined with optical microscopy and pole figures were generated from X-ray diffraction data. Microstructures and crystalline textures were compared between formed and unformed and annealed and unannealed metal specimens, and strains were measured from the formed workpieces.",
    "advisors": ["Christopher Schuh"],
    "text": "Evolution of microstructure and crystalline texture in aluminum sheet metal subjected to high strain rate biaxial deformation Electrohydraulic forming was used to biaxially stretch commercial Aluminum 5052 sheet metal workpieces at a high strain rate. Annealed and unannealed workpieces were formed. Specimens were taken from unformed metal and from the formed workpieces. Microstructures were examined with optical microscopy and pole figures were generated from X-ray diffraction data. Microstructures and crystalline textures were compared between formed and unformed and annealed and unannealed metal specimens, and strains were measured from the formed workpieces."
}, {
    "id": "oai:dspace.mit.edu:1721.1/35055",
    "title": "Electronic properties of phenylated ligand-capped nanoparticle films",
    "abstract": "An investigation was carried out of the electronic characteristics of drop-cast films comprised of phenylated ligand-capped gold nanoparticles. In homoligand-type films, the dominant mechanism of charge transfer was expected to involve orbital overlap and end group-effected wave function displacement, whereas heteroligand-type films were expected to conduct through less efficient hopping mechanisms. Films utilizing the former mechanism are expected to have great applicability within microelectronics and rapid-prototyping technologies due to the small scale (2-6nm) of functionalized nanoparticles and the structural flexibility of interdigitation as a form of inter-particle bonding. The comparative conductances of the cast films reveal a strong correlation with the ligand Hammaker constant (effectively a measure of the work function of the conjugated bond with the gold core of the nanoparticle and the charge displacement effected by the electronegativity or polarity of the ligand end group). The conductance was also greatly affected by the size of ligand end groups - a rough measure of the close-packing ability of a given ligand both within the ligand shell and amongst the shells of adjacent nanoparticles. The following experiments illustrate these correlations, as well as the effects of ligand spacing and shell composition on the dominant charge transfer mechanism.",
    "advisors": ["Francesco Stellacci"],
    "text": "Electronic properties of phenylated ligand-capped nanoparticle films An investigation was carried out of the electronic characteristics of drop-cast films comprised of phenylated ligand-capped gold nanoparticles. In homoligand-type films, the dominant mechanism of charge transfer was expected to involve orbital overlap and end group-effected wave function displacement, whereas heteroligand-type films were expected to conduct through less efficient hopping mechanisms. Films utilizing the former mechanism are expected to have great applicability within microelectronics and rapid-prototyping technologies due to the small scale (2-6nm) of functionalized nanoparticles and the structural flexibility of interdigitation as a form of inter-particle bonding. The comparative conductances of the cast films reveal a strong correlation with the ligand Hammaker constant (effectively a measure of the work function of the conjugated bond with the gold core of the nanoparticle and the charge displacement effected by the electronegativity or polarity of the ligand end group). The conductance was also greatly affected by the size of ligand end groups - a rough measure of the close-packing ability of a given ligand both within the ligand shell and amongst the shells of adjacent nanoparticles. The following experiments illustrate these correlations, as well as the effects of ligand spacing and shell composition on the dominant charge transfer mechanism."
}, {
    "id": "oai:dspace.mit.edu:1721.1/104316",
    "title": "Electrochemical behavior of a liquid tin electrode in molten ternary salt electrolyte containing sodium chloride, aluminum chloride, and tin chloride",
    "abstract": "One of the key limitations in the wide-scale adoption of mature renewable energy technologies is the lack of grid-level energy storage solutions. One important figure of merit in these battery systems is a high rate capability to match fluctuating demands for electricity. Molten salt batteries are an attractive option for stationary storage due to fast kinetics and good cycling capability, but high temperatures (>300 C) limit available materials. In this thesis, the molten NaCl-AlCl3-SnCl2 electrolyte and liquid Sn electrode couple at 250 C is investigated as part of the potential cell Na I NaCl-AlCl 3-SnCl2 I Sn for a lower temperature molten salt battery. An electrochemical study of the kinetics in the molten salt electrolyte and at the liquid Sn electrode-electrolyte interface is conducted using cyclic voltammetry and the galvanostatic pulse method. The liquid metal electrode is found to have suitably fast kinetics with an exchange current density of 92 mA/cm2. Parameters for a new Na+ conducting membrane are proposed, requiring an ionic conductivity of 0.056 S/cm, which would allow for a hypothetical Na I NaCl-AlC 3-SnCl2 I Sn battery to operate with an energy efficiency of 70%.",
    "advisors": ["Donald R. Sadoway"],
    "text": "Electrochemical behavior of a liquid tin electrode in molten ternary salt electrolyte containing sodium chloride, aluminum chloride, and tin chloride One of the key limitations in the wide-scale adoption of mature renewable energy technologies is the lack of grid-level energy storage solutions. One important figure of merit in these battery systems is a high rate capability to match fluctuating demands for electricity. Molten salt batteries are an attractive option for stationary storage due to fast kinetics and good cycling capability, but high temperatures (>300 C) limit available materials. In this thesis, the molten NaCl-AlCl3-SnCl2 electrolyte and liquid Sn electrode couple at 250 C is investigated as part of the potential cell Na I NaCl-AlCl 3-SnCl2 I Sn for a lower temperature molten salt battery. An electrochemical study of the kinetics in the molten salt electrolyte and at the liquid Sn electrode-electrolyte interface is conducted using cyclic voltammetry and the galvanostatic pulse method. The liquid metal electrode is found to have suitably fast kinetics with an exchange current density of 92 mA/cm2. Parameters for a new Na+ conducting membrane are proposed, requiring an ionic conductivity of 0.056 S/cm, which would allow for a hypothetical Na I NaCl-AlC 3-SnCl2 I Sn battery to operate with an energy efficiency of 70%."
}, {
    "id": "oai:dspace.mit.edu:1721.1/43801",
    "title": "Characterization of new surface morphologies in a hydrogen-bonded multilayer system",
    "abstract": "This work presents an analysis of surface morphology changes in poly(acrylic acid)/polyacrylamide (PAA/PAAm) hydrogen-bonded multilayers. These changes were induced by immersion of the films in aqueous solutions of poly(allylamine hydrochloride), or PAH, at different levels of pH. Positive charges on PAH are attracted to negative charges on PAA, forming ionic bonds and locally decreasing the hydrophilicity of the multilayer. The degree of ionization for each polyelectrolyte, controlled by the pH of the treatment solution, determines the molecular conformations and the extent of electrostatic interactions. These factors, in turn, determine the resulting morphology of the film. Different surface morphologies appeared in four different pH regimes. Highly acidic solutions retained the film's original smooth surface, but wrinkled, honeycomb, or globular morphologies appeared as the pH increased. The three different surface morphologies correlate with the linear, pearl necklace, and globular conformations of PAH.",
    "advisors": ["Michael F. Rubner"],
    "text": "Characterization of new surface morphologies in a hydrogen-bonded multilayer system This work presents an analysis of surface morphology changes in poly(acrylic acid)/polyacrylamide (PAA/PAAm) hydrogen-bonded multilayers. These changes were induced by immersion of the films in aqueous solutions of poly(allylamine hydrochloride), or PAH, at different levels of pH. Positive charges on PAH are attracted to negative charges on PAA, forming ionic bonds and locally decreasing the hydrophilicity of the multilayer. The degree of ionization for each polyelectrolyte, controlled by the pH of the treatment solution, determines the molecular conformations and the extent of electrostatic interactions. These factors, in turn, determine the resulting morphology of the film. Different surface morphologies appeared in four different pH regimes. Highly acidic solutions retained the film's original smooth surface, but wrinkled, honeycomb, or globular morphologies appeared as the pH increased. The three different surface morphologies correlate with the linear, pearl necklace, and globular conformations of PAH."
}, {
    "id": "oai:dspace.mit.edu:1721.1/57874",
    "title": "Physical analysis of collagen-GAG composite scaffolds for nucleus pulposus tissue regeneration",
    "abstract": "In this study biomaterial scaffolds for regeneration of nucleus pulposus were developed by freeze drying slurries with different proportions of collagen II (CII), chondroitin-6-sulfate (CS), and hyaluronic acid (HA). The scaffolds were analyzed using biochemical assays to determine final composition. Chemically cross-linked scaffolds were analyzed to determine pore size and cross-link density. It was determined that every material type contained large enough pore size (275 gm) to seed nucleus pulposus cells and mesenchymal stem cells. The addition of CS to the scaffold increased pore size. It was also found that increasing levels of CS and HA resulted in lower cross-link density. These materials will be used next in In Vitro studies to determine their viability as regenerative tissue engineering constructs.",
    "advisors": ["Myron Spector"],
    "text": "Physical analysis of collagen-GAG composite scaffolds for nucleus pulposus tissue regeneration In this study biomaterial scaffolds for regeneration of nucleus pulposus were developed by freeze drying slurries with different proportions of collagen II (CII), chondroitin-6-sulfate (CS), and hyaluronic acid (HA). The scaffolds were analyzed using biochemical assays to determine final composition. Chemically cross-linked scaffolds were analyzed to determine pore size and cross-link density. It was determined that every material type contained large enough pore size (275 gm) to seed nucleus pulposus cells and mesenchymal stem cells. The addition of CS to the scaffold increased pore size. It was also found that increasing levels of CS and HA resulted in lower cross-link density. These materials will be used next in In Vitro studies to determine their viability as regenerative tissue engineering constructs."
}, {
    "id": "oai:dspace.mit.edu:1721.1/58449",
    "title": "Modeling the reaction mechanism of membrane penetration by striated amphiphitic gold nanoparticles",
    "abstract": "The desire to desire targeted drug delivery devices capable of releasing therapeutic payloads within the cytosol of cells has led to research on nanoparticles as suitable drug carriers. Recently, it was shown that gold nanoparticles coated in striped, alternating layers of hydrophobic and hydrophilic ligands are capable of non-disruptively penetrating a lipid bilayer, a discovery with potential implications in drug delivery. While the reaction mechanism is not known, initial experimental results indicate that endocytosis and membrane poration could be ruled as possible mechanisms. In this work, we explore the reaction mechanism of membrane penetration using a coarse-grained Brownian Dynamics model. We also define a Monte Carlo simulation for modeling ligand motion on the nanoparticle surface based on a single order parameter, and describe a method for approximating the interaction energy with the bilayer as a function of this parameter. Our simulations demonstrate the dependence of nanoparticles penetration on the surface mobility, not explicit conformation, of coated ligands. They demonstrate that while nanoparticles with static ligands in a striped conformation are unable to penetrate the bilayer, enabling surface mobility allows penetration by the induced formation of a small, transient pore of a comparable size to the nanoparticle. Our results offer an enhanced understanding of the nanoparticles-bilayer interaction and an identification of the property necessary for membrane penetration.",
    "advisors": ["Alfredo Alexander-Katz"],
    "text": "Modeling the reaction mechanism of membrane penetration by striated amphiphitic gold nanoparticles The desire to desire targeted drug delivery devices capable of releasing therapeutic payloads within the cytosol of cells has led to research on nanoparticles as suitable drug carriers. Recently, it was shown that gold nanoparticles coated in striped, alternating layers of hydrophobic and hydrophilic ligands are capable of non-disruptively penetrating a lipid bilayer, a discovery with potential implications in drug delivery. While the reaction mechanism is not known, initial experimental results indicate that endocytosis and membrane poration could be ruled as possible mechanisms. In this work, we explore the reaction mechanism of membrane penetration using a coarse-grained Brownian Dynamics model. We also define a Monte Carlo simulation for modeling ligand motion on the nanoparticle surface based on a single order parameter, and describe a method for approximating the interaction energy with the bilayer as a function of this parameter. Our simulations demonstrate the dependence of nanoparticles penetration on the surface mobility, not explicit conformation, of coated ligands. They demonstrate that while nanoparticles with static ligands in a striped conformation are unable to penetrate the bilayer, enabling surface mobility allows penetration by the induced formation of a small, transient pore of a comparable size to the nanoparticle. Our results offer an enhanced understanding of the nanoparticles-bilayer interaction and an identification of the property necessary for membrane penetration."
}, {
    "id": "oai:dspace.mit.edu:1721.1/89981",
    "title": "Modeling trabecular microstructure evolution via genetic algorithm",
    "abstract": "Connecting structure to properties, and optimizing properties by controlling structure is one of the fundamental goals of materials science and engineering. No where is this connection more apparent than with biomaterials, whose unparalleled properties are the result of the evolution via cumulative selection of highly specialized structures. Beyond biomaterials, cumulative selection offers a generalizable model for materials optimization via accumulative of beneficial mutations in a material's genome that improve the properties for a given function. A genetic algorithm is one method for applying the principals of cumulative selection to material's optimization. One of unique property that cumulative selection generated was the ability of trabecular bone to optimize and adjust its structure in vivo in response to changes in its loading conditions. This work presents a model for trabecular microstructure evolution using a genetic algorithm, the same mechanism through which that ability evolved. The algorithm begins by translating a trabecular genome into a developed structure. It then simulates the structure's response under an applied load and selects for the genome which translates into the best structure. The selected genome is then replicated and mutated. Simulations of microstructure evolution consist of iterating through this process across multiple generations. A series of simulations was conducted demonstrating the ability of the algorithm to improve trabecular architecture. The systems tended to converge to a uniform stress distribution, after which additional generations of evolution had no effect on performance. During the simulations it was found that the length of the computation was most sensitive to the number of offspring per generation. Although focused on trabecular microstructure, this work establishes the use of a genetic algorithm as a general tool for material's optimization.",
    "advisors": ["W. Craig Carter"],
    "text": "Modeling trabecular microstructure evolution via genetic algorithm Connecting structure to properties, and optimizing properties by controlling structure is one of the fundamental goals of materials science and engineering. No where is this connection more apparent than with biomaterials, whose unparalleled properties are the result of the evolution via cumulative selection of highly specialized structures. Beyond biomaterials, cumulative selection offers a generalizable model for materials optimization via accumulative of beneficial mutations in a material's genome that improve the properties for a given function. A genetic algorithm is one method for applying the principals of cumulative selection to material's optimization. One of unique property that cumulative selection generated was the ability of trabecular bone to optimize and adjust its structure in vivo in response to changes in its loading conditions. This work presents a model for trabecular microstructure evolution using a genetic algorithm, the same mechanism through which that ability evolved. The algorithm begins by translating a trabecular genome into a developed structure. It then simulates the structure's response under an applied load and selects for the genome which translates into the best structure. The selected genome is then replicated and mutated. Simulations of microstructure evolution consist of iterating through this process across multiple generations. A series of simulations was conducted demonstrating the ability of the algorithm to improve trabecular architecture. The systems tended to converge to a uniform stress distribution, after which additional generations of evolution had no effect on performance. During the simulations it was found that the length of the computation was most sensitive to the number of offspring per generation. Although focused on trabecular microstructure, this work establishes the use of a genetic algorithm as a general tool for material's optimization."
}, {
    "id": "oai:dspace.mit.edu:1721.1/32846",
    "title": "Characterization of polystyrene-block-poly (acrylic acid) micelles",
    "abstract": "Several parameters that affect the formation, size and spatial distribution of micelles of poly(styrene-block-acrylic acid) (PS-b-PAA) in organic solvents or assembled on solid substrates have been investigated. The micelles were characterized in the solvated state using Dynamic Light Scattering, and were imaged and characterized in the dry thin film state using Atomic Force Microscopy. Micelle size in solution followed scaling laws based on the ratio of the two block copolymer segments. Micelle size was not affected by the addition of PS homopolymer or salt, whereas micelle diameter did increase with the addition of PAA homopolymer both in solution and in the dry state on sold supports. Furthermore, micelles formed in toluene, but they did not form in tetrahydrofurane, chloroform or hexane. In terms of spatial distribution in the dry state, the only parameters which affected spacing, and therefore density, were annealing conditions and addition of PAA homopolymer. Annealing near or below the glass transition temperature for 16 hours increased the order of the films, as was demonstrated by Fast Fourier Transforms of their AFM images. Annealing for longer periods of time or at temperatures significantly above the glass transition temperature destroyed the micelles.",
    "advisors": ["Darrell J. Irvine"],
    "text": "Characterization of polystyrene-block-poly (acrylic acid) micelles Several parameters that affect the formation, size and spatial distribution of micelles of poly(styrene-block-acrylic acid) (PS-b-PAA) in organic solvents or assembled on solid substrates have been investigated. The micelles were characterized in the solvated state using Dynamic Light Scattering, and were imaged and characterized in the dry thin film state using Atomic Force Microscopy. Micelle size in solution followed scaling laws based on the ratio of the two block copolymer segments. Micelle size was not affected by the addition of PS homopolymer or salt, whereas micelle diameter did increase with the addition of PAA homopolymer both in solution and in the dry state on sold supports. Furthermore, micelles formed in toluene, but they did not form in tetrahydrofurane, chloroform or hexane. In terms of spatial distribution in the dry state, the only parameters which affected spacing, and therefore density, were annealing conditions and addition of PAA homopolymer. Annealing near or below the glass transition temperature for 16 hours increased the order of the films, as was demonstrated by Fast Fourier Transforms of their AFM images. Annealing for longer periods of time or at temperatures significantly above the glass transition temperature destroyed the micelles."
}, {
    "id": "oai:dspace.mit.edu:1721.1/58375",
    "title": "Analysis of hydraulic power transduction in regenerative rotary shock absorbers as function of working fluid kinematic viscosity",
    "abstract": "This investigation seeks to investigate the relationship of kinematic fluid viscosity to the effective power transduction seen by a hydraulic motor. Applications of this research specifically relate to energy recovery from a vehicle suspension system through the shock absorbers. A regenerative, hydraulic-based, rotary shock absorber was designed and fabricated for the purposes of this investigation. The kinematic viscosities ranging from 100 cSt to 200 cSt were used in the fluid circuit and tested for maximal efficiency of the hydraulic system. Balance between shear-force losses in the fluid circuit, and effective transfer of momentum at the water-wheel type hydraulic motor demonstrates that optimized performance of the system is attained when a midpoint is reached in the kinematic viscosity of the fluid.",
    "advisors": ["Caroline Ross"],
    "text": "Analysis of hydraulic power transduction in regenerative rotary shock absorbers as function of working fluid kinematic viscosity This investigation seeks to investigate the relationship of kinematic fluid viscosity to the effective power transduction seen by a hydraulic motor. Applications of this research specifically relate to energy recovery from a vehicle suspension system through the shock absorbers. A regenerative, hydraulic-based, rotary shock absorber was designed and fabricated for the purposes of this investigation. The kinematic viscosities ranging from 100 cSt to 200 cSt were used in the fluid circuit and tested for maximal efficiency of the hydraulic system. Balance between shear-force losses in the fluid circuit, and effective transfer of momentum at the water-wheel type hydraulic motor demonstrates that optimized performance of the system is attained when a midpoint is reached in the kinematic viscosity of the fluid."
}, {
    "id": "oai:dspace.mit.edu:1721.1/98663",
    "title": "Fabrication of tissue scaffolds using projection micro-stereolithography",
    "abstract": "In vitro liver models are a critical tool in pharmaceutical research, yet standard hepatocyte cultures fail to capture the complexity of in vivo tissue behavior. One of the most critical features of the in vivo liver is the extensive microvasculature which allows for the delivery of nutrients and metabolites without exposing hepatocytes to de-differentiating fluidic shear stresses. A new liver tissue scaffold design able to capture this histological organization may therefore improve the functional longevity of seeded hepatocytes. The additive manufacturing technique of projection micro-stereolithography (PuSL) proved capable of building non-cytotoxic and highly complex 3D structures with microvasculature on the order of 20 um inner diameter. While extensive biological testing remains to be carried out, the built structures reveal much promise in PuSL as a method of tissue scaffold fabrication in terms of in vivo mimicking architecture.",
    "advisors": ["Niels Holten-Andersen"],
    "text": "Fabrication of tissue scaffolds using projection micro-stereolithography In vitro liver models are a critical tool in pharmaceutical research, yet standard hepatocyte cultures fail to capture the complexity of in vivo tissue behavior. One of the most critical features of the in vivo liver is the extensive microvasculature which allows for the delivery of nutrients and metabolites without exposing hepatocytes to de-differentiating fluidic shear stresses. A new liver tissue scaffold design able to capture this histological organization may therefore improve the functional longevity of seeded hepatocytes. The additive manufacturing technique of projection micro-stereolithography (PuSL) proved capable of building non-cytotoxic and highly complex 3D structures with microvasculature on the order of 20 um inner diameter. While extensive biological testing remains to be carried out, the built structures reveal much promise in PuSL as a method of tissue scaffold fabrication in terms of in vivo mimicking architecture."
}, {
    "id": "oai:dspace.mit.edu:1721.1/114089",
    "title": "Optical absorption of bismuth silicon oxide (BiSiO) crystals",
    "abstract": "The purpose of this work is to characterize the optical absorption in bismuth silicon oxide (BiSiO) crystals grown using the Bridgman technique and to identify electronic transitions responsible for absorption. Optical measurements were taken in the range of 0.4 - 11 pm at 300 K and 77 K using a spectrometer. The results show that near the band edge, there is evidence of indirect transitions at 2.3 eV and excition transitions at 1.8 eV. Low temperature measurements revealed peaks of free carrier absorption in the visible light range at 1.7 eV and 2.1 eV. Illuminated samples at low temperature revealed empty donor levels in the visible range at 1.6-1.9 eV and 2.1 eV, indicating the presence of the photochromic effect and photorefractivity.",
    "advisors": ["August F. Witt"],
    "text": "Optical absorption of bismuth silicon oxide (BiSiO) crystals The purpose of this work is to characterize the optical absorption in bismuth silicon oxide (BiSiO) crystals grown using the Bridgman technique and to identify electronic transitions responsible for absorption. Optical measurements were taken in the range of 0.4 - 11 pm at 300 K and 77 K using a spectrometer. The results show that near the band edge, there is evidence of indirect transitions at 2.3 eV and excition transitions at 1.8 eV. Low temperature measurements revealed peaks of free carrier absorption in the visible light range at 1.7 eV and 2.1 eV. Illuminated samples at low temperature revealed empty donor levels in the visible range at 1.6-1.9 eV and 2.1 eV, indicating the presence of the photochromic effect and photorefractivity."
}, {
    "id": "oai:dspace.mit.edu:1721.1/114086",
    "title": "Materials characterization and transmission analysis in erbium-doped gallium nitride microresonator structures",
    "abstract": "GaN:Er is an attractive material for room temperature 1.54 pm luminescence enhancement devices for use in telecommunications because it does not experience thermal quenching at room temperature like Si:Er and can be electronically pumped. GaN:Er layers grown by molecular beam epitaxy (MBE) on single crystal substrates have shown excellent room temperature 1.54 [mu]m luminescence, but to integrate GaN:Er into microresonator devices it is necessary to grow a good quality GaN:Er film on an amorphous substrate. This thesis examines the optical properties and morphology of GaN:Er layers grown on SiN and SiO substrates, and evaluates two microresonator devices with incorporated GaN:Er layers. GaN:Er layers grown by MBE on SiO and SiN substrates were shown to give room temperature luminescence comparable to that of GaN:Er grown on (11 1)Si. GaN:Er layers grown on a buffered oxide etched SiN substrate showed the best luminescence. The ability to grow good quality layers on amorphous substrates allows GaN:Er to be used in waveguide devices, the first of which studied was the microring resonator. Microring resonators were made by depositing a blanket GaN:Er layer on patterned SiN microring structures. These structures were damaged, and transmission measurements were not possible. When looking at surface roughness measurements it appears that channel waveguide structures are unsuitable for GaN:Er grown on amorphous substrates, and so a ridge waveguide structure is proposed to lower this surface roughness scattering loss. A microcavity with a GaN:Er defect layer and a-Si/a-SiO stacks was fabricated and tested for luminescence enhancement. The refractive index of GaN:Er was determined by reflectance measurements to be 2.1. The layer was not of uniform thickness which led to a broad resonance peak, but a distortion of the spectrum including a lower luminescence at the 1517 nm peak and a higher luminescence at the 1557 nm peak were observed, which suggests enhancement by the microcavity.",
    "advisors": ["Kazumi Wada", "Lionel C. Kimerling"],
    "text": "Materials characterization and transmission analysis in erbium-doped gallium nitride microresonator structures GaN:Er is an attractive material for room temperature 1.54 pm luminescence enhancement devices for use in telecommunications because it does not experience thermal quenching at room temperature like Si:Er and can be electronically pumped. GaN:Er layers grown by molecular beam epitaxy (MBE) on single crystal substrates have shown excellent room temperature 1.54 [mu]m luminescence, but to integrate GaN:Er into microresonator devices it is necessary to grow a good quality GaN:Er film on an amorphous substrate. This thesis examines the optical properties and morphology of GaN:Er layers grown on SiN and SiO substrates, and evaluates two microresonator devices with incorporated GaN:Er layers. GaN:Er layers grown by MBE on SiO and SiN substrates were shown to give room temperature luminescence comparable to that of GaN:Er grown on (11 1)Si. GaN:Er layers grown on a buffered oxide etched SiN substrate showed the best luminescence. The ability to grow good quality layers on amorphous substrates allows GaN:Er to be used in waveguide devices, the first of which studied was the microring resonator. Microring resonators were made by depositing a blanket GaN:Er layer on patterned SiN microring structures. These structures were damaged, and transmission measurements were not possible. When looking at surface roughness measurements it appears that channel waveguide structures are unsuitable for GaN:Er grown on amorphous substrates, and so a ridge waveguide structure is proposed to lower this surface roughness scattering loss. A microcavity with a GaN:Er defect layer and a-Si/a-SiO stacks was fabricated and tested for luminescence enhancement. The refractive index of GaN:Er was determined by reflectance measurements to be 2.1. The layer was not of uniform thickness which led to a broad resonance peak, but a distortion of the spectrum including a lower luminescence at the 1517 nm peak and a higher luminescence at the 1557 nm peak were observed, which suggests enhancement by the microcavity."
}, {
    "id": "oai:dspace.mit.edu:1721.1/35075",
    "title": "A study investigating copper smelting remains from San Bartolo, Chile",
    "abstract": "Introduction: Research on the metallurgy of archaeological artifacts has focused primarily on the examination of objects to reveal their design, their composition, the properties of the material people selected to achieve the design, and the fabrication processes used in managing the metal to produce the end product. Recently that focus has begun to broaden, and archaeologists are taking a step back to investigate the earliest stages of prehistoric metal processing that precede object manufacture, namely ore mining and extractive metallurgy. However, little archaeological work on mining and extraction has been accomplished to date, in part because so few metal processing sites have been identified. These sites are very difficult to find because of the lack of standing architecture, particularly smelting installations. Prehistoric smelting furnaces tend to be small and are either excavated beneath the ground surface or are above ground but made of impermanent materials.",
    "advisors": ["Heather N. Lechtman"],
    "text": "A study investigating copper smelting remains from San Bartolo, Chile Introduction: Research on the metallurgy of archaeological artifacts has focused primarily on the examination of objects to reveal their design, their composition, the properties of the material people selected to achieve the design, and the fabrication processes used in managing the metal to produce the end product. Recently that focus has begun to broaden, and archaeologists are taking a step back to investigate the earliest stages of prehistoric metal processing that precede object manufacture, namely ore mining and extractive metallurgy. However, little archaeological work on mining and extraction has been accomplished to date, in part because so few metal processing sites have been identified. These sites are very difficult to find because of the lack of standing architecture, particularly smelting installations. Prehistoric smelting furnaces tend to be small and are either excavated beneath the ground surface or are above ground but made of impermanent materials."
}, {
    "id": "oai:dspace.mit.edu:1721.1/98662",
    "title": "Using electrochemical impedance spectroscopy to characterize vertically-aligned carbon nanotube forest porosimetry",
    "abstract": "Carbon nanotubes have generated much research interest and potential applications due to their unique properties such as their high tensile strength, high thermal conductivity, and unique semiconductor properties. Vertically-aligned carbon nanotubes (VA-CNTs) have been used in applications for electrochemical systems in energy storage systems and desalination systems. Typical methods of characterizing the morphology and composition of CNTs are limited in providing information on the packing density of CNTs, and therefore, an effective method for in situ characterization of VA-CNT electrodes is needed. This method explores the use of impedance spectroscopy and other electrochemical methods to characterize VA-CNTs in situ. VA-CNTs forests were grown via chemical vapor densification on pre-oxidized silicon wafers, mechanically densified to achieve varying volume fractions (1%, 2%, 5%, and 10%), and tested in a three-electrode electrochemical cell. Electrochemical techniques (cyclic voltammetry, impedance spectroscopy, and potentiostatic techniques) were used to measure the performance of the VA-CNTs in 1 M and 500 mM electrolyte solutions. Optimization of the experimental setup design and data collection methods yielded data that resulted in the expected cyclic voltammetry response and impedance behavior of porous electrodes. A transmission line model-pore size distribution (TLM-PSD) model was applied to the data collected in order to predict and model porosimetry characteristics. Porous behavior was observed in the VA-CNT electrodes of all volume fractions tested, and the impedance spectra showed that the volume fraction affected the overall impedance but not the characteristic shape of the spectra. Comparison between the impedance data collected in 1 M NaCl and 500 mM NaCl showed the expected corresponding inverse correlation with solution conductivity. Parameters that describe the VA-CNT electrode porosity were calculated and predicted using electrochemical data and the TLM-PSD model. The porous volume Vtot and total ionic conductance Yp values calculated using the model applied to the impedance spectroscopy data showed trends as expected for the different volume fractions of VA-CNT. The results show that electrochemical impedance spectroscopy can be used to characterize certain physical characteristics of the VA-CNT electrodes and further development of the model can yield insights into the porous geometry of VA-CNT forests.",
    "advisors": ["Evelyn N. Wang"],
    "text": "Using electrochemical impedance spectroscopy to characterize vertically-aligned carbon nanotube forest porosimetry Carbon nanotubes have generated much research interest and potential applications due to their unique properties such as their high tensile strength, high thermal conductivity, and unique semiconductor properties. Vertically-aligned carbon nanotubes (VA-CNTs) have been used in applications for electrochemical systems in energy storage systems and desalination systems. Typical methods of characterizing the morphology and composition of CNTs are limited in providing information on the packing density of CNTs, and therefore, an effective method for in situ characterization of VA-CNT electrodes is needed. This method explores the use of impedance spectroscopy and other electrochemical methods to characterize VA-CNTs in situ. VA-CNTs forests were grown via chemical vapor densification on pre-oxidized silicon wafers, mechanically densified to achieve varying volume fractions (1%, 2%, 5%, and 10%), and tested in a three-electrode electrochemical cell. Electrochemical techniques (cyclic voltammetry, impedance spectroscopy, and potentiostatic techniques) were used to measure the performance of the VA-CNTs in 1 M and 500 mM electrolyte solutions. Optimization of the experimental setup design and data collection methods yielded data that resulted in the expected cyclic voltammetry response and impedance behavior of porous electrodes. A transmission line model-pore size distribution (TLM-PSD) model was applied to the data collected in order to predict and model porosimetry characteristics. Porous behavior was observed in the VA-CNT electrodes of all volume fractions tested, and the impedance spectra showed that the volume fraction affected the overall impedance but not the characteristic shape of the spectra. Comparison between the impedance data collected in 1 M NaCl and 500 mM NaCl showed the expected corresponding inverse correlation with solution conductivity. Parameters that describe the VA-CNT electrode porosity were calculated and predicted using electrochemical data and the TLM-PSD model. The porous volume Vtot and total ionic conductance Yp values calculated using the model applied to the impedance spectroscopy data showed trends as expected for the different volume fractions of VA-CNT. The results show that electrochemical impedance spectroscopy can be used to characterize certain physical characteristics of the VA-CNT electrodes and further development of the model can yield insights into the porous geometry of VA-CNT forests."
}, {
    "id": "oai:dspace.mit.edu:1721.1/89971",
    "title": "Gluten-free bread : characterization and development of pre- and post- baked gluten free bread",
    "abstract": "The study was conducted to characterize the effects of xanthan gum on gluten-free bread formulations. An improved gluten-free flour blend consisting of brown rice flour, quinoa flour, and sorghum flour was used with the aim of developing a gluten-free bread formulation comparable to traditional gluten-based bread and commercial gluten-free bread mix. Rheological measurements were taken to analyze the effects of xanthan gum on pre-baked dough formulations. Higher concentrations of xanthan gum were found to decrease the loss factor thus strengthening the elastic properties of the dough, elongating the linear viscoelastic region and increasing the viscosity of the dough. Furthermore, the xanthan gum samples were not independent of frequency and the loss factor decreased as frequency increased. Porosity of samples was also analyzed using imaging technology to determine the average pore size. Pore size increased as xanthan gum concentration increased indicating the ability for xanthan gum to retain gas during the proofing stage before baking. It was concluded that xanthan gum was necessary for a loaf with nice crumb texture, loaf color, and moisture content though different than gluten-based and commercial brand gluten-free bread mix. 0.3% xanthan gum concentration provided the most desirable post-baked crumb texture, loaf volume, and moisture content",
    "advisors": ["Niels Holten-Andersen"],
    "text": "Gluten-free bread : characterization and development of pre- and post- baked gluten free bread The study was conducted to characterize the effects of xanthan gum on gluten-free bread formulations. An improved gluten-free flour blend consisting of brown rice flour, quinoa flour, and sorghum flour was used with the aim of developing a gluten-free bread formulation comparable to traditional gluten-based bread and commercial gluten-free bread mix. Rheological measurements were taken to analyze the effects of xanthan gum on pre-baked dough formulations. Higher concentrations of xanthan gum were found to decrease the loss factor thus strengthening the elastic properties of the dough, elongating the linear viscoelastic region and increasing the viscosity of the dough. Furthermore, the xanthan gum samples were not independent of frequency and the loss factor decreased as frequency increased. Porosity of samples was also analyzed using imaging technology to determine the average pore size. Pore size increased as xanthan gum concentration increased indicating the ability for xanthan gum to retain gas during the proofing stage before baking. It was concluded that xanthan gum was necessary for a loaf with nice crumb texture, loaf color, and moisture content though different than gluten-based and commercial brand gluten-free bread mix. 0.3% xanthan gum concentration provided the most desirable post-baked crumb texture, loaf volume, and moisture content"
}, {
    "id": "oai:dspace.mit.edu:1721.1/67167",
    "title": "An economic analysis of aluminum sheet production and prospects of aluminum for the automotive unibody",
    "abstract": "In order to lower fuel consumption and reduce emissions, aluminum is being considered as an alternative to steel in large scale production of autobodies. This study evaluates the prospects of aluminum sheets as a cost efficient alternative to steel in autobodies with the unibody design. The study focuses on the processing technologies and alloy selection for aluminum automotive sheets and looks at the impact of these on the total part forming cost of the unibody. Technical cost modeling was used to analyze the costs of traditional direct chill casting and subsequent rolling of aluminum alloy sheet and compared the technology to the alternative continuous casting fabrication method. A change to continuous casting displayed large potential cost savings and was believed to be crucial in order for aluminum to be competitive with steel. A large cost penalty is associated with the alloying and heat treatment of 6xxx series sheet for outer body panels as opposed to 5xxx series sheet for interior panels. Changes in production method for 6xxx series sheet or a replacement by 5xxx series sheet will have large impact on the cost of the autobody. The volatility in the price of aluminum ingot has a critical influence on the price of sheet. Changes in the price level have been shown to be equally critical for the final sheet cost as substantial technical improvements. Recent developments of high strength steel have shown promise for substantial weight reduction in steel automobiles and make the challenge even greater for aluminum as its possible successor.",
    "advisors": ["Joel P. Clark"],
    "text": "An economic analysis of aluminum sheet production and prospects of aluminum for the automotive unibody In order to lower fuel consumption and reduce emissions, aluminum is being considered as an alternative to steel in large scale production of autobodies. This study evaluates the prospects of aluminum sheets as a cost efficient alternative to steel in autobodies with the unibody design. The study focuses on the processing technologies and alloy selection for aluminum automotive sheets and looks at the impact of these on the total part forming cost of the unibody. Technical cost modeling was used to analyze the costs of traditional direct chill casting and subsequent rolling of aluminum alloy sheet and compared the technology to the alternative continuous casting fabrication method. A change to continuous casting displayed large potential cost savings and was believed to be crucial in order for aluminum to be competitive with steel. A large cost penalty is associated with the alloying and heat treatment of 6xxx series sheet for outer body panels as opposed to 5xxx series sheet for interior panels. Changes in production method for 6xxx series sheet or a replacement by 5xxx series sheet will have large impact on the cost of the autobody. The volatility in the price of aluminum ingot has a critical influence on the price of sheet. Changes in the price level have been shown to be equally critical for the final sheet cost as substantial technical improvements. Recent developments of high strength steel have shown promise for substantial weight reduction in steel automobiles and make the challenge even greater for aluminum as its possible successor."
}, {
    "id": "oai:dspace.mit.edu:1721.1/17613",
    "title": "Computer simulations for a scholastic theory of granular drainage",
    "abstract": "There is a surprising lack of good models for granular flow. In 2002, Bazant proposed a new stochastic kinematic model of granular drainage from a silo. The new model rests on the notion that flow in the silo is caused by the migration of extended regions of excess interstitial space upward from the orifice at the bottom. An implementation of this model with the purpose of simulating the behavior of particles in the silo was developed by the author, and several results were obtained using simulations carried out with this implementation. As regards particle streamlines, average velocity profiles, predictions of particle mixing and of particle diffusivity, it was found that qualitative and quantitative agreement with experiments was excellent, in particular for a specific version of the implementation. This version uses a self-correlated random walk to describe the motion of the excess interstitial space through the silo. The model can also be used to make predictions about many other features of the granular flow (such as granular temperature), that are not as accessible through experiment's, and for which empirical behavior is not well known. In particular, the implementation of the model developed in this work can be used to simulate three dimensional flow, whereas existing experimental techniques are limited to observing two dimensions.",
    "advisors": ["Martin Bazant"],
    "text": "Computer simulations for a scholastic theory of granular drainage There is a surprising lack of good models for granular flow. In 2002, Bazant proposed a new stochastic kinematic model of granular drainage from a silo. The new model rests on the notion that flow in the silo is caused by the migration of extended regions of excess interstitial space upward from the orifice at the bottom. An implementation of this model with the purpose of simulating the behavior of particles in the silo was developed by the author, and several results were obtained using simulations carried out with this implementation. As regards particle streamlines, average velocity profiles, predictions of particle mixing and of particle diffusivity, it was found that qualitative and quantitative agreement with experiments was excellent, in particular for a specific version of the implementation. This version uses a self-correlated random walk to describe the motion of the excess interstitial space through the silo. The model can also be used to make predictions about many other features of the granular flow (such as granular temperature), that are not as accessible through experiment's, and for which empirical behavior is not well known. In particular, the implementation of the model developed in this work can be used to simulate three dimensional flow, whereas existing experimental techniques are limited to observing two dimensions."
}, {
    "id": "oai:dspace.mit.edu:1721.1/89966",
    "title": "Swelling properties of hydrogel coatings on neural devices",
    "abstract": "Glial scarring is a major problem seen in brain electrode implants that can hinder electrode function. One major contributing factor is the mechanical mismatch between the stiff electrode and the soft brain tissue'. Hydrogel coatings are being investigated to determine their effectiveness in providing the necessary biocompatibility. Polyethylene glycol hydrogels of various formulations were fabricated and produced elastic moduli ranging from 13kPa to 687 kPa, which lie within two orders of magnitude of the elastic moduli of the brain (6kPa). Dehydration of the hydrogels provides the mechanical rigidity necessary for implantation into the brain. The surrounding aqueous environment allows the dried hydrogel to return to its swollen state. The swelling process in the brain phantom is slower than in unconstrained swelling. The equilibrium swollen hydrogel was also slightly smaller in the constrained state, implying the strain is being distributed between the hydrogel and the brain phantom.",
    "advisors": ["Michael J. Cima"],
    "text": "Swelling properties of hydrogel coatings on neural devices Glial scarring is a major problem seen in brain electrode implants that can hinder electrode function. One major contributing factor is the mechanical mismatch between the stiff electrode and the soft brain tissue'. Hydrogel coatings are being investigated to determine their effectiveness in providing the necessary biocompatibility. Polyethylene glycol hydrogels of various formulations were fabricated and produced elastic moduli ranging from 13kPa to 687 kPa, which lie within two orders of magnitude of the elastic moduli of the brain (6kPa). Dehydration of the hydrogels provides the mechanical rigidity necessary for implantation into the brain. The surrounding aqueous environment allows the dried hydrogel to return to its swollen state. The swelling process in the brain phantom is slower than in unconstrained swelling. The equilibrium swollen hydrogel was also slightly smaller in the constrained state, implying the strain is being distributed between the hydrogel and the brain phantom."
}, {
    "id": "oai:dspace.mit.edu:1721.1/119069",
    "title": "Cobalt demand : past, present, and future",
    "abstract": "Cobalt has become more and more popular in the realms of academia, industry, and media due to its integral role in many of the most commonly-used lithium-ion battery cathodes today. Many issues have been evaluated regarding the controversial labor and volatile sociopolitical environments associated with cobalt mining and concerns over the ability of cobalt supply to continue to meet demand, especially the increasing demand due to the electric vehicle revolution. Cobalt is a critical element in a variety of products outside of the battery industry, including: superalloys, hard-facing metals, cutting tools, magnets, chemical catalysts, and pigments. In this thesis, I assessed the criticality of cobalt demand in non-battery sectors with the intention of assessing whether demand of cobalt in its traditional, inelastic sectors will supply be a limiting factor of technological progress by 2030 and by 2050. In order to do so, data was collected on the past and present demands of cobalt in its four primary sectors, outside of batteries: superalloys, cutting tools and hard-facing metals, magnets, and chemical catalysts. Future demand projections were made based on the historic data as well as via a bottom-up approach from industry projections for future product demand and cobalt intensity of products. Substitutes for cobalt in these applications were also investigated and are discussed below. The prices at which substitutes become more favorable than cobalt were also evaluated.",
    "advisors": ["Elsa A. Olivetti"],
    "text": "Cobalt demand : past, present, and future Cobalt has become more and more popular in the realms of academia, industry, and media due to its integral role in many of the most commonly-used lithium-ion battery cathodes today. Many issues have been evaluated regarding the controversial labor and volatile sociopolitical environments associated with cobalt mining and concerns over the ability of cobalt supply to continue to meet demand, especially the increasing demand due to the electric vehicle revolution. Cobalt is a critical element in a variety of products outside of the battery industry, including: superalloys, hard-facing metals, cutting tools, magnets, chemical catalysts, and pigments. In this thesis, I assessed the criticality of cobalt demand in non-battery sectors with the intention of assessing whether demand of cobalt in its traditional, inelastic sectors will supply be a limiting factor of technological progress by 2030 and by 2050. In order to do so, data was collected on the past and present demands of cobalt in its four primary sectors, outside of batteries: superalloys, cutting tools and hard-facing metals, magnets, and chemical catalysts. Future demand projections were made based on the historic data as well as via a bottom-up approach from industry projections for future product demand and cobalt intensity of products. Substitutes for cobalt in these applications were also investigated and are discussed below. The prices at which substitutes become more favorable than cobalt were also evaluated."
}, {
    "id": "oai:dspace.mit.edu:1721.1/44814",
    "title": "Field induced switching in multilayer rhombic magnetic rings",
    "abstract": "Multilayer rhombic magnetic rings are researched as a structure for the \"pseudo spin valve\" device that could possibly become useful in magnetic materials applications such as MRAM, digital logic, and sensors through the use of multiple resistance states exhibited within these devices. The magnetization reversal characteristics of these structures are explored in an effort to fully understand interactions occurring within the devices and their resulting effect on giant magnetoresistance (GMR). Contact configuration and angular dependence of applied field are also examined. Using submicron thickness rhombic rings with long axis dimension -1.5gjm, major loop magnetization sweeps were conducted, as well as minor loops in order to excite several resistance states within the devices. It was found from major loop applied field sweeps that rhombic multilayer rings exhibit five stable magnetoresistive states, with an additional state excited through execution of a minor loop field sweep. In addition, using the contact configurations known as \"classical\" and \"wheatstone bridge\" provide additional information on interactions that are occurring within the structures. It was found that both contact configurations were sensitive to similar changes in the devices, however, through different means of sensing. The major difference results in a larger GMR output in the wheatstone bridge configuration (-20%) versus the classical configuration (-1%). Preliminary work in angular dependence has shown the ability to alter resistance plateaus by changing the angle of applied field. Ultimately shown through this work is the amount of research that is still needed to truly understand these devices, as they contain more complex stable and metastable states of magnetization than generations and shapes before them.",
    "advisors": ["Caroline A. Ross"],
    "text": "Field induced switching in multilayer rhombic magnetic rings Multilayer rhombic magnetic rings are researched as a structure for the \"pseudo spin valve\" device that could possibly become useful in magnetic materials applications such as MRAM, digital logic, and sensors through the use of multiple resistance states exhibited within these devices. The magnetization reversal characteristics of these structures are explored in an effort to fully understand interactions occurring within the devices and their resulting effect on giant magnetoresistance (GMR). Contact configuration and angular dependence of applied field are also examined. Using submicron thickness rhombic rings with long axis dimension -1.5gjm, major loop magnetization sweeps were conducted, as well as minor loops in order to excite several resistance states within the devices. It was found from major loop applied field sweeps that rhombic multilayer rings exhibit five stable magnetoresistive states, with an additional state excited through execution of a minor loop field sweep. In addition, using the contact configurations known as \"classical\" and \"wheatstone bridge\" provide additional information on interactions that are occurring within the structures. It was found that both contact configurations were sensitive to similar changes in the devices, however, through different means of sensing. The major difference results in a larger GMR output in the wheatstone bridge configuration (-20%) versus the classical configuration (-1%). Preliminary work in angular dependence has shown the ability to alter resistance plateaus by changing the angle of applied field. Ultimately shown through this work is the amount of research that is still needed to truly understand these devices, as they contain more complex stable and metastable states of magnetization than generations and shapes before them."
}, {
    "id": "oai:dspace.mit.edu:1721.1/89963",
    "title": "Fabrication of nanoscale magnetic domains using block-copolymer lithography",
    "abstract": "The tendency of PS-b-PDMS to phase separate, the tunability of the resulting morphology and the sufficient etch contrast between PS and PDMS makes the block copolymer ideal for creating patterns that can be transferred onto magnetic media. The aim of this study was to determine optimal BCP film thicknesses that produce spherical micro-domains with long range order on a Co substrate brushed with PS-OH and to transfer this pattern onto the underlying Co. Thin films of PS-b-PDMS were spun-cast onto Co substrates brushed with PS-OH, solvent annealed and consequently etched in a series of steps that removed one constituent polymer at a time. Patterns with periods between 36-39nm and with great long-range order were observed for BCP film thicknesses in the 32-35 nm range and successful transfer of the pattern onto Co film was achieved, resulting in fabrication of nanoscale, discrete domains.",
    "advisors": ["Caroline Ross"],
    "text": "Fabrication of nanoscale magnetic domains using block-copolymer lithography The tendency of PS-b-PDMS to phase separate, the tunability of the resulting morphology and the sufficient etch contrast between PS and PDMS makes the block copolymer ideal for creating patterns that can be transferred onto magnetic media. The aim of this study was to determine optimal BCP film thicknesses that produce spherical micro-domains with long range order on a Co substrate brushed with PS-OH and to transfer this pattern onto the underlying Co. Thin films of PS-b-PDMS were spun-cast onto Co substrates brushed with PS-OH, solvent annealed and consequently etched in a series of steps that removed one constituent polymer at a time. Patterns with periods between 36-39nm and with great long-range order were observed for BCP film thicknesses in the 32-35 nm range and successful transfer of the pattern onto Co film was achieved, resulting in fabrication of nanoscale, discrete domains."
}, {
    "id": "oai:dspace.mit.edu:1721.1/119063",
    "title": "Bioconjugation of aminated DNA as a method of rapid polymer library generation for Corona Phase Molecular Recognition",
    "abstract": "An experimental study was performed to determine the effects on Corona Phase Molecular Recognition (CoMoRe) of bioconjugating a host of small molecules to DNA wrapped single-walled carbon nanotubes. In addition, the study observed the effects of the DNA sequence length on the subsequent effectiveness of the small molecules to alter the corona phase. The conjugation of small molecules was shown to alter both the intensity and the position of the fluorescence and absorbance profile. The length of the DNA sequence was found to change the small molecule's ability to alter the fluorecence spectra of the wrapped nanotubes. The EDC/sulfo-NHS reaction was done to conjugate the small molecules to two identical DNA sequences with varying lengthes. Through the methods of ultraviolet-visibile-near infrared absorption spectroscopy, near infrared fluorescence spectroscopy, and high-performance liquid chromatography characterization and structural analysis were performed. The results showed the successful conjugation of the small molecules to the amino-modified DNA and an alteration in the corona phase. The small molecules were found to bind to the DNA at multiple locations and the length of the sequence was found to have an effect on the corona phase.",
    "advisors": ["Micheal Strano"],
    "text": "Bioconjugation of aminated DNA as a method of rapid polymer library generation for Corona Phase Molecular Recognition An experimental study was performed to determine the effects on Corona Phase Molecular Recognition (CoMoRe) of bioconjugating a host of small molecules to DNA wrapped single-walled carbon nanotubes. In addition, the study observed the effects of the DNA sequence length on the subsequent effectiveness of the small molecules to alter the corona phase. The conjugation of small molecules was shown to alter both the intensity and the position of the fluorescence and absorbance profile. The length of the DNA sequence was found to change the small molecule's ability to alter the fluorecence spectra of the wrapped nanotubes. The EDC/sulfo-NHS reaction was done to conjugate the small molecules to two identical DNA sequences with varying lengthes. Through the methods of ultraviolet-visibile-near infrared absorption spectroscopy, near infrared fluorescence spectroscopy, and high-performance liquid chromatography characterization and structural analysis were performed. The results showed the successful conjugation of the small molecules to the amino-modified DNA and an alteration in the corona phase. The small molecules were found to bind to the DNA at multiple locations and the length of the sequence was found to have an effect on the corona phase."
}, {
    "id": "oai:dspace.mit.edu:1721.1/98646",
    "title": "Data analysis to understand coordination and topological environments in oxides",
    "abstract": "Local coordination and topology of ions determine several important properties of materials, including electronic structure, migration barrier, and diffusivity. In this thesis, we employ the Materials Project Database to investigate the coordination preferences of cations and topology of coordination polyhedra in oxides. We calculate the coordination environment preferences of several common cations in oxides, identifying lithium, sodium, calcium, and magnesium ion's preferred coordination numbers are 4- fold/6-fold, 6-fold, 6-fold, and 6-/4-fold coordination respectively. We also develop a method to quantify the connectivity between two polyhedra and determine whether they are point-sharing, edge-sharing, or face-sharing. We find that 4-fold coordinated lithium polyhedra mainly point-share while the 6-fold coordinated lithium polyhedra connectivites are face-sharing. We then build a tool to identify and insert \"empty polyhedra\" (i.e. coordination polyhedra which are bounded by ions but contain no central ion) which can help to provide a better descriptor of the structure topology. We also find that most connections with lithium polyhedra are with empty polyhedra and that in a connected set of two lithium polyhedra and one empty polyhedron, the coordination polyhedra tend to be either 6-4-6 or 4-6-4 with the empty polyhedron in the center. Finally, we utilize the database to evaluate Pauling's first and second rules, which are guidelines for current understanding of coordination and topology, and observe that the rules are generally accurate only within a 30% error margin.",
    "advisors": ["Gerbrand Ceder"],
    "text": "Data analysis to understand coordination and topological environments in oxides Local coordination and topology of ions determine several important properties of materials, including electronic structure, migration barrier, and diffusivity. In this thesis, we employ the Materials Project Database to investigate the coordination preferences of cations and topology of coordination polyhedra in oxides. We calculate the coordination environment preferences of several common cations in oxides, identifying lithium, sodium, calcium, and magnesium ion's preferred coordination numbers are 4- fold/6-fold, 6-fold, 6-fold, and 6-/4-fold coordination respectively. We also develop a method to quantify the connectivity between two polyhedra and determine whether they are point-sharing, edge-sharing, or face-sharing. We find that 4-fold coordinated lithium polyhedra mainly point-share while the 6-fold coordinated lithium polyhedra connectivites are face-sharing. We then build a tool to identify and insert \"empty polyhedra\" (i.e. coordination polyhedra which are bounded by ions but contain no central ion) which can help to provide a better descriptor of the structure topology. We also find that most connections with lithium polyhedra are with empty polyhedra and that in a connected set of two lithium polyhedra and one empty polyhedron, the coordination polyhedra tend to be either 6-4-6 or 4-6-4 with the empty polyhedron in the center. Finally, we utilize the database to evaluate Pauling's first and second rules, which are guidelines for current understanding of coordination and topology, and observe that the rules are generally accurate only within a 30% error margin."
}, {
    "id": "oai:dspace.mit.edu:1721.1/58072",
    "title": "Effects of varying ethanol and water concentrations as a gold nanoparticle gel solvent",
    "abstract": "Striped gold nanoparticles are unique in several of their characteristics and applications. Recent experiments have determined a new medium with which contain the nanoparticles is that of a chemical gel. The nanoparticles for use in these studies do not require a polymer base in order to form a gel phase. However, a concrete analysis of the transition temperature between the gel and liquid phases had yet to be performed. The work performed in this experiment has determined a portion of the phase transition curve for different concentrations of ethanol and water as a solvent in this nanoparticle gel. The results of this project showed that, as expected, with an increased concentration of dissolved gold nanoparticles, the gel to liquid transition temperature increased.",
    "advisors": ["Francesco Stellacci"],
    "text": "Effects of varying ethanol and water concentrations as a gold nanoparticle gel solvent Striped gold nanoparticles are unique in several of their characteristics and applications. Recent experiments have determined a new medium with which contain the nanoparticles is that of a chemical gel. The nanoparticles for use in these studies do not require a polymer base in order to form a gel phase. However, a concrete analysis of the transition temperature between the gel and liquid phases had yet to be performed. The work performed in this experiment has determined a portion of the phase transition curve for different concentrations of ethanol and water as a solvent in this nanoparticle gel. The results of this project showed that, as expected, with an increased concentration of dissolved gold nanoparticles, the gel to liquid transition temperature increased."
}, {
    "id": "oai:dspace.mit.edu:1721.1/43210",
    "title": "Using first principles Destiny Functional Theory methods to model the Seebeck coefficient of bulk silicon",
    "abstract": "Thermoelectrics are gaining significant amounts of attention considering their relevance today in the areas of sustainable energy generation and energy efficiency. In this thesis, the thermoelectric properties of bulk Silicon were modeled using ab initio density functional theory methods to determine the Si band structure. Specifically, three different models for determining the Seebeck coefficient - Parabolic Bands, Boltzmann's theory, and the 'Pudding Mold' approximation to Boltzmann's theory - were studied in depth and compared with experimental values. Here we show first principles calculations to yield Seebeck coefficients for n-type Silicon to be on the order of 300 gtV/K at -300 K, and -500 gtV/K at 300 K for the Parabolic Bands and Boltzmann approach, respectively. While the 'Pudding Mold' Theory failed in its approximations of the Seebeck coefficients, the calculations using the other two theories were found to agree closely with experimentally determined Seebeck coefficients.",
    "advisors": ["Gerbrand Ceder"],
    "text": "Using first principles Destiny Functional Theory methods to model the Seebeck coefficient of bulk silicon Thermoelectrics are gaining significant amounts of attention considering their relevance today in the areas of sustainable energy generation and energy efficiency. In this thesis, the thermoelectric properties of bulk Silicon were modeled using ab initio density functional theory methods to determine the Si band structure. Specifically, three different models for determining the Seebeck coefficient - Parabolic Bands, Boltzmann's theory, and the 'Pudding Mold' approximation to Boltzmann's theory - were studied in depth and compared with experimental values. Here we show first principles calculations to yield Seebeck coefficients for n-type Silicon to be on the order of 300 gtV/K at -300 K, and -500 gtV/K at 300 K for the Parabolic Bands and Boltzmann approach, respectively. While the 'Pudding Mold' Theory failed in its approximations of the Seebeck coefficients, the calculations using the other two theories were found to agree closely with experimentally determined Seebeck coefficients."
}, {
    "id": "oai:dspace.mit.edu:1721.1/119602",
    "title": "Electrolyte selection for cobalt-free solid-state batteries",
    "abstract": "Lithium-ion batteries are widespread in use due to their thermal stability and high energy density. The most common design uses an organic electrolyte and lithium-cobalt electrode. While safe under typical operating conditions, the use of an organic electrolyte subjects the battery user to certain risks; in particular, Li-ion liquid batteries are explosive when exposed to air and subject to thermal runoff, making them highly sensitive to any physical damage. The use of cobalt also poses a moral concern, as the mining and sourcing of cobalt is geographically restricted and most commonly sourced from countries that have a history of foreign exploitation and child labor. An all solid state battery is suggested as a possible alternative battery that reduces operation risks and maintains similar performance characteristics. Lithium-lanthanum-zirconium oxide is presented as a suitable electrolyte replacement. Coupled with cobalt-free electrodes, this battery design would provide a safer, more responsible battery.",
    "advisors": ["Jennifer L. M. Rupp"],
    "text": "Electrolyte selection for cobalt-free solid-state batteries Lithium-ion batteries are widespread in use due to their thermal stability and high energy density. The most common design uses an organic electrolyte and lithium-cobalt electrode. While safe under typical operating conditions, the use of an organic electrolyte subjects the battery user to certain risks; in particular, Li-ion liquid batteries are explosive when exposed to air and subject to thermal runoff, making them highly sensitive to any physical damage. The use of cobalt also poses a moral concern, as the mining and sourcing of cobalt is geographically restricted and most commonly sourced from countries that have a history of foreign exploitation and child labor. An all solid state battery is suggested as a possible alternative battery that reduces operation risks and maintains similar performance characteristics. Lithium-lanthanum-zirconium oxide is presented as a suitable electrolyte replacement. Coupled with cobalt-free electrodes, this battery design would provide a safer, more responsible battery."
}, {
    "id": "oai:dspace.mit.edu:1721.1/32844",
    "title": "Composite gelatin delivery system for bone regeneration",
    "abstract": "In this thesis, the chemical/mechanical properties and biocompatibility of gelatin were investigated to produce a gelatin scaffold for the release of bone morphogenetic proteins (BMPs) from composite particles. This delivery system, designed to regenerate bone, holds much promise as an alternative to bone grafts. The chemical properties of gelatin were examined through zeta potential measurements, swelling studies, optical microscopy, environmental scanning electron microscopy (ESEM), and collagenase degradation. Compressive tests and mercury porosimetry were performed to study the mechanical and structural properties of the scaffold. The biocompatibility of the scaffold was determined through cell optical imaging and DNA quantification studies. Based on findings of this research, the material choices were made and the synthesis method for the gelatin scaffold was developed. Gelatin A, 300B, derived from bovine collagen, with an isoelectric point of [approx.] 9, was selected. Crosslinking was accomplished by reacting 10 w/v% glutaraldehyde with 10 w/v% gelatin solution. The most effective crosslinking condition was found to be 5 hours at room temperature. Glycine rinses were conducted to cap any non- reacted (toxic) aldehyde groups, and the necessary length of time was found to be at least 48 hours at 37C. Finally, based on pore size distribution and mechanical stability, an optimal lyophilization method was developed with initial freezing at -20C for 1 day, followed by lyophilization of the scaffold for 1-2 days. In terms of mechanical properties of the gelatin and amount of protein delivered, the most effective loading of poly(lactic-co-glycolic acid)/apatite/protein composite particles was found to be 10% of the mass of the gelatin.",
    "advisors": ["Jackie Y. Ying"],
    "text": "Composite gelatin delivery system for bone regeneration In this thesis, the chemical/mechanical properties and biocompatibility of gelatin were investigated to produce a gelatin scaffold for the release of bone morphogenetic proteins (BMPs) from composite particles. This delivery system, designed to regenerate bone, holds much promise as an alternative to bone grafts. The chemical properties of gelatin were examined through zeta potential measurements, swelling studies, optical microscopy, environmental scanning electron microscopy (ESEM), and collagenase degradation. Compressive tests and mercury porosimetry were performed to study the mechanical and structural properties of the scaffold. The biocompatibility of the scaffold was determined through cell optical imaging and DNA quantification studies. Based on findings of this research, the material choices were made and the synthesis method for the gelatin scaffold was developed. Gelatin A, 300B, derived from bovine collagen, with an isoelectric point of [approx.] 9, was selected. Crosslinking was accomplished by reacting 10 w/v% glutaraldehyde with 10 w/v% gelatin solution. The most effective crosslinking condition was found to be 5 hours at room temperature. Glycine rinses were conducted to cap any non- reacted (toxic) aldehyde groups, and the necessary length of time was found to be at least 48 hours at 37C. Finally, based on pore size distribution and mechanical stability, an optimal lyophilization method was developed with initial freezing at -20C for 1 day, followed by lyophilization of the scaffold for 1-2 days. In terms of mechanical properties of the gelatin and amount of protein delivered, the most effective loading of poly(lactic-co-glycolic acid)/apatite/protein composite particles was found to be 10% of the mass of the gelatin."
}, {
    "id": "oai:dspace.mit.edu:1721.1/118565",
    "title": "Exploring strengthening mechanisms for Class C and Class F fly ash in load bearing floor tile applications",
    "abstract": "Approximately 62.8 trillion kJ are consumed annually worldwide in the manufacturing process of traditional clay tiles. With this in mind, the goal of this project was to develop an eco-friendly alternative to clay tiles that maintain the ASTM building code standards. Through experimentation, a fly ash tile was produced that consumes 99% less energy in the manufacturing process than commercial clay tiles. The final product is a fly ash tile composed of two classes of fly ash, water, and several additives to strengthen the material. Standard ASTM tests were conducted. This fly ash tile is an energy efficient clay-tile alternative that excels in many mechanical properties.",
    "advisors": ["Linn Hobbs"],
    "text": "Exploring strengthening mechanisms for Class C and Class F fly ash in load bearing floor tile applications Approximately 62.8 trillion kJ are consumed annually worldwide in the manufacturing process of traditional clay tiles. With this in mind, the goal of this project was to develop an eco-friendly alternative to clay tiles that maintain the ASTM building code standards. Through experimentation, a fly ash tile was produced that consumes 99% less energy in the manufacturing process than commercial clay tiles. The final product is a fly ash tile composed of two classes of fly ash, water, and several additives to strengthen the material. Standard ASTM tests were conducted. This fly ash tile is an energy efficient clay-tile alternative that excels in many mechanical properties."
}, {
    "id": "oai:dspace.mit.edu:1721.1/111257",
    "title": "Investigating coordinate network based films through mechanical and optical properties",
    "abstract": "Both biological and synthetic materials crosslinked via metal coordinate dynamic chemistry display interesting advanced behavior. In particular, coordinate networks have been shown to form self-healing, self-assembling, and stimuli-responsive behaviors through its tunable optical and mechanical properties as well as its ability to for dynamic networks. However, while the majority of research has focused on characterization of bulk coordinate networks, coordinate complexes have also been shown to be useful in molecular film formation [1 and 2]. This study investigates the mechanical and optical properties of tannic acid and 4 arm catechol polyethylene glycol based coordinate network films. It shows that these films can contribute to energy dissipation and undergo pH-induced optical shifts when used as coatings on soft hydrogels. It also provides evidence that the molecular architecture of the network formers may have considerable effect on the properties and behavior of coordinate network films. Ultimately this work lays the foundation for further investigation of the underlying mechanisms and engineering potential of coordinate network based films.",
    "advisors": ["Niels Holten-Andersen"],
    "text": "Investigating coordinate network based films through mechanical and optical properties Both biological and synthetic materials crosslinked via metal coordinate dynamic chemistry display interesting advanced behavior. In particular, coordinate networks have been shown to form self-healing, self-assembling, and stimuli-responsive behaviors through its tunable optical and mechanical properties as well as its ability to for dynamic networks. However, while the majority of research has focused on characterization of bulk coordinate networks, coordinate complexes have also been shown to be useful in molecular film formation [1 and 2]. This study investigates the mechanical and optical properties of tannic acid and 4 arm catechol polyethylene glycol based coordinate network films. It shows that these films can contribute to energy dissipation and undergo pH-induced optical shifts when used as coatings on soft hydrogels. It also provides evidence that the molecular architecture of the network formers may have considerable effect on the properties and behavior of coordinate network films. Ultimately this work lays the foundation for further investigation of the underlying mechanisms and engineering potential of coordinate network based films."
}, {
    "id": "oai:dspace.mit.edu:1721.1/98664",
    "title": "Thermal modulation during solvent annealing of PS-PDMS block copolymer",
    "abstract": "The self-assembly of block copolymers (BCP) has been a promising area of research for nanolithography applications in microelectronics because of their ability to produce nano-scale level periodic structures with long-range order. Ideal BCPs for generating these nano-scale patterns fall within the strong segregation limit (SSL) and have a high interaction parameter to drive BCP phase transitions. BCP morphologies can vary from equilibrium structures such as spheres, cylinders, and gyroid, to metastable structures such as hexagonal perforated lamellar (HPL). A variety of processing techniques including solvent vapor annealing (SVA) have been developed in order to facilitate the phase transitions of BCPs from disordered to ordered states. SVA parameters which can affect the final film morphology include the swelling thickness of the film and solvent removal rate. Thermal modulation of the substrate was used to explore the effects of rapid solvent evaporation during the annealing process on the morphologies of the PS-b-PDMS system. Additional cycles of solvent update and film reswelling were introduced into the annealing procedure to induce greater long-range ordering of film morphologies. Although a range of morphologies were explored, there was special focus on developing a procedure for mono-layer HPL structures for nanolithography applications.",
    "advisors": ["Caroline Ross"],
    "text": "Thermal modulation during solvent annealing of PS-PDMS block copolymer The self-assembly of block copolymers (BCP) has been a promising area of research for nanolithography applications in microelectronics because of their ability to produce nano-scale level periodic structures with long-range order. Ideal BCPs for generating these nano-scale patterns fall within the strong segregation limit (SSL) and have a high interaction parameter to drive BCP phase transitions. BCP morphologies can vary from equilibrium structures such as spheres, cylinders, and gyroid, to metastable structures such as hexagonal perforated lamellar (HPL). A variety of processing techniques including solvent vapor annealing (SVA) have been developed in order to facilitate the phase transitions of BCPs from disordered to ordered states. SVA parameters which can affect the final film morphology include the swelling thickness of the film and solvent removal rate. Thermal modulation of the substrate was used to explore the effects of rapid solvent evaporation during the annealing process on the morphologies of the PS-b-PDMS system. Additional cycles of solvent update and film reswelling were introduced into the annealing procedure to induce greater long-range ordering of film morphologies. Although a range of morphologies were explored, there was special focus on developing a procedure for mono-layer HPL structures for nanolithography applications."
}, {
    "id": "oai:dspace.mit.edu:1721.1/58378",
    "title": "Cross-sectional transmission electron microscopy study of femtosecond laser-irradiated selenium-doped 'black' silicon",
    "abstract": "'Black silicon' refers to silicon that has been treated in a laser-ablation process to incorporate large amounts of chalcogen dopants. The material has been found to have greatly increased absorbance of visible and infared wavelength light in comparison to undoped crystalline silicon. Selenium-doped black silicon that had been annealed at different temperatures were studied using transmission electron microscopy (TEM) and electron diffraction. The goal of the investigation was to characterize the structure of the laser-altered regions of the material. In addition, energy dispersive X-ray spectroscopy (EDX) was conducted in a scanning transmission electron microscope (STEM) in order to map spatial distribution of the selenium and the silicon were located within the material. The results of the TEM study showed roughly conical peaks of varying shapes protruding about 1 [mu]m from the surface of the material. The material is altered up to a depth of up to 1-2 [mu]m, where polycrystalline or amorphous layers were observed. Electron diffraction studies revealed increased crystallinity in the annealed sample. A continuous, sharp interface between the affected region and unaltered substrate was found and particles of diameter 5-100 nm embedded within the silicon were observed. The STEM-EDX studies showed that the selenium was dispersed inhomogenously throughout the material. The selenium is concentrated near the interface of the unaltered Si substrate and the laser-altered layer and a high local concentration of selenium in the embedded particles was recorded. The findings in this study provide a first look at the underlying structure of black silicon and will lead to future work characterizing the material.",
    "advisors": ["Silvija Gradeak"],
    "text": "Cross-sectional transmission electron microscopy study of femtosecond laser-irradiated selenium-doped 'black' silicon 'Black silicon' refers to silicon that has been treated in a laser-ablation process to incorporate large amounts of chalcogen dopants. The material has been found to have greatly increased absorbance of visible and infared wavelength light in comparison to undoped crystalline silicon. Selenium-doped black silicon that had been annealed at different temperatures were studied using transmission electron microscopy (TEM) and electron diffraction. The goal of the investigation was to characterize the structure of the laser-altered regions of the material. In addition, energy dispersive X-ray spectroscopy (EDX) was conducted in a scanning transmission electron microscope (STEM) in order to map spatial distribution of the selenium and the silicon were located within the material. The results of the TEM study showed roughly conical peaks of varying shapes protruding about 1 [mu]m from the surface of the material. The material is altered up to a depth of up to 1-2 [mu]m, where polycrystalline or amorphous layers were observed. Electron diffraction studies revealed increased crystallinity in the annealed sample. A continuous, sharp interface between the affected region and unaltered substrate was found and particles of diameter 5-100 nm embedded within the silicon were observed. The STEM-EDX studies showed that the selenium was dispersed inhomogenously throughout the material. The selenium is concentrated near the interface of the unaltered Si substrate and the laser-altered layer and a high local concentration of selenium in the embedded particles was recorded. The findings in this study provide a first look at the underlying structure of black silicon and will lead to future work characterizing the material."
}, {
    "id": "oai:dspace.mit.edu:1721.1/81138",
    "title": "Searching for M13 bacteriophage with high affinity for nanodiamond particles",
    "abstract": "Nanodiamonds have potential in biomedical uses, as they are non-toxic and exhibit non-blinking fluorescence behavior when they are enriched with nitrogen vacancy centers. In order for them to be useful in biomedical applications, they need to be functionalized. In this experiment, a pIII library of M13 bacteriophage were panned versus 100 nm nanodiamonds enriched with approximately 500 nitrogen vacancy centers to find phage that have an affinity for these nanoparticles. The phage DNA was sequenced and found to have the protein sequence SKMYHTP. At this point, although we have sequences of peptide that bind to nanodiamond, we are unable to determine the affinity or best binders without additional biopanning rounds and testing.",
    "advisors": ["Angela Belcher"],
    "text": "Searching for M13 bacteriophage with high affinity for nanodiamond particles Nanodiamonds have potential in biomedical uses, as they are non-toxic and exhibit non-blinking fluorescence behavior when they are enriched with nitrogen vacancy centers. In order for them to be useful in biomedical applications, they need to be functionalized. In this experiment, a pIII library of M13 bacteriophage were panned versus 100 nm nanodiamonds enriched with approximately 500 nitrogen vacancy centers to find phage that have an affinity for these nanoparticles. The phage DNA was sequenced and found to have the protein sequence SKMYHTP. At this point, although we have sequences of peptide that bind to nanodiamond, we are unable to determine the affinity or best binders without additional biopanning rounds and testing."
}, {
    "id": "oai:dspace.mit.edu:1721.1/35067",
    "title": "Incorporation of silica into baroplastic core-shell nanoparticles",
    "abstract": "Core-shell baroplastics are nanophase materials that exhibit pressure-induced flow at low temperatures and high pressures. Core-shell baroplastics used in this work are comprised of a low Tg poly(butyl acrylate) (PBA) core and a high Tg polystyrene (PS) shell. These novel polymer systems can be molded into fully formed, 3-D shapes with the application of high pressure at room temperature. While the mechanical properties are equivalent to or better than those of commercial thermoplastic elastomers, more can be done to improve upon individual aspects of the mechanical properties, such as elastic modulus. This work looks at creating baroplastic nanocomposites with the goal of improving upon the mechanical properties. To accomplish this goal, two incorporation strategies for introducing silica nanoparticles were developed. The pre-emulsion strategy incorporated hydrophobized silica nanoparticles inside the core-shell nanoparticles to create core-shell-shell nanoparticles. The post-emulsion strategy incorporated charged silica nanoparticles after core-shell emulsion, with the intention of creating crystalline structures with silica and core-shell nanoparticles.",
    "advisors": ["Anne M. Mayes"],
    "text": "Incorporation of silica into baroplastic core-shell nanoparticles Core-shell baroplastics are nanophase materials that exhibit pressure-induced flow at low temperatures and high pressures. Core-shell baroplastics used in this work are comprised of a low Tg poly(butyl acrylate) (PBA) core and a high Tg polystyrene (PS) shell. These novel polymer systems can be molded into fully formed, 3-D shapes with the application of high pressure at room temperature. While the mechanical properties are equivalent to or better than those of commercial thermoplastic elastomers, more can be done to improve upon individual aspects of the mechanical properties, such as elastic modulus. This work looks at creating baroplastic nanocomposites with the goal of improving upon the mechanical properties. To accomplish this goal, two incorporation strategies for introducing silica nanoparticles were developed. The pre-emulsion strategy incorporated hydrophobized silica nanoparticles inside the core-shell nanoparticles to create core-shell-shell nanoparticles. The post-emulsion strategy incorporated charged silica nanoparticles after core-shell emulsion, with the intention of creating crystalline structures with silica and core-shell nanoparticles."
}, {
    "id": "oai:dspace.mit.edu:1721.1/98649",
    "title": "Structure-property relations of nanostructured carbon systems as a function of processing",
    "abstract": "Due to their intrinsic properties and nanometer scale, carbon nanotubes (CNTs) are commonly used to enhance the material properties of engineering materials. However, structural defects can significantly alter the intrinsic properties of CNTs, thereby limiting the physical properties of aligned CNT nanocomposite architectures. Previous studies have shown the difficulty in getting quantitative data for CNT quality once embedded within a carbon matrix. Therefore, studies that focused on the CNTs and carbon matrix separately were necessary. A study on the CNTs and carbon matrix response to pyrolyzation temperatures has recently been completed and is used to inform and motivate the research reported here. This research will focus primarily on the effects of different temperature ramping rates (TRR's) during pyrolysis of phenolic resin to form the ceramic matrix. Preliminary X-Ray Diffraction (XRD), Raman spectroscopy and Vickers Hardness results indicate that increasing the temperature ramping rate (in the range of 10C/min - 40C/min) increases the prevalence of defects in the nanocomposite system as well as increasing the standard error of both crystallite sizes and hardness, while maintaining the mean of the distribution. Future studies exploring aligned CNT carbon matrix nanocomposites (A-CMNCs) and more extreme temperature ramping rates are proposed.",
    "advisors": ["Brian L. Wardle"],
    "text": "Structure-property relations of nanostructured carbon systems as a function of processing Due to their intrinsic properties and nanometer scale, carbon nanotubes (CNTs) are commonly used to enhance the material properties of engineering materials. However, structural defects can significantly alter the intrinsic properties of CNTs, thereby limiting the physical properties of aligned CNT nanocomposite architectures. Previous studies have shown the difficulty in getting quantitative data for CNT quality once embedded within a carbon matrix. Therefore, studies that focused on the CNTs and carbon matrix separately were necessary. A study on the CNTs and carbon matrix response to pyrolyzation temperatures has recently been completed and is used to inform and motivate the research reported here. This research will focus primarily on the effects of different temperature ramping rates (TRR's) during pyrolysis of phenolic resin to form the ceramic matrix. Preliminary X-Ray Diffraction (XRD), Raman spectroscopy and Vickers Hardness results indicate that increasing the temperature ramping rate (in the range of 10C/min - 40C/min) increases the prevalence of defects in the nanocomposite system as well as increasing the standard error of both crystallite sizes and hardness, while maintaining the mean of the distribution. Future studies exploring aligned CNT carbon matrix nanocomposites (A-CMNCs) and more extreme temperature ramping rates are proposed."
}, {
    "id": "oai:dspace.mit.edu:1721.1/35062",
    "title": "Effect of volume fraction of solids on the compressive stress-strain behavior of collagen-glycosaminoglycan scaffolds",
    "abstract": "This thesis aims to examine the effect of volume fraction of solids in collagen-glycosaminoglycan (GAG) scaffolds on the compressive-strain behavior of the structure and compare these results to the open-cell foam model. Collagen-GAG (CG) scaffolds have been used for regenerating skin, conjunctiva, and peripheral nerves with varying levels of success. In these uses, the temporary scaffolds are often deployed with a non-degradable support structure such as a waterproof film or a silicone neural tube which are removed after healing is complete if it is outside the body (for skin regeneration) or are expected to remain permanently in the body (for nerve regeneration). Unfortunately, leaving non-degradable implants in the body could provoke immune responses. At the same time, to remove supports that have been implanted in the body after healing has been completed would result in more injury to the site and other medical complications. For a truly temporary implant, the scaffold must in its entirety be degradable. Thus, the bulk mechanical properties of the scaffold are important to study. Previous research has concentrated on the effects of cells on the scaffolds on a microlevel. However, the scaffold must also be able to bear mechanical stress from surrounding tissues to keep the wound open and provide mechanical support for the body, if, for example, collagen or bone is being regenerated. Here, the bulk mechanical properties of the scaffold are tested under uniaxial, unconfined compression. The Young's modulus and critical stress are calculated from the experimental data and compared to the values predicted by the open-celled foam model. There is very good agreement between the low density scaffolds, with variability in the results increasing with increasing density and with hydration of the specimens. Further research should focus on the",
    "advisors": ["Lorna Gibson"],
    "text": "Effect of volume fraction of solids on the compressive stress-strain behavior of collagen-glycosaminoglycan scaffolds This thesis aims to examine the effect of volume fraction of solids in collagen-glycosaminoglycan (GAG) scaffolds on the compressive-strain behavior of the structure and compare these results to the open-cell foam model. Collagen-GAG (CG) scaffolds have been used for regenerating skin, conjunctiva, and peripheral nerves with varying levels of success. In these uses, the temporary scaffolds are often deployed with a non-degradable support structure such as a waterproof film or a silicone neural tube which are removed after healing is complete if it is outside the body (for skin regeneration) or are expected to remain permanently in the body (for nerve regeneration). Unfortunately, leaving non-degradable implants in the body could provoke immune responses. At the same time, to remove supports that have been implanted in the body after healing has been completed would result in more injury to the site and other medical complications. For a truly temporary implant, the scaffold must in its entirety be degradable. Thus, the bulk mechanical properties of the scaffold are important to study. Previous research has concentrated on the effects of cells on the scaffolds on a microlevel. However, the scaffold must also be able to bear mechanical stress from surrounding tissues to keep the wound open and provide mechanical support for the body, if, for example, collagen or bone is being regenerated. Here, the bulk mechanical properties of the scaffold are tested under uniaxial, unconfined compression. The Young's modulus and critical stress are calculated from the experimental data and compared to the values predicted by the open-celled foam model. There is very good agreement between the low density scaffolds, with variability in the results increasing with increasing density and with hydration of the specimens. Further research should focus on the"
}, {
    "id": "oai:dspace.mit.edu:1721.1/58070",
    "title": "The effects of polydispersity on the morphology of polystyrene-polyferrocenyldimethylsilane block copolymer thin films",
    "abstract": "Introduction: As the size of electronic and magnetic devices decreases, nanoscale patterning becomes an increasingly important area of research. Two different approaches have been taken to pattern media: top-down methods such as lithography, and bottom-up methods such as self-assembly. Top-down assembly methods have the advantages of precision and accuracy, but are hard to scale for certain industrial applications due to their low throughput. Self-assembly methods are more easily scalable for applications requiring mass production. Thus, self-assembly has attracted attention and is an area of ongoing research for its potential to create high-throughput, periodic nanoscale patterns. Block copolymers are a class of commonly-studied materials for nanoscale selfassembly. Block copolymers are long molecules that consist of \"blocks\" of chemically differing polymers attached end-to-end. Under the right conditions, these blocks will phase separate, spontaneously forming periodic microdomains. Diblock copolymers, which have only two blocks, have been found to form a variety of well-ordered morphologies with nanoscale periodicity ...",
    "advisors": ["Caroline A. Ross"],
    "text": "The effects of polydispersity on the morphology of polystyrene-polyferrocenyldimethylsilane block copolymer thin films Introduction: As the size of electronic and magnetic devices decreases, nanoscale patterning becomes an increasingly important area of research. Two different approaches have been taken to pattern media: top-down methods such as lithography, and bottom-up methods such as self-assembly. Top-down assembly methods have the advantages of precision and accuracy, but are hard to scale for certain industrial applications due to their low throughput. Self-assembly methods are more easily scalable for applications requiring mass production. Thus, self-assembly has attracted attention and is an area of ongoing research for its potential to create high-throughput, periodic nanoscale patterns. Block copolymers are a class of commonly-studied materials for nanoscale selfassembly. Block copolymers are long molecules that consist of \"blocks\" of chemically differing polymers attached end-to-end. Under the right conditions, these blocks will phase separate, spontaneously forming periodic microdomains. Diblock copolymers, which have only two blocks, have been found to form a variety of well-ordered morphologies with nanoscale periodicity ..."
}, {
    "id": "oai:dspace.mit.edu:1721.1/89964",
    "title": "Construction of a classification hierarchy for process underspecification to streamline life-cycle assessment",
    "abstract": "Concerns over global warming potential and environmental degradation have created a demand for accurate assessment of the impact of various products and processes. Life cycle assessment (LCA), a quantitative assessment method, has been employed primarily to products, analyzing the energy inputs and environmental consequences for the manufacture and use of specific goods. While it has not seen widespread use in assessment of industrial processes, its methodology can be adapted for such purposes; indeed, LCA may be a powerful tool for analyzing processes. This thesis aims to explore the viability of LCA as applied to the process industry. Building on previous research designed to provide high-quality assessment despite varying levels of uncertainty associated with material inputs, this research constructs a system which classifies processes into a hierarchy based on their degree of underspecification. Simulations are performed using Oracle's Crystal Ball software to assess the usefulness and accuracy of the classification system. The system and its components are modified and tested again to achieve better results. Owing to time constraints and fundamental differences between energy inputs for processing different types of materials, the classification system presented herein concerns itself only with metals. Nonetheless, this system seeks to provide a logical approach to process underspecification, and lays the foundation for similar systems for other processes and other types of materials.",
    "advisors": ["Elsa Olivetti"],
    "text": "Construction of a classification hierarchy for process underspecification to streamline life-cycle assessment Concerns over global warming potential and environmental degradation have created a demand for accurate assessment of the impact of various products and processes. Life cycle assessment (LCA), a quantitative assessment method, has been employed primarily to products, analyzing the energy inputs and environmental consequences for the manufacture and use of specific goods. While it has not seen widespread use in assessment of industrial processes, its methodology can be adapted for such purposes; indeed, LCA may be a powerful tool for analyzing processes. This thesis aims to explore the viability of LCA as applied to the process industry. Building on previous research designed to provide high-quality assessment despite varying levels of uncertainty associated with material inputs, this research constructs a system which classifies processes into a hierarchy based on their degree of underspecification. Simulations are performed using Oracle's Crystal Ball software to assess the usefulness and accuracy of the classification system. The system and its components are modified and tested again to achieve better results. Owing to time constraints and fundamental differences between energy inputs for processing different types of materials, the classification system presented herein concerns itself only with metals. Nonetheless, this system seeks to provide a logical approach to process underspecification, and lays the foundation for similar systems for other processes and other types of materials."
}, {
    "id": "oai:dspace.mit.edu:1721.1/104147",
    "title": "Photoconductivity and minority carrier lifetime in tin sulfide and gallium arsenide semiconductors for photovoltaics",
    "abstract": "The growth and maintenance of the modern technological world requires immediate solutions in the field of clean, renewable energy. One prominent solution is the rapid advancement of solar cell technologies due to the wide availability of solar energy and the growing versatility of harnessing it. As efficiencies for these devices creep upwards, it becomes increasingly more important to find the greatest inhibiting factor. Through a solar cell simulator program (SCAPS), improvements in the minority-carrier lifetime of cell materials show not only significant improvements in cell efficiencies, but also an un-masking of improvements by other properties, which are inhibited when the lifetime is too short. This work aims to calculate the mobilitylifetime products ([mu][tau]) of gallium arsenide (GaAs) and annealed and un-annealed tin sulfide (SnS) with respective p-doping carrier concentrations of 1018 cm-3, 1016 cm-3, and 1015 cm-3 through photoconductivity measurements. Films are 1 [mu]m thick and have a four-bar and two-bar contact configuration to model carrier conductivity as a sheet. For calculations, two methods of modeling charge carrier generation are considered; a uniform generation throughout the film and a depth and wavelength-dependent generation. This work found values on the order of 10-1 cm2 V-1, 10-4 cm2 V-1, and 10-5 cm2 V-1, for GaAs, annealed SnS, and un-annealed SnS, respectively, for both methods of calculation. The simplified approach considering a uniform generation yielded lower results than the depth and wavelength dependent calculations by about a factor of two. All values were three to four orders of magnitude higher than those found in the literature. For this reason, it is believed that the majority-carrier is dominating measurements due to an inhibited minority-carrier lifetime.",
    "advisors": ["Tonio Buonassisi"],
    "text": "Photoconductivity and minority carrier lifetime in tin sulfide and gallium arsenide semiconductors for photovoltaics The growth and maintenance of the modern technological world requires immediate solutions in the field of clean, renewable energy. One prominent solution is the rapid advancement of solar cell technologies due to the wide availability of solar energy and the growing versatility of harnessing it. As efficiencies for these devices creep upwards, it becomes increasingly more important to find the greatest inhibiting factor. Through a solar cell simulator program (SCAPS), improvements in the minority-carrier lifetime of cell materials show not only significant improvements in cell efficiencies, but also an un-masking of improvements by other properties, which are inhibited when the lifetime is too short. This work aims to calculate the mobilitylifetime products ([mu][tau]) of gallium arsenide (GaAs) and annealed and un-annealed tin sulfide (SnS) with respective p-doping carrier concentrations of 1018 cm-3, 1016 cm-3, and 1015 cm-3 through photoconductivity measurements. Films are 1 [mu]m thick and have a four-bar and two-bar contact configuration to model carrier conductivity as a sheet. For calculations, two methods of modeling charge carrier generation are considered; a uniform generation throughout the film and a depth and wavelength-dependent generation. This work found values on the order of 10-1 cm2 V-1, 10-4 cm2 V-1, and 10-5 cm2 V-1, for GaAs, annealed SnS, and un-annealed SnS, respectively, for both methods of calculation. The simplified approach considering a uniform generation yielded lower results than the depth and wavelength dependent calculations by about a factor of two. All values were three to four orders of magnitude higher than those found in the literature. For this reason, it is believed that the majority-carrier is dominating measurements due to an inhibited minority-carrier lifetime."
}, {
    "id": "oai:dspace.mit.edu:1721.1/58379",
    "title": "Polymer substrates with microneedles for epidermis injection",
    "abstract": "Injections of medicine into the body are commonplace, whether they be intravenous or capsules. The benefit of using a macroneedle for injecting cargo into the circulatory system is its simplicity. However, introduction of the needle intravenously can also include foreign matter if the needle is unsterile. Due to macroneedles ability to pierce skin and veins for effortless insertion, it can also damage unintentional areas if a patient resists the needle, or if it is poorly inserted. Thus the body can be subjected to undesirable materials beyond the intension medicine cargo. Current research reevaluates methods of introducing cargo medicine into the body. Popular models consider polymer substrates with different surface designs and medicine release. Thin polymer substrates allow flexible construction for adhering to tissue while specfic polymers with high Young's modulus create strength for rigidity. Cargo can be placed within or on top of the substrate itself for release to the epidermis or dermis in stages, which is difficult for both oral medicine and macroneedles. A spectic substrate system with microneedles can prevent irflammation or tear of the epidermis but still puncture for cargo release. Depending on the substrate contact surface area, a larger microneedle array can be utilized, for a higher success rate of release beyond individual microneedles. Microneedles can carry and release medicine either internally or externally through the epidermis. In the latter, Langerhans cells can be utilized for activating the immune system by releasing antigenes. Aims of this thesis show the effects of polymer microneedle substrates with methods for constructing the substrate arrays that are flexible adherent to the epidermis, rigid enough for puncturing the stratum corneum, but not weak enough to buckle or be brittle.",
    "advisors": ["Darrell Irvine"],
    "text": "Polymer substrates with microneedles for epidermis injection Injections of medicine into the body are commonplace, whether they be intravenous or capsules. The benefit of using a macroneedle for injecting cargo into the circulatory system is its simplicity. However, introduction of the needle intravenously can also include foreign matter if the needle is unsterile. Due to macroneedles ability to pierce skin and veins for effortless insertion, it can also damage unintentional areas if a patient resists the needle, or if it is poorly inserted. Thus the body can be subjected to undesirable materials beyond the intension medicine cargo. Current research reevaluates methods of introducing cargo medicine into the body. Popular models consider polymer substrates with different surface designs and medicine release. Thin polymer substrates allow flexible construction for adhering to tissue while specfic polymers with high Young's modulus create strength for rigidity. Cargo can be placed within or on top of the substrate itself for release to the epidermis or dermis in stages, which is difficult for both oral medicine and macroneedles. A spectic substrate system with microneedles can prevent irflammation or tear of the epidermis but still puncture for cargo release. Depending on the substrate contact surface area, a larger microneedle array can be utilized, for a higher success rate of release beyond individual microneedles. Microneedles can carry and release medicine either internally or externally through the epidermis. In the latter, Langerhans cells can be utilized for activating the immune system by releasing antigenes. Aims of this thesis show the effects of polymer microneedle substrates with methods for constructing the substrate arrays that are flexible adherent to the epidermis, rigid enough for puncturing the stratum corneum, but not weak enough to buckle or be brittle."
}, {
    "id": "oai:dspace.mit.edu:1721.1/35056",
    "title": "Work functions of functionalized singled-walled carbon nanotubes",
    "abstract": "Introduction: Carbon nanotube (CNT) structures were discovered by Sumio Iijima in 1991 at NEC laboratories in Japan. Since their discovery, scientists and engineers have been fascinated by their electrical and mechanical properties. Their unique characteristics, in addition to their nanoscale size, have generated much excitement about the possible applications of this novel. material.",
    "advisors": ["Nicola Marzari"],
    "text": "Work functions of functionalized singled-walled carbon nanotubes Introduction: Carbon nanotube (CNT) structures were discovered by Sumio Iijima in 1991 at NEC laboratories in Japan. Since their discovery, scientists and engineers have been fascinated by their electrical and mechanical properties. Their unique characteristics, in addition to their nanoscale size, have generated much excitement about the possible applications of this novel. material."
}, {
    "id": "oai:dspace.mit.edu:1721.1/89983",
    "title": "Thermo-mechanical stress relief analysis in PMMA and 6000 series aluminum",
    "abstract": "Stress relief of materials produced in bulk is a key part of the manufacturing process. The most common kinds are either thermal or mechanical and are commonly applied to commercial metal alloys. A third type, thermo-mechanical, utilizes thermal gradients to induce residual stresses of an equal and opposite nature to balance compressive and tensile stresses existing in the material after solutionizing. The experiment detailed in this work shows the effect of thermal gradients on residual stresses in polymethyl methacrylate (PMMA). A downhill quench from 95 C to 15C is able to create a deflection of 2.36 millimeters, evidence of residual stress. A subsequent uphill quench from -40 to 100 degrees reduced the deflection by 37 percent. The finite element simulation of a 6000 series aluminum block verifies that under properly controlled processing parameters, it is possible to induce opposite stresses to relieve residual stresses in a quenched material. Additional limitations to the uphill quench technique are detailed in the following work so that thermo-mechanical stress relief may be properly applied to a range of materials.",
    "advisors": ["Thomas W. Eagar"],
    "text": "Thermo-mechanical stress relief analysis in PMMA and 6000 series aluminum Stress relief of materials produced in bulk is a key part of the manufacturing process. The most common kinds are either thermal or mechanical and are commonly applied to commercial metal alloys. A third type, thermo-mechanical, utilizes thermal gradients to induce residual stresses of an equal and opposite nature to balance compressive and tensile stresses existing in the material after solutionizing. The experiment detailed in this work shows the effect of thermal gradients on residual stresses in polymethyl methacrylate (PMMA). A downhill quench from 95 C to 15C is able to create a deflection of 2.36 millimeters, evidence of residual stress. A subsequent uphill quench from -40 to 100 degrees reduced the deflection by 37 percent. The finite element simulation of a 6000 series aluminum block verifies that under properly controlled processing parameters, it is possible to induce opposite stresses to relieve residual stresses in a quenched material. Additional limitations to the uphill quench technique are detailed in the following work so that thermo-mechanical stress relief may be properly applied to a range of materials."
}, {
    "id": "oai:dspace.mit.edu:1721.1/43211",
    "title": "Synthetic routes to monodisperse gold nanoparticles stabilized by different-length alkanethiols",
    "abstract": "My thesis explored three different synthesis routes toward obtaining monodisperse clutches of well-ordered nanoparticles stabilized by various alkanethiols. The first two synthesis methods were based on a two-phase system employing first tetraoctylammonium bromide (TOAB) as a phase transfer catalyst and then didodecyldimethylammonium bromide (DDAB). Though these methods approximated what could be considered monodisperse nanoparticles ([sigma]< 5%) by reaching distributions of a [sigma]~-19% for TOAB and [sigma]- 13% for DDAB at their best, they were easily surpassed by the degree of monodispersity achieved by a one-phase method. This one-phase method, which does not use inverse micelles to control the reduction process, was able to reach distribution levels where o<10%. More specifically, the method proved robust enough to synthesize monodisperse, well-ordered nanoparticles with the following alkanethiols: octanethiol, nonanethiol, decanethiol, dodecanethiol, pentadecanethiol; and the following distributions: [sigma]~7%, [sigma]~9%, -[sigma]~7%, [sigma]~4%, and ~ [sigma]8%, respectively.",
    "advisors": ["Francesco Stellacci"],
    "text": "Synthetic routes to monodisperse gold nanoparticles stabilized by different-length alkanethiols My thesis explored three different synthesis routes toward obtaining monodisperse clutches of well-ordered nanoparticles stabilized by various alkanethiols. The first two synthesis methods were based on a two-phase system employing first tetraoctylammonium bromide (TOAB) as a phase transfer catalyst and then didodecyldimethylammonium bromide (DDAB). Though these methods approximated what could be considered monodisperse nanoparticles ([sigma]< 5%) by reaching distributions of a [sigma]~-19% for TOAB and [sigma]- 13% for DDAB at their best, they were easily surpassed by the degree of monodispersity achieved by a one-phase method. This one-phase method, which does not use inverse micelles to control the reduction process, was able to reach distribution levels where o<10%. More specifically, the method proved robust enough to synthesize monodisperse, well-ordered nanoparticles with the following alkanethiols: octanethiol, nonanethiol, decanethiol, dodecanethiol, pentadecanethiol; and the following distributions: [sigma]~7%, [sigma]~9%, -[sigma]~7%, [sigma]~4%, and ~ [sigma]8%, respectively."
}, {
    "id": "oai:dspace.mit.edu:1721.1/101857",
    "title": "Study of the microstructure and mechanical properties of hummingbird wing-bones",
    "abstract": "A study of the microstructural and mechanical properties of wing bones of an Anna's hummingbird (Calypte Anna) and a tree swallow (Tachycineta bicolor) was conducted to determine whether the hummingbird bones exhibited unique features due to the high wing loading of the bird. It was hypothesized that the hummingbird's ability to hover, flapping its wings at an incredible rate, would create higher principal stresses that would require an internal bone structure notably different from that of the swallow. Micro-computed tomography, scanning electron microscopy, and nano-indentation experiments were conducted to determine if such anomalies existed. Additional macroscale measurements were taken to compare relative proportions of each of the bones of interest, which included the humerus, radius, and ulna. Upon examining the images produced via micro-computed tomography and scanning electron microscopy, it was found that trabecular bone was present, suggesting the mechanical advantages of the trabeculae were required. Nano-indentation results proved to be relatively inconclusive, but generally provided results reasonably close to expected literature values. Further experimentation would be required to determine if the deviation from expected values were meaningful.",
    "advisors": ["Lorna Gibson"],
    "text": "Study of the microstructure and mechanical properties of hummingbird wing-bones A study of the microstructural and mechanical properties of wing bones of an Anna's hummingbird (Calypte Anna) and a tree swallow (Tachycineta bicolor) was conducted to determine whether the hummingbird bones exhibited unique features due to the high wing loading of the bird. It was hypothesized that the hummingbird's ability to hover, flapping its wings at an incredible rate, would create higher principal stresses that would require an internal bone structure notably different from that of the swallow. Micro-computed tomography, scanning electron microscopy, and nano-indentation experiments were conducted to determine if such anomalies existed. Additional macroscale measurements were taken to compare relative proportions of each of the bones of interest, which included the humerus, radius, and ulna. Upon examining the images produced via micro-computed tomography and scanning electron microscopy, it was found that trabecular bone was present, suggesting the mechanical advantages of the trabeculae were required. Nano-indentation results proved to be relatively inconclusive, but generally provided results reasonably close to expected literature values. Further experimentation would be required to determine if the deviation from expected values were meaningful."
}, {
    "id": "oai:dspace.mit.edu:1721.1/32727",
    "title": "Nanoscale properties of poly(ethylene terephthalate) vascular grafts",
    "abstract": "Vascular grafts are prosthetic tubes that serve as artificial replacements for damaged blood vessels. Poly(ethylene-terephthalate), PET, has been successfully used in large diameter grafts; however, small caliber grafts are still a major challenge in biomaterials. Due to surface forces, blood plasma proteins adsorb to the graft, resulting in inflammation, infection, thrombus formation, and ultimately, vessel reclosure. The object of this project was to characterize and analyze the nanoscale surface properties of three different commercial vascular grafts, woven collagen-coated, knitted collagen- coated, and knitted heparin-bonded, all PET-based. The study was performed in order to ascertain differences in biocompatibility due to surface coating and morphology. Scanning Electron Microscopy, Atomic Force Microscopy and High Resolution Force Spectroscopy techniques were used to characterize the surface of the samples as well as to measure the forces between these surfaces and blood plasma proteins. The results will serve as a basis for the understanding of the nanoscale interactions between the biomaterial and blood plasma proteins. Such interactions are brought about by the different surface topologies and components, therefore a thorough understanding of surface properties will act as a building block for further changes in small caliber vascular grafts in order to enhance their biocompatibility.",
    "advisors": ["Christine Ortiz"],
    "text": "Nanoscale properties of poly(ethylene terephthalate) vascular grafts Vascular grafts are prosthetic tubes that serve as artificial replacements for damaged blood vessels. Poly(ethylene-terephthalate), PET, has been successfully used in large diameter grafts; however, small caliber grafts are still a major challenge in biomaterials. Due to surface forces, blood plasma proteins adsorb to the graft, resulting in inflammation, infection, thrombus formation, and ultimately, vessel reclosure. The object of this project was to characterize and analyze the nanoscale surface properties of three different commercial vascular grafts, woven collagen-coated, knitted collagen- coated, and knitted heparin-bonded, all PET-based. The study was performed in order to ascertain differences in biocompatibility due to surface coating and morphology. Scanning Electron Microscopy, Atomic Force Microscopy and High Resolution Force Spectroscopy techniques were used to characterize the surface of the samples as well as to measure the forces between these surfaces and blood plasma proteins. The results will serve as a basis for the understanding of the nanoscale interactions between the biomaterial and blood plasma proteins. Such interactions are brought about by the different surface topologies and components, therefore a thorough understanding of surface properties will act as a building block for further changes in small caliber vascular grafts in order to enhance their biocompatibility."
}, {
    "id": "oai:dspace.mit.edu:1721.1/35065",
    "title": "Curvature driven phase separation in mixed ligand coated gold nanoparticles",
    "abstract": "Monolayer-coated gold nanoparticles have been the subject of extensive studies in fields ranging from physics to medicine. The properties of these nanomaterials such as solubility and surface energy are often attributed solely to the chemical functionalities of the ligand head-groups. However, the morphology of these monomolecular layers on gold nanoparticles plays as important of a role as the surface chemistry. Intriguing phase-separation phenomena have been observed for mixed self-assembled monolayers (SAM) of octanethiol (OT) and mercaptopropionic acid (MPA) on the surface of gold nanoparticles. These ordered structures are studied here through scanning tunneling microscope (STM) images, as a function of the gold core diameter, which is a measure of the particle's curvature. The packing of OT homoligand nanoparticles is found to have a head-group spacing of 0.54 nm, which differs from that on flat gold (111) surfaces, 0.5 nm. The OT:MPA heteroligand nanoparticles are observed to phase-separate into ordered ribbon-like domains, with spacings that depend on the nanoparticle diameter. A geometric framework that includes a continuous and crystallographic description is established to best describe the observed behaviors.",
    "advisors": ["Francesco Stellacci"],
    "text": "Curvature driven phase separation in mixed ligand coated gold nanoparticles Monolayer-coated gold nanoparticles have been the subject of extensive studies in fields ranging from physics to medicine. The properties of these nanomaterials such as solubility and surface energy are often attributed solely to the chemical functionalities of the ligand head-groups. However, the morphology of these monomolecular layers on gold nanoparticles plays as important of a role as the surface chemistry. Intriguing phase-separation phenomena have been observed for mixed self-assembled monolayers (SAM) of octanethiol (OT) and mercaptopropionic acid (MPA) on the surface of gold nanoparticles. These ordered structures are studied here through scanning tunneling microscope (STM) images, as a function of the gold core diameter, which is a measure of the particle's curvature. The packing of OT homoligand nanoparticles is found to have a head-group spacing of 0.54 nm, which differs from that on flat gold (111) surfaces, 0.5 nm. The OT:MPA heteroligand nanoparticles are observed to phase-separate into ordered ribbon-like domains, with spacings that depend on the nanoparticle diameter. A geometric framework that includes a continuous and crystallographic description is established to best describe the observed behaviors."
}, {
    "id": "oai:dspace.mit.edu:1721.1/112499",
    "title": "Semi-solid redox flow battery",
    "abstract": "materials used in Li-ion batteries and the design and functioning of a redox flow cell. The use of Li-ion battery materials offers significant increases in energy and power density (200 Wh/kg compared to 25-35 Wh/kg for current commercial vanadium redox batteries). The implementation of a redox flow system allows for energy to be stored outside the cell and for the power and energy of the battery to be decoupled. A proof of concept is achieved by successful cycling of anode and cathode suspensions under intermittent flow conditions. The importance of materials' stability to cell life, energy and power densities is discussed. The high energy densities may enable the use of the proposed system in a variety of application, ranging from grid-level storage to fully electric charge.",
    "advisors": ["Yet-Ming Chiang"],
    "text": "Semi-solid redox flow battery materials used in Li-ion batteries and the design and functioning of a redox flow cell. The use of Li-ion battery materials offers significant increases in energy and power density (200 Wh/kg compared to 25-35 Wh/kg for current commercial vanadium redox batteries). The implementation of a redox flow system allows for energy to be stored outside the cell and for the power and energy of the battery to be decoupled. A proof of concept is achieved by successful cycling of anode and cathode suspensions under intermittent flow conditions. The importance of materials' stability to cell life, energy and power densities is discussed. The high energy densities may enable the use of the proposed system in a variety of application, ranging from grid-level storage to fully electric charge."
}, {
    "id": "oai:dspace.mit.edu:1721.1/98668",
    "title": "Equilibrium configurations of oxygen bubbles on surfaces for applications in nanostructured hematite electrodes",
    "abstract": "The variability of a nanostructured material's fundamental properties as compared to its bulk state has led to the rich field of nanotechnology and the quest to uncover unique properties of structures at the nanoscale. An active application for these materials is in the nanostructuring of [alpha]-FeO (hematite) for photoelectrochemical (PEC) splitting of water to generate hydrogen. A model of a bubble on a nanorod was developed in this work to facilitate the understanding of equilibrium configurations of oxygen bubbles on a nanostructured hematite electrode. The equilibrium configurations are computed using Surface Evolver, a program which models surfaces shaped by various constraints and forces. A nanorod with a top surface dimension of 100 by 100 nm was the subject of the bulk of this work. The energy of different starting configurations of the bubble and increasing volume of the bubble were compared to that of a free spherical bubble. The energy of the bubble approaches the total surface energy of a free spherical bubble, indicating that a bubble that has nucleated on the surface of a nanorod will approach a shape that has nearly the same energy as a detached spherical bubble. For applications in PEC splitting of water, this result indicates that from an equilibrium and lowest energy perspective, an oxygen bubble could nucleate on the surface of a nanorod, grow in volume, and detach or pinch-off from the nanorod.",
    "advisors": ["W. Craig Carter"],
    "text": "Equilibrium configurations of oxygen bubbles on surfaces for applications in nanostructured hematite electrodes The variability of a nanostructured material's fundamental properties as compared to its bulk state has led to the rich field of nanotechnology and the quest to uncover unique properties of structures at the nanoscale. An active application for these materials is in the nanostructuring of [alpha]-FeO (hematite) for photoelectrochemical (PEC) splitting of water to generate hydrogen. A model of a bubble on a nanorod was developed in this work to facilitate the understanding of equilibrium configurations of oxygen bubbles on a nanostructured hematite electrode. The equilibrium configurations are computed using Surface Evolver, a program which models surfaces shaped by various constraints and forces. A nanorod with a top surface dimension of 100 by 100 nm was the subject of the bulk of this work. The energy of different starting configurations of the bubble and increasing volume of the bubble were compared to that of a free spherical bubble. The energy of the bubble approaches the total surface energy of a free spherical bubble, indicating that a bubble that has nucleated on the surface of a nanorod will approach a shape that has nearly the same energy as a detached spherical bubble. For applications in PEC splitting of water, this result indicates that from an equilibrium and lowest energy perspective, an oxygen bubble could nucleate on the surface of a nanorod, grow in volume, and detach or pinch-off from the nanorod."
}, {
    "id": "oai:dspace.mit.edu:1721.1/98652",
    "title": "Mechanical and electrical characterization of carbon Black-doped closed-cell Polydimethylsiloxane (PDMS) foam",
    "abstract": "Carbon Black-doped Polydimethylsiloxane (CB-PDMS) can be used as a pressure sensing material due to its piezoresistive properties. The sensitivity of such a sensor is in part dependent on the stiffness of the material. A closed-cell CB-PDMS foam is being explored as a possible flexible, lightweight, and waterproof underwater sensing material for use in unmanned underwater vehicles and other hydrodynamic sensing purposes. The percolation threshold for conduction through the CB-PDMS foam is theorized, and a number of different concentrations based on the theorized threshold are explored in order to determine the optimum weight percent of Carbon Black dopant to achieve a high sensitivity, low stiffness sensing CB-PDMS foam. Sinusoidal mechanical pressure patterns were applied and voltage response measured. An optimum dopant weight percent out of the concentrations tested was found at 5.5 wt% CB-PDMS.",
    "advisors": ["Jeffrey Lang"],
    "text": "Mechanical and electrical characterization of carbon Black-doped closed-cell Polydimethylsiloxane (PDMS) foam Carbon Black-doped Polydimethylsiloxane (CB-PDMS) can be used as a pressure sensing material due to its piezoresistive properties. The sensitivity of such a sensor is in part dependent on the stiffness of the material. A closed-cell CB-PDMS foam is being explored as a possible flexible, lightweight, and waterproof underwater sensing material for use in unmanned underwater vehicles and other hydrodynamic sensing purposes. The percolation threshold for conduction through the CB-PDMS foam is theorized, and a number of different concentrations based on the theorized threshold are explored in order to determine the optimum weight percent of Carbon Black dopant to achieve a high sensitivity, low stiffness sensing CB-PDMS foam. Sinusoidal mechanical pressure patterns were applied and voltage response measured. An optimum dopant weight percent out of the concentrations tested was found at 5.5 wt% CB-PDMS."
}, {
    "id": "oai:dspace.mit.edu:1721.1/32725",
    "title": "Characterization of nano-arrays fabricated via self-assembly of block copolymers",
    "abstract": "This research focused on methods for regulating arrangement of self-assembled block copolymers by understanding fabrication conditions and their effects on the polymers on flat and patterned substrates. Block copolymer self-assembly is a simple and low cost process for creating lithographic masks with features under 100nm in dimension. These patterns can be transferred to more permanent materials for applications in electronics, magnetic devices, as well as sensors and filters. Polystyrene-poly(ferrocenyldimethylsilane) block copolymer thin films were characterized in terms of their spin curves, PSF spherical domain cross sectional area distributions, and correlation distances. Optimal fabrication conditions were selected from studying polymer behavior on flat substrates and then used for templated substrate studies. Substrates that were templated with grooves produced quantized numbers of rows of spherical domains ranging from 4 to 7. Behavior in these grooves was characterized in terms of groove width constraints, cross sectional domain area distributions, and row ordering. For all templated arrays, the lengths of ordered regions were more than 2 fold higher than the diameters of ordered regions of arrays on flat substrates. The characterization accomplished in this work will be used to compare block copolymers with similar volume fractions of the blocks that allow sphere microdomain formation but of different molecular weights. The ultimate goals are to establish how the molecular weight of this block copolymer affects its self assembly on templated and on flat substrates and to use this factor as well as fabrication conditions and template geometries to engineer arrays with desirable properties.",
    "advisors": ["Caroline Ross"],
    "text": "Characterization of nano-arrays fabricated via self-assembly of block copolymers This research focused on methods for regulating arrangement of self-assembled block copolymers by understanding fabrication conditions and their effects on the polymers on flat and patterned substrates. Block copolymer self-assembly is a simple and low cost process for creating lithographic masks with features under 100nm in dimension. These patterns can be transferred to more permanent materials for applications in electronics, magnetic devices, as well as sensors and filters. Polystyrene-poly(ferrocenyldimethylsilane) block copolymer thin films were characterized in terms of their spin curves, PSF spherical domain cross sectional area distributions, and correlation distances. Optimal fabrication conditions were selected from studying polymer behavior on flat substrates and then used for templated substrate studies. Substrates that were templated with grooves produced quantized numbers of rows of spherical domains ranging from 4 to 7. Behavior in these grooves was characterized in terms of groove width constraints, cross sectional domain area distributions, and row ordering. For all templated arrays, the lengths of ordered regions were more than 2 fold higher than the diameters of ordered regions of arrays on flat substrates. The characterization accomplished in this work will be used to compare block copolymers with similar volume fractions of the blocks that allow sphere microdomain formation but of different molecular weights. The ultimate goals are to establish how the molecular weight of this block copolymer affects its self assembly on templated and on flat substrates and to use this factor as well as fabrication conditions and template geometries to engineer arrays with desirable properties."
}, {
    "id": "oai:dspace.mit.edu:1721.1/114088",
    "title": "Diffuse double-layer interaction for nonspherical colloidal particles",
    "abstract": "The DLVO theory of colloids is used to consider the stability of clay colloid particles. An approach to colloid physics using classical electrostatic methods is presented. Specifically, the electrical double layer is examined using computational methods. To this end, the Poisson-Boltzman equation is solved numerically for geometries corresponding to interacting clay particles. The interaction energies of double layers is calculated for several particle configurations.",
    "advisors": ["W. Craig Carter", "Richard L. Smith"],
    "text": "Diffuse double-layer interaction for nonspherical colloidal particles The DLVO theory of colloids is used to consider the stability of clay colloid particles. An approach to colloid physics using classical electrostatic methods is presented. Specifically, the electrical double layer is examined using computational methods. To this end, the Poisson-Boltzman equation is solved numerically for geometries corresponding to interacting clay particles. The interaction energies of double layers is calculated for several particle configurations."
}, {
    "id": "oai:dspace.mit.edu:1721.1/76123",
    "title": "Polymer coated superparamagnetic beads walking on polymer coated surface",
    "abstract": "Biology has provided us with many organisms that are able to propel themselves through a fluid using cilia or flagella. This provides inspiration to create controllable systems that cannot only propel an organism or device through a fluid but can also create a fluid flow. Research has focused on how to mimic the mechanisms of these organisms for the use in microfluidic devices or drug delivery. This work examines walkers that are created using superparamagnetic beads placed in a rotating external magnetic field. Dipoles align in the beads so they assemble into rotors. These rotors follow the rotating magnetic field and are able to translate across a surface. This work looks at the effect of coating the beads and the surface with a polymer, Polyethylene Glycol(PEG). PEG has been shown to undergo a transition from an expanded state to a collapsed state under certain salt concentrations and temperature ranges. By looking at this transition we can see if the use of a polymer could affect the velocity of the rotors and if PEG could be used to control the velocity of the rotors or to initiate a transition. This transition is only seen by recording the velocity of the rotors, future research using other experimental procedures might be helpful in finalizing the transition of PEG in NaCl. It was unclear from these experiments whether the velocity of the rotors is dependent on the state of the polymer.",
    "advisors": ["Alfredo Alexander-Katz"],
    "text": "Polymer coated superparamagnetic beads walking on polymer coated surface Biology has provided us with many organisms that are able to propel themselves through a fluid using cilia or flagella. This provides inspiration to create controllable systems that cannot only propel an organism or device through a fluid but can also create a fluid flow. Research has focused on how to mimic the mechanisms of these organisms for the use in microfluidic devices or drug delivery. This work examines walkers that are created using superparamagnetic beads placed in a rotating external magnetic field. Dipoles align in the beads so they assemble into rotors. These rotors follow the rotating magnetic field and are able to translate across a surface. This work looks at the effect of coating the beads and the surface with a polymer, Polyethylene Glycol(PEG). PEG has been shown to undergo a transition from an expanded state to a collapsed state under certain salt concentrations and temperature ranges. By looking at this transition we can see if the use of a polymer could affect the velocity of the rotors and if PEG could be used to control the velocity of the rotors or to initiate a transition. This transition is only seen by recording the velocity of the rotors, future research using other experimental procedures might be helpful in finalizing the transition of PEG in NaCl. It was unclear from these experiments whether the velocity of the rotors is dependent on the state of the polymer."
}, {
    "id": "oai:dspace.mit.edu:1721.1/43208",
    "title": "pH-sensitive resist materials for combined photolithographic patterning of proteins and fluid lipid bilayers",
    "abstract": "Photolithography of a pH-sensitive photoresist polymer was performed to pattern both lipid bilayers and proteins onto the same surface. The motivation behind this was to create a substrate mimicking an array of antigen- presenting cells. The substrate would consist of signaling ligand, biotin anti- CD3, bound to a lipid bilayer in a regular array of patches. The fluidity of the lipid bilayer would impart mobility to the signaling ligand. It was found that under appropriate substrate fabrication conditions, lipid bilayers and their associated ligand do segregate to the desired signaling patches. Additionally, the bilayer in these regions is fluid, and is potentially bioactive. This bodes well for our system as a future platform to study the actions of the helper T cell and antigen- presenting cell at the immunological synapse.",
    "advisors": ["Darrell J. Irvine"],
    "text": "pH-sensitive resist materials for combined photolithographic patterning of proteins and fluid lipid bilayers Photolithography of a pH-sensitive photoresist polymer was performed to pattern both lipid bilayers and proteins onto the same surface. The motivation behind this was to create a substrate mimicking an array of antigen- presenting cells. The substrate would consist of signaling ligand, biotin anti- CD3, bound to a lipid bilayer in a regular array of patches. The fluidity of the lipid bilayer would impart mobility to the signaling ligand. It was found that under appropriate substrate fabrication conditions, lipid bilayers and their associated ligand do segregate to the desired signaling patches. Additionally, the bilayer in these regions is fluid, and is potentially bioactive. This bodes well for our system as a future platform to study the actions of the helper T cell and antigen- presenting cell at the immunological synapse."
}, {
    "id": "oai:dspace.mit.edu:1721.1/69799",
    "title": "Low variance methods for Monte Carlo simulation of phonon transport",
    "abstract": "Computational studies in kinetic transport are of great use in micro and nanotechnologies. In this work, we focus on Monte Carlo methods for phonon transport, intended for studies in microscale heat transfer. After reviewing the theory of phonons, we use scientific literature to write a Monte Carlo code solving the Boltzmann Transport Equation for phonons. As a first improvement to the particle method presented, we choose to use the Boltzmann Equation in terms of energy as a more convenient and accurate formulation to develop such a code. Then, we use the concept of control variates in order to introduce the notion of deviational particles. Noticing that a thermalized system at equilibrium is inherently a solution of the Boltzmann Transport Equation, we take advantage of this deterministic piece of information: we only simulate the deviation from a nearby equilibrium, which removes a great part of the statistical uncertainty. Doing so, the standard deviation of the result that we obtain is proportional to the deviation from equilibrium. In other words, we are able to simulate signals of arbitrarily low amplitude with no additional computational cost. After exploring two other variants based on the idea of control variates, we validate our code on a few theoretical results derived from the Boltzmann equation. Finally, we present a few applications of the methods.",
    "advisors": ["Nicolas G. Hadjiconstantinou"],
    "text": "Low variance methods for Monte Carlo simulation of phonon transport Computational studies in kinetic transport are of great use in micro and nanotechnologies. In this work, we focus on Monte Carlo methods for phonon transport, intended for studies in microscale heat transfer. After reviewing the theory of phonons, we use scientific literature to write a Monte Carlo code solving the Boltzmann Transport Equation for phonons. As a first improvement to the particle method presented, we choose to use the Boltzmann Equation in terms of energy as a more convenient and accurate formulation to develop such a code. Then, we use the concept of control variates in order to introduce the notion of deviational particles. Noticing that a thermalized system at equilibrium is inherently a solution of the Boltzmann Transport Equation, we take advantage of this deterministic piece of information: we only simulate the deviation from a nearby equilibrium, which removes a great part of the statistical uncertainty. Doing so, the standard deviation of the result that we obtain is proportional to the deviation from equilibrium. In other words, we are able to simulate signals of arbitrarily low amplitude with no additional computational cost. After exploring two other variants based on the idea of control variates, we validate our code on a few theoretical results derived from the Boltzmann equation. Finally, we present a few applications of the methods."
}, {
    "id": "oai:dspace.mit.edu:1721.1/34831",
    "title": "Product design for supply chain : quantifying the costs of complexity in Hewlett-Packard's retail desktop PC business",
    "abstract": "Over the past several years, Hewlett-Packard Company's North America Consumer Computing (NACC) division has faced pressures to increase retail product variety in response to growing customer demand. As they pursue incremental revenue and market share to meet corporate milestones, their, product portfolio grows and the overall complexity of the business increases. The holistic effects of this complexity across the supply chain are not fully understood, which can lead to inefficiencies in portfolio management and, ultimately, lower profitability for the division. Faced with this growing problem, the NACC division employed an HP internal consulting group to help quantify the costs and benefits of complexity in their business and to help establish decision-making guidelines to optimize future product cycles. This project involved three main phases: identification of complexity cost drivers in the retail consumer PC business, development of a quantitative model to calculate complexity effects, and suggestion of complexity guidelines for future portfolio planning. Each phase included presentations to senior management and NACC staff as a way to build \"complexity consciousness\" throughout the organization.",
    "advisors": ["Sara Beckman", "Thomas Roemer"],
    "text": "Product design for supply chain : quantifying the costs of complexity in Hewlett-Packard's retail desktop PC business Over the past several years, Hewlett-Packard Company's North America Consumer Computing (NACC) division has faced pressures to increase retail product variety in response to growing customer demand. As they pursue incremental revenue and market share to meet corporate milestones, their, product portfolio grows and the overall complexity of the business increases. The holistic effects of this complexity across the supply chain are not fully understood, which can lead to inefficiencies in portfolio management and, ultimately, lower profitability for the division. Faced with this growing problem, the NACC division employed an HP internal consulting group to help quantify the costs and benefits of complexity in their business and to help establish decision-making guidelines to optimize future product cycles. This project involved three main phases: identification of complexity cost drivers in the retail consumer PC business, development of a quantitative model to calculate complexity effects, and suggestion of complexity guidelines for future portfolio planning. Each phase included presentations to senior management and NACC staff as a way to build \"complexity consciousness\" throughout the organization."
}, {
    "id": "oai:dspace.mit.edu:1721.1/45360",
    "title": "A technical and economic evaluation of novel pH-responsive core-shell nanoparticles : delivering innovation from laboratory to market",
    "abstract": "Many potentially powerful therapeutic strategies for the treatment of disease require the delivery of drugs into the cytosolic or nuclear compartments of cells. Members of the Irvine laboratory have developed a novel pH-responsive core-shell nanoparticle system that can achieve efficient and non-cytotoxic drug delivery into the cytosol. Another advantage is that the shell can be easily modified to bind to different types of drug agents and incorporate ligands for specific cell targeting. Experimental analysis of the newly synthesized nanoparticles with various shell structures has demonstrated that modification of the shell does not compromise their cytosolic delivery. These nanoparticles, if successful, will improve the therapeutic potential of a wide range of drugs. However, critical issues on the research side need to be resolved, and an appropriate intellectual property strategy should be initiated in the near future. Applications to siRNA delivery and vaccines have been examined in depth, as cytosolic delivery is one of the main challenges in these fields. Partnerships with large pharmaceutical companies are critical in order to acquire key patents on siRNA/antigen. Even though the market is competitive, there is a strong demand for innovative delivery platforms; provided that the overall profile of the core-shell nanoparticles is comparable to that of emerging drug delivery systems, and a strong intellectual property portfolio is developed, the Irvine technology should be able to compete in the market. After analyzing risks on the business side, including the FDA approval process, a suggested business strategy is outlined, through which value can be successfully obtained throughout the existing pharmaceutical supply chain from the novel drug delivery system.",
    "advisors": ["Darrell J. Irvine"],
    "text": "A technical and economic evaluation of novel pH-responsive core-shell nanoparticles : delivering innovation from laboratory to market Many potentially powerful therapeutic strategies for the treatment of disease require the delivery of drugs into the cytosolic or nuclear compartments of cells. Members of the Irvine laboratory have developed a novel pH-responsive core-shell nanoparticle system that can achieve efficient and non-cytotoxic drug delivery into the cytosol. Another advantage is that the shell can be easily modified to bind to different types of drug agents and incorporate ligands for specific cell targeting. Experimental analysis of the newly synthesized nanoparticles with various shell structures has demonstrated that modification of the shell does not compromise their cytosolic delivery. These nanoparticles, if successful, will improve the therapeutic potential of a wide range of drugs. However, critical issues on the research side need to be resolved, and an appropriate intellectual property strategy should be initiated in the near future. Applications to siRNA delivery and vaccines have been examined in depth, as cytosolic delivery is one of the main challenges in these fields. Partnerships with large pharmaceutical companies are critical in order to acquire key patents on siRNA/antigen. Even though the market is competitive, there is a strong demand for innovative delivery platforms; provided that the overall profile of the core-shell nanoparticles is comparable to that of emerging drug delivery systems, and a strong intellectual property portfolio is developed, the Irvine technology should be able to compete in the market. After analyzing risks on the business side, including the FDA approval process, a suggested business strategy is outlined, through which value can be successfully obtained throughout the existing pharmaceutical supply chain from the novel drug delivery system."
}, {
    "id": "oai:dspace.mit.edu:1721.1/120393",
    "title": "Towards material-informed tectonics",
    "abstract": "This thesis introduces, demonstrates, and implements a unified computational design framework for material distribution modeling that enables the production of geometrically complex, materially heterogeneous, and functionally graded objects, across scales, media, and platforms. Receiving user-defined performance mappings as input, the workflow generates and evaluates instructions for designated fabrication systems, informed by the extrinsic constraints presented by the hardware and the intrinsic characteristics embedded in the materials utilized. As a proof of concept to the generalizable approach, three novel design-to-fabrication processes within the framework are introduced with material and materialization precedents and implemented through computational and robotic platforms: implicit modeling for the fabrication of photopolymers, trajectory optimizing for the fabrication of water-based material, and toolpath planning for the fabrication of fiber-based material. Titled Material-informed Tectonics, the framework extends the domain of parametric design processes from geometry to material, expands the potential application of volumetric material modeling techniques beyond high resolution multi-material 3D printing systems, and bridges between the virtual and the physical by integrating material information into the tectonic relationship between manufactured objects and manufacturing methods; thereby outlining an approach towards a synthesis of material properties, computational design, digital fabrication, and the environment.",
    "advisors": ["Neri Oxman"],
    "text": "Towards material-informed tectonics This thesis introduces, demonstrates, and implements a unified computational design framework for material distribution modeling that enables the production of geometrically complex, materially heterogeneous, and functionally graded objects, across scales, media, and platforms. Receiving user-defined performance mappings as input, the workflow generates and evaluates instructions for designated fabrication systems, informed by the extrinsic constraints presented by the hardware and the intrinsic characteristics embedded in the materials utilized. As a proof of concept to the generalizable approach, three novel design-to-fabrication processes within the framework are introduced with material and materialization precedents and implemented through computational and robotic platforms: implicit modeling for the fabrication of photopolymers, trajectory optimizing for the fabrication of water-based material, and toolpath planning for the fabrication of fiber-based material. Titled Material-informed Tectonics, the framework extends the domain of parametric design processes from geometry to material, expands the potential application of volumetric material modeling techniques beyond high resolution multi-material 3D printing systems, and bridges between the virtual and the physical by integrating material information into the tectonic relationship between manufactured objects and manufacturing methods; thereby outlining an approach towards a synthesis of material properties, computational design, digital fabrication, and the environment."
}, {
    "id": "oai:dspace.mit.edu:1721.1/37379",
    "title": "Assessment of colloidal self-assembly for photonic crystal",
    "abstract": "A suspension of monodisperse colloids has an interesting property of self-assembling into a three-dimensional ordered structure. This crystalline material has attracted significant interest on the implementation of photonic crystals, which have practical applications in reflectors, filters, resonators, and waveguides. In this thesis, self-assembly of colloidal crystals and photonic crystal technologies are reviewed. Potential colloidal photonic and non-photonic devices were presented and their values/limitations were discussed. Colloidal photonic crystals were assessed on their technical capabilities, growth techniques and fabrication cost. In this assessment, the bulk colloidal photonic crystals are found to be inherently robust against stacking disorder, cracks and voids. The high reflectance performance and lattice parameter tailoring are useful for implementing reflectors, optical switch and sensors. Besides, the anomalous dispersion characteristic near to the band edges or near to flat bands of the photonic band diagram is suited for superprism and light harvesting applications. Potentially, the unique characteristics of colloidal photonic crystal could be capitalized in a low cost micro-fabrication model. Finally, the study has shown that it is more technically and commercially viable to implement bulk colloidal photonic crystal applications rather than lithographically-defined types.",
    "advisors": ["Yet-Ming Chiang"],
    "text": "Assessment of colloidal self-assembly for photonic crystal A suspension of monodisperse colloids has an interesting property of self-assembling into a three-dimensional ordered structure. This crystalline material has attracted significant interest on the implementation of photonic crystals, which have practical applications in reflectors, filters, resonators, and waveguides. In this thesis, self-assembly of colloidal crystals and photonic crystal technologies are reviewed. Potential colloidal photonic and non-photonic devices were presented and their values/limitations were discussed. Colloidal photonic crystals were assessed on their technical capabilities, growth techniques and fabrication cost. In this assessment, the bulk colloidal photonic crystals are found to be inherently robust against stacking disorder, cracks and voids. The high reflectance performance and lattice parameter tailoring are useful for implementing reflectors, optical switch and sensors. Besides, the anomalous dispersion characteristic near to the band edges or near to flat bands of the photonic band diagram is suited for superprism and light harvesting applications. Potentially, the unique characteristics of colloidal photonic crystal could be capitalized in a low cost micro-fabrication model. Finally, the study has shown that it is more technically and commercially viable to implement bulk colloidal photonic crystal applications rather than lithographically-defined types."
}, {
    "id": "oai:dspace.mit.edu:1721.1/33620",
    "title": "Development of bi-layer mineralized bone and cartilage regeneration template",
    "abstract": "Porous collagen-glycosaminoglycan (CG) scaffolds have been studied extensively and proven to be capable of tissue regeneration in vivo for applications including skin regeneration templates, hollow nerve guides and conjunctiva regeneration. While the current CG scaffold has been thoroughly examined both mechanically and clinically, it has yet to prove appropriate for load- bearing applications. This study will investigate the mechanical properties of a mineralized CG scaffold and its application potential in a load-bearing environment. Through the introduction of calcium-phosphate mineral into the standard CG formulation the matrix analog will be available for bone regeneration. Utilizing a patented triple co-precipitation technique developed at Massachusetts Institute of Technology and Cambridge University, a homogeneous mineralized scaffold will be manufactured. Comparison to healthy trabecular bone as well as the selection of the most appropriate extracellular matrix analog will be presented. The key to commercial success is the introduction of a bi-layer bone and cartilage regeneration template to address concerns and difficulties in cartilage repair today. This dual combination is termed a layered osteochondral scaffold.",
    "advisors": ["Lorna J. Gibson"],
    "text": "Development of bi-layer mineralized bone and cartilage regeneration template Porous collagen-glycosaminoglycan (CG) scaffolds have been studied extensively and proven to be capable of tissue regeneration in vivo for applications including skin regeneration templates, hollow nerve guides and conjunctiva regeneration. While the current CG scaffold has been thoroughly examined both mechanically and clinically, it has yet to prove appropriate for load- bearing applications. This study will investigate the mechanical properties of a mineralized CG scaffold and its application potential in a load-bearing environment. Through the introduction of calcium-phosphate mineral into the standard CG formulation the matrix analog will be available for bone regeneration. Utilizing a patented triple co-precipitation technique developed at Massachusetts Institute of Technology and Cambridge University, a homogeneous mineralized scaffold will be manufactured. Comparison to healthy trabecular bone as well as the selection of the most appropriate extracellular matrix analog will be presented. The key to commercial success is the introduction of a bi-layer bone and cartilage regeneration template to address concerns and difficulties in cartilage repair today. This dual combination is termed a layered osteochondral scaffold."
}, {
    "id": "oai:dspace.mit.edu:1721.1/33624",
    "title": "Understanding the economics and material platform of bidirectional transceiver for plastic optical fiber",
    "abstract": "Limitations of electrical wires result in distortion and dispersion of the signal for long distances. That have emerged optical communication as the only way of communication for long distances. For medium distances optics can support the high data rates required by the latest applications. Optical networks are becoming the dominant transmission medium as the data rate required by different applications increases. The bottleneck for implementing optical instead of electric networks for medium distances, like local area network, is the cost of the optical components and the cost of replacing the existing copper network. This thesis will discuss the possible cost benefits that come from the use of different materials like plastic optical fiber instead of silica fiber or Si, Si/Ge instead of InP or GaAs for the transceiver as well as the trade offs between the performance and cost when discrete transceiver is replaced by the monolithically integrated transceiver, by using a process based cost model.",
    "advisors": ["Randolph E. Kirchain, Jr"],
    "text": "Understanding the economics and material platform of bidirectional transceiver for plastic optical fiber Limitations of electrical wires result in distortion and dispersion of the signal for long distances. That have emerged optical communication as the only way of communication for long distances. For medium distances optics can support the high data rates required by the latest applications. Optical networks are becoming the dominant transmission medium as the data rate required by different applications increases. The bottleneck for implementing optical instead of electric networks for medium distances, like local area network, is the cost of the optical components and the cost of replacing the existing copper network. This thesis will discuss the possible cost benefits that come from the use of different materials like plastic optical fiber instead of silica fiber or Si, Si/Ge instead of InP or GaAs for the transceiver as well as the trade offs between the performance and cost when discrete transceiver is replaced by the monolithically integrated transceiver, by using a process based cost model."
}, {
    "id": "oai:dspace.mit.edu:1721.1/62674",
    "title": "Technology assessment and feasibility study of high-throughput single cell force spectroscopy",
    "abstract": "In the last decade, the field of single cell mechanics has emerged with the development of high resolution experimental and computational methods, providing significant amount of information about individual cells instead of the averaged characteristics provided by classical assays from large populations of cells. These single cell mechanical properties correlate closely with the intracellular organelle arrangement and organization, which are determined by load bearing cytoskeleton network comprised of biommolecules. This thesis will assess the feasibility of a high throughput single cell force spectroscopy using an atomic force microscopy (AFM)-based platform. A conventional AFM set-up employs a single cantilever probe for force measurement by using laser to detect the deflection of the cantilever structure, and usually can only handle one cell at a time. To improve the throughput of the device, a modified scheme to make use of cantilever based array is proposed and studied in this project. In addition, to complement the use of AFM array, a novel cell chip design is also presented for the fine positioning of cells in coordination with AFM cantilevers. The advantages and challenges of the system are analyzed too. To assess the feasibility of developing this technology, the commercialization possibility is discussed with intellectual property research, market analysis, cost modeling and supply chain positioning. Conclusion about this technology and its market prospect is drawn at the end of the thesis.",
    "advisors": ["Christine Ortiz"],
    "text": "Technology assessment and feasibility study of high-throughput single cell force spectroscopy In the last decade, the field of single cell mechanics has emerged with the development of high resolution experimental and computational methods, providing significant amount of information about individual cells instead of the averaged characteristics provided by classical assays from large populations of cells. These single cell mechanical properties correlate closely with the intracellular organelle arrangement and organization, which are determined by load bearing cytoskeleton network comprised of biommolecules. This thesis will assess the feasibility of a high throughput single cell force spectroscopy using an atomic force microscopy (AFM)-based platform. A conventional AFM set-up employs a single cantilever probe for force measurement by using laser to detect the deflection of the cantilever structure, and usually can only handle one cell at a time. To improve the throughput of the device, a modified scheme to make use of cantilever based array is proposed and studied in this project. In addition, to complement the use of AFM array, a novel cell chip design is also presented for the fine positioning of cells in coordination with AFM cantilevers. The advantages and challenges of the system are analyzed too. To assess the feasibility of developing this technology, the commercialization possibility is discussed with intellectual property research, market analysis, cost modeling and supply chain positioning. Conclusion about this technology and its market prospect is drawn at the end of the thesis."
}, {
    "id": "oai:dspace.mit.edu:1721.1/45357",
    "title": "Evaluation of continuous glucose monitoring systems",
    "abstract": "There has been much hype in the research and development of continuous glucose monitoring technologies, driven by the enormous and rapidly expanding glucose monitoring market and the large and growing base of diabetes patients. Continuous glucose monitoring has shown significant benefits over traditional intermittent blood glucose testing in reducing the risks of developing long-term complications associated with diabetes, by maintaining blood glucose concentrations to near-normoglycemic levels and reducing glycemic variability. In this thesis, commercially available continuous glucose monitoring systems as well as those still in development are evaluated. SWOT analysis shows that continuous glucose monitoring has a promising future, but there remain a number of challenges to be overcome, such as accuracy, sensor span, data handling, cost and reimbursement issues. It is concluded that continuous glucose monitoring will be the roadmap for future diabetes management. Ongoing technological advances in continuous glucose monitoring systems will hopefully close the loop for a fully automated artificial pancreas and develop a cure for Type I diabetes.",
    "advisors": ["Michael J. Cima"],
    "text": "Evaluation of continuous glucose monitoring systems There has been much hype in the research and development of continuous glucose monitoring technologies, driven by the enormous and rapidly expanding glucose monitoring market and the large and growing base of diabetes patients. Continuous glucose monitoring has shown significant benefits over traditional intermittent blood glucose testing in reducing the risks of developing long-term complications associated with diabetes, by maintaining blood glucose concentrations to near-normoglycemic levels and reducing glycemic variability. In this thesis, commercially available continuous glucose monitoring systems as well as those still in development are evaluated. SWOT analysis shows that continuous glucose monitoring has a promising future, but there remain a number of challenges to be overcome, such as accuracy, sensor span, data handling, cost and reimbursement issues. It is concluded that continuous glucose monitoring will be the roadmap for future diabetes management. Ongoing technological advances in continuous glucose monitoring systems will hopefully close the loop for a fully automated artificial pancreas and develop a cure for Type I diabetes."
}, {
    "id": "oai:dspace.mit.edu:1721.1/45958",
    "title": "Commercial potential for thermal & magnetic sensitive polymer in drug delivery applications",
    "abstract": "Thermal and magnetically sensitive polymers are a new class of materials with unique properties suitable for applications in drug delivery. Specifically, these polymers can be combined with a drug reservoir to make a drug delivery device that can be triggered externally. Such a device could be implanted subcutaneously and allow for temporal control of drug release and localized delivery. Current experiments have shown that a prototype device is capable of delivering both small and large molecule drugs. Attractive medical applications for this technology were discovered and their respective markets examined. Additionally, the scientific literature and intellectual property in this field were analyzed for competing technologies that would hinder development of this invention. Novel attributes of this technology were also identified and specific competitive advantages made evident. To facilitate the commercialization of this novel technology, a business model has been proposed that identifies possible risks and provides strategies for overcoming them. Using this model, a timeline for future research and development has been constructed that traces the technology from its current state to a final product that can be launched commercially. The requirements for regulatory approval have also been investigated and a plausible manufacturing process has been established. Furthermore, a cost model and pricing analysis has been conducted to determine if a viable business proposition around this technology can be made.",
    "advisors": ["Daniel S. Kohane"],
    "text": "Commercial potential for thermal & magnetic sensitive polymer in drug delivery applications Thermal and magnetically sensitive polymers are a new class of materials with unique properties suitable for applications in drug delivery. Specifically, these polymers can be combined with a drug reservoir to make a drug delivery device that can be triggered externally. Such a device could be implanted subcutaneously and allow for temporal control of drug release and localized delivery. Current experiments have shown that a prototype device is capable of delivering both small and large molecule drugs. Attractive medical applications for this technology were discovered and their respective markets examined. Additionally, the scientific literature and intellectual property in this field were analyzed for competing technologies that would hinder development of this invention. Novel attributes of this technology were also identified and specific competitive advantages made evident. To facilitate the commercialization of this novel technology, a business model has been proposed that identifies possible risks and provides strategies for overcoming them. Using this model, a timeline for future research and development has been constructed that traces the technology from its current state to a final product that can be launched commercially. The requirements for regulatory approval have also been investigated and a plausible manufacturing process has been established. Furthermore, a cost model and pricing analysis has been conducted to determine if a viable business proposition around this technology can be made."
}, {
    "id": "oai:dspace.mit.edu:1721.1/81063",
    "title": "Development of a porous piezoresistive material and its applications to underwater pressure sensors and tactile sensors",
    "abstract": "MEMS (Microelectromechanical System) pressure sensor arrays are gaining attention in the field of underwater navigation because they are seen as alternatives to current sonar and vision-based systems that fail to navigate unmanned undersea vehicles (UUVs) in dark, unsteady and cluttered environments. Other advantages of MEMS pressure sensor arrays include lower power consumption and that their passive nature makes them covert. The goal of this work focuses on the development of a flexible pressure sensor array for UUVs, where the sensor array is inspired by the ability of fish to form three-dimensional maps of their surroundings. Fish are able to decipher various pressure waves from their surroundings using the array of pressure sensors in their lateral line sensory organs that can detect minute pressure differences. Similarly, by measuring pressure variations using an engineered pressure-sensor array on the surface of an UUV, this project hopes to aid UUVs in the identification and location of obstacles for navigation. The active material of the pressure sensor array is a porous polydimethylsiloxane (PDMS)-carbon black composite made out of a sugar sacrificial scaffold that shows great promise for satisfying the proposed applications. The proposed device structure is flexible, easily fabricated, cost efficient and can be implemented on a large-area and curved UUV surface. Although hysteresis occurs during the electromechanical test, the piezoresistivity of this porous PDMS-carbon black composite is reversible and reproducible. Compared to its non-porous counterpart, this porous composite shows a six-times increase in piezoresistivity and a greatly reduced Young's Modulus. When tested underwater, this porous composite was able to differentiate water waves that had a frequency of 1 Hz and 2 Hz, which is promising for its underwater application. This porous composite was also extended to the application of tactile sensors using a different device architecture, which showed excellent response under mechanical testing.",
    "advisors": ["Vladimir Bulovi", "Jeffrey H. Lang"],
    "text": "Development of a porous piezoresistive material and its applications to underwater pressure sensors and tactile sensors MEMS (Microelectromechanical System) pressure sensor arrays are gaining attention in the field of underwater navigation because they are seen as alternatives to current sonar and vision-based systems that fail to navigate unmanned undersea vehicles (UUVs) in dark, unsteady and cluttered environments. Other advantages of MEMS pressure sensor arrays include lower power consumption and that their passive nature makes them covert. The goal of this work focuses on the development of a flexible pressure sensor array for UUVs, where the sensor array is inspired by the ability of fish to form three-dimensional maps of their surroundings. Fish are able to decipher various pressure waves from their surroundings using the array of pressure sensors in their lateral line sensory organs that can detect minute pressure differences. Similarly, by measuring pressure variations using an engineered pressure-sensor array on the surface of an UUV, this project hopes to aid UUVs in the identification and location of obstacles for navigation. The active material of the pressure sensor array is a porous polydimethylsiloxane (PDMS)-carbon black composite made out of a sugar sacrificial scaffold that shows great promise for satisfying the proposed applications. The proposed device structure is flexible, easily fabricated, cost efficient and can be implemented on a large-area and curved UUV surface. Although hysteresis occurs during the electromechanical test, the piezoresistivity of this porous PDMS-carbon black composite is reversible and reproducible. Compared to its non-porous counterpart, this porous composite shows a six-times increase in piezoresistivity and a greatly reduced Young's Modulus. When tested underwater, this porous composite was able to differentiate water waves that had a frequency of 1 Hz and 2 Hz, which is promising for its underwater application. This porous composite was also extended to the application of tactile sensors using a different device architecture, which showed excellent response under mechanical testing."
}, {
    "id": "oai:dspace.mit.edu:1721.1/8426",
    "title": "Evaluation of layer-by-layer assembly of polyelectrolyte multilayers in cell patterning technology",
    "abstract": "The layer-by-layer assembly of polyelectrolytes into multilayered films is an attractive approach for fabricating novel biomaterials, as it offers tremendous control over the internal composition and surface properties of their layered architectures. In this work, polyelectrolyte multilayers (PEMs) were evaluated as a platform for applications in controlling the spatial adhesion of living cells. An overview is presented on current developments and competing technologies within research and industry with respect to cell patterning and cell-based devices. Interviewed individuals in research and industry suggested a variety of potential applications of PEMs in cell patterning technology. A patent search on the core technologies (i.e. PEMs and patterning methods) and on applications in cell patterning, cell-based screening, and cell-based biosensors revealed ample opportunity for starting a new venture with a platform based on the layer-by-layer assembly of PEMs. A brief business plan for starting a new venture with a platform based on the layer-by-layer assembly of PEMs is proposed to initially target the high throughput screening and cell-based biosensor markets.",
    "advisors": ["Michael F. Rubner"],
    "text": "Evaluation of layer-by-layer assembly of polyelectrolyte multilayers in cell patterning technology The layer-by-layer assembly of polyelectrolytes into multilayered films is an attractive approach for fabricating novel biomaterials, as it offers tremendous control over the internal composition and surface properties of their layered architectures. In this work, polyelectrolyte multilayers (PEMs) were evaluated as a platform for applications in controlling the spatial adhesion of living cells. An overview is presented on current developments and competing technologies within research and industry with respect to cell patterning and cell-based devices. Interviewed individuals in research and industry suggested a variety of potential applications of PEMs in cell patterning technology. A patent search on the core technologies (i.e. PEMs and patterning methods) and on applications in cell patterning, cell-based screening, and cell-based biosensors revealed ample opportunity for starting a new venture with a platform based on the layer-by-layer assembly of PEMs. A brief business plan for starting a new venture with a platform based on the layer-by-layer assembly of PEMs is proposed to initially target the high throughput screening and cell-based biosensor markets."
}, {
    "id": "oai:dspace.mit.edu:1721.1/62686",
    "title": "Behavior of alloy 617 at 650C in low oxygen pressure environments",
    "abstract": "The behavior of alloy 617 at 650C in low oxygen partial pressure environments has been studied under static loading. Of particular interest was the crack growth rate in these conditions. For that, tests were conducted at a constant stress intensity factor of 49.45 MPa'm (45 ksidinch) using a direct current potential drop measurement system to determine crack length. High purity argon gas allowed establishing an oxygen partial pressure as low as 10-22 atm and premixed oxygen/argon gases were used to vary the oxygen potential. To go dee er into the understanding of the phenomena involved, a creep test (constant load of 1.21x10 N in argon environment) and a corrosion experiment (50 ppm oxygen in argon for 500 hours without any load) were also added. The crack growth tests led to a particularly unexpected result: whatever the oxygen potential, the crack growth rate increased with time and the plot for the crack length versus time displayed a recurrent parabolic shape without any change in the environment. No unique crack growth rate could be defined in a given environment and the influence of the environment on the crack growth rate was not clearly visible. Multiple features were found to surround the main crack: secondary cracks parallel to the principal one, intergranular cracking ahead of the crack tip, wedge cracks at grain boundaries and aggregates of Cr-rich carbides near the lips of the crack. Moreover no extensive oxide scale was formed on the surface of the sample exposed to the corrosive environment (50 ppm oxygen in argon) for 500 hours. The non-constant crack growth rates, together with the observed cracking features, were attributed to the competition between creep deformation and mechanical fracture, likely environmentally enhanced. An exponential law was found to fit the data for the crack growth rate as a function of time for a K of 49.45 MPalm (45 ksi'inch). The effects of the environment were overcome by mechanical and thermal processes leading to damage accumulation and so, a reaction of alloy 617 to the external stress and temperature highly dependent on time. This behavior was compared with the one of alloys Haynes 230 and Incoloy 908 in the same conditions.",
    "advisors": ["Ronald G. Ballinger"],
    "text": "Behavior of alloy 617 at 650C in low oxygen pressure environments The behavior of alloy 617 at 650C in low oxygen partial pressure environments has been studied under static loading. Of particular interest was the crack growth rate in these conditions. For that, tests were conducted at a constant stress intensity factor of 49.45 MPa'm (45 ksidinch) using a direct current potential drop measurement system to determine crack length. High purity argon gas allowed establishing an oxygen partial pressure as low as 10-22 atm and premixed oxygen/argon gases were used to vary the oxygen potential. To go dee er into the understanding of the phenomena involved, a creep test (constant load of 1.21x10 N in argon environment) and a corrosion experiment (50 ppm oxygen in argon for 500 hours without any load) were also added. The crack growth tests led to a particularly unexpected result: whatever the oxygen potential, the crack growth rate increased with time and the plot for the crack length versus time displayed a recurrent parabolic shape without any change in the environment. No unique crack growth rate could be defined in a given environment and the influence of the environment on the crack growth rate was not clearly visible. Multiple features were found to surround the main crack: secondary cracks parallel to the principal one, intergranular cracking ahead of the crack tip, wedge cracks at grain boundaries and aggregates of Cr-rich carbides near the lips of the crack. Moreover no extensive oxide scale was formed on the surface of the sample exposed to the corrosive environment (50 ppm oxygen in argon) for 500 hours. The non-constant crack growth rates, together with the observed cracking features, were attributed to the competition between creep deformation and mechanical fracture, likely environmentally enhanced. An exponential law was found to fit the data for the crack growth rate as a function of time for a K of 49.45 MPalm (45 ksi'inch). The effects of the environment were overcome by mechanical and thermal processes leading to damage accumulation and so, a reaction of alloy 617 to the external stress and temperature highly dependent on time. This behavior was compared with the one of alloys Haynes 230 and Incoloy 908 in the same conditions."
}, {
    "id": "oai:dspace.mit.edu:1721.1/39596",
    "title": "Optimization of a fast-response distribution network",
    "abstract": "Inditex is one of the world's largest fashion distributors, operating 3,100 stores in 64 countries; its brands currently include Zara, Pull and Bear, Massimo Dutti, Bershka, Stradivarius, Oysho, Zara Home and Kiddy's Class. The group's flagship company is Zara, which is the world's largest \"fast fashion\" company: through unique and carefully integrated design, manufacturing and distribution processes, Zara routinely achieves design-to-shelf leadtimes of 6 weeks against an industry average of 6 months, and introduces 11,000 references per season against an industry average of 3,000. Throughout the season, Zara currently ships every new incoming product to all 950 stores comprising its distribution network at the same time. Its operations group has recognized a large opportunity in customizing the assortment of products offered in each store based on local sales, and staggering shipments to stores of each new reference in order to acquire more accurate sales forecast and enable better subsequent inventory allocation decisions. My thesis will detail the development and implementation of new optimization models for dynamically allocating inventory across Zara's distribution network.",
    "advisors": ["Jrmie Gallien, David Roylance", "Felipe Caro"],
    "text": "Optimization of a fast-response distribution network Inditex is one of the world's largest fashion distributors, operating 3,100 stores in 64 countries; its brands currently include Zara, Pull and Bear, Massimo Dutti, Bershka, Stradivarius, Oysho, Zara Home and Kiddy's Class. The group's flagship company is Zara, which is the world's largest \"fast fashion\" company: through unique and carefully integrated design, manufacturing and distribution processes, Zara routinely achieves design-to-shelf leadtimes of 6 weeks against an industry average of 6 months, and introduces 11,000 references per season against an industry average of 3,000. Throughout the season, Zara currently ships every new incoming product to all 950 stores comprising its distribution network at the same time. Its operations group has recognized a large opportunity in customizing the assortment of products offered in each store based on local sales, and staggering shipments to stores of each new reference in order to acquire more accurate sales forecast and enable better subsequent inventory allocation decisions. My thesis will detail the development and implementation of new optimization models for dynamically allocating inventory across Zara's distribution network."
}, {
    "id": "oai:dspace.mit.edu:1721.1/37678",
    "title": "Commercialization of Quantum Dot White Light Emitting Diode technology",
    "abstract": "It is well known that the use of high-brightness LEDs for illumination has the potential to substitute conventional lighting and revolutionize the lighting industry over the next 10 to 20 years. However, successful penetration of this extremely large lighting market would require vast improvements in power conversion efficiencies, color index, light output per device and drastic reduction in cost. Quantum Dot white LED (QD WLED) technology may be one of the best choices, due to its higher energy efficiency, larger color render in index, better versatility and more importantly lower cost, compared to conventional blue LED plus YAG: Ce yellow phosphor technology. Due to the fundamental difference of the material structure, QD LEDs will win a steady position among existing white LED patents and a hybrid fabless plus IP business model has the best position to promote this technology to maximize its benefits and potential for the entire LED industry.",
    "advisors": ["Eugene A. Fitzgerald"],
    "text": "Commercialization of Quantum Dot White Light Emitting Diode technology It is well known that the use of high-brightness LEDs for illumination has the potential to substitute conventional lighting and revolutionize the lighting industry over the next 10 to 20 years. However, successful penetration of this extremely large lighting market would require vast improvements in power conversion efficiencies, color index, light output per device and drastic reduction in cost. Quantum Dot white LED (QD WLED) technology may be one of the best choices, due to its higher energy efficiency, larger color render in index, better versatility and more importantly lower cost, compared to conventional blue LED plus YAG: Ce yellow phosphor technology. Due to the fundamental difference of the material structure, QD LEDs will win a steady position among existing white LED patents and a hybrid fabless plus IP business model has the best position to promote this technology to maximize its benefits and potential for the entire LED industry."
}, {
    "id": "oai:dspace.mit.edu:1721.1/42151",
    "title": "White LED for general illumination applications",
    "abstract": "In the 21st century, mankind faces problem of energy crisis through depletion of fossil fuels as well as global warning through the production of excessive greenhouse gases. Hence, there is an urgent need to look for new sources of renewable energy or ways to utilize energy more effectively. Solid state lighting (SSL) is a major area of research interest to use energy in a more efficient manner. Early light emitting devices (LEDs) were originally limited their use for low power indication lights. Later research produces high brightness LEDs (HB-LEDs) as well as blue color LEDs. This brings to reality of the entire visible light spectrum. White light is also made possible. As with other technologies, numerous obstacles will have to be surmounted in bringing LEDs from the laboratory to the marketplace. LEDs will also have to compete with established technologies such as incandescent and fluorescent lighting. This thesis will describe the current state of high powered LEDs, examine challenges faced by LEDs and look at future markets. Evaluation in the potential of LEDs for general illumination will be carried out through cost modeling and performance analysis.",
    "advisors": ["Thomas W. Eagar"],
    "text": "White LED for general illumination applications In the 21st century, mankind faces problem of energy crisis through depletion of fossil fuels as well as global warning through the production of excessive greenhouse gases. Hence, there is an urgent need to look for new sources of renewable energy or ways to utilize energy more effectively. Solid state lighting (SSL) is a major area of research interest to use energy in a more efficient manner. Early light emitting devices (LEDs) were originally limited their use for low power indication lights. Later research produces high brightness LEDs (HB-LEDs) as well as blue color LEDs. This brings to reality of the entire visible light spectrum. White light is also made possible. As with other technologies, numerous obstacles will have to be surmounted in bringing LEDs from the laboratory to the marketplace. LEDs will also have to compete with established technologies such as incandescent and fluorescent lighting. This thesis will describe the current state of high powered LEDs, examine challenges faced by LEDs and look at future markets. Evaluation in the potential of LEDs for general illumination will be carried out through cost modeling and performance analysis."
}, {
    "id": "oai:dspace.mit.edu:1721.1/8122",
    "title": "Development of an advanced materials system for tooling produced by three-dimensional printing",
    "abstract": "Three Dimensional Printing (3DP) is a manufacturing technique in which a powdered material is used to build parts with complicated geometries directly from a three dimensional computer model. This technique has been used in the past to manufacture metal parts with complicated geometries. The three different materials used in the making of tools by 3DP are binder, metal powder and the low melting infiitrant. The original material system used for hard tools is 420 martensitic stainless steel powder, a Cu-lOSn alloy as the infiltrant and acrysol as the binder. Although almost 50 tools have been manufactured using this materials system, there were several concerns which had to be dealt with, including dimensional control, porosity control, erosion and interaction between the skeleton and the infiltrant. Another requirement is a \"hardenable\" material system that is soft initially and that can be hardened after machining. This thesis describes the development of a new hardenable material system in which most of the concerns with the 420/bronze system are eliminated. The new material system that was selected uses a Mo powder skeleton and a 56Cu-22Ni- 22Mn infiltrant. The Cu-Ni-Mn alloy is age hardenable. Acrysol was retained as the binder material again. Experiments were carried out to study the printing, debinding/sintering and infiltration steps. An extra age hardening step was also introduced after infiltration. The process parameters for each of the steps were developed and several tools have been manufactured successfully. The thesis describes each of the post processing steps in detail with regards to experiments that were performed or literature that was obtained from past work. The tools that were manufactured with the new material system showed better properties on the whole than those manufactured with the previous material system.",
    "advisors": ["Samuel M. Allen"],
    "text": "Development of an advanced materials system for tooling produced by three-dimensional printing Three Dimensional Printing (3DP) is a manufacturing technique in which a powdered material is used to build parts with complicated geometries directly from a three dimensional computer model. This technique has been used in the past to manufacture metal parts with complicated geometries. The three different materials used in the making of tools by 3DP are binder, metal powder and the low melting infiitrant. The original material system used for hard tools is 420 martensitic stainless steel powder, a Cu-lOSn alloy as the infiltrant and acrysol as the binder. Although almost 50 tools have been manufactured using this materials system, there were several concerns which had to be dealt with, including dimensional control, porosity control, erosion and interaction between the skeleton and the infiltrant. Another requirement is a \"hardenable\" material system that is soft initially and that can be hardened after machining. This thesis describes the development of a new hardenable material system in which most of the concerns with the 420/bronze system are eliminated. The new material system that was selected uses a Mo powder skeleton and a 56Cu-22Ni- 22Mn infiltrant. The Cu-Ni-Mn alloy is age hardenable. Acrysol was retained as the binder material again. Experiments were carried out to study the printing, debinding/sintering and infiltration steps. An extra age hardening step was also introduced after infiltration. The process parameters for each of the steps were developed and several tools have been manufactured successfully. The thesis describes each of the post processing steps in detail with regards to experiments that were performed or literature that was obtained from past work. The tools that were manufactured with the new material system showed better properties on the whole than those manufactured with the previous material system."
}, {
    "id": "oai:dspace.mit.edu:1721.1/7973",
    "title": "Commercialization potential of quantum dot light emitting devices",
    "abstract": "The use of quantum dots as discrete emitters in hybrid organic/inorganic light emitting devices is an attractive approach for producing novel display products. These structures exhibit narrow-band emission tunable across the visible spectrum - characteristics allowing for display devices not possible with current OLEO materials. In this work, quantum dot light emitting devices (QD-LEDs) using small molecule host materials are evaluated as a potential platform for the growing OLEO industry. Specific applications are suggested and the primary technology hurdles identified. A search of relevant patents pertaining to quantum dot synthesis and device structure was conducted to reveal a significant opportunity for the commercialization of QD-LED devices. A business model has been devised based upon several developing companies in the OLEO industry with a focus on licensing of technology as the primary source of revenue.",
    "advisors": ["Vladimir Bulovic"],
    "text": "Commercialization potential of quantum dot light emitting devices The use of quantum dots as discrete emitters in hybrid organic/inorganic light emitting devices is an attractive approach for producing novel display products. These structures exhibit narrow-band emission tunable across the visible spectrum - characteristics allowing for display devices not possible with current OLEO materials. In this work, quantum dot light emitting devices (QD-LEDs) using small molecule host materials are evaluated as a potential platform for the growing OLEO industry. Specific applications are suggested and the primary technology hurdles identified. A search of relevant patents pertaining to quantum dot synthesis and device structure was conducted to reveal a significant opportunity for the commercialization of QD-LED devices. A business model has been devised based upon several developing companies in the OLEO industry with a focus on licensing of technology as the primary source of revenue."
}, {
    "id": "oai:dspace.mit.edu:1721.1/33617",
    "title": "Evaluation of monolayer protected metal nanoparticle technology",
    "abstract": "Self assembling nanostructured nanoparticles represent a new class of synthesized materials with unique functionality. Such monolayer protected metal nanoparticles are capable of resisting protein adsorption, and if utilized as a coating could have broad application in a wide range of industries from consumer products to maritime shipping to medical instruments. The formation of proteic films can adversely affect the performance of materials and is often a limiting factor in device effectiveness. In many instances such as sensors or medical implants, regular cleaning or disposal of the instrument is not a viable option, thus there exists a demand for additional means to prevent nonspecific protein adsorption. Existing protein resistant coating options are still not completely effective, and monolayer protected metal nanoparticle coatings could be a superior means by which to prevent protein adsorption onto material surfaces.",
    "advisors": ["Francesco Stellacci"],
    "text": "Evaluation of monolayer protected metal nanoparticle technology Self assembling nanostructured nanoparticles represent a new class of synthesized materials with unique functionality. Such monolayer protected metal nanoparticles are capable of resisting protein adsorption, and if utilized as a coating could have broad application in a wide range of industries from consumer products to maritime shipping to medical instruments. The formation of proteic films can adversely affect the performance of materials and is often a limiting factor in device effectiveness. In many instances such as sensors or medical implants, regular cleaning or disposal of the instrument is not a viable option, thus there exists a demand for additional means to prevent nonspecific protein adsorption. Existing protein resistant coating options are still not completely effective, and monolayer protected metal nanoparticle coatings could be a superior means by which to prevent protein adsorption onto material surfaces."
}, {
    "id": "oai:dspace.mit.edu:1721.1/89985",
    "title": "Determination of the synthesis diagram of sodium cobalt oxide and electrochemical study",
    "abstract": "A complete and uniform synthesis diagram of NaxCoO has been proposed based on forty-one samples synthesized at various temperatures from 450C to 750C by solid-state reactions with initial Na:Co ratio ranging from 0.60 to 1.05. Four monophasic domains of' O3, O3', P3' and P2 and four biphasic regions were revealed based on an XRD analysis. The sodium contents in these phases were determined according to the d00j-x relations obtained by an in situ XRD experiment and it is found O3, O3' and P3' phase almost form with only one stoichiometry, that is x=1.00, 0.83 and 0.67 respectively, by solid-state reaction while P2 phase forms in a slightly larger composition range from 0.68 to 0.76. Galvanostatic charging on O3-Na.CoO battery reveals several plateaus and steep steps on the voltage curve, the corresponding phase transitions and solid solution behaviors were studied by a simultaneous in situ XRD experiment. The composition driven structural evolution in three layer NaXCoO follows the sequence: O3-O3'-P3'-P3-P3', with a generally increased interslab distance dl.",
    "advisors": ["Gerbrand Ceder"],
    "text": "Determination of the synthesis diagram of sodium cobalt oxide and electrochemical study A complete and uniform synthesis diagram of NaxCoO has been proposed based on forty-one samples synthesized at various temperatures from 450C to 750C by solid-state reactions with initial Na:Co ratio ranging from 0.60 to 1.05. Four monophasic domains of' O3, O3', P3' and P2 and four biphasic regions were revealed based on an XRD analysis. The sodium contents in these phases were determined according to the d00j-x relations obtained by an in situ XRD experiment and it is found O3, O3' and P3' phase almost form with only one stoichiometry, that is x=1.00, 0.83 and 0.67 respectively, by solid-state reaction while P2 phase forms in a slightly larger composition range from 0.68 to 0.76. Galvanostatic charging on O3-Na.CoO battery reveals several plateaus and steep steps on the voltage curve, the corresponding phase transitions and solid solution behaviors were studied by a simultaneous in situ XRD experiment. The composition driven structural evolution in three layer NaXCoO follows the sequence: O3-O3'-P3'-P3-P3', with a generally increased interslab distance dl."
}, {
    "id": "oai:dspace.mit.edu:1721.1/34758",
    "title": "Ergonomic product and process design",
    "abstract": "Ergonomic injuries are not the result of acute events. An ergonomic injury develops gradually from continued actions combining force, motion repetition, posture, and duration. Because these injuries accrue over time, it is often difficult to determine their causes. Lacking a clear causal link, it is difficult to justify investments that are intended to prevent ergonomic injuries. A large computer manufacturer, Dell Inc, is targeting significant reductions in their factory injury rates. This thesis describes the evaluation of two desktop computer manufacturing facilities. As part of this work, OSHA logs from 2002 were analyzed, injury costs were collected, factory workers were surveyed, and biomaterials associated with ergonomic injuries were studied. The analysis of the OSHA logs determined that 70% of factory injuries were ergonomic in nature and that a majority of the ergonomic injuries occurred as a result of work in the computer assembly (build) area. The costs associated with ergonomic injuries were computed on a cost per box (CPB) basis, a common metric used throughout Dell factories to determine financial impact. In order to evaluate, improve, and monitor the ergonomic factors on the factory floor, an evaluation tool for product and process design was developed. This tool incorporates risk factors of force, motion repetition, and posture while determining ergonomic scores for products and process steps. Tool validation was achieved by comparing ergonomic scores with worker product preferences, as revealed by an employee survey. Currently, the ergonomic evaluation tool is being used by the Environmental, Health, and Safety (EHS) Department at Dell. A greater understanding of the causes behind ergonomic injuries, combined",
    "advisors": ["Roy E. Welsch", "Jack T. Dennerlein"],
    "text": "Ergonomic product and process design Ergonomic injuries are not the result of acute events. An ergonomic injury develops gradually from continued actions combining force, motion repetition, posture, and duration. Because these injuries accrue over time, it is often difficult to determine their causes. Lacking a clear causal link, it is difficult to justify investments that are intended to prevent ergonomic injuries. A large computer manufacturer, Dell Inc, is targeting significant reductions in their factory injury rates. This thesis describes the evaluation of two desktop computer manufacturing facilities. As part of this work, OSHA logs from 2002 were analyzed, injury costs were collected, factory workers were surveyed, and biomaterials associated with ergonomic injuries were studied. The analysis of the OSHA logs determined that 70% of factory injuries were ergonomic in nature and that a majority of the ergonomic injuries occurred as a result of work in the computer assembly (build) area. The costs associated with ergonomic injuries were computed on a cost per box (CPB) basis, a common metric used throughout Dell factories to determine financial impact. In order to evaluate, improve, and monitor the ergonomic factors on the factory floor, an evaluation tool for product and process design was developed. This tool incorporates risk factors of force, motion repetition, and posture while determining ergonomic scores for products and process steps. Tool validation was achieved by comparing ergonomic scores with worker product preferences, as revealed by an employee survey. Currently, the ergonomic evaluation tool is being used by the Environmental, Health, and Safety (EHS) Department at Dell. A greater understanding of the causes behind ergonomic injuries, combined"
}, {
    "id": "oai:dspace.mit.edu:1721.1/33394",
    "title": "Low temperature transient liquid phase bonding of copper",
    "abstract": "This thesis describes a Pb-free solder alternative that is capable of fluxless bonding. The main advantage of this process is that it offers the benefits of low fabrication temperature (125C) while producing a joint capable of withstanding low stresses at very high service temperatures (300C+). The ternary alloy system of Bi-In-Sn was investigated in the bonding of copper substrates. All bonds were made at 125C with 25psi of fixturing pressure. Primary solidification was observed in as little as 15 minutes. The mechanical properties of the joints were shear tested both at room temperature and at 1000C to simulate a conventional service environment. With sufficient dwell time ([approx.] 250h), joints would not fail in shear even at stresses that caused significant substrate deformation. Scanning electron microscopy was used to examine the joints and the evolution of the diffusion process.",
    "advisors": ["Thomas W. Eagar"],
    "text": "Low temperature transient liquid phase bonding of copper This thesis describes a Pb-free solder alternative that is capable of fluxless bonding. The main advantage of this process is that it offers the benefits of low fabrication temperature (125C) while producing a joint capable of withstanding low stresses at very high service temperatures (300C+). The ternary alloy system of Bi-In-Sn was investigated in the bonding of copper substrates. All bonds were made at 125C with 25psi of fixturing pressure. Primary solidification was observed in as little as 15 minutes. The mechanical properties of the joints were shear tested both at room temperature and at 1000C to simulate a conventional service environment. With sufficient dwell time ([approx.] 250h), joints would not fail in shear even at stresses that caused significant substrate deformation. Scanning electron microscopy was used to examine the joints and the evolution of the diffusion process."
}, {
    "id": "oai:dspace.mit.edu:1721.1/69796",
    "title": "Effects of formulation conditions on micellar interactions and solution rheology in multi-component micellar systems",
    "abstract": "Surfactants are crucial to the personal care industry due to their unique surface activity, cleansing, and self assembly properties. Typically, multi-component systems are used in order to maximize mildness, hard water tolerance, and foaming. System morphology and viscosity are controlled through chemistry and solution conditions. An experimental study was conducted to determine how variations in solution chemistry (surfactant headgroup and blend stoichiometry) and solution conditions (pH and [NaCl]: [anionic + zwitterionic surfactant] ratio) affect the structure and rheology of surfactant solutions. This study examined binary systems of Sodium Laureth Sulfate (SLES) and Lauramidopropyl Betaine (LAPB) or SLES and Lauramidopropyl Hydroxysultaine (LAPHS) as well as ternary systems of SLES/LAPB/PEG-80 Sorbitan Laurate (PEG-80 SL) and SLES/LAPB/Polysorbite-20 (Tween-20). Using dynamic light scattering and rheometic measurements, system morphology was determined. In the SLES/LAPB system, it was found that there was a break in system viscosity at a critical [NaCl]: [anionic + zwitterionic surfactant] ratio, 0.16:1 (R*). Micelles only had the ability to entangle, thus increasing viscosity, above this ratio. When the system pH decreased such that pH ~ pKa of LAPB, all [NaCl]:[anionic + zwitterionic surfactant] ratios had the ability to entangle, and entanglement began at lower surfactant concentrations. At these pH values, LAPB protonated and created a pseudo-ternary system with SLES, LAPB0 , and LAPB*. There was no measured variation in system morphology in the SLES/LAPHS system with [NaCl]: [anionic + zwitterionic surfactant] ratio, most likely because the minimum ratio achievable was above R* due to a high salt content in the raw materials. In addition, there was no measured variation in system morphology in the SLES/LAPHS system with variation in pH, most likely because the system was not tested at pH ~ pKa of LAPHS. The addition of a third surfactant drastically decreased the system viscosity and drove the system towards the formation of spherical micelles because the nonionic surfactant of choice decreased the packing parameter due to its relatively large size as compared to that of SLES and LAPB.",
    "advisors": ["T. Alan Hatton"],
    "text": "Effects of formulation conditions on micellar interactions and solution rheology in multi-component micellar systems Surfactants are crucial to the personal care industry due to their unique surface activity, cleansing, and self assembly properties. Typically, multi-component systems are used in order to maximize mildness, hard water tolerance, and foaming. System morphology and viscosity are controlled through chemistry and solution conditions. An experimental study was conducted to determine how variations in solution chemistry (surfactant headgroup and blend stoichiometry) and solution conditions (pH and [NaCl]: [anionic + zwitterionic surfactant] ratio) affect the structure and rheology of surfactant solutions. This study examined binary systems of Sodium Laureth Sulfate (SLES) and Lauramidopropyl Betaine (LAPB) or SLES and Lauramidopropyl Hydroxysultaine (LAPHS) as well as ternary systems of SLES/LAPB/PEG-80 Sorbitan Laurate (PEG-80 SL) and SLES/LAPB/Polysorbite-20 (Tween-20). Using dynamic light scattering and rheometic measurements, system morphology was determined. In the SLES/LAPB system, it was found that there was a break in system viscosity at a critical [NaCl]: [anionic + zwitterionic surfactant] ratio, 0.16:1 (R*). Micelles only had the ability to entangle, thus increasing viscosity, above this ratio. When the system pH decreased such that pH ~ pKa of LAPB, all [NaCl]:[anionic + zwitterionic surfactant] ratios had the ability to entangle, and entanglement began at lower surfactant concentrations. At these pH values, LAPB protonated and created a pseudo-ternary system with SLES, LAPB0 , and LAPB*. There was no measured variation in system morphology in the SLES/LAPHS system with [NaCl]: [anionic + zwitterionic surfactant] ratio, most likely because the minimum ratio achievable was above R* due to a high salt content in the raw materials. In addition, there was no measured variation in system morphology in the SLES/LAPHS system with variation in pH, most likely because the system was not tested at pH ~ pKa of LAPHS. The addition of a third surfactant drastically decreased the system viscosity and drove the system towards the formation of spherical micelles because the nonionic surfactant of choice decreased the packing parameter due to its relatively large size as compared to that of SLES and LAPB."
}, {
    "id": "oai:dspace.mit.edu:1721.1/88380",
    "title": "Fiber inspired neural probes",
    "abstract": "Limitations in the currently available technology for neural probes impede our progress towards a comprehensive brain activity map. The lack of understanding the brain function leads to limited options for the treatment of neurological disorders. In this thesis, I employed a two-step thermal drawing process (TDP), widely used in fabrication of optical fibers, to create arrays of metal microelectrodes embedded in a polymer cladding. The pitch and size of the electrodes were determined on the macroscale and preserved during the TDP. I have applied these fiber-inspired probes to record spontaneous and stimulated neural activity in vivo.",
    "advisors": ["Polina Anikeeva"],
    "text": "Fiber inspired neural probes Limitations in the currently available technology for neural probes impede our progress towards a comprehensive brain activity map. The lack of understanding the brain function leads to limited options for the treatment of neurological disorders. In this thesis, I employed a two-step thermal drawing process (TDP), widely used in fabrication of optical fibers, to create arrays of metal microelectrodes embedded in a polymer cladding. The pitch and size of the electrodes were determined on the macroscale and preserved during the TDP. I have applied these fiber-inspired probes to record spontaneous and stimulated neural activity in vivo."
}, {
    "id": "oai:dspace.mit.edu:1721.1/79564",
    "title": "Systematic study of the Taylor method for production of cu-based shape memory alloy microwires : a master's thesis",
    "abstract": "The Taylor method is a proven way to produce Cu-based shape memory microwires that aren't plagued by problems typical in polycrystalline copper SMAs produced by other methods. Here we set out to expand and refine this processing method to take the first critical steps toward large-scale continuous production. Using a semi-automated processing route, we draw continuous, uniform fibers up to 5 meters in length with diameters in the range 10 - 35 microns. Particular attention is paid to microwires made from a Cu-Sn shape memory alloy. In addition, because the properties of shape memory microwires depend on their diameter, processing parameters were varied to understand their impact on the diameters of the resulting wires.",
    "advisors": ["Christopher A. Schuh"],
    "text": "Systematic study of the Taylor method for production of cu-based shape memory alloy microwires : a master's thesis The Taylor method is a proven way to produce Cu-based shape memory microwires that aren't plagued by problems typical in polycrystalline copper SMAs produced by other methods. Here we set out to expand and refine this processing method to take the first critical steps toward large-scale continuous production. Using a semi-automated processing route, we draw continuous, uniform fibers up to 5 meters in length with diameters in the range 10 - 35 microns. Particular attention is paid to microwires made from a Cu-Sn shape memory alloy. In addition, because the properties of shape memory microwires depend on their diameter, processing parameters were varied to understand their impact on the diameters of the resulting wires."
}, {
    "id": "oai:dspace.mit.edu:1721.1/28869",
    "title": "Process-based cost modeling of tool-steels parts by transient liquid-phase infiltration of powder-metal preforms",
    "abstract": "(cont.) cost between these two processes was related mainly to their powder scrap rates, 15 % for the Pressing-TLI and 80% for the 3DP-TLI. The high scrap rate value of the 3DP process originates from the fact that powder is sieved before printing, eliminating the coarse and very fine particles. A possible option to decrease this value is to recycle or sell the extra powder, which will reduce the fabrication cost significantly. The model also shows that the main cost for both processes is the powder cost. TLI technical parameters such as heating and cooling rates were included in the model in order to predict the cost behavior when those are manipulated. Because the powder cost dominates the total fabrication cost, variations in the heating and cooling rates do not significantly affect the cost.",
    "advisors": ["Samuel Allen"],
    "text": "Process-based cost modeling of tool-steels parts by transient liquid-phase infiltration of powder-metal preforms (cont.) cost between these two processes was related mainly to their powder scrap rates, 15 % for the Pressing-TLI and 80% for the 3DP-TLI. The high scrap rate value of the 3DP process originates from the fact that powder is sieved before printing, eliminating the coarse and very fine particles. A possible option to decrease this value is to recycle or sell the extra powder, which will reduce the fabrication cost significantly. The model also shows that the main cost for both processes is the powder cost. TLI technical parameters such as heating and cooling rates were included in the model in order to predict the cost behavior when those are manipulated. Because the powder cost dominates the total fabrication cost, variations in the heating and cooling rates do not significantly affect the cost."
}, {
    "id": "oai:dspace.mit.edu:1721.1/16626",
    "title": "Bimetallic bars with local control of composition by three-dimensional printing",
    "abstract": "Three Dimensional Printing (3DP) is a process that enables the fabrication of geometrically complex parts directly from computer-aided design (CAD) models. The success of 3DP as an alternative manufacturing technology to bulk machining of materials for complex parts has been demonstrated. By proof of concept, 3DP has demonstrated the ability to create parts with Local Control of the Composition (LCC). LCC allows tailoring the material properties in regions of a part for functional purposes. In this work, LCC was studied and demonstrated by fabricating bimetallic bars consisting of two layers of Fe-Ni alloys with different composition and, hence, different thermal expansion properties; the coefficient of thermal expansion (CTE) of Fe-Ni system is sensitive to its composition. Two types of the binder/dopant slurries were made for making the LCC bars. One type consisted of dispersions of FeO particles in water, and the other consisted of dispersion of NiO in water. The LCC bars were successfully made by printing the FeO/NiO slurries into Fe-30Ni base powders. After heat treatment to impart strength to the printed bars, the bars were successfully retrieved from unbound powders. The bars, then, were annealed at 1400 C for 2 hours for sintering and homogenization. The final composition of the base powders were changed accordingly. In the layers on which an FeO slurry was printed, the Fe composition of the layers increased on average to 72wt%. Similarly, the Ni composition of the Ni-enriched layers of the bars increased on average to 33wt%. The densification and local homogenization resulting from reduction and sintering treatments were not satisfactory.",
    "advisors": ["Samuel M. Allen", "Emanuel M. Sachs"],
    "text": "Bimetallic bars with local control of composition by three-dimensional printing Three Dimensional Printing (3DP) is a process that enables the fabrication of geometrically complex parts directly from computer-aided design (CAD) models. The success of 3DP as an alternative manufacturing technology to bulk machining of materials for complex parts has been demonstrated. By proof of concept, 3DP has demonstrated the ability to create parts with Local Control of the Composition (LCC). LCC allows tailoring the material properties in regions of a part for functional purposes. In this work, LCC was studied and demonstrated by fabricating bimetallic bars consisting of two layers of Fe-Ni alloys with different composition and, hence, different thermal expansion properties; the coefficient of thermal expansion (CTE) of Fe-Ni system is sensitive to its composition. Two types of the binder/dopant slurries were made for making the LCC bars. One type consisted of dispersions of FeO particles in water, and the other consisted of dispersion of NiO in water. The LCC bars were successfully made by printing the FeO/NiO slurries into Fe-30Ni base powders. After heat treatment to impart strength to the printed bars, the bars were successfully retrieved from unbound powders. The bars, then, were annealed at 1400 C for 2 hours for sintering and homogenization. The final composition of the base powders were changed accordingly. In the layers on which an FeO slurry was printed, the Fe composition of the layers increased on average to 72wt%. Similarly, the Ni composition of the Ni-enriched layers of the bars increased on average to 33wt%. The densification and local homogenization resulting from reduction and sintering treatments were not satisfactory."
}, {
    "id": "oai:dspace.mit.edu:1721.1/44422",
    "title": "Analytical study and cost modeling of secondary aluminum consumption for alloy producers under uncertain demands",
    "abstract": "A series of case studies on raw materials inventory strategy for both wrought and cast aluminum alloy productions were conducted under recourse-based modeling framework with the explicit considerations of the demand uncertainty compared to the traditional strategy based on point forecast of future demand. The result shows significant economic and environmental benefits by pre-purchasing excess amount of cheaper but dirtier secondary raw materials to hedge the riskier higher-than-expected demand scenario. Further observations demonstrate that factors such as salvage value of residual scraps, cost advantage of secondary materials over primary materials, the degree of the demand uncertainty, etc. all have direct impacts on the hedging behavior. An analytical study on a simplified case scenario suggested a close form expression to well explain the hedging behavior and the impacts of various factors observed in case studies. The thesis then explored the effects of commonality shared by secondary materials in their application in multiple final products. Four propositions were reached.",
    "advisors": ["Randolph Kirchain"],
    "text": "Analytical study and cost modeling of secondary aluminum consumption for alloy producers under uncertain demands A series of case studies on raw materials inventory strategy for both wrought and cast aluminum alloy productions were conducted under recourse-based modeling framework with the explicit considerations of the demand uncertainty compared to the traditional strategy based on point forecast of future demand. The result shows significant economic and environmental benefits by pre-purchasing excess amount of cheaper but dirtier secondary raw materials to hedge the riskier higher-than-expected demand scenario. Further observations demonstrate that factors such as salvage value of residual scraps, cost advantage of secondary materials over primary materials, the degree of the demand uncertainty, etc. all have direct impacts on the hedging behavior. An analytical study on a simplified case scenario suggested a close form expression to well explain the hedging behavior and the impacts of various factors observed in case studies. The thesis then explored the effects of commonality shared by secondary materials in their application in multiple final products. Four propositions were reached."
}, {
    "id": "oai:dspace.mit.edu:1721.1/62676",
    "title": "Technological and economic comparison of battery technologies for U.S.A electric grid stabilization applications",
    "abstract": "Energy storage can provide many benefits to the electric grid of the United States of America. With recent pushes to stabilize renewable energy and implement a Smart Grid, battery technology can play a pivotal role in the advancement of energy storage of the grid. While there are many types of batteries that have been brought to market in recent years, four commonly mentioned practical systems are sodium sulfur, flow batteries, long life lead acid, and lithium ion batteries. A new type of battery, the \"liquid metal battery\" boasts low cost and easy maintenance while also providing superior power and capacity. However, this technology is still in its developmental stage. This study implements a framework for analyzing these five technologies for implementation in real-life scenarios. Firstly, a technological comparison of battery types and application requirements is conducted in order to see which technology is best suited for different applications. Next, an in depth cost analysis is done for each technology, so they can be compared on a total cost of ownership (#/kWh cycled) basis. Lastly, each technology is evaluated for each application through a financial analysis. This analysis encompasses current estimates on market valuation and provides net present values of investments for each battery type and application.",
    "advisors": ["Donald R Sadoway"],
    "text": "Technological and economic comparison of battery technologies for U.S.A electric grid stabilization applications Energy storage can provide many benefits to the electric grid of the United States of America. With recent pushes to stabilize renewable energy and implement a Smart Grid, battery technology can play a pivotal role in the advancement of energy storage of the grid. While there are many types of batteries that have been brought to market in recent years, four commonly mentioned practical systems are sodium sulfur, flow batteries, long life lead acid, and lithium ion batteries. A new type of battery, the \"liquid metal battery\" boasts low cost and easy maintenance while also providing superior power and capacity. However, this technology is still in its developmental stage. This study implements a framework for analyzing these five technologies for implementation in real-life scenarios. Firstly, a technological comparison of battery types and application requirements is conducted in order to see which technology is best suited for different applications. Next, an in depth cost analysis is done for each technology, so they can be compared on a total cost of ownership (#/kWh cycled) basis. Lastly, each technology is evaluated for each application through a financial analysis. This analysis encompasses current estimates on market valuation and provides net present values of investments for each battery type and application."
}, {
    "id": "oai:dspace.mit.edu:1721.1/45351",
    "title": "Commercialization of gallium nitride nanorod arrays on silicon for solid-state lighting",
    "abstract": "One important component in energy usage is lighting, which is currently dominated by incandescent and fluorescent lamps. However, due to potentially higher efficiencies and thus higher energy savings, solid-state lighting (SSL) is seriously being considered as a replacement. Currently, state-of-the-art white LEDs are made up of thin films of GaN and InGaN grown on sapphire substrates. A new LED structure design is proposed, in which GaN nanorod arrays are grown on silicon substrates. This new structure could be fabricated using anodized aluminum oxide's (AAO) ordered arrangement of pores as a template for growth of the nanorod array. AAO is selected for its high porosity and simple controllability of pore size and separation, which can in turn produce high density monocrystalline nanorod arrays with adjustable rod size and separation. Several advantages are enjoyed by LEDs based on rod arrays: lower cost, better yield and reliability and higher efficiencies. Two more LED designs, other than the current state-of-the-art GaN LED and the proposed LED structure, are included for comparisons. It is found that the proposed LED structure design is the best after considering costs and efficiency. For commercialization of this new LED design, the market penetration plan is to have a partnership with one of the major players in the current white LED industry. This has the advantage of having minimal capital investment and the product could be sold under an established brand. A simplified projection of earnings is calculated to illustrate sustainability of this business plan.",
    "advisors": ["Carl V. Thompson II", "Soo-Jin Chua"],
    "text": "Commercialization of gallium nitride nanorod arrays on silicon for solid-state lighting One important component in energy usage is lighting, which is currently dominated by incandescent and fluorescent lamps. However, due to potentially higher efficiencies and thus higher energy savings, solid-state lighting (SSL) is seriously being considered as a replacement. Currently, state-of-the-art white LEDs are made up of thin films of GaN and InGaN grown on sapphire substrates. A new LED structure design is proposed, in which GaN nanorod arrays are grown on silicon substrates. This new structure could be fabricated using anodized aluminum oxide's (AAO) ordered arrangement of pores as a template for growth of the nanorod array. AAO is selected for its high porosity and simple controllability of pore size and separation, which can in turn produce high density monocrystalline nanorod arrays with adjustable rod size and separation. Several advantages are enjoyed by LEDs based on rod arrays: lower cost, better yield and reliability and higher efficiencies. Two more LED designs, other than the current state-of-the-art GaN LED and the proposed LED structure, are included for comparisons. It is found that the proposed LED structure design is the best after considering costs and efficiency. For commercialization of this new LED design, the market penetration plan is to have a partnership with one of the major players in the current white LED industry. This has the advantage of having minimal capital investment and the product could be sold under an established brand. A simplified projection of earnings is calculated to illustrate sustainability of this business plan."
}, {
    "id": "oai:dspace.mit.edu:1721.1/93046",
    "title": "Towards a lithium-ion fiber battery",
    "abstract": "One of the key objectives in the realm of flexible electronics and flexible power sources is to achieve large-area, low-cost, scalable production of flexible systems. In this thesis we propose a new Li-ion battery architecture in a fiber form that could be the building block to large-area, conformal, flexible power sources, achieved through fiber thermal drawing. This architecture is based on the key-finding of using thermally induced phase separation as a method to introduce porous structures inside thermally drawn fibers for the very first time. This new versatile process allows us to incorporate ionically conductive gel-polymer electrolytes in fiber cores in a very simple way, with ionic conductivities suitable for a battery application. The rest of our proposed infiber battery architecture is composed of composite electrodes, which we fabricate and characterize. A model system is tested and a detailed pathway towards the first successful fabrication of a Li-ion fiber battery is given.",
    "advisors": ["Yoel Fink"],
    "text": "Towards a lithium-ion fiber battery One of the key objectives in the realm of flexible electronics and flexible power sources is to achieve large-area, low-cost, scalable production of flexible systems. In this thesis we propose a new Li-ion battery architecture in a fiber form that could be the building block to large-area, conformal, flexible power sources, achieved through fiber thermal drawing. This architecture is based on the key-finding of using thermally induced phase separation as a method to introduce porous structures inside thermally drawn fibers for the very first time. This new versatile process allows us to incorporate ionically conductive gel-polymer electrolytes in fiber cores in a very simple way, with ionic conductivities suitable for a battery application. The rest of our proposed infiber battery architecture is composed of composite electrodes, which we fabricate and characterize. A model system is tested and a detailed pathway towards the first successful fabrication of a Li-ion fiber battery is given."
}, {
    "id": "oai:dspace.mit.edu:1721.1/30262",
    "title": "Packaging for a drug delivery microelectromechanical system",
    "abstract": "Local drug delivery is a fast expanding field, and has been a center of attention for researchers in medicine in the last decade. Its advantages over systemic drug delivery are clear in cancer therapy, with localized tumors. A silicon microelectromechanical drug delivery device was fabricated for the purpose of delivering chemotherapeutic agents such-as carmustine, a potent brain cancer drug, directly to the site of the tumor. Limitations in the delivery capacity of the device led to the design of a new package. This package is made from thermally bonded Pyrex 7740 frames that are anodically bonded to the drug delivery chip. It increases the capacity of the chip, is smaller than the previous package and possesses true hermeticity, because of the bonding processes involved. This work describes the fabrication steps of the new package and a problem with the thermal bonding of Pyrex frames preventing the achievement of a package true to the original design. A temporary solution was devised and the completed package was tested with regards to its intended goals. It managed to increase the load capacity of the chip by a, factor of 10, with potential for more, while decreasing the overall size of the package. Short-term hermeticity was achieved for this package by using a UV-cured epoxy to bond some pieces, which was not in the original design. Future work will focus on finding a permanent solution to the aforementioned problem, and directions for it were suggested.",
    "advisors": ["Michael J. Cima"],
    "text": "Packaging for a drug delivery microelectromechanical system Local drug delivery is a fast expanding field, and has been a center of attention for researchers in medicine in the last decade. Its advantages over systemic drug delivery are clear in cancer therapy, with localized tumors. A silicon microelectromechanical drug delivery device was fabricated for the purpose of delivering chemotherapeutic agents such-as carmustine, a potent brain cancer drug, directly to the site of the tumor. Limitations in the delivery capacity of the device led to the design of a new package. This package is made from thermally bonded Pyrex 7740 frames that are anodically bonded to the drug delivery chip. It increases the capacity of the chip, is smaller than the previous package and possesses true hermeticity, because of the bonding processes involved. This work describes the fabrication steps of the new package and a problem with the thermal bonding of Pyrex frames preventing the achievement of a package true to the original design. A temporary solution was devised and the completed package was tested with regards to its intended goals. It managed to increase the load capacity of the chip by a, factor of 10, with potential for more, while decreasing the overall size of the package. Short-term hermeticity was achieved for this package by using a UV-cured epoxy to bond some pieces, which was not in the original design. Future work will focus on finding a permanent solution to the aforementioned problem, and directions for it were suggested."
}, {
    "id": "oai:dspace.mit.edu:1721.1/44207",
    "title": "Non-fluorine precursor solutions for high critical current density REBaCuOx films",
    "abstract": "The past two decades have seen advancements in high temperature superconducting cables for use in applications such as electrical transmission lines, propulsion systems, and mobile power generation systems. This work describes the development of a non-fluorine precursor solution for YBCO films with high critical current densities (Jc). An aqueous nitrate precursor solution system was selected from three possible precursor solution systems. It was further developed to produce YBCO films with Jc > 1 MA/cm2. Films up to ~800 nm thickness were made, and Jc > 1 MA/cm2 was obtained for films of over ~400 nm thickness. The developed aqueous solution contained a rheology modifier (hydroxyethyl cellulose / HEC), nitrates of Y, Ba, and Cu, and chelating agents (polyethylene glycol / PEG and sucrose). The total organic content was ~12 wt% of the entire solution, and the total cation concentration was ~0.7 M. The rheology modifying polymer determined the thickness of the deposited films. This allowed for the deposition of films with higher thickness than would be dictated by the total dissolved cations alone. A low temperature decomposition process was developed based on analyses of the chemical reactions that take place in the precursor films as they were heated. This process produced smooth and defect-free intermediate films that were stable under ambient conditions. These films were then heat treated to convert them into YBCO films. Recommendations for future work include further improvements to the precursor solution, including more effective chelating agents and possible alternative solvent systems. Intermediate films thicker than 2.5 [mu]m still tended to have surface defects.",
    "advisors": ["Michael J. Cima"],
    "text": "Non-fluorine precursor solutions for high critical current density REBaCuOx films The past two decades have seen advancements in high temperature superconducting cables for use in applications such as electrical transmission lines, propulsion systems, and mobile power generation systems. This work describes the development of a non-fluorine precursor solution for YBCO films with high critical current densities (Jc). An aqueous nitrate precursor solution system was selected from three possible precursor solution systems. It was further developed to produce YBCO films with Jc > 1 MA/cm2. Films up to ~800 nm thickness were made, and Jc > 1 MA/cm2 was obtained for films of over ~400 nm thickness. The developed aqueous solution contained a rheology modifier (hydroxyethyl cellulose / HEC), nitrates of Y, Ba, and Cu, and chelating agents (polyethylene glycol / PEG and sucrose). The total organic content was ~12 wt% of the entire solution, and the total cation concentration was ~0.7 M. The rheology modifying polymer determined the thickness of the deposited films. This allowed for the deposition of films with higher thickness than would be dictated by the total dissolved cations alone. A low temperature decomposition process was developed based on analyses of the chemical reactions that take place in the precursor films as they were heated. This process produced smooth and defect-free intermediate films that were stable under ambient conditions. These films were then heat treated to convert them into YBCO films. Recommendations for future work include further improvements to the precursor solution, including more effective chelating agents and possible alternative solvent systems. Intermediate films thicker than 2.5 [mu]m still tended to have surface defects."
}, {
    "id": "oai:dspace.mit.edu:1721.1/111227",
    "title": "In-situ investigation of the oxidation kinetics of Fe-12Cr-2Si using time-resolved transient grating spectroscopy",
    "abstract": "The design and validation of new alloys for engineering applications is limited by the speed at which materials may be tested. In particular, there exist few methods by which the thermal, mechanical, and structural properties of materials may be monitored in conditions that are dynamically changing their microstructure. These conditions, such as heat treatments, radiation exposure, or corrosive environments, are common when material performance needs to be validated. To offset this lack of capability, new non-destructive experimental tools must be developed to facilitate on-line, realtime testing of materials undergoing some type of evolution. In this thesis, a flexible, all-optical methodology known as dual heterodyne phase collection transient grating spectroscopy is developed for this purpose. This method adapts a traditional spectroscopic technique sensitive to thermal and mechanical properties for real-time use. A formalism is also developed to quantify both elastic and thermal transport properties of materials with second-scale resolution. These new tools are then used to study the short-timescale oxidation kinetics of Fe-12Cr-2Si, a model alloy with oxide layer formation properties similar to large classes of Fr-Cr alloys. By monitoring the effect of thin oxide layers on surface thermal transport, there exists a pathway to continuously determine the thickness of a tens of nanometers thick growing oxide layer in real-time. Despite the lack of clarity in the particular set of experimental results presented here, the potential for the methods developed in this thesis is large. In-situ materials testing of this type may allow for a drastic increase in the pace of materials development by reducing the need for post-evolution, destructive materials testing between each design iteration.",
    "advisors": ["Michael P. Short"],
    "text": "In-situ investigation of the oxidation kinetics of Fe-12Cr-2Si using time-resolved transient grating spectroscopy The design and validation of new alloys for engineering applications is limited by the speed at which materials may be tested. In particular, there exist few methods by which the thermal, mechanical, and structural properties of materials may be monitored in conditions that are dynamically changing their microstructure. These conditions, such as heat treatments, radiation exposure, or corrosive environments, are common when material performance needs to be validated. To offset this lack of capability, new non-destructive experimental tools must be developed to facilitate on-line, realtime testing of materials undergoing some type of evolution. In this thesis, a flexible, all-optical methodology known as dual heterodyne phase collection transient grating spectroscopy is developed for this purpose. This method adapts a traditional spectroscopic technique sensitive to thermal and mechanical properties for real-time use. A formalism is also developed to quantify both elastic and thermal transport properties of materials with second-scale resolution. These new tools are then used to study the short-timescale oxidation kinetics of Fe-12Cr-2Si, a model alloy with oxide layer formation properties similar to large classes of Fr-Cr alloys. By monitoring the effect of thin oxide layers on surface thermal transport, there exists a pathway to continuously determine the thickness of a tens of nanometers thick growing oxide layer in real-time. Despite the lack of clarity in the particular set of experimental results presented here, the potential for the methods developed in this thesis is large. In-situ materials testing of this type may allow for a drastic increase in the pace of materials development by reducing the need for post-evolution, destructive materials testing between each design iteration."
}, {
    "id": "oai:dspace.mit.edu:1721.1/34413",
    "title": "Data mining for structure type prediction",
    "abstract": "Determining the stable structure types of an alloy is critical to determining many properties of that material. This can be done through experiment or computation. Both methods can be expensive and time consuming. Computational methods require energy calculations of hundreds of structure types. Computation time would be greatly improved if this large number of possible structure types was reduced. A method is discussed here to predict the stable structure types for an alloy based on compiled data. This would include experimentally observed stable structure types and calculated energies of structure types. In this paper I will describe the state of this technology. This will include an overview of past and current work. Curtarolo et al. showed a factor of three improvement in the number of calculations required to determine a given percentage of the ground state structure types for an alloy system by using correlations among a database of over 6000 calculated energies.I will show correlations among experimentally determined stable structure types appearing in the same alloy system through statistics computed from the Pauling File Inorganic Materials Database Binaries edition. I will compare a method to predict stable structure types based on correlations among pairs of structure types that appear in the same alloy system with a method based simply on the frequency of occurrence of each structure type. I will show a factor of two improvement in the number of calculations required to determine the ground state structure types between these two methods. This paper will examine the potential market value for a software tool used to predict likely stable structure types. A timeline for introduction of this product and an analysis of the market for such a tool will be included. There is no established market for structure type prediction software, but the market will be similar to that of materials database software and energy calculation software.The potential market is small, but the production and maintenance costs are also small. These small costs, combined with the potential of this tool to improve greatly over time, make this a potentially promising investment. These methods are still in development. The key to the value of this tool lies in the accuracy of the prediction methods developed over the next few years.",
    "advisors": ["Gerbrand Ceder"],
    "text": "Data mining for structure type prediction Determining the stable structure types of an alloy is critical to determining many properties of that material. This can be done through experiment or computation. Both methods can be expensive and time consuming. Computational methods require energy calculations of hundreds of structure types. Computation time would be greatly improved if this large number of possible structure types was reduced. A method is discussed here to predict the stable structure types for an alloy based on compiled data. This would include experimentally observed stable structure types and calculated energies of structure types. In this paper I will describe the state of this technology. This will include an overview of past and current work. Curtarolo et al. showed a factor of three improvement in the number of calculations required to determine a given percentage of the ground state structure types for an alloy system by using correlations among a database of over 6000 calculated energies.I will show correlations among experimentally determined stable structure types appearing in the same alloy system through statistics computed from the Pauling File Inorganic Materials Database Binaries edition. I will compare a method to predict stable structure types based on correlations among pairs of structure types that appear in the same alloy system with a method based simply on the frequency of occurrence of each structure type. I will show a factor of two improvement in the number of calculations required to determine the ground state structure types between these two methods. This paper will examine the potential market value for a software tool used to predict likely stable structure types. A timeline for introduction of this product and an analysis of the market for such a tool will be included. There is no established market for structure type prediction software, but the market will be similar to that of materials database software and energy calculation software.The potential market is small, but the production and maintenance costs are also small. These small costs, combined with the potential of this tool to improve greatly over time, make this a potentially promising investment. These methods are still in development. The key to the value of this tool lies in the accuracy of the prediction methods developed over the next few years."
}, {
    "id": "oai:dspace.mit.edu:1721.1/45353",
    "title": "Financial viability and technical evaluation of dendritic cell-carrying \"vaccination nodes\" for immunotherapy",
    "abstract": "Cancer immunotherapy attempts to stimulate the immune system to reject and destroy tumor cells. Despite the amount of ongoing intensive research to prevent cancer, tumor cells continue to evade immune responses. Currently, dendritic cell vaccines are in development, in which autologous antigen-loaded dendritic cells are injected back into the patient in order to generate an appropriate immune response. Improving upon this idea, members of the Irvine laboratory are in development of an injectable dendritic cell based formulation that gels in situ around the tumor site. In this way, immune cells (most notably T cells) can be recruited and become activated against specific tumor antigens, and (hopefully) kill tumor cells. Recent studies have shown the potential benefit of incorporation of cytokine interleukin-15 complexed with its soluble receptor interleukin-5R[alpha], which is discussed. Economic considerations are also discussed, including topics such as intellectual property, barriers to entry, initial markets and market drivers, and entry into the current supply chain considerations. A business strategy is outlined and evaluated.",
    "advisors": ["Darrell J. Irvine"],
    "text": "Financial viability and technical evaluation of dendritic cell-carrying \"vaccination nodes\" for immunotherapy Cancer immunotherapy attempts to stimulate the immune system to reject and destroy tumor cells. Despite the amount of ongoing intensive research to prevent cancer, tumor cells continue to evade immune responses. Currently, dendritic cell vaccines are in development, in which autologous antigen-loaded dendritic cells are injected back into the patient in order to generate an appropriate immune response. Improving upon this idea, members of the Irvine laboratory are in development of an injectable dendritic cell based formulation that gels in situ around the tumor site. In this way, immune cells (most notably T cells) can be recruited and become activated against specific tumor antigens, and (hopefully) kill tumor cells. Recent studies have shown the potential benefit of incorporation of cytokine interleukin-15 complexed with its soluble receptor interleukin-5R[alpha], which is discussed. Economic considerations are also discussed, including topics such as intellectual property, barriers to entry, initial markets and market drivers, and entry into the current supply chain considerations. A business strategy is outlined and evaluated."
}, {
    "id": "oai:dspace.mit.edu:1721.1/61907",
    "title": "Design, build and test of an axial flow hydrokinetic turbine with fatigue analysis",
    "abstract": "OpenProp is an open source propeller and turbine design and analysis code that has been in development since 2007 by MIT graduate students under the supervision of Professor Richard Kimball. In order to test the performance predictions of OpenProp for axial flow hydrokinetic turbines, a test fixture was designed and constructed, and a model scale turbine was tested. Tests were conducted in the MIT water tunnel for tip speed ratios ranging from 1.55 to 7.73. Additional code was also written and added to OpenProp in order to implement ABS steel vessels rules for propellers and calculate blade stress. The blade stress code was used to conduct a fatigue analysis for a model scale propeller using a quasi-steady approach. Turbine test results showed that OpenProp provides good performance predictions for the on-design operational condition but that further work is needed to improve performance predictions for the off-design operational condition. Fatigue analysis results show that reasonable estimates of propeller blade fatigue life can be obtained using a relatively simple method. Calculated blade stress distributions agree with previously published data obtained with more sophisticated and time consuming calculation techniques.",
    "advisors": ["Mark S. Welsh, Richard W. Kimball", "Ronald G. Ballinger"],
    "text": "Design, build and test of an axial flow hydrokinetic turbine with fatigue analysis OpenProp is an open source propeller and turbine design and analysis code that has been in development since 2007 by MIT graduate students under the supervision of Professor Richard Kimball. In order to test the performance predictions of OpenProp for axial flow hydrokinetic turbines, a test fixture was designed and constructed, and a model scale turbine was tested. Tests were conducted in the MIT water tunnel for tip speed ratios ranging from 1.55 to 7.73. Additional code was also written and added to OpenProp in order to implement ABS steel vessels rules for propellers and calculate blade stress. The blade stress code was used to conduct a fatigue analysis for a model scale propeller using a quasi-steady approach. Turbine test results showed that OpenProp provides good performance predictions for the on-design operational condition but that further work is needed to improve performance predictions for the off-design operational condition. Fatigue analysis results show that reasonable estimates of propeller blade fatigue life can be obtained using a relatively simple method. Calculated blade stress distributions agree with previously published data obtained with more sophisticated and time consuming calculation techniques."
}, {
    "id": "oai:dspace.mit.edu:1721.1/44384",
    "title": "The impact of mass decompounding on assessing the value of vehicle lightweighting",
    "abstract": "Among consumers and manufacturers alike, there is an increasing realization about the need for fuel efficient vehicles. One effective way to accomplish this is through vehicle lightweighting, which can be achieved by material substitution, novel vehicle component design, and changes in processing. Although primary vehicle mass reduction is often associated with additional costs to the automaker, a decision to lightweight may, depending on when in the vehicle development process the decision is taken, result in additional secondary mass savings such that the value derived from lightweighting is greater than the costs. In this study, the concept of secondary mass savings, or mass decompounding, is developed using regression analysis. Moreover, the full, both primary and secondary, mass savings potential is assessed at different times in the vehicle development process. Lastly, powertrain and market trend modeling are employed to estimate the value of the compounded mass savings in terms of improved fuel economy and acceleration. This methodology is applied to a collected vehicle dataset in order to generate a model by which the value of and the subsystem-specific amount of secondary mass savings may be easily estimated during the early stages of vehicle development. In summary, this analysis may be employed to evaluate the economic competitiveness of vehicle lightweighting options at different times in the vehicle development process.",
    "advisors": ["Randolph E. Kirchain, Jr.", "Richard Roth"],
    "text": "The impact of mass decompounding on assessing the value of vehicle lightweighting Among consumers and manufacturers alike, there is an increasing realization about the need for fuel efficient vehicles. One effective way to accomplish this is through vehicle lightweighting, which can be achieved by material substitution, novel vehicle component design, and changes in processing. Although primary vehicle mass reduction is often associated with additional costs to the automaker, a decision to lightweight may, depending on when in the vehicle development process the decision is taken, result in additional secondary mass savings such that the value derived from lightweighting is greater than the costs. In this study, the concept of secondary mass savings, or mass decompounding, is developed using regression analysis. Moreover, the full, both primary and secondary, mass savings potential is assessed at different times in the vehicle development process. Lastly, powertrain and market trend modeling are employed to estimate the value of the compounded mass savings in terms of improved fuel economy and acceleration. This methodology is applied to a collected vehicle dataset in order to generate a model by which the value of and the subsystem-specific amount of secondary mass savings may be easily estimated during the early stages of vehicle development. In summary, this analysis may be employed to evaluate the economic competitiveness of vehicle lightweighting options at different times in the vehicle development process."
}, {
    "id": "oai:dspace.mit.edu:1721.1/33400",
    "title": "Structural, vibrational and thermodynamic properties of carbon allotropes from first-principles : diamond, graphite, and nanotubes",
    "abstract": "The structural, dynamical, and thermodynamic properties of different carbon allotropes are computed using a combination of ab-initio methods: density-functional theory for total-energy calculations and density-functional perturbation theory for lattice dynamics. For diamond, graphite, graphene, and armchair or zigzag single-walled nanotubes we first calculate the ground-state properties: lattice parameters, elastic constants and phonon dispersions and density of states. Very good agreement with available experimental data is found for all these, with the exception of the c/a ratio in graphite and the associated elastic constants and phonon dispersions. Agreement with experiments is recovered once the experimental c/a is chosen for the calculations. Results for carbon nanotubes confirm and expand available, but scarce, experimental data. The vibrational free energy and the thermal expansion, the temperature dependence of the elastic moduli and the specific heat are calculated using the quasi-harmonic approximation. Graphite shows a distinctive in-plane negative thermal-expansion coefficient that reaches its lowest value around room temperature, in very good agreement with experiments. The predicted value for the thermal-contraction coefficient of narrow single-walled nanotubes is half that of graphite, while for graphene it is found to be three times as large.",
    "advisors": ["Nicola Marzari"],
    "text": "Structural, vibrational and thermodynamic properties of carbon allotropes from first-principles : diamond, graphite, and nanotubes The structural, dynamical, and thermodynamic properties of different carbon allotropes are computed using a combination of ab-initio methods: density-functional theory for total-energy calculations and density-functional perturbation theory for lattice dynamics. For diamond, graphite, graphene, and armchair or zigzag single-walled nanotubes we first calculate the ground-state properties: lattice parameters, elastic constants and phonon dispersions and density of states. Very good agreement with available experimental data is found for all these, with the exception of the c/a ratio in graphite and the associated elastic constants and phonon dispersions. Agreement with experiments is recovered once the experimental c/a is chosen for the calculations. Results for carbon nanotubes confirm and expand available, but scarce, experimental data. The vibrational free energy and the thermal expansion, the temperature dependence of the elastic moduli and the specific heat are calculated using the quasi-harmonic approximation. Graphite shows a distinctive in-plane negative thermal-expansion coefficient that reaches its lowest value around room temperature, in very good agreement with experiments. The predicted value for the thermal-contraction coefficient of narrow single-walled nanotubes is half that of graphite, while for graphene it is found to be three times as large."
}, {
    "id": "oai:dspace.mit.edu:1721.1/44423",
    "title": "DNA as a programmable material : de novo gene synthesis and error correction",
    "abstract": "Deoxyribonucleic acid (DNA), the polymeric molecule that carries the genetic code of all living organisms, is arguably one of the most programmable assembly materials available to chemists, biologists, and materials scientists. Scientists have used DNA to build many different structures for various applications in disparate areas of research from traditional biological applications to more recent non-biological applications. Although DNA isn't typically thought of as an assembly material by people not doing research in the area, the availability of decreasing cost synthetic oligonucleotides has led to advances in gene fabrication technology which in turn has enabled synthetic biology to flourish. Using DNA as a building material for small and large constructs of DNA is reliant on having effective gene synthesis techniques. Construction of synthetic DNA is limited by errors that pervade the final product. To address this problem, effective error correction methods are pivotal. Having extremely robust gene synthesis and error correction techniques will allow researchers to generate very large scale constructs potentially necessary in applications such as genome re-engineering.",
    "advisors": ["Joseph Jacobson"],
    "text": "DNA as a programmable material : de novo gene synthesis and error correction Deoxyribonucleic acid (DNA), the polymeric molecule that carries the genetic code of all living organisms, is arguably one of the most programmable assembly materials available to chemists, biologists, and materials scientists. Scientists have used DNA to build many different structures for various applications in disparate areas of research from traditional biological applications to more recent non-biological applications. Although DNA isn't typically thought of as an assembly material by people not doing research in the area, the availability of decreasing cost synthetic oligonucleotides has led to advances in gene fabrication technology which in turn has enabled synthetic biology to flourish. Using DNA as a building material for small and large constructs of DNA is reliant on having effective gene synthesis techniques. Construction of synthetic DNA is limited by errors that pervade the final product. To address this problem, effective error correction methods are pivotal. Having extremely robust gene synthesis and error correction techniques will allow researchers to generate very large scale constructs potentially necessary in applications such as genome re-engineering."
}, {
    "id": "oai:dspace.mit.edu:1721.1/37883",
    "title": "An evaluation of indium antimonide quantum well transistor technology",
    "abstract": "Motivated by the super high electron mobility of Indium Antimonide (InSb), researchers have seen great potential to use this new material in high switching speed and low power transistors. In Dec, 2005, Intel and its partner, QinetiQ, Ltd, announced 85nm gate length enhancement and depletion mode InSb quantum well transistors. Such transistors can operate as high as 305GHz and power consumption is reduced by a factor of 10. In this thesis, the emerging InSb transistor technology is discussed in details. Given its superior performance, it may complement silicon transistor to extend Moore's law in the next decade. The prospect of InSb transistor is also compared with other nanotechnology transistors, such as carbon nanotube and silicon nanowire. Several potential markets are figured out, namely, microprocessor, low noise amplifier and millimeter wave device. Related patents are evaluated. It is found that most of the patents are held by Intel's partner, QinetiQ Ltd. and thus patents issue would not block the launch of products. A joint venture or strategy alliance model is proposed to reduce the risk of investment. In addition, a cost model is presented at the end. It is concluded that cheap silicon substrate and large enough production scale are two crucial factors for the commercialization success of InSb transistor technology.",
    "advisors": ["Thomas W. Eagar"],
    "text": "An evaluation of indium antimonide quantum well transistor technology Motivated by the super high electron mobility of Indium Antimonide (InSb), researchers have seen great potential to use this new material in high switching speed and low power transistors. In Dec, 2005, Intel and its partner, QinetiQ, Ltd, announced 85nm gate length enhancement and depletion mode InSb quantum well transistors. Such transistors can operate as high as 305GHz and power consumption is reduced by a factor of 10. In this thesis, the emerging InSb transistor technology is discussed in details. Given its superior performance, it may complement silicon transistor to extend Moore's law in the next decade. The prospect of InSb transistor is also compared with other nanotechnology transistors, such as carbon nanotube and silicon nanowire. Several potential markets are figured out, namely, microprocessor, low noise amplifier and millimeter wave device. Related patents are evaluated. It is found that most of the patents are held by Intel's partner, QinetiQ Ltd. and thus patents issue would not block the launch of products. A joint venture or strategy alliance model is proposed to reduce the risk of investment. In addition, a cost model is presented at the end. It is concluded that cheap silicon substrate and large enough production scale are two crucial factors for the commercialization success of InSb transistor technology."
}, {
    "id": "oai:dspace.mit.edu:1721.1/36218",
    "title": "Nanoparticle-chiral nematic liquid crystal composites",
    "abstract": "The advancement of the fabrication of a one-dimensional photonic crystal without time-reversal and space-inversion symmetries was pursued. Theoretical studies predict that such a system would exhibit unusual optical properties, including indirect photonic band gaps and backward wave propagating eigenmodes. Such a system can be created experimentally by combing magnetooptical nanoparticles with a chiral nematic liquid crystal. The fabrication of this material system was advanced through two distinct phases of research. The first phase seeks to produce magnetooptical yttrium iron garnet (YIG) nanoparticles with an average diameter on the order of 15-50 nm. It was determined that a commercially available yttrium iron oxide nanopowder (purchased from Sigma-Aldrich Corporation) exhibited YIG and orthorhombic yttrium iron oxide (YFeO3) phases after being calcined at 800 C for two hours. These nanoparticles were slightly smaller than desired, having diameters on the order of 10-20 nm. Direct nanoparticle synthesis via coprecipitation in microemulsions produced superior results, resulting in a pure YIG material with diameters on the order of 30-50 nm.",
    "advisors": ["Edwin L. Thomas"],
    "text": "Nanoparticle-chiral nematic liquid crystal composites The advancement of the fabrication of a one-dimensional photonic crystal without time-reversal and space-inversion symmetries was pursued. Theoretical studies predict that such a system would exhibit unusual optical properties, including indirect photonic band gaps and backward wave propagating eigenmodes. Such a system can be created experimentally by combing magnetooptical nanoparticles with a chiral nematic liquid crystal. The fabrication of this material system was advanced through two distinct phases of research. The first phase seeks to produce magnetooptical yttrium iron garnet (YIG) nanoparticles with an average diameter on the order of 15-50 nm. It was determined that a commercially available yttrium iron oxide nanopowder (purchased from Sigma-Aldrich Corporation) exhibited YIG and orthorhombic yttrium iron oxide (YFeO3) phases after being calcined at 800 C for two hours. These nanoparticles were slightly smaller than desired, having diameters on the order of 10-20 nm. Direct nanoparticle synthesis via coprecipitation in microemulsions produced superior results, resulting in a pure YIG material with diameters on the order of 30-50 nm."
}, {
    "id": "oai:dspace.mit.edu:1721.1/54200",
    "title": "Implementations of electric vehicle system based on solar energy in Singapore assessment of lithium ion batteries for automobiles",
    "abstract": "In this thesis report, both quantitative and qualitative approaches are used to provide a comprehensive analysis of lithium ion (Li-ion) batteries for plug-in hybrid electric vehicle (PHEV) and battery electric vehicle (BEV) from technological and economical perspectives. Five key factors including power density, energy density, safety, durability, and cost are employed to compare four types of Li-ion batteries. Utility analysis indicates that all the Li-ion batteries are able to satisfy both power density and energy density targets, but only two of them are able to meet safety and durability requirements. Currently, the main challenge for their automotive application is cost reduction, since the cheapest LiFePO battery costs $247.8/kWh which is 1.65 times the cost target established by USABC. Economical values of PHEV and BEV are presented from an end user's point of view. Various sensitivity analysis have been used to identify the impact of key factors such as battery pack cost reduction, driving distance, gasoline price, and government subsidizations on cost effectiveness of PHEV and BEV. Results show that $4,270 and $7,726 of U.S. government subsidizations to an individual user are needed for PHEV and BEV to breakeven. Lastly, the lithium ion battery based electric vehicle systems have also been evaluated in the implementation models in Singapore. The conclusion is that it is not feasible to adopt electric vehicle system in Singapore under current government incentives.",
    "advisors": ["Yet-Ming Chiang", "Andy Chu"],
    "text": "Implementations of electric vehicle system based on solar energy in Singapore assessment of lithium ion batteries for automobiles In this thesis report, both quantitative and qualitative approaches are used to provide a comprehensive analysis of lithium ion (Li-ion) batteries for plug-in hybrid electric vehicle (PHEV) and battery electric vehicle (BEV) from technological and economical perspectives. Five key factors including power density, energy density, safety, durability, and cost are employed to compare four types of Li-ion batteries. Utility analysis indicates that all the Li-ion batteries are able to satisfy both power density and energy density targets, but only two of them are able to meet safety and durability requirements. Currently, the main challenge for their automotive application is cost reduction, since the cheapest LiFePO battery costs $247.8/kWh which is 1.65 times the cost target established by USABC. Economical values of PHEV and BEV are presented from an end user's point of view. Various sensitivity analysis have been used to identify the impact of key factors such as battery pack cost reduction, driving distance, gasoline price, and government subsidizations on cost effectiveness of PHEV and BEV. Results show that $4,270 and $7,726 of U.S. government subsidizations to an individual user are needed for PHEV and BEV to breakeven. Lastly, the lithium ion battery based electric vehicle systems have also been evaluated in the implementation models in Singapore. The conclusion is that it is not feasible to adopt electric vehicle system in Singapore under current government incentives."
}, {
    "id": "oai:dspace.mit.edu:1721.1/112385",
    "title": "Active hyperspectral imaging of chemicals on surfaces",
    "abstract": "Active hyperspectral imaging (HSI) is a promising technique for the detection of chemicals at standoff distances. In active HSI, a target is illuminated by a laser source at many different wavelengths and a camera obtains an image of the illuminated scene at each wavelength. In this research, the feasibility of hyperspectral imaging for the detection of particles on surfaces was demonstrated using potassium chlorate particles on car panels at distances of 5 m, 10 m, and 20 m. Using the Adaptive Cosine Estimation (ACE) algorithm which compares the observed reflectance spectra to a reference spectrum, potassium chlorate fingerprints are easily visible at many different sample angles. However, in general, there is a large amount of variation in the shape and magnitude of spectra in a hyperspectral image that depend on factors such as particle size, viewing geometry, and surface reflectivity. Thus, Mie Theory calculations are performed on simulated materials and combined with information from sources such as Hapke [4], [20] to give qualitative insight into the expected shape and magnitude of reflectance spectra from sparse particles on a surface. The shape of the spectra depends on whether the particles are strongly absorbing or weakly absorbing. Strongly absorbing particles tend to have reflectance maxima near the resonant frequency, whereas weakly absorbing particles tend to have reflectance minima. For highly reflective substrates, the reflectance decreases sharply as the sample angle increases and becomes dominated by backward scattering from the particle which has a flatter spectrum around the Christiansen frequency, the frequency at which the refractive index of the particle is closest to one. The double interaction model, which uses Mie Theory to calculate the contributions to the reflectance along two different light paths, is used to accurately account for how the shape and magnitude of the reflectance spectra of sodium chlorate particles on gold and silica surfaces changes as a function of sample angle and laser angle. A method for approximating the mean particle size based on the location of the peak near the Christiansen frequency is derived. This method, when applied to the sodium chlorate sample, yields a result for the mean particle diameter that is approximately half of the value determined using a microscope. The Hapke Isotropic Multiple Scattering Approximation (IMSA), combined with Mie Theory, is used to give qualitative insight into the expected shape and magnitude of reflectance spectra from bulk powders. Compared with the reflectance spectra from sparse particles, the spectra from bulk powders are much simpler and less dependent on the viewing geometry. The Hapke IMSA model is able to accurately account for the observed changes in the reflectance from bulk sodium chlorate powder at multiple sample angles and laser angles. A final scenario of interest is thin films on rough or porous surfaces. Using a model that takes into account diffusely reflected and specularly reflected light, the observed reflectance spectra from diethyl phthalate (DEP) on a brick is fitted to a high degree of accuracy. This suggests a promising method for using hyperspectral imaging to determine the thickness of liquids on porous surfaces. Finally, the issue of speckle in hyperspectral imaging was examined using simulations based on Fourier optics and information from sources such as Goodman [6], [17]. Speckle is a limiting factor in hyperspectral imaging because it is noise that scales with the signal, and thus cannot be eliminated by increasing the signal strength. Equations from various sources are presented that describe the reduction in speckle contrast for spatial, spectral, polarization, temporal, and angular averaging. Original equations for the reduction in contrast for spectral and angular averaging are derived.",
    "advisors": ["Linn Hobbs"],
    "text": "Active hyperspectral imaging of chemicals on surfaces Active hyperspectral imaging (HSI) is a promising technique for the detection of chemicals at standoff distances. In active HSI, a target is illuminated by a laser source at many different wavelengths and a camera obtains an image of the illuminated scene at each wavelength. In this research, the feasibility of hyperspectral imaging for the detection of particles on surfaces was demonstrated using potassium chlorate particles on car panels at distances of 5 m, 10 m, and 20 m. Using the Adaptive Cosine Estimation (ACE) algorithm which compares the observed reflectance spectra to a reference spectrum, potassium chlorate fingerprints are easily visible at many different sample angles. However, in general, there is a large amount of variation in the shape and magnitude of spectra in a hyperspectral image that depend on factors such as particle size, viewing geometry, and surface reflectivity. Thus, Mie Theory calculations are performed on simulated materials and combined with information from sources such as Hapke [4], [20] to give qualitative insight into the expected shape and magnitude of reflectance spectra from sparse particles on a surface. The shape of the spectra depends on whether the particles are strongly absorbing or weakly absorbing. Strongly absorbing particles tend to have reflectance maxima near the resonant frequency, whereas weakly absorbing particles tend to have reflectance minima. For highly reflective substrates, the reflectance decreases sharply as the sample angle increases and becomes dominated by backward scattering from the particle which has a flatter spectrum around the Christiansen frequency, the frequency at which the refractive index of the particle is closest to one. The double interaction model, which uses Mie Theory to calculate the contributions to the reflectance along two different light paths, is used to accurately account for how the shape and magnitude of the reflectance spectra of sodium chlorate particles on gold and silica surfaces changes as a function of sample angle and laser angle. A method for approximating the mean particle size based on the location of the peak near the Christiansen frequency is derived. This method, when applied to the sodium chlorate sample, yields a result for the mean particle diameter that is approximately half of the value determined using a microscope. The Hapke Isotropic Multiple Scattering Approximation (IMSA), combined with Mie Theory, is used to give qualitative insight into the expected shape and magnitude of reflectance spectra from bulk powders. Compared with the reflectance spectra from sparse particles, the spectra from bulk powders are much simpler and less dependent on the viewing geometry. The Hapke IMSA model is able to accurately account for the observed changes in the reflectance from bulk sodium chlorate powder at multiple sample angles and laser angles. A final scenario of interest is thin films on rough or porous surfaces. Using a model that takes into account diffusely reflected and specularly reflected light, the observed reflectance spectra from diethyl phthalate (DEP) on a brick is fitted to a high degree of accuracy. This suggests a promising method for using hyperspectral imaging to determine the thickness of liquids on porous surfaces. Finally, the issue of speckle in hyperspectral imaging was examined using simulations based on Fourier optics and information from sources such as Goodman [6], [17]. Speckle is a limiting factor in hyperspectral imaging because it is noise that scales with the signal, and thus cannot be eliminated by increasing the signal strength. Equations from various sources are presented that describe the reduction in speckle contrast for spatial, spectral, polarization, temporal, and angular averaging. Original equations for the reduction in contrast for spectral and angular averaging are derived."
}, {
    "id": "oai:dspace.mit.edu:1721.1/8428",
    "title": "Characterization of bonded copper interconnects for three-dimensional integrated circuits",
    "abstract": "The unprecedented growth of the semiconductor industry is demanding ultra large-scale integrated (ULSI) circuits with increasing performance at minimum cost and power dissipation. As the critical dimensions in ULSI design continue to shrink, system performance of integrated circuits will be increasingly dominated by interconnect delay. Three-dimensional (3-D) ICs can reduce interconnect delay problems by offering flexibility in system design, placement and routing. 3-D ICs can be formed by vertical integration of multiple device layers using wafer bonding, recrystallization or selective epitaxial growth. The flexibility to place devices along the vertical dimension allows higher device density and reduced total interconnect lengths in 3-D ICs. One approach to fabrication of 3D integrated circuits is to bond previously-processed device layers using metal-metal bonds that also serve as layer-to-layer interconnects. Evaluation of the feasibility of wafer bonding for 3-D integration relies on our ability to characterize bonded interconnects. The reliability of devices containing multi-layer thin film structures is strongly influenced by the adhesion properties of the many interfaces present. Interface fracture failure is highly likely given the high thermal stresses developed during processing and also during service. A four-point bend test technique has been used to evaluate the strength of Cu-Cu bonds. Test structures were fabricated by bonding wafers containing copper lines (with Ta barrier) that were patterned on silicon dioxide. Tests on the thermocompression-bonded copper lines yielded reproducible fracture toughness values (1-10 J/m2 ) for bonds created at 300C-400C. The effect of process parameters on bond strength was studied. It was found that surface copper oxide removal prior to bonding using a forming gas purge (95%Ar-5%H2 ) resulted in higher toughness values and lesser variations compared to a N2 purge. Also, bond strength was found to increase with increasing bonding temperature. Thicker bonded films resulted in stronger bonds. Interface failure was found to be most likely at the Cu-Cu and Ta-Silicon dioxide interfaces. The results obtained from different process conditions were used to optimize the bonding process.",
    "advisors": ["Carl V. Thompson"],
    "text": "Characterization of bonded copper interconnects for three-dimensional integrated circuits The unprecedented growth of the semiconductor industry is demanding ultra large-scale integrated (ULSI) circuits with increasing performance at minimum cost and power dissipation. As the critical dimensions in ULSI design continue to shrink, system performance of integrated circuits will be increasingly dominated by interconnect delay. Three-dimensional (3-D) ICs can reduce interconnect delay problems by offering flexibility in system design, placement and routing. 3-D ICs can be formed by vertical integration of multiple device layers using wafer bonding, recrystallization or selective epitaxial growth. The flexibility to place devices along the vertical dimension allows higher device density and reduced total interconnect lengths in 3-D ICs. One approach to fabrication of 3D integrated circuits is to bond previously-processed device layers using metal-metal bonds that also serve as layer-to-layer interconnects. Evaluation of the feasibility of wafer bonding for 3-D integration relies on our ability to characterize bonded interconnects. The reliability of devices containing multi-layer thin film structures is strongly influenced by the adhesion properties of the many interfaces present. Interface fracture failure is highly likely given the high thermal stresses developed during processing and also during service. A four-point bend test technique has been used to evaluate the strength of Cu-Cu bonds. Test structures were fabricated by bonding wafers containing copper lines (with Ta barrier) that were patterned on silicon dioxide. Tests on the thermocompression-bonded copper lines yielded reproducible fracture toughness values (1-10 J/m2 ) for bonds created at 300C-400C. The effect of process parameters on bond strength was studied. It was found that surface copper oxide removal prior to bonding using a forming gas purge (95%Ar-5%H2 ) resulted in higher toughness values and lesser variations compared to a N2 purge. Also, bond strength was found to increase with increasing bonding temperature. Thicker bonded films resulted in stronger bonds. Interface failure was found to be most likely at the Cu-Cu and Ta-Silicon dioxide interfaces. The results obtained from different process conditions were used to optimize the bonding process."
}, {
    "id": "oai:dspace.mit.edu:1721.1/28874",
    "title": "MEMS-based resonant sensor arrays : selective detection of volatile and toxic chemicals",
    "abstract": "With growing concerns about homeland security, public health, and environmental cleanliness, there is a strong need today for robust chemical sensing systems that are portable in addition to being highly sensitive. While there are many options available for gaseous chemical detection and identification, not all are well-suited toward the creation of a portable device. Boston MicroSystems, Inc. (BMS) has developed a resonant chemical sensor that is predicted to meet the performance needs of the current market in terms of gas sensitivity, operational reliability, and overall device portability. Desirable device characteristics are attained through integrating aluminum nitride and silicon carbide in processes that are protected through a strong base of intellectual property. By developing a standardized platform for gas detection based on this sensor technology, barriers to entering the targeted markets may be overcome.",
    "advisors": ["Harry L. Tuller"],
    "text": "MEMS-based resonant sensor arrays : selective detection of volatile and toxic chemicals With growing concerns about homeland security, public health, and environmental cleanliness, there is a strong need today for robust chemical sensing systems that are portable in addition to being highly sensitive. While there are many options available for gaseous chemical detection and identification, not all are well-suited toward the creation of a portable device. Boston MicroSystems, Inc. (BMS) has developed a resonant chemical sensor that is predicted to meet the performance needs of the current market in terms of gas sensitivity, operational reliability, and overall device portability. Desirable device characteristics are attained through integrating aluminum nitride and silicon carbide in processes that are protected through a strong base of intellectual property. By developing a standardized platform for gas detection based on this sensor technology, barriers to entering the targeted markets may be overcome."
}, {
    "id": "oai:dspace.mit.edu:1721.1/39555",
    "title": "Ligand shell morphology of water-soluble mixed-monolayer protected gold nanoparticles",
    "abstract": "Nanoparticles comprise a versatile class of nanomaterials that consist of particles that have a characteristic length scale less than 100nm. They are on a similar length scale as many biological elements, so it is fitting that they are being used increasingly in biological systems for a variety of applications. Interesting properties of water-soluble metal nanoparticles that could lead to novel biological applications include bio-catalytic, sensing, and light scattering capabilities. We will present here the characterization of novel highly water-soluble gold nanoparticles that can be used as model systems to study the fundamental mechanisms of cellular uptake and intracellular trafficking.",
    "advisors": ["Francesco Stellacci"],
    "text": "Ligand shell morphology of water-soluble mixed-monolayer protected gold nanoparticles Nanoparticles comprise a versatile class of nanomaterials that consist of particles that have a characteristic length scale less than 100nm. They are on a similar length scale as many biological elements, so it is fitting that they are being used increasingly in biological systems for a variety of applications. Interesting properties of water-soluble metal nanoparticles that could lead to novel biological applications include bio-catalytic, sensing, and light scattering capabilities. We will present here the characterization of novel highly water-soluble gold nanoparticles that can be used as model systems to study the fundamental mechanisms of cellular uptake and intracellular trafficking."
}, {
    "id": "oai:dspace.mit.edu:1721.1/45384",
    "title": "Technological assessment of silicon on lattice engineered substrate (SOLES) for optical applications",
    "abstract": "Over the past decade, much effort had been placed to integrate optoelectronic and electronic devices. Silicon on lattice engineered substrate (SOLES) had been developed for such purpose. As SOLES technology mature, a technological analysis is important so as to evaluate its potentials. In this report, a technological assessment will be done on the SOLES technology. This technology will be evaluated technologically and economically. The IP landscape in which SOLES would be applied is also looked into so as to ensure that there would be no infringement of intellectual properties. It has been concluded in this report that LED printer market would be a good introduction application for SOLES. A fabless company model is suggested as the start-up approach. A simple cost model had been done in this report. Cost of fabrication is expected to be reduced by 13% by changing current fabrication process to SOLES process. On the final note, if SOLES could capture a market share in the world market of 0.1% to 1% , the expected profit was projected to be about USD$1.32 M over the next five years, with an expected sales of about 210 thousand unit, averaging about USD$264K per annum.",
    "advisors": ["Eugene A. Fitzgerald"],
    "text": "Technological assessment of silicon on lattice engineered substrate (SOLES) for optical applications Over the past decade, much effort had been placed to integrate optoelectronic and electronic devices. Silicon on lattice engineered substrate (SOLES) had been developed for such purpose. As SOLES technology mature, a technological analysis is important so as to evaluate its potentials. In this report, a technological assessment will be done on the SOLES technology. This technology will be evaluated technologically and economically. The IP landscape in which SOLES would be applied is also looked into so as to ensure that there would be no infringement of intellectual properties. It has been concluded in this report that LED printer market would be a good introduction application for SOLES. A fabless company model is suggested as the start-up approach. A simple cost model had been done in this report. Cost of fabrication is expected to be reduced by 13% by changing current fabrication process to SOLES process. On the final note, if SOLES could capture a market share in the world market of 0.1% to 1% , the expected profit was projected to be about USD$1.32 M over the next five years, with an expected sales of about 210 thousand unit, averaging about USD$264K per annum."
}, {
    "id": "oai:dspace.mit.edu:1721.1/92083",
    "title": "Submarine propulsion shaft life : probabilistic prediction and extension through prevention of water ingress",
    "abstract": "Submarine propulsion shafts have demonstrated acceptable reliability performance when inspected and refurbished at least every 6 years. Designers wish to extend the inspection interval to 12 years without sacrificing reliability. This interval is unprecedented, as no known submarine shafting system is currently operated with this inspection cycle, nor are any known commercial vessel shafts. Experience and improved design have eliminated many threats to the life of a submarine shaft, but inspections of existing shafts show a high percentage with signs of wetting, leaving designers with less-than-acceptable confidence to approve this longer inspection interval due to the possibility of corrosion fatigue failure. This thesis uses probabilistic models from literature for pitting and cracking of wetted shafts, along with Monte Carlo simulations, to predict results of shafts inspections. Each possible water ingress distribution is analyzed by simulating shafts under 6 years of exposure to the water ingress, pitting, and cracking models in order to estimate the effects of corrosion fatigue. A water ingress distribution that predicts inspection results closest to actual inspection results is identified. Some information about water ingress is inferred from this distribution. Next, using the same literature models, a water ingress distribution that predicts similar performance at 12 years is identified. It is shown that the time a shaft is in service prior to becoming wetted must increase substantially. Predicted failure rates are low, but they are still higher than acceptable. This thesis recommends that inspection procedures are updated to provide more robust information for future analyses, which would better identify the appropriate distributions and greatly reduce uncertainty.",
    "advisors": ["Alexander H. Slocum", "Ronald G. Ballinger"],
    "text": "Submarine propulsion shaft life : probabilistic prediction and extension through prevention of water ingress Submarine propulsion shafts have demonstrated acceptable reliability performance when inspected and refurbished at least every 6 years. Designers wish to extend the inspection interval to 12 years without sacrificing reliability. This interval is unprecedented, as no known submarine shafting system is currently operated with this inspection cycle, nor are any known commercial vessel shafts. Experience and improved design have eliminated many threats to the life of a submarine shaft, but inspections of existing shafts show a high percentage with signs of wetting, leaving designers with less-than-acceptable confidence to approve this longer inspection interval due to the possibility of corrosion fatigue failure. This thesis uses probabilistic models from literature for pitting and cracking of wetted shafts, along with Monte Carlo simulations, to predict results of shafts inspections. Each possible water ingress distribution is analyzed by simulating shafts under 6 years of exposure to the water ingress, pitting, and cracking models in order to estimate the effects of corrosion fatigue. A water ingress distribution that predicts inspection results closest to actual inspection results is identified. Some information about water ingress is inferred from this distribution. Next, using the same literature models, a water ingress distribution that predicts similar performance at 12 years is identified. It is shown that the time a shaft is in service prior to becoming wetted must increase substantially. Predicted failure rates are low, but they are still higher than acceptable. This thesis recommends that inspection procedures are updated to provide more robust information for future analyses, which would better identify the appropriate distributions and greatly reduce uncertainty."
}, {
    "id": "oai:dspace.mit.edu:1721.1/28871",
    "title": "The impact of improved materials in poly(vinyl chloride)-based endotracheal tubes",
    "abstract": "Endotracheal tubes (ETs) are used to aid artificial ventilation in millions of medical patients every year and are known to invoke the proliferative phase in the cell linings. The technical objective of this work was to investigate in vitro the interaction between epithelial cells and current poly(vinyl chloride)-based ET materials, as well as some ET samples embedded with materials intended to improve biocompatibility properties of the tubes. Cells were grown in wells with small samples of ETs and proliferation and migration were observed using phase microscopy. ETs appeared to increase cell growth wherever cells came into contact with the material. The cell morphology altered once in contact with the ET sample. Cell growth on and around the ETs with embedded material appeared to slow, but had significant visible changes in cell morphology. The need for continued research in this area of research and development and future steps are addressed. A proposal for starting a company around a safer material for use in endotracheal tubes was developed and showed significant barriers to entry for a small medical device company with a single product. Subsequently, the most appropriate approach for bringing a new ET to the market would be by way of a licensing with an existing manufacturer.",
    "advisors": ["Christine Ortiz"],
    "text": "The impact of improved materials in poly(vinyl chloride)-based endotracheal tubes Endotracheal tubes (ETs) are used to aid artificial ventilation in millions of medical patients every year and are known to invoke the proliferative phase in the cell linings. The technical objective of this work was to investigate in vitro the interaction between epithelial cells and current poly(vinyl chloride)-based ET materials, as well as some ET samples embedded with materials intended to improve biocompatibility properties of the tubes. Cells were grown in wells with small samples of ETs and proliferation and migration were observed using phase microscopy. ETs appeared to increase cell growth wherever cells came into contact with the material. The cell morphology altered once in contact with the ET sample. Cell growth on and around the ETs with embedded material appeared to slow, but had significant visible changes in cell morphology. The need for continued research in this area of research and development and future steps are addressed. A proposal for starting a company around a safer material for use in endotracheal tubes was developed and showed significant barriers to entry for a small medical device company with a single product. Subsequently, the most appropriate approach for bringing a new ET to the market would be by way of a licensing with an existing manufacturer."
}, {
    "id": "oai:dspace.mit.edu:1721.1/62746",
    "title": "Phenomenological study of Au and Pt nanowires grown in porous alumina scaffolds",
    "abstract": "Porous anodic aluminum oxide, commonly known as AAO, has been widely used as a scaffold to synthesize nanowires and nanotubes. The porous alumina structure can be obtained from a simple electrochemical oxidation process, applying a positive voltage to an aluminum film placed in an electrolyte, and resulting in the formation of periodically arranged arrays of pores. It is possible to tune pore diameters and pore spacing by adjusting parameters such as the type of electrolyte, the pH, and the applied voltage. Once the barrier oxide is removed from the bottom of the pores, porous alumina that has been formed on conducting substrates can be used for growth of metal nanowires using electrodeposition. We synthesized Au and Pt nanowire arrays on Au or Pt substrates. During electrodeposition, Au nanowires that grew out of the pores developed a pyramidlike faceted shape. This was not observed for overgrown Pt nanowires. To understand this phenomenon, the microstructure and crystallographic characteristics of the overgrown Au and Pt nanowires were studied using SEM, TEM and XRD. It was found that the overgrown Au caps were single crystalline with (111) facets and textured along the [100] direction, while the Au nanowires in the pores were poly-crystalline with a [11 11] texture. Pt nanowires grown in pores were also polycrystalline and had a [111] texture, but the grain size was much smaller than that of the Au. In contrast with Au, no change of texture or microstructure was observed when Pt grew out of pores. The structure change observed for Au involves nucleation of a new crystal with a (100) texture. This is thought to be related to the changes in the overpotential that occur when the Au emerges from the pores.",
    "advisors": ["Carl V. Thompson"],
    "text": "Phenomenological study of Au and Pt nanowires grown in porous alumina scaffolds Porous anodic aluminum oxide, commonly known as AAO, has been widely used as a scaffold to synthesize nanowires and nanotubes. The porous alumina structure can be obtained from a simple electrochemical oxidation process, applying a positive voltage to an aluminum film placed in an electrolyte, and resulting in the formation of periodically arranged arrays of pores. It is possible to tune pore diameters and pore spacing by adjusting parameters such as the type of electrolyte, the pH, and the applied voltage. Once the barrier oxide is removed from the bottom of the pores, porous alumina that has been formed on conducting substrates can be used for growth of metal nanowires using electrodeposition. We synthesized Au and Pt nanowire arrays on Au or Pt substrates. During electrodeposition, Au nanowires that grew out of the pores developed a pyramidlike faceted shape. This was not observed for overgrown Pt nanowires. To understand this phenomenon, the microstructure and crystallographic characteristics of the overgrown Au and Pt nanowires were studied using SEM, TEM and XRD. It was found that the overgrown Au caps were single crystalline with (111) facets and textured along the [100] direction, while the Au nanowires in the pores were poly-crystalline with a [11 11] texture. Pt nanowires grown in pores were also polycrystalline and had a [111] texture, but the grain size was much smaller than that of the Au. In contrast with Au, no change of texture or microstructure was observed when Pt grew out of pores. The structure change observed for Au involves nucleation of a new crystal with a (100) texture. This is thought to be related to the changes in the overpotential that occur when the Au emerges from the pores."
}, {
    "id": "oai:dspace.mit.edu:1721.1/69674",
    "title": "Chromium (III), Titanium (III), and Vanadium (IV) sensitization of rare earth complexes for luminescent solar concentrator applications",
    "abstract": "High optical concentrations without excess heating in a stationary system can be achieved with a luminescent solar concentrator (LSC). Neodymium (Nd) and ytterbium (Yb) are excellent infrared LSC materials: inexpensive, abundant, efficient, and spectrally well-matched to high-performance silicon solar cells. These rare earth ions are reasonably transparent to their own radiation and capable of generating high optical concentrations. Neodymium's and ytterbium's disadvantage is their relatively poor absorption overlap with the visible spectrum. Transition metals such as chromium (Cr), titanium (Ti), and vanadium (V) have broadband absorption covering the visible and near-infrared and can efficiently sensitize neodymium and ytterbium through a non-radiative energy transfer process. Chromium, titanium, and vanadium containing glasses were fabricated using a custom designed glass making furnace. The optical properties including molar absorption coefficient, photoluminescence spectrum, and energy transfer characteristics were investigated to determine the suitability for LSC applications. Glasses containing Cr or V co-doped with Nd or Yb demonstrated energy transfer from the transition metal to the rare earth, a fundamental step toward integration into a LSC. Titanium co-doped glasses did not exhibit photoluminescence or energy transfer. Chromium co-doped glasses exhibit both forward and backward energy transfer. Vanadium holds the best promise as a sensitizer for LSC applications.",
    "advisors": ["Marc Baldo", "Harry L. Tuller"],
    "text": "Chromium (III), Titanium (III), and Vanadium (IV) sensitization of rare earth complexes for luminescent solar concentrator applications High optical concentrations without excess heating in a stationary system can be achieved with a luminescent solar concentrator (LSC). Neodymium (Nd) and ytterbium (Yb) are excellent infrared LSC materials: inexpensive, abundant, efficient, and spectrally well-matched to high-performance silicon solar cells. These rare earth ions are reasonably transparent to their own radiation and capable of generating high optical concentrations. Neodymium's and ytterbium's disadvantage is their relatively poor absorption overlap with the visible spectrum. Transition metals such as chromium (Cr), titanium (Ti), and vanadium (V) have broadband absorption covering the visible and near-infrared and can efficiently sensitize neodymium and ytterbium through a non-radiative energy transfer process. Chromium, titanium, and vanadium containing glasses were fabricated using a custom designed glass making furnace. The optical properties including molar absorption coefficient, photoluminescence spectrum, and energy transfer characteristics were investigated to determine the suitability for LSC applications. Glasses containing Cr or V co-doped with Nd or Yb demonstrated energy transfer from the transition metal to the rare earth, a fundamental step toward integration into a LSC. Titanium co-doped glasses did not exhibit photoluminescence or energy transfer. Chromium co-doped glasses exhibit both forward and backward energy transfer. Vanadium holds the best promise as a sensitizer for LSC applications."
}, {
    "id": "oai:dspace.mit.edu:1721.1/29930",
    "title": "Microstructural modeling of lithium battery electrodes",
    "abstract": "The transport of charged species in lithium ion batteries was studied from a microstructural point of view. Electron transport was analyzed using percolation theory and comparison with other conductor-insulator composites. An in situ filter pressing apparatus was designed and constructed in order to determine the percolation threshold in composite electrode systems. In addition, the effect of inter-particle interactions was qualitatively examined. The percolation threshold was determined to occur between 10 and 13 volume percent conductor loading for liquid electrolyte systems. In dissolved polymer systems, polymer adsorption shifted the percolation threshold to 25 volume percent. Ion transport was analyzed using a computer model designed by Doyle and Newman. Microstructural solutions to ameliorate the rate limiting steps were proposed and tested. Battery simulations demonstrated that the rate capability of lithium batteries could be improved both by utilizing plate-like particles aligned in parallel with the current flow, and also by producing a porosity gradient in the electrode. Using particles aligned parallel to the current flow allowed the elimination of tortuosity from the ion path. Graded electrodes provided superior ion transport near the electrode surface, where the ionic current is greatest, while additional capacity was available in the depth of the electrode, where ion transport was not as critical.",
    "advisors": ["Yet-Ming Chiang"],
    "text": "Microstructural modeling of lithium battery electrodes The transport of charged species in lithium ion batteries was studied from a microstructural point of view. Electron transport was analyzed using percolation theory and comparison with other conductor-insulator composites. An in situ filter pressing apparatus was designed and constructed in order to determine the percolation threshold in composite electrode systems. In addition, the effect of inter-particle interactions was qualitatively examined. The percolation threshold was determined to occur between 10 and 13 volume percent conductor loading for liquid electrolyte systems. In dissolved polymer systems, polymer adsorption shifted the percolation threshold to 25 volume percent. Ion transport was analyzed using a computer model designed by Doyle and Newman. Microstructural solutions to ameliorate the rate limiting steps were proposed and tested. Battery simulations demonstrated that the rate capability of lithium batteries could be improved both by utilizing plate-like particles aligned in parallel with the current flow, and also by producing a porosity gradient in the electrode. Using particles aligned parallel to the current flow allowed the elimination of tortuosity from the ion path. Graded electrodes provided superior ion transport near the electrode surface, where the ionic current is greatest, while additional capacity was available in the depth of the electrode, where ion transport was not as critical."
}, {
    "id": "oai:dspace.mit.edu:1721.1/37131",
    "title": "Material evaluation and selection processes to enable design for manufacture",
    "abstract": "In order to optimize product designs it is necessary to quickly evaluate many candidate materials in terms of performance and processing costs. Evaluation using physical prototypes yields concrete results but is time intensive and costly when dealing with multiple optimization objectives. As an alternative, computer aided simulation is a reliable means of material evaluation and selection, is increasingly available to smaller companies due to the shrinking cost of computation, and is essential for handling the dual optimization objectives of manufacturability and performance in a timely and cost effective manner. To support this thesis, the author first examines iRobot Corporation's current process of experimental trial and error for evaluating and selecting a polymer material for use in the wheels of its robotic military vehicles. The author then demonstrates that the experimental derived performance results can be reasonably predicted using the viscoelastic properties of polymers, as captured in such models as the standard linear solid model, and that this predictability can be used to quickly simulate wheel performance with computer aided engineering (CAE) tools.",
    "advisors": ["Stephen C. Graves", "Randolph E. Kirchain, Jr"],
    "text": "Material evaluation and selection processes to enable design for manufacture In order to optimize product designs it is necessary to quickly evaluate many candidate materials in terms of performance and processing costs. Evaluation using physical prototypes yields concrete results but is time intensive and costly when dealing with multiple optimization objectives. As an alternative, computer aided simulation is a reliable means of material evaluation and selection, is increasingly available to smaller companies due to the shrinking cost of computation, and is essential for handling the dual optimization objectives of manufacturability and performance in a timely and cost effective manner. To support this thesis, the author first examines iRobot Corporation's current process of experimental trial and error for evaluating and selecting a polymer material for use in the wheels of its robotic military vehicles. The author then demonstrates that the experimental derived performance results can be reasonably predicted using the viscoelastic properties of polymers, as captured in such models as the standard linear solid model, and that this predictability can be used to quickly simulate wheel performance with computer aided engineering (CAE) tools."
}, {
    "id": "oai:dspace.mit.edu:1721.1/54563",
    "title": "Implementations of electric vehicle system based on solar energy in Singapore : assessment of solar photovoltaic systems",
    "abstract": "To evaluate the feasibility of solar energy based Electric Vehicle Transportation System in Singapore, the state of the art Photovoltaic Systems have been reviewed in this report with a focus on solar cell technologies. Various solar cell technologies were evaluated based on characteristics such as efficiency, reliability and cost to identify a best working one under Singapore's hot and humid climate. Commercial CdTe modules were found to have the best efficiency to cost ratio, making them the best module choice in land-scarce and tropical Singapore. Based on the market price and characteristics of CdTe modules from manufacturer First Solar Ltd, two PV systems based on an apartment model and a private house model were evaluated. The cost of electricity from a relatively large scale grid-tied PV system is found to be at around US$0.173/kWh which is not market competitive with the utility electricity price of US$0.109/kWh in Singapore. But with enough capital funding and government incentives such as rebate or feed-in price tariff, PV electricity generation could become economically feasible. The small private house system is found not economical as a means of household electricity generation even with current status of government rebate. When carbon trading is considered, the current trading price has to be increased by around 7 times of the current value or 3 times of the predicted price at 2016 to offset the difference with the utility electricity price.",
    "advisors": ["Yet-Ming Chiang", "Andy Chu"],
    "text": "Implementations of electric vehicle system based on solar energy in Singapore : assessment of solar photovoltaic systems To evaluate the feasibility of solar energy based Electric Vehicle Transportation System in Singapore, the state of the art Photovoltaic Systems have been reviewed in this report with a focus on solar cell technologies. Various solar cell technologies were evaluated based on characteristics such as efficiency, reliability and cost to identify a best working one under Singapore's hot and humid climate. Commercial CdTe modules were found to have the best efficiency to cost ratio, making them the best module choice in land-scarce and tropical Singapore. Based on the market price and characteristics of CdTe modules from manufacturer First Solar Ltd, two PV systems based on an apartment model and a private house model were evaluated. The cost of electricity from a relatively large scale grid-tied PV system is found to be at around US$0.173/kWh which is not market competitive with the utility electricity price of US$0.109/kWh in Singapore. But with enough capital funding and government incentives such as rebate or feed-in price tariff, PV electricity generation could become economically feasible. The small private house system is found not economical as a means of household electricity generation even with current status of government rebate. When carbon trading is considered, the current trading price has to be increased by around 7 times of the current value or 3 times of the predicted price at 2016 to offset the difference with the utility electricity price."
}, {
    "id": "oai:dspace.mit.edu:1721.1/62740",
    "title": "The challenges of organic polymer solar cells",
    "abstract": "The technical and commercial prospects of polymer solar cells were evaluated. Polymer solar cells are an attractive approach to fabricate and deploy roll-to-roll processed solar cells that are reasonably efficient (total PV system efficiency>10%), scalable and inexpensive to make and install (<100 $/m2). At a cost of less than 1$/Wp, PV systems will be able to generate electricity in most geographical locations at costs competitive to coal's electricity (at 5-6 cents/KWh) and will make electricity available to more people around the world (-20% of the world population is without electricity). In this chapter, we explore organic polymer solar cell technology. The first chapter discusses the potential impact of solar cells on electricity markets and the developing world and its promise as a sustainable scalable low carbon energy technology. The second chapter discusses some of the complexity in designing polymer solar cells from new materials and the physics involved in some detail. I also discuss the need to develop new solution processed transparent conductors, cost effective encapsulation and long life flexible substrates. The third chapter discusses polymer solar cells cost estimates and how innovative designs for new modules could reduce installation costs. In the final chapter I discussed the prospects for commercialization of polymer solar cells in several niche markets and in grid electricity markets; the commiseration prospects are dim especially with the uncertainty in the potential improvement in polymer solar cell stability.",
    "advisors": ["Jeffrey C. Grossman"],
    "text": "The challenges of organic polymer solar cells The technical and commercial prospects of polymer solar cells were evaluated. Polymer solar cells are an attractive approach to fabricate and deploy roll-to-roll processed solar cells that are reasonably efficient (total PV system efficiency>10%), scalable and inexpensive to make and install (<100 $/m2). At a cost of less than 1$/Wp, PV systems will be able to generate electricity in most geographical locations at costs competitive to coal's electricity (at 5-6 cents/KWh) and will make electricity available to more people around the world (-20% of the world population is without electricity). In this chapter, we explore organic polymer solar cell technology. The first chapter discusses the potential impact of solar cells on electricity markets and the developing world and its promise as a sustainable scalable low carbon energy technology. The second chapter discusses some of the complexity in designing polymer solar cells from new materials and the physics involved in some detail. I also discuss the need to develop new solution processed transparent conductors, cost effective encapsulation and long life flexible substrates. The third chapter discusses polymer solar cells cost estimates and how innovative designs for new modules could reduce installation costs. In the final chapter I discussed the prospects for commercialization of polymer solar cells in several niche markets and in grid electricity markets; the commiseration prospects are dim especially with the uncertainty in the potential improvement in polymer solar cell stability."
}, {
    "id": "oai:dspace.mit.edu:1721.1/53247",
    "title": "Genetically engineered phage fibers and coatings for antibacterial applications",
    "abstract": "Multifunctionality can be imparted to protein-based fibers and coatings via either synthetic or biological approaches. Here, we demonstrate potent antimicrobial functionality of genetically engineered, phage-based fibers and fiber coatings, processed at room temperature. Facile genetic engineering of the M13 virus (bacteriophage) genome leverages the well-known antibacterial properties of silver ions to kill bacteria. Predominant expression of negatively-charged glutamic acid (E3) peptides on the pVIII major coat proteins of M13 bacteriophage (or phage) enables solution-based, electrostatic binding of silver ions and subsequent reduction to metallic silver along the phage length. Antibacterial fibers of micrometer-scale diameters are constructed from such E3-modified phage, via wet-spinning and glutaraldehyde-crosslinking of the E3-modified phage. Silverization of the free-standing fibers is confirmed via energy-dispersive spectroscopy (EDS) and inductively-coupled plasma atomic emission spectroscopy (ICP-AES), showing -0.61,pg/cm of silver on E3-Ag fibers. This degree of silverization is threefold greater than that attainable for the unmodified M13-Ag fibers. Conferred bactericidal functionality is determined via live-dead staining and a modified disk-diffusion (Kirby-Bauer) measure of zone of inhibition (Zol) against Staphylococcus epidermidis and Escherichia coli bacterial strains. Live-dead staining and Zol distance measurements indicate increased bactericidal activity in the genetically engineered virus fibers attached to silver.",
    "advisors": ["Krystyn J. van Vliet", "Angela M. Belcher"],
    "text": "Genetically engineered phage fibers and coatings for antibacterial applications Multifunctionality can be imparted to protein-based fibers and coatings via either synthetic or biological approaches. Here, we demonstrate potent antimicrobial functionality of genetically engineered, phage-based fibers and fiber coatings, processed at room temperature. Facile genetic engineering of the M13 virus (bacteriophage) genome leverages the well-known antibacterial properties of silver ions to kill bacteria. Predominant expression of negatively-charged glutamic acid (E3) peptides on the pVIII major coat proteins of M13 bacteriophage (or phage) enables solution-based, electrostatic binding of silver ions and subsequent reduction to metallic silver along the phage length. Antibacterial fibers of micrometer-scale diameters are constructed from such E3-modified phage, via wet-spinning and glutaraldehyde-crosslinking of the E3-modified phage. Silverization of the free-standing fibers is confirmed via energy-dispersive spectroscopy (EDS) and inductively-coupled plasma atomic emission spectroscopy (ICP-AES), showing -0.61,pg/cm of silver on E3-Ag fibers. This degree of silverization is threefold greater than that attainable for the unmodified M13-Ag fibers. Conferred bactericidal functionality is determined via live-dead staining and a modified disk-diffusion (Kirby-Bauer) measure of zone of inhibition (Zol) against Staphylococcus epidermidis and Escherichia coli bacterial strains. Live-dead staining and Zol distance measurements indicate increased bactericidal activity in the genetically engineered virus fibers attached to silver."
}, {
    "id": "oai:dspace.mit.edu:1721.1/44424",
    "title": "Novel approaches to low temperature transient liquid phase bonding in the In-Sn/Cu and In-Sn-Bi/Cu systems",
    "abstract": "A fluxless low temperature transient liquid phase (LTTLP) bonding process was studied as a method of producing Cu/Cu joints below 125C and 75C using interlayer alloys from the In-Sn and In-Sn-Bi systems. Using thermodynamic models, three different compositions (wt. %) of base alloys were chosen to accomplish this task: 50In-43.6Sn-6.4Bi (Tm = 110C) and eutectic 50.9In-49.1Sn (Tm = 120C) alloys were used for bonding at 125C and a eutectic 48.3In-15.6Sn-36.1Bi (Tm = 60C) alloy was used for bonding at 75C. In addition, novel approaches to TLP bonding, including the addition of base material to the interlayer alloy and application of an electroless Ni diffusion barrier layer, were employed in an attempt to optimize this joining method. The LTTLP processes were assessed based on their abilities to produce joints with minimal thickness, high reflow temperatures, and good mechanical properties at room/elevated temperatures. It was found that interlayer alloys containing higher Bi contents produced the thinnest joints, with the 48.3In-15.6Sn-36.1Bi alloy producing joints on the order of 10 gm. Increases in nominal Cu composition of the interlayer alloy tended to form larger joints. Application of the Ni layer was observed to decrease the growth rate of the eutectic In-Sn joints made with 5 wt % Cu additions. Shear tests were performed on the joints at room (25C) and operating (service) temperatures (100C). Most of the TLP joints had room temperature shear strengths around 13,000 - 17,000 psi (= 90 - 120 MPa), although increases in strength were observed for eutectic In-Sn joints with 2.5 and 5 wt% Cu additions. At operating temperature, TLP joints made within the In-Sn-Cu system were found to have strengths an order of magnitude higher than those made in the In-Sn-Bi-Cu system.",
    "advisors": ["Thomas W. Eagar"],
    "text": "Novel approaches to low temperature transient liquid phase bonding in the In-Sn/Cu and In-Sn-Bi/Cu systems A fluxless low temperature transient liquid phase (LTTLP) bonding process was studied as a method of producing Cu/Cu joints below 125C and 75C using interlayer alloys from the In-Sn and In-Sn-Bi systems. Using thermodynamic models, three different compositions (wt. %) of base alloys were chosen to accomplish this task: 50In-43.6Sn-6.4Bi (Tm = 110C) and eutectic 50.9In-49.1Sn (Tm = 120C) alloys were used for bonding at 125C and a eutectic 48.3In-15.6Sn-36.1Bi (Tm = 60C) alloy was used for bonding at 75C. In addition, novel approaches to TLP bonding, including the addition of base material to the interlayer alloy and application of an electroless Ni diffusion barrier layer, were employed in an attempt to optimize this joining method. The LTTLP processes were assessed based on their abilities to produce joints with minimal thickness, high reflow temperatures, and good mechanical properties at room/elevated temperatures. It was found that interlayer alloys containing higher Bi contents produced the thinnest joints, with the 48.3In-15.6Sn-36.1Bi alloy producing joints on the order of 10 gm. Increases in nominal Cu composition of the interlayer alloy tended to form larger joints. Application of the Ni layer was observed to decrease the growth rate of the eutectic In-Sn joints made with 5 wt % Cu additions. Shear tests were performed on the joints at room (25C) and operating (service) temperatures (100C). Most of the TLP joints had room temperature shear strengths around 13,000 - 17,000 psi (= 90 - 120 MPa), although increases in strength were observed for eutectic In-Sn joints with 2.5 and 5 wt% Cu additions. At operating temperature, TLP joints made within the In-Sn-Cu system were found to have strengths an order of magnitude higher than those made in the In-Sn-Bi-Cu system."
}, {
    "id": "oai:dspace.mit.edu:1721.1/101797",
    "title": "Synthesis of functionalized few layer graphene via electrochemical expansion",
    "abstract": "Single layer graphene is a nearly transparent two-dimensional honeycomb sp2 hybridized carbon lattice, and has received immense attention for its potential application in next-generation electronic devices, composite materials, and energy storage devices. This attention is a result of its desirable and intriguing electrical, mechanical, and chemical properties. However, mass production of high-quality, solution-processable graphene via a simple low-cost method remains a major challenge. Recently, electrochemical exfoliation of graphite has attracted attention as an easy, fast, and environmentally friendly approach to the production of high-quality graphene. This route solution phase approach complements the original micromechanical cleavage production of high quality graphite samples and also involved a chemically activated intermediate state that facilitates functionalization. In this thesis we demonstrate a highly efficient electrochemical exfoliation of graphite in organic solvent containing tetraalkylammonium salts, avoiding oxidation of graphene and the associated defect generation encountered with the broadly used Hummer's method. The expansion and charging of the graphite by intercalation of cations facilitates the functionalization of the graphene basal surfaces. Electrochemically enhanced diazonium functionalization of the expanded graphite was performed. The exfoliated graphene platelets were analyzed by Raman spectroscopy, to quantify defect states and the degree of exfoliation. Additional microscopy techniques provided additional insight into the chemical state and structure of the graphene sheets.",
    "advisors": ["Timothy M. Swager"],
    "text": "Synthesis of functionalized few layer graphene via electrochemical expansion Single layer graphene is a nearly transparent two-dimensional honeycomb sp2 hybridized carbon lattice, and has received immense attention for its potential application in next-generation electronic devices, composite materials, and energy storage devices. This attention is a result of its desirable and intriguing electrical, mechanical, and chemical properties. However, mass production of high-quality, solution-processable graphene via a simple low-cost method remains a major challenge. Recently, electrochemical exfoliation of graphite has attracted attention as an easy, fast, and environmentally friendly approach to the production of high-quality graphene. This route solution phase approach complements the original micromechanical cleavage production of high quality graphite samples and also involved a chemically activated intermediate state that facilitates functionalization. In this thesis we demonstrate a highly efficient electrochemical exfoliation of graphite in organic solvent containing tetraalkylammonium salts, avoiding oxidation of graphene and the associated defect generation encountered with the broadly used Hummer's method. The expansion and charging of the graphite by intercalation of cations facilitates the functionalization of the graphene basal surfaces. Electrochemically enhanced diazonium functionalization of the expanded graphite was performed. The exfoliated graphene platelets were analyzed by Raman spectroscopy, to quantify defect states and the degree of exfoliation. Additional microscopy techniques provided additional insight into the chemical state and structure of the graphene sheets."
}, {
    "id": "oai:dspace.mit.edu:1721.1/37683",
    "title": "Technical and economic feasibility of a high-temperature self-assembling battery",
    "abstract": "A conceptual high-temperature battery system for large-scale grid power applications was proposed, described, and evaluated. Unlike conventional battery technologies whose maximum current rate is constrained by at least one solid phase, the novel three-layer liquid-phase electrode-electrolyte-electrode cell facilitates high diffusivity and facile interfacial kinetics, which results in rapid ion transport and low activation overpotential. In addition to the extremely high currents enabled by the absence of solid-liquid interfaces, this cell configuration represents a robust design that can be easily manufactured. The molten components will self assemble due to their immiscibility and different densities. Another key feature is that a molten metalloid acts as the positive electrode, while an alkali or alkaline earth metal acts as the negative electrode, providing two electronically conductive molten electrodes. The cell is estimated to have a lifespan of 10-15 years with >3,000 deep-discharge cycles, to require minimal maintenance, and to supply 1-5 A/cm2 at 0.9 V with 80% DC-DC cycle-efficiency.",
    "advisors": ["Donald R. Sadoway", "Gerbrand Ceder"],
    "text": "Technical and economic feasibility of a high-temperature self-assembling battery A conceptual high-temperature battery system for large-scale grid power applications was proposed, described, and evaluated. Unlike conventional battery technologies whose maximum current rate is constrained by at least one solid phase, the novel three-layer liquid-phase electrode-electrolyte-electrode cell facilitates high diffusivity and facile interfacial kinetics, which results in rapid ion transport and low activation overpotential. In addition to the extremely high currents enabled by the absence of solid-liquid interfaces, this cell configuration represents a robust design that can be easily manufactured. The molten components will self assemble due to their immiscibility and different densities. Another key feature is that a molten metalloid acts as the positive electrode, while an alkali or alkaline earth metal acts as the negative electrode, providing two electronically conductive molten electrodes. The cell is estimated to have a lifespan of 10-15 years with >3,000 deep-discharge cycles, to require minimal maintenance, and to supply 1-5 A/cm2 at 0.9 V with 80% DC-DC cycle-efficiency."
}, {
    "id": "oai:dspace.mit.edu:1721.1/42143",
    "title": "A market analysis for high efficiency multi-junction solar cells grown on SiGe",
    "abstract": "Applications, markets and a cost model are presented for III-V multi-junction solar cells built on compositionally graded SiGe buffer layers currently being developed by professors Steven Ringell of Ohio State University and Eugene Fitzgerald of MIT. Potential markets are similar to those currently occupied by high efficiency multi-junction space solar cells grown on a Germanium substrate. Initial cost analysis shows that at production volumes similar to those of the state of the art, cost could be reduced by a factor of' four. Significant market share may be gained in both the space and terrestrial PV markets due to improved performance associated with superior materials properties advantages as well as production cost reductions.",
    "advisors": ["Eugene Fitzgerald"],
    "text": "A market analysis for high efficiency multi-junction solar cells grown on SiGe Applications, markets and a cost model are presented for III-V multi-junction solar cells built on compositionally graded SiGe buffer layers currently being developed by professors Steven Ringell of Ohio State University and Eugene Fitzgerald of MIT. Potential markets are similar to those currently occupied by high efficiency multi-junction space solar cells grown on a Germanium substrate. Initial cost analysis shows that at production volumes similar to those of the state of the art, cost could be reduced by a factor of' four. Significant market share may be gained in both the space and terrestrial PV markets due to improved performance associated with superior materials properties advantages as well as production cost reductions."
}, {
    "id": "oai:dspace.mit.edu:1721.1/62606",
    "title": "Potential applications of the natural design of internal explosion chambers in the bombardier beetle (Carabidae, Brachinus)",
    "abstract": "The Bombardier Beetle (Carabidae, Brachinus) has a unique form of defense mechanism which involves the explosive mixing of hydroquinones and hydrogen peroxide in its internal explosion chambers and using the resultant high pressure to spray out a heated corrosive fluid containing p-benzoquinones in a controlled direction [1][2]. Three salient features of the internal explosion chambers were found to be instrumental in withstanding the high pressures generated from the explosive mixing and protecting the Bombardier Beetle's internal organs [3]. Using simulations performed with finite element analysis, it was discovered that such design features employed by the Bombardier Beetle are suitable for incorporation into helmet designs. An in-depth analysis of the market potential of such a design with respect to the motorcycle helmet market is presented along with implementation strategies and proposed business plans.",
    "advisors": ["Christine Ortiz"],
    "text": "Potential applications of the natural design of internal explosion chambers in the bombardier beetle (Carabidae, Brachinus) The Bombardier Beetle (Carabidae, Brachinus) has a unique form of defense mechanism which involves the explosive mixing of hydroquinones and hydrogen peroxide in its internal explosion chambers and using the resultant high pressure to spray out a heated corrosive fluid containing p-benzoquinones in a controlled direction [1][2]. Three salient features of the internal explosion chambers were found to be instrumental in withstanding the high pressures generated from the explosive mixing and protecting the Bombardier Beetle's internal organs [3]. Using simulations performed with finite element analysis, it was discovered that such design features employed by the Bombardier Beetle are suitable for incorporation into helmet designs. An in-depth analysis of the market potential of such a design with respect to the motorcycle helmet market is presented along with implementation strategies and proposed business plans."
}, {
    "id": "oai:dspace.mit.edu:1721.1/101853",
    "title": "Numerically generating topology of the liner finish in internal combustion engines",
    "abstract": "Internal combustion (IC) engines are broadly utilized today. The friction caused by piston rings in IC engines contributes around 20% of the mechanical friction losses. The liner finish is the most critical parameter to define the tension and other design parameters of the piston rings for proper sealing. This work is focused on developing numerical approaches to generating liner finishes based on certain values of topology parameters. The generated surface is able to simulate the lubrication and dry contact behaviors of the original surface, so that the method is used to study the effects of various topology parameters on friction losses. First, methods to analyze, generate, test, and compare honed liner surfaces have been developed. The algorithm to analyze the topology parameters of honed surfaces is described. The honed surfaces are numerically generated and compared with the experimental data. Moreover, the topological variables are changed and the corresponding friction behaviors are studied. The relations between topology variables and friction losses are illustrated. We also developed a quantitative relation between two ISO standards describing the honed liner finish, so that the manufacturing industry can use the surface generation method in convenience. Second, attempts were made to simulate the break-in processes for honed liner finish. Measured and numerically generated surfaces are simulated and compared. The friction and pressure behaviors for lightly and heavily worn surfaces are compared with experimental data. Moreover, by tuning the worn parameters, the friction effective mean pressure (FMEP) curve can match the experimental data. Finally, the algorithm to numerically generate thermally sprayed liner finish is described. The hydrodynamic and dry contact friction behaviors for generated surfaces are compared with experimental data. Critical topology parameters are tuned and their effects on friction losses are studied. Moreover, the effects of the pores created by the plasma spraying processes on the lubrication behaviors are simulated.",
    "advisors": ["Tian Tian"],
    "text": "Numerically generating topology of the liner finish in internal combustion engines Internal combustion (IC) engines are broadly utilized today. The friction caused by piston rings in IC engines contributes around 20% of the mechanical friction losses. The liner finish is the most critical parameter to define the tension and other design parameters of the piston rings for proper sealing. This work is focused on developing numerical approaches to generating liner finishes based on certain values of topology parameters. The generated surface is able to simulate the lubrication and dry contact behaviors of the original surface, so that the method is used to study the effects of various topology parameters on friction losses. First, methods to analyze, generate, test, and compare honed liner surfaces have been developed. The algorithm to analyze the topology parameters of honed surfaces is described. The honed surfaces are numerically generated and compared with the experimental data. Moreover, the topological variables are changed and the corresponding friction behaviors are studied. The relations between topology variables and friction losses are illustrated. We also developed a quantitative relation between two ISO standards describing the honed liner finish, so that the manufacturing industry can use the surface generation method in convenience. Second, attempts were made to simulate the break-in processes for honed liner finish. Measured and numerically generated surfaces are simulated and compared. The friction and pressure behaviors for lightly and heavily worn surfaces are compared with experimental data. Moreover, by tuning the worn parameters, the friction effective mean pressure (FMEP) curve can match the experimental data. Finally, the algorithm to numerically generate thermally sprayed liner finish is described. The hydrodynamic and dry contact friction behaviors for generated surfaces are compared with experimental data. Critical topology parameters are tuned and their effects on friction losses are studied. Moreover, the effects of the pores created by the plasma spraying processes on the lubrication behaviors are simulated."
}, {
    "id": "oai:dspace.mit.edu:1721.1/34833",
    "title": "Implementation of a manufacturing process platform",
    "abstract": "As companies grow and innovate, they offer an increasing number of products. Product proliferation must be managed through a product development process, which is supported by key competencies of the company in the form of platforms. Product and technology platforms have been essential to the success of innovative companies. By leveraging core abilities, companies are able to bring products to market faster and at a lower cost for quality. In this research I present the concept of a manufacturing process platform and a framework for identifying and institutionalizing the platform. I present a case study of a manufacturing group in Eastman Kodak Company which has performed analysis of manufacturing processes and is attempting to implement a manufacturing process platform. Research for this thesis was conducted during a six and a half month internship with Eastman Kodak Company's High Performance Imaging Systems Manufacturing group in Rochester, NY. The internship was affiliated with the Massachusetts Institute of Technology's Leaders for Manufacturing Program.",
    "advisors": ["Roy Welsch", "Randolph Kirchain"],
    "text": "Implementation of a manufacturing process platform As companies grow and innovate, they offer an increasing number of products. Product proliferation must be managed through a product development process, which is supported by key competencies of the company in the form of platforms. Product and technology platforms have been essential to the success of innovative companies. By leveraging core abilities, companies are able to bring products to market faster and at a lower cost for quality. In this research I present the concept of a manufacturing process platform and a framework for identifying and institutionalizing the platform. I present a case study of a manufacturing group in Eastman Kodak Company which has performed analysis of manufacturing processes and is attempting to implement a manufacturing process platform. Research for this thesis was conducted during a six and a half month internship with Eastman Kodak Company's High Performance Imaging Systems Manufacturing group in Rochester, NY. The internship was affiliated with the Massachusetts Institute of Technology's Leaders for Manufacturing Program."
}, {
    "id": "oai:dspace.mit.edu:1721.1/39544",
    "title": "Novel molecular architectures from iptycene polymers",
    "abstract": "This thesis explored the incorporation of iptycenes into polymers as a means to enhance the mechanical properties. Iptycene structures were targeted because they possess a unique structural property called internal molecular free volume. When these bulky pendant groups are incorporated into the backbone of a polymer chain, they produce a novel chain architecture called molecular barbed wire. This molecular barbed wire architecture not only influences individual polymer chain dynamics but also induces lateral interactions between polymer chains through the minimization of internal molecular free volume. Work began by developing the concepts of internal molecular free volume and how to exploit its minimization. A formal definition for this property was created along with methods to identify and quantify its existence. Experimental results document the effects of introducing this property into polyesters and polycarbonates. Two specific novel inter-chain interactions were established: molecular threading and molecular interlocking. These steric interactions between polymers chains generate a non-bonded, network morphology and actively enhance the mechanical properties during deformation.",
    "advisors": ["Edwin L. Thomas"],
    "text": "Novel molecular architectures from iptycene polymers This thesis explored the incorporation of iptycenes into polymers as a means to enhance the mechanical properties. Iptycene structures were targeted because they possess a unique structural property called internal molecular free volume. When these bulky pendant groups are incorporated into the backbone of a polymer chain, they produce a novel chain architecture called molecular barbed wire. This molecular barbed wire architecture not only influences individual polymer chain dynamics but also induces lateral interactions between polymer chains through the minimization of internal molecular free volume. Work began by developing the concepts of internal molecular free volume and how to exploit its minimization. A formal definition for this property was created along with methods to identify and quantify its existence. Experimental results document the effects of introducing this property into polyesters and polycarbonates. Two specific novel inter-chain interactions were established: molecular threading and molecular interlocking. These steric interactions between polymers chains generate a non-bonded, network morphology and actively enhance the mechanical properties during deformation."
}, {
    "id": "oai:dspace.mit.edu:1721.1/30254",
    "title": "Mechanical characterization and in vivo operation of an implantable drug delivery MEMS device",
    "abstract": "The goal of this thesis was to advance an implantable drug delivery MEMS (MicroElectroMechanical Systems) device developed in our laboratory. This device was designed to locally deliver multiple substances in complex release profiles in order to maximize the effectiveness of drug therapies. It consists of an array of microreservoirs etched into a silicon substrate. Different types and dosages of drugs can be contained in these reservoirs capped by thin gold membranes. The drug release is achieved by the application of a small anodic potential on the gold membrane in a chloride containing medium (such as the body fluid). The gold membrane will corrode and disintegrate so that the drug contained within the reservoir is free to diffuse into the surrounding medium. Previous researchers have demonstrated in vitro and in vivo release of tracer molecules as well as a radiolabled chemotherapeutic agent (carmustine, or BCNU) from the device. However, systematic characterization of the mechanical and electrochemical behavior of gold membranes on the drug delivery device was necessary in order to achieve more reliable device performance and to demonstrate efficacy of BCNU delivered from the MEMS device against an experimental tumor model. A bulge test apparatus was constructed to characterize the mechanical properties of gold membranes. Uniform pressure was applied from underneath the gold membrane and the membrane deflection was measured using optical interferometry. Analyzing the deflection and pressure data allowed extraction of the elastic modulus and residual stress of the gold membrane.",
    "advisors": ["Michael J. Cima"],
    "text": "Mechanical characterization and in vivo operation of an implantable drug delivery MEMS device The goal of this thesis was to advance an implantable drug delivery MEMS (MicroElectroMechanical Systems) device developed in our laboratory. This device was designed to locally deliver multiple substances in complex release profiles in order to maximize the effectiveness of drug therapies. It consists of an array of microreservoirs etched into a silicon substrate. Different types and dosages of drugs can be contained in these reservoirs capped by thin gold membranes. The drug release is achieved by the application of a small anodic potential on the gold membrane in a chloride containing medium (such as the body fluid). The gold membrane will corrode and disintegrate so that the drug contained within the reservoir is free to diffuse into the surrounding medium. Previous researchers have demonstrated in vitro and in vivo release of tracer molecules as well as a radiolabled chemotherapeutic agent (carmustine, or BCNU) from the device. However, systematic characterization of the mechanical and electrochemical behavior of gold membranes on the drug delivery device was necessary in order to achieve more reliable device performance and to demonstrate efficacy of BCNU delivered from the MEMS device against an experimental tumor model. A bulge test apparatus was constructed to characterize the mechanical properties of gold membranes. Uniform pressure was applied from underneath the gold membrane and the membrane deflection was measured using optical interferometry. Analyzing the deflection and pressure data allowed extraction of the elastic modulus and residual stress of the gold membrane."
}, {
    "id": "oai:dspace.mit.edu:1721.1/46389",
    "title": "First-principles transition-metal catalysis : efficient and accurate approaches for studying enzymatic systems",
    "abstract": "(cont.) We apply our approach to several paradigmatic systems: spin state splittings and structural properties of Fe2 and other small molecules as well as the addition-elimination reactions of hydrogen and methane on FeO+ to form water and methanol, respectively. We find that errors from common density functionals which are over 1.0 eV are greatly reduced to on average 0.1 eV when the DFT+U approach is implemented as compared against experiment and highly accurate but expensive quantum chemistry. We also improve structural and vibrational properties, ground state spin identification for a given configuration, and qualitative descriptions of reaction mechanism. Thanks to the minimal overhead of our DFT+U approach, we have also studied properties of systems of over one thousand electrons in size: in particular, the spin density profiles of functionalized cobalt porphyrins on a metal slab support and the reaction mechanism of the halogenating non-heme Fe(II) enzyme, SyrB2. Efficient and accurate study of transition metal chemistry paves the way for predictive and targeted design of catalysts that provide unique solutions for green chemistry and optimal harnessing of alternative energy sources.",
    "advisors": ["Nicola Marzari"],
    "text": "First-principles transition-metal catalysis : efficient and accurate approaches for studying enzymatic systems (cont.) We apply our approach to several paradigmatic systems: spin state splittings and structural properties of Fe2 and other small molecules as well as the addition-elimination reactions of hydrogen and methane on FeO+ to form water and methanol, respectively. We find that errors from common density functionals which are over 1.0 eV are greatly reduced to on average 0.1 eV when the DFT+U approach is implemented as compared against experiment and highly accurate but expensive quantum chemistry. We also improve structural and vibrational properties, ground state spin identification for a given configuration, and qualitative descriptions of reaction mechanism. Thanks to the minimal overhead of our DFT+U approach, we have also studied properties of systems of over one thousand electrons in size: in particular, the spin density profiles of functionalized cobalt porphyrins on a metal slab support and the reaction mechanism of the halogenating non-heme Fe(II) enzyme, SyrB2. Efficient and accurate study of transition metal chemistry paves the way for predictive and targeted design of catalysts that provide unique solutions for green chemistry and optimal harnessing of alternative energy sources."
}, {
    "id": "oai:dspace.mit.edu:1721.1/39482",
    "title": "Particle sensors based on amplified quenching of conjugated polymers for biosensing applications",
    "abstract": "Conjugated polymers (CP)s display unique material properties that allow for implementation as sensors. For sensors to operate in complex biological environments, it is important to address the issues of sensitivity and specificity. To develop these attributes in a biosensor design, CPbased core-shell particles have been investigated as potential material platforms to detect protease activity. CP-based particles have greater sensitivity versus CPs in solution due to interchain and intrachain interactions afforded in the solid state. The CP core of the particle can be made using layer-by-layer assembly, a versatile technique that forms uniform polymeric films through non-covalent interactions. To measure the response of CP core particles in aqueous environments, a quantitative ratiometric approach was developed to account for system fluctuations encountered with particle dispersions. This method can help assess the molecular design of polymers and quenchers in a systematic approach. CP core particles, because of their electrostatic charge, suffer from nonspecific interactions with other charged species, and thus encapsulating CP particles with a hydrogel shell should create sensor materials with higher specificity.",
    "advisors": ["Timothy M. Swager", "Michael F. Rubner"],
    "text": "Particle sensors based on amplified quenching of conjugated polymers for biosensing applications Conjugated polymers (CP)s display unique material properties that allow for implementation as sensors. For sensors to operate in complex biological environments, it is important to address the issues of sensitivity and specificity. To develop these attributes in a biosensor design, CPbased core-shell particles have been investigated as potential material platforms to detect protease activity. CP-based particles have greater sensitivity versus CPs in solution due to interchain and intrachain interactions afforded in the solid state. The CP core of the particle can be made using layer-by-layer assembly, a versatile technique that forms uniform polymeric films through non-covalent interactions. To measure the response of CP core particles in aqueous environments, a quantitative ratiometric approach was developed to account for system fluctuations encountered with particle dispersions. This method can help assess the molecular design of polymers and quenchers in a systematic approach. CP core particles, because of their electrostatic charge, suffer from nonspecific interactions with other charged species, and thus encapsulating CP particles with a hydrogel shell should create sensor materials with higher specificity."
}, {
    "id": "oai:dspace.mit.edu:1721.1/81062",
    "title": "Mechanisms for intrinsic stress evolution during and after polycrystalline film growth",
    "abstract": "Growth of polycrystalline films involves poorly understood kinetic processes that occur far from equilibrium and lead to complex co-evolution of the surface, microstructure and intrinsic stress of the films. Here we present a comprehensive study consisting of in situ stress measurements, microstructure characterization, and analytical modeling for various polycrystalline systems. We find that in systems of high atomic mobility, the stress change after polycrystalline film growth can be attributed to a fast reversible surface process and a slow irreversible bulk process. The fast process is weakly dependent on temperature and is associated with changes in the shape of grain surfaces. The slow process is strongly dependent on temperature and is mostly associated with grain growth in the bulk of the film. We also discovered a turnaround phenomenon in which, under conditions of intermediate atomic mobility, the stress evolves from a tensile toward a compressive state, and then turns around to evolve toward a tensile state. This stress turnaround phenomenon is strongly dependent on the substrate temperature and deposition rate, and can be attributed to an increase of the grain size during film deposition. Grain growth during deposition not only leads to a tensile component of the intrinsic stress, but also changes the grain size dependence of the compressive component. The compressive component results from incorporation of excess adatoms in grain boundaries, and the magnitude of the compressive stress is controlled by a competition between adatom incorporation in 2D islands and incorporation at grain boundaries. We also investigated the effect of the angle of incidence of the flux of depositing atoms on stress and structure evolution during polycrystalline film growth. We find that as the angle of incidence increases, the coalescence thickness increases and the stress becomes less compressive or more tensile. We attribute these phenomena to the enhanced surface roughness, the shadowing effect, the steering effect and the presence of Ehrlich-Schwoebel barriers during oblique angle deposition. All these effects lead to suppression of the adatom-grain boundary incorporation process. Based on this thesis work, intrinsic stresses in polycrystalline films can be categorized into three types: Type I, the intermediate type and Type II. These behaviors are observed in systems of low, intermediate and high atomic mobility, respectively. Compressive stresses develop in Type II behavior and tensile stresses develop in Type I behavior. The transition of the stress behavior from Type I, to the intermediate type and to Type II is continuous and can be achieved by adjusting deposition conditions. Whether the post-coalescence stress is tensile, or compressive, or evolving from compressive to tensile depends on the homologous temperature, the deposition rate and the angle of the incidence of the flux of depositing atoms.",
    "advisors": ["Carl V. Thompson"],
    "text": "Mechanisms for intrinsic stress evolution during and after polycrystalline film growth Growth of polycrystalline films involves poorly understood kinetic processes that occur far from equilibrium and lead to complex co-evolution of the surface, microstructure and intrinsic stress of the films. Here we present a comprehensive study consisting of in situ stress measurements, microstructure characterization, and analytical modeling for various polycrystalline systems. We find that in systems of high atomic mobility, the stress change after polycrystalline film growth can be attributed to a fast reversible surface process and a slow irreversible bulk process. The fast process is weakly dependent on temperature and is associated with changes in the shape of grain surfaces. The slow process is strongly dependent on temperature and is mostly associated with grain growth in the bulk of the film. We also discovered a turnaround phenomenon in which, under conditions of intermediate atomic mobility, the stress evolves from a tensile toward a compressive state, and then turns around to evolve toward a tensile state. This stress turnaround phenomenon is strongly dependent on the substrate temperature and deposition rate, and can be attributed to an increase of the grain size during film deposition. Grain growth during deposition not only leads to a tensile component of the intrinsic stress, but also changes the grain size dependence of the compressive component. The compressive component results from incorporation of excess adatoms in grain boundaries, and the magnitude of the compressive stress is controlled by a competition between adatom incorporation in 2D islands and incorporation at grain boundaries. We also investigated the effect of the angle of incidence of the flux of depositing atoms on stress and structure evolution during polycrystalline film growth. We find that as the angle of incidence increases, the coalescence thickness increases and the stress becomes less compressive or more tensile. We attribute these phenomena to the enhanced surface roughness, the shadowing effect, the steering effect and the presence of Ehrlich-Schwoebel barriers during oblique angle deposition. All these effects lead to suppression of the adatom-grain boundary incorporation process. Based on this thesis work, intrinsic stresses in polycrystalline films can be categorized into three types: Type I, the intermediate type and Type II. These behaviors are observed in systems of low, intermediate and high atomic mobility, respectively. Compressive stresses develop in Type II behavior and tensile stresses develop in Type I behavior. The transition of the stress behavior from Type I, to the intermediate type and to Type II is continuous and can be achieved by adjusting deposition conditions. Whether the post-coalescence stress is tensile, or compressive, or evolving from compressive to tensile depends on the homologous temperature, the deposition rate and the angle of the incidence of the flux of depositing atoms."
}, {
    "id": "oai:dspace.mit.edu:1721.1/45950",
    "title": "Bio-inspired optical components",
    "abstract": "Guiding electro-magnetic radiation is fundamental to optics. Lenses, mirrors, and photonic crystals all accomplish this task by different routes. Understanding the interaction of light with materials is fundamental to improving and extending optical science and engineering as well as producing novel optical elements. Improvement in this understanding should not only include work to understand the interaction with traditional engineering materials but also should target the understanding of the interaction of electromagnetic radiation with biological structures as millions of years of evolution have sorted out numerous ways to modulate light (e.g. the fish eye or the skin of the octopus). The goal of this thesis work is to fabricate novel optical elements by taking cues from nature and extending the state of the art in light guiding behavior. Here, optical elements are defined as structured materials that guide or direct electromagnetic radiation in a predetermined manner. The work presented in this thesis encompasses biologically inspired tunable multilayer reflectors made from block copolymers and improvements to liquid filled lenses which mimic the human eye.In this thesis a poly(styrene)-poly(2-vinylpyridine) block copolymer was used to create a bio-mimetic, one-dimensional, multilayer reflector. The wavelengths of light reflected from this multilayer reflector or Bragg stack were tuned by the application of stimuli which included temperature, change in the solvent environment, pH, salt concentration in the solvent, and electrochemistry.",
    "advisors": ["Edwin L. Thomas"],
    "text": "Bio-inspired optical components Guiding electro-magnetic radiation is fundamental to optics. Lenses, mirrors, and photonic crystals all accomplish this task by different routes. Understanding the interaction of light with materials is fundamental to improving and extending optical science and engineering as well as producing novel optical elements. Improvement in this understanding should not only include work to understand the interaction with traditional engineering materials but also should target the understanding of the interaction of electromagnetic radiation with biological structures as millions of years of evolution have sorted out numerous ways to modulate light (e.g. the fish eye or the skin of the octopus). The goal of this thesis work is to fabricate novel optical elements by taking cues from nature and extending the state of the art in light guiding behavior. Here, optical elements are defined as structured materials that guide or direct electromagnetic radiation in a predetermined manner. The work presented in this thesis encompasses biologically inspired tunable multilayer reflectors made from block copolymers and improvements to liquid filled lenses which mimic the human eye.In this thesis a poly(styrene)-poly(2-vinylpyridine) block copolymer was used to create a bio-mimetic, one-dimensional, multilayer reflector. The wavelengths of light reflected from this multilayer reflector or Bragg stack were tuned by the application of stimuli which included temperature, change in the solvent environment, pH, salt concentration in the solvent, and electrochemistry."
}, {
    "id": "oai:dspace.mit.edu:1721.1/46681",
    "title": "Modification of space charge transport in nanocrystalline cerium oxide by heterogeneous doping",
    "abstract": "In the search for new materials for energy conversion and storage technologies such as solid oxide fuel cells, nano-ionic materials have become increasingly relevant because unique physical and transport properties that occur on the nanoscale may potentially lead to improved device performance. Nanocrystalline cerium oxide, in particular, has been the subject of intense scrutiny, as researchers have attempted to link trends in electrical conductivity with the properties of space charge layers within the material. In this thesis, efforts designed to intentionally modify the space charge potential, and thus the space charge profiles and the macroscopic conductivity, are described.Nanocrystalline CeO2 thin films with a columnar microstructure were grown by pulsed laser deposition. A novel heterogeneous doping technique was developed in which thin NiO and Gd203 diffusion sources were deposited on the ceria surface and annealed in the temperature range of 7008000C in order to diffuse the cations into the ceria layer exclusively along grain boundaries. Time-offlight secondary ion mass spectrometry (ToF-SIMS) was utilized to measure the diffusion profiles. A single diffusion mechanism, identified as grain boundary diffusion, was observed. Using the constant source solution to the diffusion equation, grain boundary diffusion coefficients on the order of 10-15 to 10-13 cm2/s were obtained for Ni, as well as Mg diffusion emanating from the underlying substrate. Microfabricated Pt electrodes were deposited on the sample surface, and electrical measurements were made using impedance spectroscopy and two-point DC techniques. The asdeposited thin films displayed a total conductivity and activation energy consistent with reference values in the literature. After in-diffusion, the electrical conductivity decreased by one order of magnitude. Novel electron-blocking electrodes, consisting of dense yttria-stabilized zirconia and porous Pt layers were fabricated in order to deconvolute the ionic and electronic contributions to the total conductivity. In the as-deposited state, the ionic conductivity was determined to be pO2-independent, and the electronic conductivity displayed a slope of -0.30. The ionic transference number in the as-deposited state was 0.34.",
    "advisors": ["Harry L. Tuller"],
    "text": "Modification of space charge transport in nanocrystalline cerium oxide by heterogeneous doping In the search for new materials for energy conversion and storage technologies such as solid oxide fuel cells, nano-ionic materials have become increasingly relevant because unique physical and transport properties that occur on the nanoscale may potentially lead to improved device performance. Nanocrystalline cerium oxide, in particular, has been the subject of intense scrutiny, as researchers have attempted to link trends in electrical conductivity with the properties of space charge layers within the material. In this thesis, efforts designed to intentionally modify the space charge potential, and thus the space charge profiles and the macroscopic conductivity, are described.Nanocrystalline CeO2 thin films with a columnar microstructure were grown by pulsed laser deposition. A novel heterogeneous doping technique was developed in which thin NiO and Gd203 diffusion sources were deposited on the ceria surface and annealed in the temperature range of 7008000C in order to diffuse the cations into the ceria layer exclusively along grain boundaries. Time-offlight secondary ion mass spectrometry (ToF-SIMS) was utilized to measure the diffusion profiles. A single diffusion mechanism, identified as grain boundary diffusion, was observed. Using the constant source solution to the diffusion equation, grain boundary diffusion coefficients on the order of 10-15 to 10-13 cm2/s were obtained for Ni, as well as Mg diffusion emanating from the underlying substrate. Microfabricated Pt electrodes were deposited on the sample surface, and electrical measurements were made using impedance spectroscopy and two-point DC techniques. The asdeposited thin films displayed a total conductivity and activation energy consistent with reference values in the literature. After in-diffusion, the electrical conductivity decreased by one order of magnitude. Novel electron-blocking electrodes, consisting of dense yttria-stabilized zirconia and porous Pt layers were fabricated in order to deconvolute the ionic and electronic contributions to the total conductivity. In the as-deposited state, the ionic conductivity was determined to be pO2-independent, and the electronic conductivity displayed a slope of -0.30. The ionic transference number in the as-deposited state was 0.34."
}, {
    "id": "oai:dspace.mit.edu:1721.1/46268",
    "title": "Light emitting characteristics and dielectric properties of polyelectrolyte multilayer thin films",
    "abstract": "This thesis focuses on the use of a new sequential adsorption technique to deposit thin polyelectrolyte multilayer films. This involves alternately dipping a substrate into dilute aqueous solutions of a positively charged polyelectrolyte followed by a negatively charged polyelectrolyte, with a rinsing step in between. By repeating this process an arbitrary number of times, a thin film can be built up due to the electrostatic interaction between the two oppositely charged polyelectrolytes. This technique was used to create thin film electroluminescent devices based on poly(p-phenylene vinylene) (PPV) using a water soluble precursor to PPV and poly(acrylic acid) (PAA). The structure of such films has been shown to be highly dependent on the conditions of the dipping solutions. The pH of the solutions controls the degree of ionization of the PAA which influences the deposition process by affecting both the conformation of the PAA in solution as well as the charge density of the PAA on the surface. These films exhibited a light output of greater than 1000 cd/m 2 (about 10 times the brightness of a computer monitor), significantly higher than that typically reported for films of pure PPV. A time dependent charging process together with a reduction in the turn-on voltage with charging, and a non-rectifying device behavior, suggest an electrochemical mode of operation. In such a case, ions present in the film play an active role by modifying the electrical injection characteristics. More fundamental studies on the impedance and dielectric characteristics of sequentially adsorbed films were performed on layers of poly(allylamine hydrochloride) (PAH) with PAA as well as PAH with sulfonated polystyrene (SPS). This provided some insight into the level of ionic conductivity present in these films. Typically ionic conductivities were observed that ranged from about 10-12 S/cm at room temperature up to about 10-8 to 10-9 S/cm at 1 100C. The apparent dielectric constant also increased to relatively large values at low frequencies implying the buildup of ions at the interface. The PAH/SPS system required much higher temperatures than the PAHIPAA system before any significant change in the electrical characteristics were observed suggesting that ionic motion is much more hindered in PAH/SPS films.",
    "advisors": ["Michael F. Rubner"],
    "text": "Light emitting characteristics and dielectric properties of polyelectrolyte multilayer thin films This thesis focuses on the use of a new sequential adsorption technique to deposit thin polyelectrolyte multilayer films. This involves alternately dipping a substrate into dilute aqueous solutions of a positively charged polyelectrolyte followed by a negatively charged polyelectrolyte, with a rinsing step in between. By repeating this process an arbitrary number of times, a thin film can be built up due to the electrostatic interaction between the two oppositely charged polyelectrolytes. This technique was used to create thin film electroluminescent devices based on poly(p-phenylene vinylene) (PPV) using a water soluble precursor to PPV and poly(acrylic acid) (PAA). The structure of such films has been shown to be highly dependent on the conditions of the dipping solutions. The pH of the solutions controls the degree of ionization of the PAA which influences the deposition process by affecting both the conformation of the PAA in solution as well as the charge density of the PAA on the surface. These films exhibited a light output of greater than 1000 cd/m 2 (about 10 times the brightness of a computer monitor), significantly higher than that typically reported for films of pure PPV. A time dependent charging process together with a reduction in the turn-on voltage with charging, and a non-rectifying device behavior, suggest an electrochemical mode of operation. In such a case, ions present in the film play an active role by modifying the electrical injection characteristics. More fundamental studies on the impedance and dielectric characteristics of sequentially adsorbed films were performed on layers of poly(allylamine hydrochloride) (PAH) with PAA as well as PAH with sulfonated polystyrene (SPS). This provided some insight into the level of ionic conductivity present in these films. Typically ionic conductivities were observed that ranged from about 10-12 S/cm at room temperature up to about 10-8 to 10-9 S/cm at 1 100C. The apparent dielectric constant also increased to relatively large values at low frequencies implying the buildup of ions at the interface. The PAH/SPS system required much higher temperatures than the PAHIPAA system before any significant change in the electrical characteristics were observed suggesting that ionic motion is much more hindered in PAH/SPS films."
}, {
    "id": "oai:dspace.mit.edu:1721.1/33609",
    "title": "New polymeric biomaterial interfaces for biosensor applications",
    "abstract": "To fabricate living cell-based immunological sensors, we have examined two PEO-based biomaterials that can be patterned to generate cellular array templates: poly(allylamine)-g- poly(ethylene glycol) graft-copolymer and poly(ethylene glycol) dimethacrylate hydrogel. Poly(allylamine)-g-poly(ethylene glycol) polycation graft-copolymers were designed, synthesized, and characterized in order to combine bio-functionality with patternability on charged polyelectrolyte multilayer surfaces. Polymer-on-polymer stamping (POPS) techniques were used to create micron scale patterned regions on negatively charged multilayer surfaces via direct stamping of these graft copolymers. The long PEG side chains effectively resisted adsorption of antibodies or other proteins, and created a bio-inert area when patterned by POPS. On the other hand, desired proteins can be covalently attached to the graft copolymer by introducing proper coupling agents. Arrays of proteins were produced by either simple adsorption or coupling of proteins onto the graft copolymer patterned surfaces. The protein arrays were utilized as templates in fabricating cellular arrays of non-adherent B cells.",
    "advisors": ["Robert E. Cohen", "Paula T. Hammond"],
    "text": "New polymeric biomaterial interfaces for biosensor applications To fabricate living cell-based immunological sensors, we have examined two PEO-based biomaterials that can be patterned to generate cellular array templates: poly(allylamine)-g- poly(ethylene glycol) graft-copolymer and poly(ethylene glycol) dimethacrylate hydrogel. Poly(allylamine)-g-poly(ethylene glycol) polycation graft-copolymers were designed, synthesized, and characterized in order to combine bio-functionality with patternability on charged polyelectrolyte multilayer surfaces. Polymer-on-polymer stamping (POPS) techniques were used to create micron scale patterned regions on negatively charged multilayer surfaces via direct stamping of these graft copolymers. The long PEG side chains effectively resisted adsorption of antibodies or other proteins, and created a bio-inert area when patterned by POPS. On the other hand, desired proteins can be covalently attached to the graft copolymer by introducing proper coupling agents. Arrays of proteins were produced by either simple adsorption or coupling of proteins onto the graft copolymer patterned surfaces. The protein arrays were utilized as templates in fabricating cellular arrays of non-adherent B cells."
}, {
    "id": "oai:dspace.mit.edu:1721.1/16704",
    "title": "Attractive electrostatic self-assembly of ordered and disordered heterogeneous colloids",
    "abstract": "Ionic colloidal crystals are here defined as multicomponent ordered colloidal structures stabilized by attractive electrostatic interactions. These crystals are colloidal analogues to ionic materials including zincblende, rocksalt, cesium chloride, and fluorite. A thermodynamic study revealed that the screening ratio, charge ratio, and monodispersity are critical parameters in ionic colloidal crystal (ICC) formation. Experimentally, small ordered regions were observed under ideal thermodynamic conditions. However, no larger crystalline regions were found in these samples. The kinetics of ICC formation was studied using a variety of computational techniques, including Brownian dynamics, Monte Carlo, and a Newton's method solver. These techniques have each elucidated properties and processing conditions that are important to crystallization. The Brownian dynamics and Monte Carlo simulations showed that the previous experiments were highly undercooled. Furthermore, a narrow crystallization window was found, demonstrating the need to create particle systems that meet the narrow parameter space where ICCs should be stable. Pair interaction potentials were evaluated for their accuracy using a Poisson-Boltzmann (PB) equation solver. The PB solver was also used to further refine crystalline formation energies so that systems can be more accurately tailored. A surprising result from the PB solver showed that the lowest formation energy occurs when the quantity of surface charges on both particles are equal. Although this result is not predicted by any colloidal pair potentials, it was verified experimentally. This further illustrates that thermal mobility in these systems can be sufficient to maintain a stable solution despite attractive electrostatic interactions. Tailoring particle systems to balance the thermal and electrostatic interactions should allow widespread crystallization. However, these conditions require highly monodisperse particles to be fabricated with controlled surface charge and sizes. Currently these particles are not widely available and further research in this area should aid in the full realization of the ICC concept. In conclusion, all results are integrated to predict which particle systems should be produced to allow the formation of large ordered structures.",
    "advisors": ["Yet-Ming Chiang", "W. Craig Carter"],
    "text": "Attractive electrostatic self-assembly of ordered and disordered heterogeneous colloids Ionic colloidal crystals are here defined as multicomponent ordered colloidal structures stabilized by attractive electrostatic interactions. These crystals are colloidal analogues to ionic materials including zincblende, rocksalt, cesium chloride, and fluorite. A thermodynamic study revealed that the screening ratio, charge ratio, and monodispersity are critical parameters in ionic colloidal crystal (ICC) formation. Experimentally, small ordered regions were observed under ideal thermodynamic conditions. However, no larger crystalline regions were found in these samples. The kinetics of ICC formation was studied using a variety of computational techniques, including Brownian dynamics, Monte Carlo, and a Newton's method solver. These techniques have each elucidated properties and processing conditions that are important to crystallization. The Brownian dynamics and Monte Carlo simulations showed that the previous experiments were highly undercooled. Furthermore, a narrow crystallization window was found, demonstrating the need to create particle systems that meet the narrow parameter space where ICCs should be stable. Pair interaction potentials were evaluated for their accuracy using a Poisson-Boltzmann (PB) equation solver. The PB solver was also used to further refine crystalline formation energies so that systems can be more accurately tailored. A surprising result from the PB solver showed that the lowest formation energy occurs when the quantity of surface charges on both particles are equal. Although this result is not predicted by any colloidal pair potentials, it was verified experimentally. This further illustrates that thermal mobility in these systems can be sufficient to maintain a stable solution despite attractive electrostatic interactions. Tailoring particle systems to balance the thermal and electrostatic interactions should allow widespread crystallization. However, these conditions require highly monodisperse particles to be fabricated with controlled surface charge and sizes. Currently these particles are not widely available and further research in this area should aid in the full realization of the ICC concept. In conclusion, all results are integrated to predict which particle systems should be produced to allow the formation of large ordered structures."
}, {
    "id": "oai:dspace.mit.edu:1721.1/9162",
    "title": "Stability of lithium aluminum manganese oxide cathodes for rechargeable lithium batteries",
    "abstract": "Lithium manganese oxides have attracted wide attention as low-cost, nontoxic intercalation cathode materials for rechargeable lithium batteries. In this work, the stability of these compounds during synthesis and in use has been studied in several respects. (1) Phase stability of LiMnO2 polymorphs has been determined under the high temperature synthesis conditions. Effects of temperature, oxygen partial pressure, and dopant (Al) content on the phase stability have been discussed based on a possible stability mechanism. (2) The mechanism of improved cycling stability of electrochemically transformed spinel compared to conventional spinel has been identified. Atomic rearrangement from the ordered rocksalt to spinel type cation ordering results in an antiphase nanodomain structure, which becomes a ferroelastic domain structure during the cubic ---> tetragonal Jahn-Teller transformation, and thereby accommodates the transformation strains. (3) Al-doped spinels exhibit much improved capacity stability at elevated temperatures compared to undoped spinels. This effect has been discussed with respect to proposed mechanisms of Mn dissolution and capacity loss. (4) Magnetic properties are critically influenced by phase stability, cation ordering, and Mn valence in lithium manganese oxides. In the paramagnetic temperature regime, it has been observed that antiferromagnetic interactions between the Mn ions are strongest in the orthorhombic phase among LiMnO2 polymorphs having the average Mn valence of 3+, while decreasing Mn valence strengthens the antiferromagnetic interactions in LixMn2O4 spinel. At temperatures below the paramagnetic temperature regime, spin-glass behavior is observed in both LixMn2O4 and monoclinic LiMnO2 compounds, which is attributed to geometrical frustration due to structure ( cation ordering) and magnetic disorder due to a disordered distribution of Mn valence. As spin-glass behavior is commonly observed in both well-crystallized, conventional spinel and highly disordered, transformed spinel, magnetic characterization cannot easily be used to distinguish the two different spinels.",
    "advisors": ["Yet-Ming Chiang"],
    "text": "Stability of lithium aluminum manganese oxide cathodes for rechargeable lithium batteries Lithium manganese oxides have attracted wide attention as low-cost, nontoxic intercalation cathode materials for rechargeable lithium batteries. In this work, the stability of these compounds during synthesis and in use has been studied in several respects. (1) Phase stability of LiMnO2 polymorphs has been determined under the high temperature synthesis conditions. Effects of temperature, oxygen partial pressure, and dopant (Al) content on the phase stability have been discussed based on a possible stability mechanism. (2) The mechanism of improved cycling stability of electrochemically transformed spinel compared to conventional spinel has been identified. Atomic rearrangement from the ordered rocksalt to spinel type cation ordering results in an antiphase nanodomain structure, which becomes a ferroelastic domain structure during the cubic ---> tetragonal Jahn-Teller transformation, and thereby accommodates the transformation strains. (3) Al-doped spinels exhibit much improved capacity stability at elevated temperatures compared to undoped spinels. This effect has been discussed with respect to proposed mechanisms of Mn dissolution and capacity loss. (4) Magnetic properties are critically influenced by phase stability, cation ordering, and Mn valence in lithium manganese oxides. In the paramagnetic temperature regime, it has been observed that antiferromagnetic interactions between the Mn ions are strongest in the orthorhombic phase among LiMnO2 polymorphs having the average Mn valence of 3+, while decreasing Mn valence strengthens the antiferromagnetic interactions in LixMn2O4 spinel. At temperatures below the paramagnetic temperature regime, spin-glass behavior is observed in both LixMn2O4 and monoclinic LiMnO2 compounds, which is attributed to geometrical frustration due to structure ( cation ordering) and magnetic disorder due to a disordered distribution of Mn valence. As spin-glass behavior is commonly observed in both well-crystallized, conventional spinel and highly disordered, transformed spinel, magnetic characterization cannot easily be used to distinguish the two different spinels."
}, {
    "id": "oai:dspace.mit.edu:1721.1/90085",
    "title": "Controlling microstructure of nanocrystalline thermoelectrics through powder processing",
    "abstract": "Bismuth Telluride and its solid solutions are currently front running thermoelectric materials because of their high figure of merit. When processed via mechanical alloying to obtain nanocrystalline structures, their efficiency is increased dramatically, due to enhanced phonon scattering at grain boundaries. However, the excess free energy of these interfaces renders them inherently susceptible to grain growth, therefore there is a need for materials with enhanced thermal stability. Despite this, little is known about the relevant processing science of these materials with respect to mechanical alloying and powder consolidation. This shortcoming is addressed here via systematic study of the processing-structure relationships that govern these processing operations. Firstly, during mechanical alloying, the primary mechanism of mixing between elemental constituents is revealed, as well as the limitations to subsequent grain refinement. The resultant behaviour is unique in the literature on mechanical alloying, due to the unusual thermal and thermodynamic properties of the compound and its elements, rendering deformation-induced heating effects especially prevalent. Next, during sintering operations of the powders, the kinetics of grain growth and porosity evolution were studied. By quantifying these processes, a thermal budget map for the nanocrystalline compound is constructed, to allow predictive powder and guidance of both processing and device operation at elevated temperatures. Finally, based on the improved understanding in processing science and thermal stability of these materials, a new class of thermally stable composites is engineered, with improved thermal stability, and hence enhanced thermoelectric properties.",
    "advisors": ["Christopher A. Schuh"],
    "text": "Controlling microstructure of nanocrystalline thermoelectrics through powder processing Bismuth Telluride and its solid solutions are currently front running thermoelectric materials because of their high figure of merit. When processed via mechanical alloying to obtain nanocrystalline structures, their efficiency is increased dramatically, due to enhanced phonon scattering at grain boundaries. However, the excess free energy of these interfaces renders them inherently susceptible to grain growth, therefore there is a need for materials with enhanced thermal stability. Despite this, little is known about the relevant processing science of these materials with respect to mechanical alloying and powder consolidation. This shortcoming is addressed here via systematic study of the processing-structure relationships that govern these processing operations. Firstly, during mechanical alloying, the primary mechanism of mixing between elemental constituents is revealed, as well as the limitations to subsequent grain refinement. The resultant behaviour is unique in the literature on mechanical alloying, due to the unusual thermal and thermodynamic properties of the compound and its elements, rendering deformation-induced heating effects especially prevalent. Next, during sintering operations of the powders, the kinetics of grain growth and porosity evolution were studied. By quantifying these processes, a thermal budget map for the nanocrystalline compound is constructed, to allow predictive powder and guidance of both processing and device operation at elevated temperatures. Finally, based on the improved understanding in processing science and thermal stability of these materials, a new class of thermally stable composites is engineered, with improved thermal stability, and hence enhanced thermoelectric properties."
}, {
    "id": "oai:dspace.mit.edu:1721.1/115711",
    "title": "Designing dynamic mechanics in self-healing nanocomposite hydrogels",
    "abstract": "The functional versatility and endurable self-healing capacity of soft materials in nature is found to originate from the dynamic supramolecular scaffolds assembled via reversible interactions. To mimic this strategy, extensive efforts have been made to design polymer networks with transient crosslinks, which lays the foundation for synthetic self-healing hydrogels. Towards the development of stronger and faster self-healing hydrogels, understanding and controlling the gel network dynamics is of critical importance, since it provides design principles for key properties such as dynamic mechanics and self-healing performance. For this purpose, a universal strategy independent of exact crosslinking chemistry would be regulating the polymer material's dynamic behavior by optimal network design, yet current understanding of the relationship between network structure and macroscopic dynamic mechanics is still limited, and implementation of complex network structure has always been challenging. In this thesis, we show how the dynamic mechanical properties in a hydrogel can be controlled by rational design of polymer network structures. Using mussel-inspired reversible catechol coordination chemistry, we developed a nanocomposite hydrogel network (NP gel) with hierarchical assembly of polymer chains on iron oxide (Fe3O4) nanoparticles as network crosslinks. With NP gel as a model system, we first investigated its unique dynamic mechanics in comparison with traditional permanent and dynamic gels, and discovered a general approach to manipulate the network dynamics by controlling the crosslink structural functionality. Then we further explored the underlying relationship between polymer network structure and two key parameters in relaxation mechanics, which elucidated universal approaches for designing relaxation patterns in supramolecular transient gel network. Finally, by utilizing these design principles, we designed a hybrid gel network using two crosslinking structures with distinct relaxation timescales. By simply adjusting the ratio of two crosslinks, we can precisely tune the material's dynamic mechanics from a viscoelastic fluid to a rigid solid. Such controllability in dynamic mechanics enabled performance optimization towards mechanically rigid and fast self-healing hydrogel materials.",
    "advisors": ["Niels Holten-Andersen"],
    "text": "Designing dynamic mechanics in self-healing nanocomposite hydrogels The functional versatility and endurable self-healing capacity of soft materials in nature is found to originate from the dynamic supramolecular scaffolds assembled via reversible interactions. To mimic this strategy, extensive efforts have been made to design polymer networks with transient crosslinks, which lays the foundation for synthetic self-healing hydrogels. Towards the development of stronger and faster self-healing hydrogels, understanding and controlling the gel network dynamics is of critical importance, since it provides design principles for key properties such as dynamic mechanics and self-healing performance. For this purpose, a universal strategy independent of exact crosslinking chemistry would be regulating the polymer material's dynamic behavior by optimal network design, yet current understanding of the relationship between network structure and macroscopic dynamic mechanics is still limited, and implementation of complex network structure has always been challenging. In this thesis, we show how the dynamic mechanical properties in a hydrogel can be controlled by rational design of polymer network structures. Using mussel-inspired reversible catechol coordination chemistry, we developed a nanocomposite hydrogel network (NP gel) with hierarchical assembly of polymer chains on iron oxide (Fe3O4) nanoparticles as network crosslinks. With NP gel as a model system, we first investigated its unique dynamic mechanics in comparison with traditional permanent and dynamic gels, and discovered a general approach to manipulate the network dynamics by controlling the crosslink structural functionality. Then we further explored the underlying relationship between polymer network structure and two key parameters in relaxation mechanics, which elucidated universal approaches for designing relaxation patterns in supramolecular transient gel network. Finally, by utilizing these design principles, we designed a hybrid gel network using two crosslinking structures with distinct relaxation timescales. By simply adjusting the ratio of two crosslinks, we can precisely tune the material's dynamic mechanics from a viscoelastic fluid to a rigid solid. Such controllability in dynamic mechanics enabled performance optimization towards mechanically rigid and fast self-healing hydrogel materials."
}, {
    "id": "oai:dspace.mit.edu:1721.1/113929",
    "title": "Accounting for temperature and local structure in atomistic calculations of interface free energies in metals",
    "abstract": "Interfaces are ubiquitous in crystalline materials and they predominantly govern the properties of metals at nanoscale. Free energy is the most significant characteristic of an interface that determines its behavior and other properties. However, interfaces have an internal structure with nanoscale features that gives the interface free energy, a location dependent variation. The interface free energy is also expected to depend on temperature, especially for entropy stabilized phases such as liquids. I present methods for calculating location-dependent energies for solid-state interfaces and temperature-dependent free energies for solid/liquid interfaces using atomistic models. I demonstrate these methods on CuNbHe and AlGa models due to the importance of location- and temperature-dependence of interface energies in these systems. My analysis sheds light on interfacial He precipitation in CuNb composites and on Ga permeation through grain boundaries (GB) in Al. Based on the Gibbsian definition of excess interface energy, I develop a method to compute location dependent energy of solid/solid interface from atomic level energies and apply it to characterize four different Cu/Nb interfaces and several GBs of Al. The solid-state interface energies are indeed location-dependent and vary at the length scale of few nanometers. For any particular interface, the variation in the energies spans a range of 100% of its average energy. The higher energy regions correspond to the underlying misfit dislocation network of the semicoherent interfaces. I compute the stress of solid/liquid interface from capillary pressure of the interface using the Young-Laplace equation and then derive its free energy. Applying this method to Cu/He and Nb/He interfaces, I find that these interface energies are dependent on He pressure and interface curvature but not explicitly on temperature. I fit analytical expressions for these interface free energies and study the impact of their curvature dependence on He cluster growth. For instance, the incorporation of curvature dependence of metal-He interface free energies accounts for a decrease of up to 1GPa in the pressure required to punch out a dislocation from the bubble into the metal. Al/Ga interface energy is found to be negative, implying that Ga can spread on all Al surfaces and GBs. The computed interface energies may be used to predict the distribution of He precipitates at CuNb interfaces while interface energies for AlGa shed light on the crystallographic character-dependence of Ga permeation through Al GBs. These results may be directly incorporated into phase field models of He behavior in CuNb composites and Ga behavior at Al GBs.",
    "advisors": ["Michael J. Demkowicz"],
    "text": "Accounting for temperature and local structure in atomistic calculations of interface free energies in metals Interfaces are ubiquitous in crystalline materials and they predominantly govern the properties of metals at nanoscale. Free energy is the most significant characteristic of an interface that determines its behavior and other properties. However, interfaces have an internal structure with nanoscale features that gives the interface free energy, a location dependent variation. The interface free energy is also expected to depend on temperature, especially for entropy stabilized phases such as liquids. I present methods for calculating location-dependent energies for solid-state interfaces and temperature-dependent free energies for solid/liquid interfaces using atomistic models. I demonstrate these methods on CuNbHe and AlGa models due to the importance of location- and temperature-dependence of interface energies in these systems. My analysis sheds light on interfacial He precipitation in CuNb composites and on Ga permeation through grain boundaries (GB) in Al. Based on the Gibbsian definition of excess interface energy, I develop a method to compute location dependent energy of solid/solid interface from atomic level energies and apply it to characterize four different Cu/Nb interfaces and several GBs of Al. The solid-state interface energies are indeed location-dependent and vary at the length scale of few nanometers. For any particular interface, the variation in the energies spans a range of 100% of its average energy. The higher energy regions correspond to the underlying misfit dislocation network of the semicoherent interfaces. I compute the stress of solid/liquid interface from capillary pressure of the interface using the Young-Laplace equation and then derive its free energy. Applying this method to Cu/He and Nb/He interfaces, I find that these interface energies are dependent on He pressure and interface curvature but not explicitly on temperature. I fit analytical expressions for these interface free energies and study the impact of their curvature dependence on He cluster growth. For instance, the incorporation of curvature dependence of metal-He interface free energies accounts for a decrease of up to 1GPa in the pressure required to punch out a dislocation from the bubble into the metal. Al/Ga interface energy is found to be negative, implying that Ga can spread on all Al surfaces and GBs. The computed interface energies may be used to predict the distribution of He precipitates at CuNb interfaces while interface energies for AlGa shed light on the crystallographic character-dependence of Ga permeation through Al GBs. These results may be directly incorporated into phase field models of He behavior in CuNb composites and Ga behavior at Al GBs."
}, {
    "id": "oai:dspace.mit.edu:1721.1/75848",
    "title": "Collagen scaffolds and injectable biopolymer gels for cardiac tissue engineering",
    "abstract": "Three-dimensional biomaterial scaffolds have begun to shown promise for cell delivery for cardiac tissue engineering. Although various polymers and material forms have been explored, there is a need for: injectable gels that meet certain design specifications; a more indepth characterization of the scaffold properties; and a deeper understanding of the relation of select properties to cellular behavior, to provide a rational basis for future in vivo studies. The first objective of this thesis was to develop and characterize novel injectable biopolymer hydrogels capable of safely undergoing covalent cross-linking in vivo to provide a mechanically tunable nanofibrillar scaffold. Soluble type I collagen gels with genipin and transglutaminase cross-linkers, and gelatin-hydroxyphenylpropionic acid (Gtn-HPA) gels, the cross-linking of which are modulated by horse radish peroxidase and hydrogen peroxide, were investigated. The gels were characterized on the basis of rheological properties, resistance to degradation, and effects on stem cell behavior. Another objective was to evaluate the simultaneous differentiation of embryonic carcinoma cells (ECCs) incorporated in the gels into the three cell types in cardiac tissue -- cardiomyocytes, neural cells, and vascular endothelial cells -- and to determine the effects of certain properties of the gels on the differentiation profile, using mesenchymal stem cells as a comparative control. The injectable collagen-genipin and Gtn-HPA gels were found to be mechanically tunable hydrogel systems that supported cell encapsulation and proliferation at safe concentrations of the respective cross-linking agents. ECCs cultured as embryoid bodies (EBs) incorporated in the collagen-genipin and Gtn-HPA gels differentiated into cardiac, neural, and endothelial cells and combinations thereof, demonstrating the capability of EBs to express multiple cell lineages within the same EB. EBs cultured in collagen gels without cross-linkers and collagen gels with 0.25 mM genipin exhibited the highest differentiation efficiency compared to those cultured in monolayer, sponge-like scaffolds, and Gtn-HPA gels. The differentiation medium and culture time also had significant effects on differentiation efficiency. Notable findings included: the increased expression of neural and endothelial markers in EBs cultured in in mixed medium conditions compared to those cultured in neural or endothelial differentiation medium alone, and the correlation between angiogenic and neurogenic differentiation in the EBs in the non-cross-linked collagen gels for all media. Collectively, these findings show promise in using collagen gels cross-linked with 0.25 mM genipin, incorporated with EBs, for cellular therapy in cardiac tissue engineering applications.",
    "advisors": ["Myron Spector", "Ioannis Yannas"],
    "text": "Collagen scaffolds and injectable biopolymer gels for cardiac tissue engineering Three-dimensional biomaterial scaffolds have begun to shown promise for cell delivery for cardiac tissue engineering. Although various polymers and material forms have been explored, there is a need for: injectable gels that meet certain design specifications; a more indepth characterization of the scaffold properties; and a deeper understanding of the relation of select properties to cellular behavior, to provide a rational basis for future in vivo studies. The first objective of this thesis was to develop and characterize novel injectable biopolymer hydrogels capable of safely undergoing covalent cross-linking in vivo to provide a mechanically tunable nanofibrillar scaffold. Soluble type I collagen gels with genipin and transglutaminase cross-linkers, and gelatin-hydroxyphenylpropionic acid (Gtn-HPA) gels, the cross-linking of which are modulated by horse radish peroxidase and hydrogen peroxide, were investigated. The gels were characterized on the basis of rheological properties, resistance to degradation, and effects on stem cell behavior. Another objective was to evaluate the simultaneous differentiation of embryonic carcinoma cells (ECCs) incorporated in the gels into the three cell types in cardiac tissue -- cardiomyocytes, neural cells, and vascular endothelial cells -- and to determine the effects of certain properties of the gels on the differentiation profile, using mesenchymal stem cells as a comparative control. The injectable collagen-genipin and Gtn-HPA gels were found to be mechanically tunable hydrogel systems that supported cell encapsulation and proliferation at safe concentrations of the respective cross-linking agents. ECCs cultured as embryoid bodies (EBs) incorporated in the collagen-genipin and Gtn-HPA gels differentiated into cardiac, neural, and endothelial cells and combinations thereof, demonstrating the capability of EBs to express multiple cell lineages within the same EB. EBs cultured in collagen gels without cross-linkers and collagen gels with 0.25 mM genipin exhibited the highest differentiation efficiency compared to those cultured in monolayer, sponge-like scaffolds, and Gtn-HPA gels. The differentiation medium and culture time also had significant effects on differentiation efficiency. Notable findings included: the increased expression of neural and endothelial markers in EBs cultured in in mixed medium conditions compared to those cultured in neural or endothelial differentiation medium alone, and the correlation between angiogenic and neurogenic differentiation in the EBs in the non-cross-linked collagen gels for all media. Collectively, these findings show promise in using collagen gels cross-linked with 0.25 mM genipin, incorporated with EBs, for cellular therapy in cardiac tissue engineering applications."
}, {
    "id": "oai:dspace.mit.edu:1721.1/89948",
    "title": "Templated self-assembly of multiferroic nanocomposites",
    "abstract": "To respond to the growing demand for smart and connected devices, such as smartphones, tablet PCs arid other mobile hardware, while meeting the needs for increased power efficiency, miniaturization and reduced manufacturing costs, new material solutions need to be considered. These should address the shortcomings of incumbent semiconductor-based technologies which provide a limited number of functionalities, suffer from high power consumption and heat dissipation, and whose conventional planar processing is increasingly complex and resource-intensive. Potential replacement materials include complex oxides, which exhibit interesting physical phenomena such as superconductivity, colossal magnetoresistance and multiferroicity. New functionalities are especially found at interfaces between two oxides, including emergent electronic states like two-dimensional electron gases, enhanced ionic transport and magnetoelectric coupling, among many other. In this this thesis, we focus on self-assembled oxide nanocomposites, which elegantly organize into vertical nanostructures via spontaneous phase-separation, naturally forming numerous such heterointerfaces. These provide a rich playground for studying interfacial effects, which could be used in future devices, and the self-assembly promises cheap arid high throughput manufacturing providing it can be integrated into useful architectures. BiFeO-CoFeO self-assembled nanocomposites, in particular, have been studied for the magnetoelectric coupling that takes place between the ferrimagnetic spinel phase, which forms discrete vertical pillars, arid the ferroelectric perovskite phase, which forms a matrix that surrounds the spinel pillars. Here, after an in-depth study of the mechanisms responsible for the formation of this self-assembled nanostructure, we develop a templating method enabling the precise control over the morphology of the film, resulting in useful structures for potential devices like magnetoelectric memories and logic devices. To study the structural, magnetic and electrical properties of our samples, a set of experimental and theoretical methods is developed, adapted to the unique requirements of these thin film nanostructures with iicron-scale ordering. Using finite element analysis and micromagnetic modeling, the effect of the strain-mediated magnetoelectic coupling on the magnetic switching of the CoFeO nanopillars is predicted. Scanning Probe Microscopy is also used to characterize the local ferroelectric and magnetic behavior, and observe, for the first time in these templated composites, electrically-induced magnetic switching of the pillar magnetization. The tools and methods developed in this thesis could pave the way towards a wider use of templated self-assembly to leverage the promising properties of oxide heterointerfaces and enable their use in future devices with low manufacturing costs.",
    "advisors": ["Caroline A. Ross"],
    "text": "Templated self-assembly of multiferroic nanocomposites To respond to the growing demand for smart and connected devices, such as smartphones, tablet PCs arid other mobile hardware, while meeting the needs for increased power efficiency, miniaturization and reduced manufacturing costs, new material solutions need to be considered. These should address the shortcomings of incumbent semiconductor-based technologies which provide a limited number of functionalities, suffer from high power consumption and heat dissipation, and whose conventional planar processing is increasingly complex and resource-intensive. Potential replacement materials include complex oxides, which exhibit interesting physical phenomena such as superconductivity, colossal magnetoresistance and multiferroicity. New functionalities are especially found at interfaces between two oxides, including emergent electronic states like two-dimensional electron gases, enhanced ionic transport and magnetoelectric coupling, among many other. In this this thesis, we focus on self-assembled oxide nanocomposites, which elegantly organize into vertical nanostructures via spontaneous phase-separation, naturally forming numerous such heterointerfaces. These provide a rich playground for studying interfacial effects, which could be used in future devices, and the self-assembly promises cheap arid high throughput manufacturing providing it can be integrated into useful architectures. BiFeO-CoFeO self-assembled nanocomposites, in particular, have been studied for the magnetoelectric coupling that takes place between the ferrimagnetic spinel phase, which forms discrete vertical pillars, arid the ferroelectric perovskite phase, which forms a matrix that surrounds the spinel pillars. Here, after an in-depth study of the mechanisms responsible for the formation of this self-assembled nanostructure, we develop a templating method enabling the precise control over the morphology of the film, resulting in useful structures for potential devices like magnetoelectric memories and logic devices. To study the structural, magnetic and electrical properties of our samples, a set of experimental and theoretical methods is developed, adapted to the unique requirements of these thin film nanostructures with iicron-scale ordering. Using finite element analysis and micromagnetic modeling, the effect of the strain-mediated magnetoelectic coupling on the magnetic switching of the CoFeO nanopillars is predicted. Scanning Probe Microscopy is also used to characterize the local ferroelectric and magnetic behavior, and observe, for the first time in these templated composites, electrically-induced magnetic switching of the pillar magnetization. The tools and methods developed in this thesis could pave the way towards a wider use of templated self-assembly to leverage the promising properties of oxide heterointerfaces and enable their use in future devices with low manufacturing costs."
}, {
    "id": "oai:dspace.mit.edu:1721.1/16636",
    "title": "An investigation of corrosion mechanisms of constructional alloys in supercritical water oxidation (SCWO) systems",
    "abstract": "Supercritical water oxidation (SCWO) is a technology that can effectively destroy aqueous organic waste above the critical point of pure water. These waste feed streams are very aggressive and pose material performance issues. As potential alloys in construction of SCWO systems, nickel-base alloys are tested. Corrosion in aqueous feed streams of ambient pH values of 2, 1 and 7 is studied both at supercritical (-425C) and subcritical (-300-360C) temperatures with a constant pressure of 24.1MPa. Dealloying of Ni and Fe, and oxidation of Cr and Mo are observed at subcritical temperatures at a pH value of 2. At a pH value of 1, even chromium is selectively dissolved and only molybdenum forms a stable oxide at the subcritical temperature. At supercritical temperatures, normal thin oxidation occurs at both pH values of pH 2 and 7. In contrast, in the neutral pH solution, dealloying is not observed at any temperature. Stress corrosion cracking (SCC) in acidic feed streams is observed both at the supercritical and subcritical temperatures. In order to understand the corrosion mechanisms, the chemistry of a feed stream, the formation of the dealloyed oxide layer, and the level of stress are investigated.",
    "advisors": ["Ronald M. Latanision", "Ronald G. Ballinger"],
    "text": "An investigation of corrosion mechanisms of constructional alloys in supercritical water oxidation (SCWO) systems Supercritical water oxidation (SCWO) is a technology that can effectively destroy aqueous organic waste above the critical point of pure water. These waste feed streams are very aggressive and pose material performance issues. As potential alloys in construction of SCWO systems, nickel-base alloys are tested. Corrosion in aqueous feed streams of ambient pH values of 2, 1 and 7 is studied both at supercritical (-425C) and subcritical (-300-360C) temperatures with a constant pressure of 24.1MPa. Dealloying of Ni and Fe, and oxidation of Cr and Mo are observed at subcritical temperatures at a pH value of 2. At a pH value of 1, even chromium is selectively dissolved and only molybdenum forms a stable oxide at the subcritical temperature. At supercritical temperatures, normal thin oxidation occurs at both pH values of pH 2 and 7. In contrast, in the neutral pH solution, dealloying is not observed at any temperature. Stress corrosion cracking (SCC) in acidic feed streams is observed both at the supercritical and subcritical temperatures. In order to understand the corrosion mechanisms, the chemistry of a feed stream, the formation of the dealloyed oxide layer, and the level of stress are investigated."
}, {
    "id": "oai:dspace.mit.edu:1721.1/62688",
    "title": "Life extension of structural components via an improved nondestructive testing methodology",
    "abstract": "An experimental study was performed to determine the flaw detection sensitivity of advanced nondestructive testing (NDT) techniques with respect to structural applications. The techniques analyzed exemplify the incorporation of digital technology into NDT and includes the following: meandering winding magnetometer array (MWM-array@) eddy current, phased-array ultrasonic (PA-UT), three dimensional computed tomography (3DCT), and digital radiography (DR). The three classes of samples inspected with these techniques consisted of alloy block specimens containing flat bottom hole (FBH) arrays, probability of detection (POD) wedding cake samples, and actual airplane engine components. Results from the sensitivity analyses were compared to current NDT techniques used industrially. An image analysis program called Cellprofiler was used to optimize the threshold correction factor for selected results. The Cellprofiler output was analyzed in conjunction with POD software, and the integration of digitally advanced NDT techniques with image analysis software resulted in approximately a threefold improvement in the minimum detectable flaw size at the 90/95 POD/CL level. An improved inspection methodology was presented which incorporated redundancy in the in-service inspection plan with the use of Bayesian updating techniques to forecast remnant life. Reliability block diagrams for structural disk and blade aircraft engine components were presented as examples of the methodology. Implementation of the proposed NDT methodology significantly increases the feasibility of a retirement-forcause (RFC) approach to be applied to aging structural components in a cost-effective manner.",
    "advisors": ["Thomas W. Eagar"],
    "text": "Life extension of structural components via an improved nondestructive testing methodology An experimental study was performed to determine the flaw detection sensitivity of advanced nondestructive testing (NDT) techniques with respect to structural applications. The techniques analyzed exemplify the incorporation of digital technology into NDT and includes the following: meandering winding magnetometer array (MWM-array@) eddy current, phased-array ultrasonic (PA-UT), three dimensional computed tomography (3DCT), and digital radiography (DR). The three classes of samples inspected with these techniques consisted of alloy block specimens containing flat bottom hole (FBH) arrays, probability of detection (POD) wedding cake samples, and actual airplane engine components. Results from the sensitivity analyses were compared to current NDT techniques used industrially. An image analysis program called Cellprofiler was used to optimize the threshold correction factor for selected results. The Cellprofiler output was analyzed in conjunction with POD software, and the integration of digitally advanced NDT techniques with image analysis software resulted in approximately a threefold improvement in the minimum detectable flaw size at the 90/95 POD/CL level. An improved inspection methodology was presented which incorporated redundancy in the in-service inspection plan with the use of Bayesian updating techniques to forecast remnant life. Reliability block diagrams for structural disk and blade aircraft engine components were presented as examples of the methodology. Implementation of the proposed NDT methodology significantly increases the feasibility of a retirement-forcause (RFC) approach to be applied to aging structural components in a cost-effective manner."
}, {
    "id": "oai:dspace.mit.edu:1721.1/53250",
    "title": "Multimaterial rectifying device fibers",
    "abstract": "Electronic and optoelectronic device processing is commonly thought to be incompatible with much simpler thermal drawing techniques used in optical fiber production. The incorporation of metals, polymer insulators, and chalcogenide semiconductors into structured fibers has reversed this paradigm and made it possible to realize optoelectronic device functionalities at fiber optic length scales and cost. In spite of the surprising robustness of this processing technique, the electronic performance and complexity of these optoelectronic fiber devices has been constrained by the small set of materials compatible with the fabrication method and the disordered nature of the semiconductor. Specifically, the high density of defects inherent to the amorphous chalcogenide semiconductors precludes the ability to create spatially extended internal electric fields necessary to create more sophisticated devices such as diodes and transistors. In this work, the design, fabrication, and characterization of the first fiber-integrated diode is described. The relevant optical, thermal, and electronic properties of candidate materials compatible with the thermal fiber drawing process are described and measured. Phase changing semiconductors are incorporated into the fiber having both amorphous properties amenable to thermal drawing and crystalline properties ideal for electronic devices. Combinations of metals and semiconductors that form both blocking and non-blocking contacts are identified and combined to form the first diode device that is compatible with the thermal drawing process. Techniques are developed to reduce the dimensions of the resulting devices by an order-of- magnitude compared to all previous multimaterial device fibers.",
    "advisors": ["Yoel Fink"],
    "text": "Multimaterial rectifying device fibers Electronic and optoelectronic device processing is commonly thought to be incompatible with much simpler thermal drawing techniques used in optical fiber production. The incorporation of metals, polymer insulators, and chalcogenide semiconductors into structured fibers has reversed this paradigm and made it possible to realize optoelectronic device functionalities at fiber optic length scales and cost. In spite of the surprising robustness of this processing technique, the electronic performance and complexity of these optoelectronic fiber devices has been constrained by the small set of materials compatible with the fabrication method and the disordered nature of the semiconductor. Specifically, the high density of defects inherent to the amorphous chalcogenide semiconductors precludes the ability to create spatially extended internal electric fields necessary to create more sophisticated devices such as diodes and transistors. In this work, the design, fabrication, and characterization of the first fiber-integrated diode is described. The relevant optical, thermal, and electronic properties of candidate materials compatible with the thermal fiber drawing process are described and measured. Phase changing semiconductors are incorporated into the fiber having both amorphous properties amenable to thermal drawing and crystalline properties ideal for electronic devices. Combinations of metals and semiconductors that form both blocking and non-blocking contacts are identified and combined to form the first diode device that is compatible with the thermal drawing process. Techniques are developed to reduce the dimensions of the resulting devices by an order-of- magnitude compared to all previous multimaterial device fibers."
}, {
    "id": "oai:dspace.mit.edu:1721.1/62616",
    "title": "Development of in-situ toughened silicon-rich alloys : a new class of castable engineering ceramics",
    "abstract": "Despite having a broad set of desirable properties, silicon's potential as a primary constituent in a structural material has not yet been realized because of its extremely low fracture toughness. Motivated by the microstructural design techniques used in toughening inherently brittle ceramic materials, this work aims to develop a silicon-rich alloy with microstructural features that provide for the same types of toughening mechanisms displayed by technical ceramics. In order to add true commercial value to these silicon-based alloys, however, the alloys must be processed using methods more flexible and less expensive than the powder processing routes currently used for engineering ceramics. This thesis will discuss the development of a class of castable silicon-based alloys referred to as silicon-disilicide (Si-XSi2) composites, which naturally form a microstructure composed of a silicon matrix and reinforcing disilicide (XSi2) phase during solidification (where X is a transition metal). Experimental work is performed to characterize the thermal, microstructural, and fracture properties of a specific set of Si-(Cr,V)Si2 alloys which are based on the Si-CrSi2-VSi2 system. First, a reliable thermodynamic description of the Si-CrSi2-VSi2 system is obtained, from which the relevant phase diagram is determined. Comparison between simulated solidification paths and experimentally observed microstructures demonstrates the use of the thermodynamic database to predict the phase evolution of the alloys during processing. Long-crack fracture toughness measurements made through chevron-notched beam (CNB) tests show that the toughness of the composite alloys are over 2-3 times that of unalloyed silicon, with in-situ tests revealing the role of microstructural toughening (via crack deflection and crack bridging) on the enhanced fracture properties. Ball-on-disk experiments reveal an order of magnitude improvement in the wear resistance of the alloys compared to silicon. This enhanced short-crack response is linked to the fine microstructural size scale of the eutectic structures, which allow toughening mechanisms to be activated during very early stages of crack growth. The range of fracture toughness and wear resistance values measured for the Si-(Cr,V)Si2 alloys elucidates the potential of these materials as viable structural materials in place of powder-processed ceramics.",
    "advisors": ["Christopher A. Schuh"],
    "text": "Development of in-situ toughened silicon-rich alloys : a new class of castable engineering ceramics Despite having a broad set of desirable properties, silicon's potential as a primary constituent in a structural material has not yet been realized because of its extremely low fracture toughness. Motivated by the microstructural design techniques used in toughening inherently brittle ceramic materials, this work aims to develop a silicon-rich alloy with microstructural features that provide for the same types of toughening mechanisms displayed by technical ceramics. In order to add true commercial value to these silicon-based alloys, however, the alloys must be processed using methods more flexible and less expensive than the powder processing routes currently used for engineering ceramics. This thesis will discuss the development of a class of castable silicon-based alloys referred to as silicon-disilicide (Si-XSi2) composites, which naturally form a microstructure composed of a silicon matrix and reinforcing disilicide (XSi2) phase during solidification (where X is a transition metal). Experimental work is performed to characterize the thermal, microstructural, and fracture properties of a specific set of Si-(Cr,V)Si2 alloys which are based on the Si-CrSi2-VSi2 system. First, a reliable thermodynamic description of the Si-CrSi2-VSi2 system is obtained, from which the relevant phase diagram is determined. Comparison between simulated solidification paths and experimentally observed microstructures demonstrates the use of the thermodynamic database to predict the phase evolution of the alloys during processing. Long-crack fracture toughness measurements made through chevron-notched beam (CNB) tests show that the toughness of the composite alloys are over 2-3 times that of unalloyed silicon, with in-situ tests revealing the role of microstructural toughening (via crack deflection and crack bridging) on the enhanced fracture properties. Ball-on-disk experiments reveal an order of magnitude improvement in the wear resistance of the alloys compared to silicon. This enhanced short-crack response is linked to the fine microstructural size scale of the eutectic structures, which allow toughening mechanisms to be activated during very early stages of crack growth. The range of fracture toughness and wear resistance values measured for the Si-(Cr,V)Si2 alloys elucidates the potential of these materials as viable structural materials in place of powder-processed ceramics."
}, {
    "id": "oai:dspace.mit.edu:1721.1/111249",
    "title": "Complex mechanical design of bio-inspired model transient network hydrogels",
    "abstract": "The mechanical properties of viscoelastic soft materials are strongly time-dependent, such that we must describe their mechanical properties with material functions. This is an inherently difficult problem for materials scientists: typically,we define structure-property relationships in terms of scalar material properties, such that modifying a material's structure affects a target material property. However, if the property of interest is function-valued, modifying the material's structure may affect different parts of the material function in undesirable ways. The increased dimensionality of the target material property therefore renders the overall materials design problem for soft materials significantly more difficult. Recently, transient interactions have been shown to vastly improve the mechanical properties of soft materials by providing increased energy dissipation through the dissociation of the reversible bonds. However, there is a wide variety of transient interactions to choose from, varying widely in binding strength, kinetics, specificity, and stoichiometry of the groups that form the association. More research needs to be done to identify what physical laws apply universally across the types of transient associations, and what differentiates the abilities of different types of interactions to control material mechanics. In this thesis,we show how transient metal-coordinate bonds inspired by the chemistry of the mussel byssal threads can be used to engineer viscoelastic material functions in an intuitive and facile manner. We show that intelligent understanding of the thermodynamics and kinetics of metal-coordinate complexes allows quasi-independent control over different regimes of the viscoelastic material function. We draw from classical polymer physics and metal-coordinate chemistry to show that our 4-arm polyethylene glycol-based hydrogels crosslinked with transient histidine:metal bonds represent a uniquely ideal system for probing fundamental questions in how the properties of transient interactions affect viscoelastic material functions. In the final part of this thesis, we extend our control over the viscoelastic material functions of hydrogels by exploiting the redox-sensitivity of histidine:metal crosslinks. In this way, we show how histidine:metal interactions are perhaps more versatile than other types of transient interactions, promising a facile way to examine structure-property relationships in transient networks.",
    "advisors": ["Niels Holten-Andersen"],
    "text": "Complex mechanical design of bio-inspired model transient network hydrogels The mechanical properties of viscoelastic soft materials are strongly time-dependent, such that we must describe their mechanical properties with material functions. This is an inherently difficult problem for materials scientists: typically,we define structure-property relationships in terms of scalar material properties, such that modifying a material's structure affects a target material property. However, if the property of interest is function-valued, modifying the material's structure may affect different parts of the material function in undesirable ways. The increased dimensionality of the target material property therefore renders the overall materials design problem for soft materials significantly more difficult. Recently, transient interactions have been shown to vastly improve the mechanical properties of soft materials by providing increased energy dissipation through the dissociation of the reversible bonds. However, there is a wide variety of transient interactions to choose from, varying widely in binding strength, kinetics, specificity, and stoichiometry of the groups that form the association. More research needs to be done to identify what physical laws apply universally across the types of transient associations, and what differentiates the abilities of different types of interactions to control material mechanics. In this thesis,we show how transient metal-coordinate bonds inspired by the chemistry of the mussel byssal threads can be used to engineer viscoelastic material functions in an intuitive and facile manner. We show that intelligent understanding of the thermodynamics and kinetics of metal-coordinate complexes allows quasi-independent control over different regimes of the viscoelastic material function. We draw from classical polymer physics and metal-coordinate chemistry to show that our 4-arm polyethylene glycol-based hydrogels crosslinked with transient histidine:metal bonds represent a uniquely ideal system for probing fundamental questions in how the properties of transient interactions affect viscoelastic material functions. In the final part of this thesis, we extend our control over the viscoelastic material functions of hydrogels by exploiting the redox-sensitivity of histidine:metal crosslinks. In this way, we show how histidine:metal interactions are perhaps more versatile than other types of transient interactions, promising a facile way to examine structure-property relationships in transient networks."
}, {
    "id": "oai:dspace.mit.edu:1721.1/9378",
    "title": "Impact of scale on performance and technology in process-intensive industry",
    "abstract": "Two surveys are performed to determine production methods, competitive strategies, and scale disadvantages for a group of small manufacturing plants. Detailed comparisons of economic, operational, and development activities are presented to identify differences between industry production standards and small-scale plants. As a group, the small-scale plants had similar production costs to the standard-scale plants. The small-scale plants experienced lower average unit fixed costs as a result of lower capital investment and indirect labor expenses. The small-scale plants operated closer to their theoretical efficiency levels than the standard-scale plants. The procedure used to collect and analyze data is referred to as the direct comparison method. The direct comparison method involves conducting detailed one-to-one comparisons of production systems at the plant-level. A pattern of operational trends are reported that contribute to the economic performance of small-scale plants. The small-scale plants compensated for scale disadvantages by having greater technology independence, conserving capital, conducting internal development, and promoting process competence.",
    "advisors": ["Thomas W. Eagar"],
    "text": "Impact of scale on performance and technology in process-intensive industry Two surveys are performed to determine production methods, competitive strategies, and scale disadvantages for a group of small manufacturing plants. Detailed comparisons of economic, operational, and development activities are presented to identify differences between industry production standards and small-scale plants. As a group, the small-scale plants had similar production costs to the standard-scale plants. The small-scale plants experienced lower average unit fixed costs as a result of lower capital investment and indirect labor expenses. The small-scale plants operated closer to their theoretical efficiency levels than the standard-scale plants. The procedure used to collect and analyze data is referred to as the direct comparison method. The direct comparison method involves conducting detailed one-to-one comparisons of production systems at the plant-level. A pattern of operational trends are reported that contribute to the economic performance of small-scale plants. The small-scale plants compensated for scale disadvantages by having greater technology independence, conserving capital, conducting internal development, and promoting process competence."
}, {
    "id": "oai:dspace.mit.edu:1721.1/111320",
    "title": "Understanding and engineering azobenzene for thermal energy storage",
    "abstract": "This thesis focuses on the understanding and engineering of a molecule known as azobenzene which holds unique properties for thermal storage applications. The azobenzene molecule undergoes structural change into a metastable state which has the ability to store energy. This thesis utilizes the energy storage and structural change properties of this molecule to develop new materials for thermal energy storage. The first is through a concept called solar thermal fuel which is storing the solar energy in rearranged bonds of the azobenzene and later releasing that energy in the form of heat. The second approach is through the structural property difference of its two states in order to moderate the phase change temperature of organic phase change materials. Essentially, the molecule azobenzene was modified and engineered to be used as a thermal battery as well as to mediate thermal energy storage in other materials. The first chapter will give a brief introduction on the concept and past examples of solar thermal fuel. Chapter 2, 3, 4 will discuss about the development of solar thermal fuel while chapter 5 discusses about a recently developed concept of using azobenzene to moderate phase change temperature. Chapter 2 shows the first demonstration of using solar thermal fuel in the solid state through functionalizing azobenzene on a polymer template. The polymer platform allows fabrication of a thin film of this material which enabled charging, discharging, and heat release using optically chargeable molecules all within the solid-state. A demonstration of solid state application was shown by constructing a macroscopic device which resulted in heat release bringing a temperature increase of as high as 10 OC. Next in chapter 3, azobenzene was engineered on the molecular lever with bulky aromatic groups (phenyl, biphenyl, and tert-butyl phenyl groups). The molecules were designed and synthesized for the purpose of increasing energy stored while promoting solid state solar thermal fuels. The design allowed fabrication of molecular based thin film, which was able to be charged with light, a great improvement from the original azobenzene, which crystallized preventing switching in the solid state. Molecular engineering proved to be a powerful and effective method in improving other solar thermal fuel properties, such as energy storage in STFs, chargeability, and also the thermal stability of the molecular thin film. In chapter 4, new diacetylene derivatives with azobenzene moieties and with varied alkyl spacers and linkers were synthesized to show photocontrolled self-assembly and disassembly of photon energy storage materials. This azobenzene decorated diacetylenes not only allowed solar energy storage but also demonstrated phase change characteristic of organic materials can be a parameter to consider in terms of designing high energy density photon energy storage materials. Chapter 5 discusses azobenzene based dopants in organic phase change material to photomoderate the phase change temperature. Three different types, 8 in total, organic phase change materials were tested to show the possibilty of this concept in a wide variety of phase change materials. A deep understanding was developed giving parameters to achieve a large phase change temperature difference in the organic phase change materials using the structual difference of the trans and the cis state of azobenzene.",
    "advisors": ["Jeffrey C. Grossman"],
    "text": "Understanding and engineering azobenzene for thermal energy storage This thesis focuses on the understanding and engineering of a molecule known as azobenzene which holds unique properties for thermal storage applications. The azobenzene molecule undergoes structural change into a metastable state which has the ability to store energy. This thesis utilizes the energy storage and structural change properties of this molecule to develop new materials for thermal energy storage. The first is through a concept called solar thermal fuel which is storing the solar energy in rearranged bonds of the azobenzene and later releasing that energy in the form of heat. The second approach is through the structural property difference of its two states in order to moderate the phase change temperature of organic phase change materials. Essentially, the molecule azobenzene was modified and engineered to be used as a thermal battery as well as to mediate thermal energy storage in other materials. The first chapter will give a brief introduction on the concept and past examples of solar thermal fuel. Chapter 2, 3, 4 will discuss about the development of solar thermal fuel while chapter 5 discusses about a recently developed concept of using azobenzene to moderate phase change temperature. Chapter 2 shows the first demonstration of using solar thermal fuel in the solid state through functionalizing azobenzene on a polymer template. The polymer platform allows fabrication of a thin film of this material which enabled charging, discharging, and heat release using optically chargeable molecules all within the solid-state. A demonstration of solid state application was shown by constructing a macroscopic device which resulted in heat release bringing a temperature increase of as high as 10 OC. Next in chapter 3, azobenzene was engineered on the molecular lever with bulky aromatic groups (phenyl, biphenyl, and tert-butyl phenyl groups). The molecules were designed and synthesized for the purpose of increasing energy stored while promoting solid state solar thermal fuels. The design allowed fabrication of molecular based thin film, which was able to be charged with light, a great improvement from the original azobenzene, which crystallized preventing switching in the solid state. Molecular engineering proved to be a powerful and effective method in improving other solar thermal fuel properties, such as energy storage in STFs, chargeability, and also the thermal stability of the molecular thin film. In chapter 4, new diacetylene derivatives with azobenzene moieties and with varied alkyl spacers and linkers were synthesized to show photocontrolled self-assembly and disassembly of photon energy storage materials. This azobenzene decorated diacetylenes not only allowed solar energy storage but also demonstrated phase change characteristic of organic materials can be a parameter to consider in terms of designing high energy density photon energy storage materials. Chapter 5 discusses azobenzene based dopants in organic phase change material to photomoderate the phase change temperature. Three different types, 8 in total, organic phase change materials were tested to show the possibilty of this concept in a wide variety of phase change materials. A deep understanding was developed giving parameters to achieve a large phase change temperature difference in the organic phase change materials using the structual difference of the trans and the cis state of azobenzene."
}, {
    "id": "oai:dspace.mit.edu:1721.1/8442",
    "title": "Dynamic actuation properties of Ni-Mn-Ga ferromagnetic shape memory alloys",
    "abstract": "Dynamic magnetic-field-induced strain actuation of up to 3% with a frequency bandwidth of least 500 Hz in Ni48.5Mn29.5Ga21 ferromagnetic shape memory alloys (FMSAs) is achieved. Hardware was designed and constructed to measure frequency bandwidth, magnetic-field-induced strain, stress and magnetization driven from an applied magnetic field. The bandwidth in this investigation was only limited by inductive reactance of the hardware, not by fundamental limitations of Ni-Mn-Ga. Degradation of the peak dynamic actuation strain occurred from 3.0% to 2.6% with increasing number of cycles from Nz1,000 to N 100,000. Measurement of strain, stress, and magnetization driven by a magnetic field permitted the comparison of measured properties versus properly defined thermodynamic properties. The peak thermodynamic piezomagnetic coefficient is d3, 1,= 2.5 x 10-7m / A compared to the experimental slope, dE/dH, of 1.0 x 10-7 m / A at N-1,000 cycles and 1.4 x 10-7 m / A at N-100,000 cycles, respectively. The thermodynamic piezomagnetic coefficient is five times greater than Terfenol-D with d31 = 5.0 x 10-m / A. The magnetic susceptibility varies between 3-10, while the twinning stiffness varies between 30-40 MPa within the average bias stress range of 0.3 to 2.8 MPa. At optimum fields and bias stresses, the mechanical energy density during cyclic deformation was 65 kJ/m3 at the expense of 20 kJ/m3 lost An important first observation of dynamic stress vs. field behavior is understood by an extension of a magnetomechanical phenomenological model.",
    "advisors": ["Samuel M. Allen", "Robert C. O'Handley"],
    "text": "Dynamic actuation properties of Ni-Mn-Ga ferromagnetic shape memory alloys Dynamic magnetic-field-induced strain actuation of up to 3% with a frequency bandwidth of least 500 Hz in Ni48.5Mn29.5Ga21 ferromagnetic shape memory alloys (FMSAs) is achieved. Hardware was designed and constructed to measure frequency bandwidth, magnetic-field-induced strain, stress and magnetization driven from an applied magnetic field. The bandwidth in this investigation was only limited by inductive reactance of the hardware, not by fundamental limitations of Ni-Mn-Ga. Degradation of the peak dynamic actuation strain occurred from 3.0% to 2.6% with increasing number of cycles from Nz1,000 to N 100,000. Measurement of strain, stress, and magnetization driven by a magnetic field permitted the comparison of measured properties versus properly defined thermodynamic properties. The peak thermodynamic piezomagnetic coefficient is d3, 1,= 2.5 x 10-7m / A compared to the experimental slope, dE/dH, of 1.0 x 10-7 m / A at N-1,000 cycles and 1.4 x 10-7 m / A at N-100,000 cycles, respectively. The thermodynamic piezomagnetic coefficient is five times greater than Terfenol-D with d31 = 5.0 x 10-m / A. The magnetic susceptibility varies between 3-10, while the twinning stiffness varies between 30-40 MPa within the average bias stress range of 0.3 to 2.8 MPa. At optimum fields and bias stresses, the mechanical energy density during cyclic deformation was 65 kJ/m3 at the expense of 20 kJ/m3 lost An important first observation of dynamic stress vs. field behavior is understood by an extension of a magnetomechanical phenomenological model."
}, {
    "id": "oai:dspace.mit.edu:1721.1/44320",
    "title": "Towards first-principles electrochemistry",
    "abstract": "This doctoral dissertation presents a comprehensive computational approach to describe quantum mechanical systems embedded in complex ionic media, primarily focusing on the first-principles representation of catalytic electrodes under electrochemical conditions. The accurate electrostatic description of electrified metal-solution interfaces represents a persistent challenge for ab-initio simulations and an essential requisite for predicting the electrical response of electrochemical convertors-i.e., the correspondence between the macroscopic voltage and the microscopic interfacial charge distribution. The approach consists of controlling the electrode voltage via its conjugate extensive variable, namely, the charge of the system. As a preliminary to the study of electrified interfaces in ionic media, we analyze charged slabs in vacuum subject to periodic boundary conditions. We show that the corrective potential (defined as the difference between the exact open-boundary potential and the periodic potential obtained from a Fourier transform) varies smoothly over space, allowing for its determination on a coarse mesh using optimized electrostatic solvers. Because this scheme takes into account exact open boundary conditions, its performance is considerably superior to that of conventional corrective methods. Extending this computational scheme, we present an efficient approach to model electrochemical systems under realistic conditions, based on a first-principles description of the interface region and on a continuum representation of the ionic solvent.",
    "advisors": ["Nicola Marzari"],
    "text": "Towards first-principles electrochemistry This doctoral dissertation presents a comprehensive computational approach to describe quantum mechanical systems embedded in complex ionic media, primarily focusing on the first-principles representation of catalytic electrodes under electrochemical conditions. The accurate electrostatic description of electrified metal-solution interfaces represents a persistent challenge for ab-initio simulations and an essential requisite for predicting the electrical response of electrochemical convertors-i.e., the correspondence between the macroscopic voltage and the microscopic interfacial charge distribution. The approach consists of controlling the electrode voltage via its conjugate extensive variable, namely, the charge of the system. As a preliminary to the study of electrified interfaces in ionic media, we analyze charged slabs in vacuum subject to periodic boundary conditions. We show that the corrective potential (defined as the difference between the exact open-boundary potential and the periodic potential obtained from a Fourier transform) varies smoothly over space, allowing for its determination on a coarse mesh using optimized electrostatic solvers. Because this scheme takes into account exact open boundary conditions, its performance is considerably superior to that of conventional corrective methods. Extending this computational scheme, we present an efficient approach to model electrochemical systems under realistic conditions, based on a first-principles description of the interface region and on a continuum representation of the ionic solvent."
}, {
    "id": "oai:dspace.mit.edu:1721.1/16702",
    "title": "BaTiO based materials for piezoelectric and electro-optic applications",
    "abstract": "Ferroelectric materials are key to many modem technologies, in particular piezoelectric actuators and electro-optic modulators. BaTiO is one of the most extensively studied ferroelectric materials. The use of BaTiO for piezoelectric applications is, however, limited due to the small piezoelectric coefficient of the room temperature-stable tetragonal phase. Furthermore, research on BaTiO for integrated optics applications remains sparse. In this work Zr-, Hf-, and KNb- doped BaTiO materials were prepared in a composition range that stabilizes the rhombohedral phase. These materials were prepared as bulk polycrystals using a standard solid-state reaction technique in order to test the piezoelectric and dielectric properties. Some compositions were then chosen for thin film deposition. The films were deposited using pulsed laser deposition on MgO and SOI substrates. Growth orientation, remnant strain and optical properties were then measured. X-ray diffraction was used to confirm the existence of a stable rhombohedral phase. Dielectric measurements confirmed the expected phase transition temperatures. A piezoelectric coefficient of d=290-470pc/N was measured for Zr- and Hf- doped BaTiO, compared with d=75pC/N for pure BaTiO. The electrostrictive coefficient of the KNb-doped material, was measured as Q33=0.37m/C, compared with Q33=0.11m/C for pure BaTiO. The maximum strain measured for the doped samples was 5-10 times higher then that of pure BaTiO. The effect of growth conditions on the orientation and strain of BaTiO thin films was studied. As the substrate temperature and laser fluency were increased the film orientation varied from (111) to (110), then to (100). Zr- and Hf- doping helped lower the forming temperature for the",
    "advisors": ["Harry L. Tuller"],
    "text": "BaTiO based materials for piezoelectric and electro-optic applications Ferroelectric materials are key to many modem technologies, in particular piezoelectric actuators and electro-optic modulators. BaTiO is one of the most extensively studied ferroelectric materials. The use of BaTiO for piezoelectric applications is, however, limited due to the small piezoelectric coefficient of the room temperature-stable tetragonal phase. Furthermore, research on BaTiO for integrated optics applications remains sparse. In this work Zr-, Hf-, and KNb- doped BaTiO materials were prepared in a composition range that stabilizes the rhombohedral phase. These materials were prepared as bulk polycrystals using a standard solid-state reaction technique in order to test the piezoelectric and dielectric properties. Some compositions were then chosen for thin film deposition. The films were deposited using pulsed laser deposition on MgO and SOI substrates. Growth orientation, remnant strain and optical properties were then measured. X-ray diffraction was used to confirm the existence of a stable rhombohedral phase. Dielectric measurements confirmed the expected phase transition temperatures. A piezoelectric coefficient of d=290-470pc/N was measured for Zr- and Hf- doped BaTiO, compared with d=75pC/N for pure BaTiO. The electrostrictive coefficient of the KNb-doped material, was measured as Q33=0.37m/C, compared with Q33=0.11m/C for pure BaTiO. The maximum strain measured for the doped samples was 5-10 times higher then that of pure BaTiO. The effect of growth conditions on the orientation and strain of BaTiO thin films was studied. As the substrate temperature and laser fluency were increased the film orientation varied from (111) to (110), then to (100). Zr- and Hf- doping helped lower the forming temperature for the"
}, {
    "id": "oai:dspace.mit.edu:1721.1/44386",
    "title": "Morphology and self-assembly behavior of side chain liquid crystalline block copolymers",
    "abstract": "There is significant interest from both the academic and industrial communities for understanding and controlling the self-assembly behavior of complex macromolecular systems and has been an active area of research in recent years. Such systems can be designed to result in a wide range of nanoscale morphologies and greater functionality can be introduced with increasing complexity.This thesis focuses on the synthesis and characterization of a class of side chain liquid crystalline block copolymers (SCLCBCPs) that are based on a low glass transition temperature (Tg) siloxane backbone. Moieties that self-assemble into smectic liquid crystalline (LC) phases are covalently attached to the polystyrene-polyvinylmethylsiloxane (PS-PVMS) block copolymer backbone. Precise control over the functionalization of the LCs onto the functional siloxane backbone allows for unique control over the self-assembly and the resulting properties of the system. The LC content significantly affects the stability of the smectic mesophase and subsequently the interactions with the inter-material dividing surface (IMDS) with the PS domains. A strong preference for homogenous anchoring of the LC moieties relative to the IMDS is observed, and increasing the LC content intensifies the preference for this arrangement. Utilizing the effects of LC anchoring to alter the self-assembly behavior is a reoccurring theme throughout this work. Additionally, the mechanical properties of these materials can be precisely manipulated over several orders of magnitude through variations in LC content and the block copolymer backbone architecture.Several methods can be used to manipulate the morphologies of these materials once synthesized including, thermal annealing and mechanical deformation.",
    "advisors": ["Paula T. Hammond", "Edwin L. Thomas"],
    "text": "Morphology and self-assembly behavior of side chain liquid crystalline block copolymers There is significant interest from both the academic and industrial communities for understanding and controlling the self-assembly behavior of complex macromolecular systems and has been an active area of research in recent years. Such systems can be designed to result in a wide range of nanoscale morphologies and greater functionality can be introduced with increasing complexity.This thesis focuses on the synthesis and characterization of a class of side chain liquid crystalline block copolymers (SCLCBCPs) that are based on a low glass transition temperature (Tg) siloxane backbone. Moieties that self-assemble into smectic liquid crystalline (LC) phases are covalently attached to the polystyrene-polyvinylmethylsiloxane (PS-PVMS) block copolymer backbone. Precise control over the functionalization of the LCs onto the functional siloxane backbone allows for unique control over the self-assembly and the resulting properties of the system. The LC content significantly affects the stability of the smectic mesophase and subsequently the interactions with the inter-material dividing surface (IMDS) with the PS domains. A strong preference for homogenous anchoring of the LC moieties relative to the IMDS is observed, and increasing the LC content intensifies the preference for this arrangement. Utilizing the effects of LC anchoring to alter the self-assembly behavior is a reoccurring theme throughout this work. Additionally, the mechanical properties of these materials can be precisely manipulated over several orders of magnitude through variations in LC content and the block copolymer backbone architecture.Several methods can be used to manipulate the morphologies of these materials once synthesized including, thermal annealing and mechanical deformation."
}, {
    "id": "oai:dspace.mit.edu:1721.1/53242",
    "title": "Atomistic calculations of rate of long-timescale microstructural evolution",
    "abstract": "The ability to investigate materials systems at the resolution of individual atoms makes computational simulations a powerful tool for the study of materials phenomena. However, microstructural evolution in complex materials is only meaningfully characterized in laboratory or industry applications by deformation rate and relevant rate coefficients, quantities that require sampling over a timescale too large for traditional atomistic methods to probe. New methods and techniques have to be developed in order to obtain useful information of rate from atomistic simulations. In this thesis, we explore a set of four problems, related to two long-timescale microstructural phenomena, creep and oxidation, and use a variety of atomistic methods appropriate to each problem to demonstrate the techniques of obtaining rate information. Creep due to vacancy-driven dislocation climb critically depends on the movement of the vacancies in the bulk towards dislocation cores, and for the first contribution of the thesis we investigate the influence of carbon solute atoms on vacancy diffusion pathways in bulk BCC Fe. Using these results, we draw explanations of the trends of the experimentally-observed rate of creep. It is well-known that vacancy energetics vary with distance from dislocation cores due to the dislocation strain field, but the effect this has on creep by the dislocation climb mechanism is not well understood. In the second contribution of the thesis, we present an investigation of the vacancy-dislocation interaction of BCC Fe.",
    "advisors": ["Sidney Yip"],
    "text": "Atomistic calculations of rate of long-timescale microstructural evolution The ability to investigate materials systems at the resolution of individual atoms makes computational simulations a powerful tool for the study of materials phenomena. However, microstructural evolution in complex materials is only meaningfully characterized in laboratory or industry applications by deformation rate and relevant rate coefficients, quantities that require sampling over a timescale too large for traditional atomistic methods to probe. New methods and techniques have to be developed in order to obtain useful information of rate from atomistic simulations. In this thesis, we explore a set of four problems, related to two long-timescale microstructural phenomena, creep and oxidation, and use a variety of atomistic methods appropriate to each problem to demonstrate the techniques of obtaining rate information. Creep due to vacancy-driven dislocation climb critically depends on the movement of the vacancies in the bulk towards dislocation cores, and for the first contribution of the thesis we investigate the influence of carbon solute atoms on vacancy diffusion pathways in bulk BCC Fe. Using these results, we draw explanations of the trends of the experimentally-observed rate of creep. It is well-known that vacancy energetics vary with distance from dislocation cores due to the dislocation strain field, but the effect this has on creep by the dislocation climb mechanism is not well understood. In the second contribution of the thesis, we present an investigation of the vacancy-dislocation interaction of BCC Fe."
}, {
    "id": "oai:dspace.mit.edu:1721.1/8768",
    "title": "Transmission and routing of optical signals in on-chip waveguides for silicon microphotonics",
    "abstract": "In this thesis, guiding and routing of optical signals in high index difference ([delta]m) waveguide systems are studied for silicon microphotonic applications. High [delta]n waveguide systems offer compact device sizes that enable highly dense integrated optics suitable for silicon microphotonics. Scattering loss due to the roughness at the core/cladding interfaces is identified as a major source of loss in a high M system. Using both experimental and theoretical approaches, the interdependence of scattering loss, waveguide dimension, and roughness is investigated. We developed a 3 dimensional model that successfully explains the scattering loss dependence on the waveguide dimension. Using this model, a loss contour map is constructed to better understand the scattering loss from interface roughness. This map provides an effective methodology to reduce roughness scattering, which we used to develop two fabrication technologies. Loss reduction from 32 dB/cm to 0.8 dB/cm is achieved for [delta]n =2.0. This is the lowest loss ever achieved for a single-mode, high An system. PolySi/Si02 waveguide systems are investigated due to the compatibility of multi-level processing. Our best PolySi/Si02 waveguide shows additional 10 dB/cm loss, coming mainly from the top surface roughness due to grain boundary grooving. compared to a Si/Si02 waveguide. Compact high An routing devices such as round bends, Y-splitters, and Multi-Mode Interference (MMI) splitters are fabricated and tested. We show that single-mode waveguide bends exhibit m size bending with low loss and single-mode splitters show splitting with good uniformity. MMis show advantages over equivalent Y-splitter based structures in terms of size and loss. Our MMI design led to the fabrication of the smallest optical 1x16 fanout ever built. High Transmission Cavity (HTC) based bends, splitters, and resonators, that are compatible with an anisotropic etching technique, are demonstrated. An index engineering map, which shows competing trends of minimum bending radius and scattering loss as tin is changed. is constructed. From this map, the optimal M can be found for a given fabrication technology. Improvement in the fabrication technology allows for higher tin and provides a scaling law in optical devices. This point is proven by our 0.8 dB/cm Si/Si02 waveguides, which lifts the upper limit of the usable [delta]n.",
    "advisors": ["Lionel C. Kimerling"],
    "text": "Transmission and routing of optical signals in on-chip waveguides for silicon microphotonics In this thesis, guiding and routing of optical signals in high index difference ([delta]m) waveguide systems are studied for silicon microphotonic applications. High [delta]n waveguide systems offer compact device sizes that enable highly dense integrated optics suitable for silicon microphotonics. Scattering loss due to the roughness at the core/cladding interfaces is identified as a major source of loss in a high M system. Using both experimental and theoretical approaches, the interdependence of scattering loss, waveguide dimension, and roughness is investigated. We developed a 3 dimensional model that successfully explains the scattering loss dependence on the waveguide dimension. Using this model, a loss contour map is constructed to better understand the scattering loss from interface roughness. This map provides an effective methodology to reduce roughness scattering, which we used to develop two fabrication technologies. Loss reduction from 32 dB/cm to 0.8 dB/cm is achieved for [delta]n =2.0. This is the lowest loss ever achieved for a single-mode, high An system. PolySi/Si02 waveguide systems are investigated due to the compatibility of multi-level processing. Our best PolySi/Si02 waveguide shows additional 10 dB/cm loss, coming mainly from the top surface roughness due to grain boundary grooving. compared to a Si/Si02 waveguide. Compact high An routing devices such as round bends, Y-splitters, and Multi-Mode Interference (MMI) splitters are fabricated and tested. We show that single-mode waveguide bends exhibit m size bending with low loss and single-mode splitters show splitting with good uniformity. MMis show advantages over equivalent Y-splitter based structures in terms of size and loss. Our MMI design led to the fabrication of the smallest optical 1x16 fanout ever built. High Transmission Cavity (HTC) based bends, splitters, and resonators, that are compatible with an anisotropic etching technique, are demonstrated. An index engineering map, which shows competing trends of minimum bending radius and scattering loss as tin is changed. is constructed. From this map, the optimal M can be found for a given fabrication technology. Improvement in the fabrication technology allows for higher tin and provides a scaling law in optical devices. This point is proven by our 0.8 dB/cm Si/Si02 waveguides, which lifts the upper limit of the usable [delta]n."
}, {
    "id": "oai:dspace.mit.edu:1721.1/117791",
    "title": "Interface-driven spin-orbit torques in magnetic heterostructures",
    "abstract": "The connection between charge and spin transport in solid state materials offers new techniques for generating and detecting spin currents and could potentially allow high-performance memory and logic devices. Simple multilayer thin films or \"heterostructures\" such as Pt/Co have broken inversion symmetry, so a charge current gives rise to net spin current. Electrical and optical measurements reveal the effect of spin current on the magnetization, including chiral spin textures such as domain walls (DWs) and skyrmions. The magnetic properties of an ultrathin magnetic film are strongly sensitive to interfacial effects such as interfacial anisotropy and the Dzyaloshinskii-Moriya Interaction (DMI), which stabilizes chiral spin textures. This thesis is motivated to systematically vary the layer structure of magnetic heterostructures to understand and quantify spin-orbit torques. I showed that switching efficiency is consistent with harmonic spin orbit torque measurements in Pt/Co/Ta. My automation software and improved electromagnet enabled a new experimental technique that highlights the role of DMI in spin-orbit torque switching. I showed that a gold spacer layer inserted between platinum and cobalt independently modulates the DMI and spin transport. I demonstrated SOT switching of a ferromagnetic insulator for the first time. I also developed a temperature-controlled, high-speed electrical and optical measuring system to observe record-breaking DW velocity in ferrimagnetic GdCo. This thesis focuses on building experimental apparatus and understanding spin-orbit torques.",
    "advisors": ["Geoffrey S. D. Beach"],
    "text": "Interface-driven spin-orbit torques in magnetic heterostructures The connection between charge and spin transport in solid state materials offers new techniques for generating and detecting spin currents and could potentially allow high-performance memory and logic devices. Simple multilayer thin films or \"heterostructures\" such as Pt/Co have broken inversion symmetry, so a charge current gives rise to net spin current. Electrical and optical measurements reveal the effect of spin current on the magnetization, including chiral spin textures such as domain walls (DWs) and skyrmions. The magnetic properties of an ultrathin magnetic film are strongly sensitive to interfacial effects such as interfacial anisotropy and the Dzyaloshinskii-Moriya Interaction (DMI), which stabilizes chiral spin textures. This thesis is motivated to systematically vary the layer structure of magnetic heterostructures to understand and quantify spin-orbit torques. I showed that switching efficiency is consistent with harmonic spin orbit torque measurements in Pt/Co/Ta. My automation software and improved electromagnet enabled a new experimental technique that highlights the role of DMI in spin-orbit torque switching. I showed that a gold spacer layer inserted between platinum and cobalt independently modulates the DMI and spin transport. I demonstrated SOT switching of a ferromagnetic insulator for the first time. I also developed a temperature-controlled, high-speed electrical and optical measuring system to observe record-breaking DW velocity in ferrimagnetic GdCo. This thesis focuses on building experimental apparatus and understanding spin-orbit torques."
}, {
    "id": "oai:dspace.mit.edu:1721.1/45951",
    "title": "Combinatorial lipid-like materials for intracellular delivery of small RNAs that activate innate antiviral immune responses and adjuvant vaccines",
    "abstract": "RNA-based therapy is an exciting new realm of experimental medicine due to the diverse roles of RNA in the human body. RNA function depends on sequence, structure, and cellular location. Whereas cytosolic short-interfering RNA (siRNA) can be used to turn off genes through RNA interference (RNAi), it was observed that the same siRNA can also activate an innate immune response. Recognition of RNA by specialized immune cells occurs through pattern recognition receptors, which have evolved to respond to RNA viruses, such as the Toll-like receptors (TLR) 7 and 8 located in the endosome of dendritic cells. Tailoring the multiple functions of RNA for a desired clinical application will require novel systems for intracellular delivery. A library of structurally-related cationic lipid-like materials, termed \"lipidoids\", was developed to facilitate uptake of small RNAs, and the role of drug delivery in controlling RNA function was investigated. In an experimental animal model of RNA interference of influenza virus, treatment with a lipidoid-siRNA nanoparticle efficiently activated a type I interferon response in a sequence-dependent manner suppressing lung viral titer over 97%. Specific chemical modifications to the siRNA prevented TLR7/8 engagement and also prevented antiviral responses, confirming an RNAi-independent mechanism of antiviral activity. Recognizing the therapeutic potential of immunostimulatory RNA (isRNA), a novel in vitro high- throughput assay was developed to screen the lipidoid library for delivery of isRNA.",
    "advisors": ["Robert S. Langer"],
    "text": "Combinatorial lipid-like materials for intracellular delivery of small RNAs that activate innate antiviral immune responses and adjuvant vaccines RNA-based therapy is an exciting new realm of experimental medicine due to the diverse roles of RNA in the human body. RNA function depends on sequence, structure, and cellular location. Whereas cytosolic short-interfering RNA (siRNA) can be used to turn off genes through RNA interference (RNAi), it was observed that the same siRNA can also activate an innate immune response. Recognition of RNA by specialized immune cells occurs through pattern recognition receptors, which have evolved to respond to RNA viruses, such as the Toll-like receptors (TLR) 7 and 8 located in the endosome of dendritic cells. Tailoring the multiple functions of RNA for a desired clinical application will require novel systems for intracellular delivery. A library of structurally-related cationic lipid-like materials, termed \"lipidoids\", was developed to facilitate uptake of small RNAs, and the role of drug delivery in controlling RNA function was investigated. In an experimental animal model of RNA interference of influenza virus, treatment with a lipidoid-siRNA nanoparticle efficiently activated a type I interferon response in a sequence-dependent manner suppressing lung viral titer over 97%. Specific chemical modifications to the siRNA prevented TLR7/8 engagement and also prevented antiviral responses, confirming an RNAi-independent mechanism of antiviral activity. Recognizing the therapeutic potential of immunostimulatory RNA (isRNA), a novel in vitro high- throughput assay was developed to screen the lipidoid library for delivery of isRNA."
}, {
    "id": "oai:dspace.mit.edu:1721.1/104183",
    "title": "Fiber drawing : beyond the scaling paradigm",
    "abstract": "The emergence of multimaterial fibers that combine a multiplicity of solid materials with disparate electrical, optical, and mechanical properties into a single fiber presents new opportunities for extending fiber applications. Different functional fiber devices have been fabricated with a thermal co-draw approach. In order to make the thermal co-draw feasible, only materials with similar viscosity at the draw temperature are used, which excludes a wide range of metal and semiconductors that have good electrical property but not compatible viscosity profile. From the fiber structure point of view, the nature of the fiber drawing process makes fabricating a large quantity of fiber with identical inner structures feasible. The scalability of thermal drawing approach offers access to large quantities of devices however constrains the devices to be translational symmetric. Lifting this symmetry to create discrete devices in fibers will increase the utility of fiber devices. Also, the surface of the fiber is rarely studied though complex inner structure have been fabricated for different functionalities. Functionalize the fiber surface would give fiber the ability to better interact with the outer environment. This thesis seeks to address the abovementioned considerations, i.e. to expand materials selection for the fiber co-draw process and to explore variance of the fiber structure including breaking the inner structure translational symmetry and functionalize the outer surface. On the material side, a chemical reaction phenomenon is observed and studied in two different fiber drawing situations. In both cases, new composition is formed during the draw and play an important role in the formed fiber devices. On the structure side, relying on the principle of Plateau-Rayleigh instability, the fiber inner structure is designed to form a series of discrete semiconductor spheres contacting two metal buses after a thermal selective breakup process. This gives rise to photodecting devices in a silica-cladding fiber which shows a large working bandwidth. The fiber surface is also studied and successfully patterned with micron-scale features during the draw process. The formed patterned fiber surface shows potential in structural coloration and directional wetting.",
    "advisors": ["Yoel Fink"],
    "text": "Fiber drawing : beyond the scaling paradigm The emergence of multimaterial fibers that combine a multiplicity of solid materials with disparate electrical, optical, and mechanical properties into a single fiber presents new opportunities for extending fiber applications. Different functional fiber devices have been fabricated with a thermal co-draw approach. In order to make the thermal co-draw feasible, only materials with similar viscosity at the draw temperature are used, which excludes a wide range of metal and semiconductors that have good electrical property but not compatible viscosity profile. From the fiber structure point of view, the nature of the fiber drawing process makes fabricating a large quantity of fiber with identical inner structures feasible. The scalability of thermal drawing approach offers access to large quantities of devices however constrains the devices to be translational symmetric. Lifting this symmetry to create discrete devices in fibers will increase the utility of fiber devices. Also, the surface of the fiber is rarely studied though complex inner structure have been fabricated for different functionalities. Functionalize the fiber surface would give fiber the ability to better interact with the outer environment. This thesis seeks to address the abovementioned considerations, i.e. to expand materials selection for the fiber co-draw process and to explore variance of the fiber structure including breaking the inner structure translational symmetry and functionalize the outer surface. On the material side, a chemical reaction phenomenon is observed and studied in two different fiber drawing situations. In both cases, new composition is formed during the draw and play an important role in the formed fiber devices. On the structure side, relying on the principle of Plateau-Rayleigh instability, the fiber inner structure is designed to form a series of discrete semiconductor spheres contacting two metal buses after a thermal selective breakup process. This gives rise to photodecting devices in a silica-cladding fiber which shows a large working bandwidth. The fiber surface is also studied and successfully patterned with micron-scale features during the draw process. The formed patterned fiber surface shows potential in structural coloration and directional wetting."
}, {
    "id": "oai:dspace.mit.edu:1721.1/98639",
    "title": "How beetles explode : new insights into the operation, structure, and materials of bombardier beetle (Brachinini) defensive glands",
    "abstract": "Bombardier beetles possess one of the most remarkable defense mechanisms in nature, using explosions inside their bodies to synthesize and eject a hot, noxious spray at attackers. The chemical reactions that enable this process are well understood, but many aspects of the beetles' two-chambered defensive glands, which house the explosions and produce the defensive spray, remain unexplored. In this Thesis, I describe our recent progress in understanding the operation, structure, and materials composition of the defensive glands-topics which have to date received little treatment in the literature-of the best-known bombardier beetles, the brachinines (Carabidae: Brachininae: Brachinini). Chapter 2 deals with the pulsed-jet character of brachinines' sprays, which is in contrast to all other types of bombardier beetles that emit their sprays as continuous streams. Brachinine sprays comprise a number of spray pulses emitted in a rapid sequence, each pulse formed in a discrete explosion event inside the reaction chamber of the defensive gland, with the frequency of pulsation ranging from 300 to 1000 Hz. Using a combination of high-speed synchrotron x-ray phase-contrast imaging of live beetles, anatomical studies of the excised defensive glands, and mathematical analyses, we determined that spray pulsation arises due to explosion-induced displacement of the inlet structures to the reaction chamber periodically cutting o the ow of reactant solution into the reaction chamber. In Chapter 3, the interior cuticular microsculpture of the reaction chamber is studied using scanning electron microscopy and synchrotron x-ray phase-contrast microtomography. The microscupture is found to be highly complex, with a number of distinct spiny microtextures localized to specific regions of the reaction chamber. Quantitative details of the spine lengths and spacings are reported, and on the basis of the similarity of some of the features to the beetles' external abdominal microscupture and the micro-textural transitions observed inside the reaction chamber, we hypothesize that the reaction chamber microsculpture is homologous with the exterior microsculpture, consistent with the fact that the defensive glands are invaginations of the abdomen. Chapter 4 reports our preliminary investigations of the materials composition of the defensive glands. We use scanning electron microscopy to examine the fibrous composite structure of the gland cuticle and employ various light microscopy techniques to understand spatial variations in the cuticle sclerotization and chemical composition. The reaction chamber is found to exhibit dramatic spatial variation in sclerotization, including several lightly sclerotized regions, and possible functions of these regions are proposed. Additionally, the inter-chamber valve is found to contain the rubber-like protein resilin, likely as an adaptation to allow the valve to consistently make and hold a tight seal during each explosion, in analogy to rubber gaskets used in technological valve applications.",
    "advisors": ["Christine Ortiz"],
    "text": "How beetles explode : new insights into the operation, structure, and materials of bombardier beetle (Brachinini) defensive glands Bombardier beetles possess one of the most remarkable defense mechanisms in nature, using explosions inside their bodies to synthesize and eject a hot, noxious spray at attackers. The chemical reactions that enable this process are well understood, but many aspects of the beetles' two-chambered defensive glands, which house the explosions and produce the defensive spray, remain unexplored. In this Thesis, I describe our recent progress in understanding the operation, structure, and materials composition of the defensive glands-topics which have to date received little treatment in the literature-of the best-known bombardier beetles, the brachinines (Carabidae: Brachininae: Brachinini). Chapter 2 deals with the pulsed-jet character of brachinines' sprays, which is in contrast to all other types of bombardier beetles that emit their sprays as continuous streams. Brachinine sprays comprise a number of spray pulses emitted in a rapid sequence, each pulse formed in a discrete explosion event inside the reaction chamber of the defensive gland, with the frequency of pulsation ranging from 300 to 1000 Hz. Using a combination of high-speed synchrotron x-ray phase-contrast imaging of live beetles, anatomical studies of the excised defensive glands, and mathematical analyses, we determined that spray pulsation arises due to explosion-induced displacement of the inlet structures to the reaction chamber periodically cutting o the ow of reactant solution into the reaction chamber. In Chapter 3, the interior cuticular microsculpture of the reaction chamber is studied using scanning electron microscopy and synchrotron x-ray phase-contrast microtomography. The microscupture is found to be highly complex, with a number of distinct spiny microtextures localized to specific regions of the reaction chamber. Quantitative details of the spine lengths and spacings are reported, and on the basis of the similarity of some of the features to the beetles' external abdominal microscupture and the micro-textural transitions observed inside the reaction chamber, we hypothesize that the reaction chamber microsculpture is homologous with the exterior microsculpture, consistent with the fact that the defensive glands are invaginations of the abdomen. Chapter 4 reports our preliminary investigations of the materials composition of the defensive glands. We use scanning electron microscopy to examine the fibrous composite structure of the gland cuticle and employ various light microscopy techniques to understand spatial variations in the cuticle sclerotization and chemical composition. The reaction chamber is found to exhibit dramatic spatial variation in sclerotization, including several lightly sclerotized regions, and possible functions of these regions are proposed. Additionally, the inter-chamber valve is found to contain the rubber-like protein resilin, likely as an adaptation to allow the valve to consistently make and hold a tight seal during each explosion, in analogy to rubber gaskets used in technological valve applications."
}, {
    "id": "oai:dspace.mit.edu:1721.1/108967",
    "title": "Ab initio investigations of solid electrolytes for lithium- and Sodium-ion batteries",
    "abstract": "Solid-state electrolytes have the potential to dramatically improve the safety and longevity of state-of-the-art battery technology by replacing the flammable organic electrolytes currently employed in Li-ion batteries. Recent advances in the development of new thiophosphate electrolytes have reenergized the field by achieving room temperature conductivities exceeding those liquid electrolytes, but a number of practical challenges to their widespread adoption still exist. This thesis applies ab initio computational methods based on density functional theory to investigate the structural origins of high conductivity in ionic conductor materials and provides a thermodynamic explanation of why the integration of these newly developed thiophosphates into high-rate cells has proven difficult in practice, often resulting in high interfacial resistance. As a result of these computational investigations, we report the prediction and synthesis of a new high performance sodium-ion conducting material: NaioSnP 2S 12, with room temperature ionic conductivity of 0.4 mS cm-1, which rivals the conductivity of the best sodium sulfide solid electrolytes to date. We computationally investigate the variants of this compound where Sn is substituted by Ge or Si and find that the latter may achieve even higher conductivity. We then investigate the relationship between anion packing and ionic transport in fast Li-ion conductors, finding that a bcc-like anion framework is desirable for achieving high ionic conductivity, and that this anion arrangement is present in a disproportionately high number of known Li-conducting materials, including Na10SnP2S12 and its structural analog Li10GeP2S2 . Using this bcc anion lattice as a screening criterion, we show that the I4 material LiZnPS4 also contains such a framework and has the potential for very high ionic conductivity. While the stoichiometric material has poor ionic conductivity, engineering of its composition to introduce interstitial lithium defects is able to exploit the low migration barrier of the bcc anion structure. Thermodynamic calculations predict a solid-solution regime in this system that extends to x = 0.5 in Li1+2xZn-xPS 4 , thus it may yield a new ionic conductor with exceptionally high lithium-ion conductivity, potentially exceeding 50 mS cm- 1 at room temperature. Finally, we develop a computational methodology to examine the thermodynamics of formation of resistive interfacial phases through mixing of the electrode and electrolyte. The results of the thermodynamic model of interfacial phase formation are well correlated with experimental observations and battery performance, and predict that thiophosphate electrolytes have especially high reactivity with high voltage oxide cathodes and a narrow electrochemical stability window. We also find that a number of known electrolytes are not inherently stable, but react in situ with the electrode to form passivating but ionically conducting barrier layers.",
    "advisors": ["Gerbrand Ceder"],
    "text": "Ab initio investigations of solid electrolytes for lithium- and Sodium-ion batteries Solid-state electrolytes have the potential to dramatically improve the safety and longevity of state-of-the-art battery technology by replacing the flammable organic electrolytes currently employed in Li-ion batteries. Recent advances in the development of new thiophosphate electrolytes have reenergized the field by achieving room temperature conductivities exceeding those liquid electrolytes, but a number of practical challenges to their widespread adoption still exist. This thesis applies ab initio computational methods based on density functional theory to investigate the structural origins of high conductivity in ionic conductor materials and provides a thermodynamic explanation of why the integration of these newly developed thiophosphates into high-rate cells has proven difficult in practice, often resulting in high interfacial resistance. As a result of these computational investigations, we report the prediction and synthesis of a new high performance sodium-ion conducting material: NaioSnP 2S 12, with room temperature ionic conductivity of 0.4 mS cm-1, which rivals the conductivity of the best sodium sulfide solid electrolytes to date. We computationally investigate the variants of this compound where Sn is substituted by Ge or Si and find that the latter may achieve even higher conductivity. We then investigate the relationship between anion packing and ionic transport in fast Li-ion conductors, finding that a bcc-like anion framework is desirable for achieving high ionic conductivity, and that this anion arrangement is present in a disproportionately high number of known Li-conducting materials, including Na10SnP2S12 and its structural analog Li10GeP2S2 . Using this bcc anion lattice as a screening criterion, we show that the I4 material LiZnPS4 also contains such a framework and has the potential for very high ionic conductivity. While the stoichiometric material has poor ionic conductivity, engineering of its composition to introduce interstitial lithium defects is able to exploit the low migration barrier of the bcc anion structure. Thermodynamic calculations predict a solid-solution regime in this system that extends to x = 0.5 in Li1+2xZn-xPS 4 , thus it may yield a new ionic conductor with exceptionally high lithium-ion conductivity, potentially exceeding 50 mS cm- 1 at room temperature. Finally, we develop a computational methodology to examine the thermodynamics of formation of resistive interfacial phases through mixing of the electrode and electrolyte. The results of the thermodynamic model of interfacial phase formation are well correlated with experimental observations and battery performance, and predict that thiophosphate electrolytes have especially high reactivity with high voltage oxide cathodes and a narrow electrochemical stability window. We also find that a number of known electrolytes are not inherently stable, but react in situ with the electrode to form passivating but ionically conducting barrier layers."
}, {
    "id": "oai:dspace.mit.edu:1721.1/69662",
    "title": "The interplay of structure and optical properties in individual semiconducting nanostructures",
    "abstract": "Semiconductor nanostructures exhibit distinct properties by virtue of nano-scale dimensionality, allowing for investigations of fundamental physics and the improvement of optoelectronic devices. Nanoscale morphological variations can drastically affect overall nanostructure properties because the investigation of nanostructure assemblies convolves nanoscale fluctuations to produce an averaged result. The investigation of individual nanostructures is thus paramount to a comprehensive analysis of nanomaterials. This thesis focuses on the study of individual GaAs, AlGaAs, and ZnO nanostructures to understand the influence of morphology on properties at the nanoscale. First, the diameter-dependent exciton-phonon coupling strengths of individual GaAs and AlGaAs nanowires were investigated by resonant micro-Raman spectroscopy near their direct bandgaps. The one-dimensional nanowire architecture was found to affect exciton lifetimes through an increase in surface state population relative to volume, resulting in Frhlich coupling strengths stronger than any previously observed. Next, ZnO nanowire growth kinetics and mechanisms were found to evolve by altering precursor concentrations. The cathodoluminescence of nanowires grown by reaction-limited kinetics were quenched at the nanowire tips, likely due to point defects associated with the high Zn supersaturation required for reaction-limited growth. Further, cathodoluminescence was quenched in the vicinity of Au nanoparticles, which were found on nanowire sidewalls due to the transition in growth mechanism, caused by excited electron transfer from the ZnO conduction band to the Au Fermi level. Finally, ZnO nanowalls were grown by significantly increasing precursor flux and diffusion lengths over that of the ZnO nanowire growth. Nanowall growth began with the Au-assisted nucleation of nanowires, whose growth kinetics was a combination of Gibbs- Thomson-limited and diffusion-limited, followed by the domination of non-assisted film growth to form nanowalls. Nanoscale morphological variations, such as thickness variations and the presence of dislocations and Au nanoparticles, were directly correlated with nanoscale variations in optical properties. These investigations prove unequivocally that nanoscale morphological variations have profound consequences on optical properties on the nanoscale. Studies of individual nano-objects are therefore prerequisite to fully understanding, and eventually employing, these promising nanostructures.",
    "advisors": ["Silvija Gradeak"],
    "text": "The interplay of structure and optical properties in individual semiconducting nanostructures Semiconductor nanostructures exhibit distinct properties by virtue of nano-scale dimensionality, allowing for investigations of fundamental physics and the improvement of optoelectronic devices. Nanoscale morphological variations can drastically affect overall nanostructure properties because the investigation of nanostructure assemblies convolves nanoscale fluctuations to produce an averaged result. The investigation of individual nanostructures is thus paramount to a comprehensive analysis of nanomaterials. This thesis focuses on the study of individual GaAs, AlGaAs, and ZnO nanostructures to understand the influence of morphology on properties at the nanoscale. First, the diameter-dependent exciton-phonon coupling strengths of individual GaAs and AlGaAs nanowires were investigated by resonant micro-Raman spectroscopy near their direct bandgaps. The one-dimensional nanowire architecture was found to affect exciton lifetimes through an increase in surface state population relative to volume, resulting in Frhlich coupling strengths stronger than any previously observed. Next, ZnO nanowire growth kinetics and mechanisms were found to evolve by altering precursor concentrations. The cathodoluminescence of nanowires grown by reaction-limited kinetics were quenched at the nanowire tips, likely due to point defects associated with the high Zn supersaturation required for reaction-limited growth. Further, cathodoluminescence was quenched in the vicinity of Au nanoparticles, which were found on nanowire sidewalls due to the transition in growth mechanism, caused by excited electron transfer from the ZnO conduction band to the Au Fermi level. Finally, ZnO nanowalls were grown by significantly increasing precursor flux and diffusion lengths over that of the ZnO nanowire growth. Nanowall growth began with the Au-assisted nucleation of nanowires, whose growth kinetics was a combination of Gibbs- Thomson-limited and diffusion-limited, followed by the domination of non-assisted film growth to form nanowalls. Nanoscale morphological variations, such as thickness variations and the presence of dislocations and Au nanoparticles, were directly correlated with nanoscale variations in optical properties. These investigations prove unequivocally that nanoscale morphological variations have profound consequences on optical properties on the nanoscale. Studies of individual nano-objects are therefore prerequisite to fully understanding, and eventually employing, these promising nanostructures."
}, {
    "id": "oai:dspace.mit.edu:1721.1/59237",
    "title": "Controlled growth and doping of core-shell GaAs-based nanowires",
    "abstract": "The use of compound semiconductor heterostructures to create electron confinement has enabled the highest frequency and lowest noise semiconductor electronics in existence. Modem technology uses two-dimensional electron gasses and there is considerable interest to explore one-dimensional electron confinement. This thesis develops the materials science toolkit needed to fabricate, characterize, and control the compositional, structural and electronic properties of core-shell GaAs/AlGaAs nanowires towards studying quasi-one-dimensional confinement and developing high mobility electronics First, nanowire growth kinetics were studied to optimize nanowire morphology. Variations in nanowire diameter were eliminated by understanding the role Ga adatom diffusion on sidewall deposition and vertical growth was enabled by understanding the importance of Ga and As mass-transport to nanowire nucleation. These results demonstrate that arrays of vertically-aligned GaAs nanowires can be produced. Then, the deposition of epitaxial AlGaAs shells on GaAs nanowires was demonstrated. By reducing the nanowire aerial density the stability of the nanowire geometry was maintained. A variety of analytical electron microscopy techniques confirmed the shell deposition to be uniform, epitaxial, defect-free, and nearly atomic sharp. These results demonstrate that core-shell nanowires possess a core-shell interface free of many of the imperfections that lithographically-defined nanowires possess. Finally, the adverse effect of the Au seed nanoparticle during n-type doping was identified and n-type doping was achieved via the removal of the Au nanoparticle prior to doping. A combination of energy dispersive X-ray spectroscopy, current-voltage, capacitance-voltage, and Kelvin probe force microscopy demonstrated that if the Au seed nanoparticle is present during the shell deposition, Au diffuses from the seed nanoparticle and creates a rectifying IV behavior. A process was presented to remove the Au nanoparticle prior to shell deposition and was shown to produce uniform n-type doping. The conductivity of GaAs/n-GaAs nanowires was calculated as a function of donor concentration and geometric factors taking into account the effects of Fermi level pinning. The control demonstrated over all of these parameters is sufficient enough for core-shell nanowires to be considered candidates for high mobility electronics.",
    "advisors": ["Silvija Gradeak"],
    "text": "Controlled growth and doping of core-shell GaAs-based nanowires The use of compound semiconductor heterostructures to create electron confinement has enabled the highest frequency and lowest noise semiconductor electronics in existence. Modem technology uses two-dimensional electron gasses and there is considerable interest to explore one-dimensional electron confinement. This thesis develops the materials science toolkit needed to fabricate, characterize, and control the compositional, structural and electronic properties of core-shell GaAs/AlGaAs nanowires towards studying quasi-one-dimensional confinement and developing high mobility electronics First, nanowire growth kinetics were studied to optimize nanowire morphology. Variations in nanowire diameter were eliminated by understanding the role Ga adatom diffusion on sidewall deposition and vertical growth was enabled by understanding the importance of Ga and As mass-transport to nanowire nucleation. These results demonstrate that arrays of vertically-aligned GaAs nanowires can be produced. Then, the deposition of epitaxial AlGaAs shells on GaAs nanowires was demonstrated. By reducing the nanowire aerial density the stability of the nanowire geometry was maintained. A variety of analytical electron microscopy techniques confirmed the shell deposition to be uniform, epitaxial, defect-free, and nearly atomic sharp. These results demonstrate that core-shell nanowires possess a core-shell interface free of many of the imperfections that lithographically-defined nanowires possess. Finally, the adverse effect of the Au seed nanoparticle during n-type doping was identified and n-type doping was achieved via the removal of the Au nanoparticle prior to doping. A combination of energy dispersive X-ray spectroscopy, current-voltage, capacitance-voltage, and Kelvin probe force microscopy demonstrated that if the Au seed nanoparticle is present during the shell deposition, Au diffuses from the seed nanoparticle and creates a rectifying IV behavior. A process was presented to remove the Au nanoparticle prior to shell deposition and was shown to produce uniform n-type doping. The conductivity of GaAs/n-GaAs nanowires was calculated as a function of donor concentration and geometric factors taking into account the effects of Fermi level pinning. The control demonstrated over all of these parameters is sufficient enough for core-shell nanowires to be considered candidates for high mobility electronics."
}, {
    "id": "oai:dspace.mit.edu:1721.1/103266",
    "title": "Theoretical advancements towards understanding crystalline metastability during materials synthesis",
    "abstract": "Predicting the conditions in which a compound adopts a metastable structure when it crystallizes out of solution is an unsolved and fundamental problem in materials synthesis, and one which if understood and harnessed, could enable the rational design of synthesis pathways towards or away from metastable structures. Although metastable phases are ubiquitous in both nature and technology, only a heuristic understanding of their underlying thermodynamics and formation mechanisms exists. In this thesis, we aim to address two important outstanding questions regarding the fundamental nature of metastability: Which metastable phases can form? Under which conditions will they form? We will employ a variety of computational and theoretical approaches to elucidate quantitative insights to these two questions. To better predict which metastable materials can be made, we first seek to understand the metastable materials that have been made. We data-mine the Materials Project, a high-throughput database of DFT-calculated energetics of ICSD structures, to explicitly quantify the energy scale of metastability for 29,902 inorganic crystalline phases. We reveal the influence of chemistry and composition on the accessible range of crystalline metastability, and identify motifs characteristic of highly metastable compounds. We further assert that not all low-energy metastable materials can necessarily be made, and argue for a concept of \"remnant metastability\" - that observable metastable phases are remnants of thermodynamic conditions where they were once the lowest free-energy phase. Recently, exciting thermochemistry experiments have demonstrated that for many compounds, as metastability of a phase increases, its surface energy decreases. This effect is significant enough to trigger a reversal of relative phase stability in nanoparticles. Because nucleation and growth starts at the nanoscale, we hypothesize that the direct precipitation of metastable phases during crystallization may be 'remnant metastability' of size-dependent nanoscale phase stability. We develop algorithms for the automated, efficient, and high-throughput calculation of surface energies via DFT. We combine these algorithms with prior theoretical frameworks to predict solid-aqueous equilibria, enabling the calculation of nucleation barriers of competing polymorphs as a function of solution chemistry, thereby predicting the solution conditions governing polymorph selection. We apply this approach to resolve the long-standing 'Calcite-Aragonite Problem' - the observation that calcium carbonate precipitates as the metastable aragonite polymorph in marine environments, rather than the stable phase calcite - which is of tremendous relevance to biomineralization, carbon sequestration, paleogeochemistry, and the vulnerability of marine life to ocean acidification. We identify a direct relationship between the calcite surface energy and solution Mg/Ca ion concentrations, showing that the calcite nucleation barrier surpasses that of metastable aragonite in solutions with Mg/Ca ratios consistent with modern seawater, allowing aragonite to dominate the kinetics of nucleation. Our ability to quantify how solution parameters distinguish between polymorphs marks an important step towards the ab initio prediction of materials synthesis pathways in solution.",
    "advisors": ["Gerbrand Ceder"],
    "text": "Theoretical advancements towards understanding crystalline metastability during materials synthesis Predicting the conditions in which a compound adopts a metastable structure when it crystallizes out of solution is an unsolved and fundamental problem in materials synthesis, and one which if understood and harnessed, could enable the rational design of synthesis pathways towards or away from metastable structures. Although metastable phases are ubiquitous in both nature and technology, only a heuristic understanding of their underlying thermodynamics and formation mechanisms exists. In this thesis, we aim to address two important outstanding questions regarding the fundamental nature of metastability: Which metastable phases can form? Under which conditions will they form? We will employ a variety of computational and theoretical approaches to elucidate quantitative insights to these two questions. To better predict which metastable materials can be made, we first seek to understand the metastable materials that have been made. We data-mine the Materials Project, a high-throughput database of DFT-calculated energetics of ICSD structures, to explicitly quantify the energy scale of metastability for 29,902 inorganic crystalline phases. We reveal the influence of chemistry and composition on the accessible range of crystalline metastability, and identify motifs characteristic of highly metastable compounds. We further assert that not all low-energy metastable materials can necessarily be made, and argue for a concept of \"remnant metastability\" - that observable metastable phases are remnants of thermodynamic conditions where they were once the lowest free-energy phase. Recently, exciting thermochemistry experiments have demonstrated that for many compounds, as metastability of a phase increases, its surface energy decreases. This effect is significant enough to trigger a reversal of relative phase stability in nanoparticles. Because nucleation and growth starts at the nanoscale, we hypothesize that the direct precipitation of metastable phases during crystallization may be 'remnant metastability' of size-dependent nanoscale phase stability. We develop algorithms for the automated, efficient, and high-throughput calculation of surface energies via DFT. We combine these algorithms with prior theoretical frameworks to predict solid-aqueous equilibria, enabling the calculation of nucleation barriers of competing polymorphs as a function of solution chemistry, thereby predicting the solution conditions governing polymorph selection. We apply this approach to resolve the long-standing 'Calcite-Aragonite Problem' - the observation that calcium carbonate precipitates as the metastable aragonite polymorph in marine environments, rather than the stable phase calcite - which is of tremendous relevance to biomineralization, carbon sequestration, paleogeochemistry, and the vulnerability of marine life to ocean acidification. We identify a direct relationship between the calcite surface energy and solution Mg/Ca ion concentrations, showing that the calcite nucleation barrier surpasses that of metastable aragonite in solutions with Mg/Ca ratios consistent with modern seawater, allowing aragonite to dominate the kinetics of nucleation. Our ability to quantify how solution parameters distinguish between polymorphs marks an important step towards the ab initio prediction of materials synthesis pathways in solution."
}, {
    "id": "oai:dspace.mit.edu:1721.1/29969",
    "title": "Hierarchical layered-silicate-- lamellar triblock copolymer nanocomposites",
    "abstract": "The fundamental role of the layered-silicates in a styrene-butadiene-styrene triblock copolymer (SBS) as a function of layered-silicate dispersion during deformation was investigated. Predominantly immiscible composites of mixed morphology provided the initial proof that dramatic alteration of the SBS deformation behavior exists, but a clear understanding of the nature of reinforcement was precluded due to the fiber symmetric orientation of the SBS and the mixed clay morphologies. Following the theory of Vaia and Giannelis, use of a more hydrophobic organically modified clay resulted in an intercalated morphology with a near single crystalline texture of the SBS due to roll-casting. Significant heterogeneous deformation was observed at ambient conditions as well as at elevated temperature as verified through Cohen's affine deformation model in combination with Kratky's scattering pattern model. The intercalated morphology shows little or modest mechanical property enhancements at all temperatures studied. Exfoliated nanocomposite was produced by functionalization of the clay surfaces with polystyrene, altering the enthalpic interactions. Entropic interactions were also controlled by varying the molecular weight of the surfactant and the grafting density and shows remarkable agreement with the theory proposed by Balazs et al. Due to the increase surface volume ratio of the clay, a flipping transition of the block copolymer morphology was observed during roll-casting producing a near single crystalline parallel/parallel clay/BCP orientation. The modulus was relatively unaffected whereas the toughness increased significantly due to an earlier onset of strain hardening.",
    "advisors": ["Edwin L. Thomas"],
    "text": "Hierarchical layered-silicate-- lamellar triblock copolymer nanocomposites The fundamental role of the layered-silicates in a styrene-butadiene-styrene triblock copolymer (SBS) as a function of layered-silicate dispersion during deformation was investigated. Predominantly immiscible composites of mixed morphology provided the initial proof that dramatic alteration of the SBS deformation behavior exists, but a clear understanding of the nature of reinforcement was precluded due to the fiber symmetric orientation of the SBS and the mixed clay morphologies. Following the theory of Vaia and Giannelis, use of a more hydrophobic organically modified clay resulted in an intercalated morphology with a near single crystalline texture of the SBS due to roll-casting. Significant heterogeneous deformation was observed at ambient conditions as well as at elevated temperature as verified through Cohen's affine deformation model in combination with Kratky's scattering pattern model. The intercalated morphology shows little or modest mechanical property enhancements at all temperatures studied. Exfoliated nanocomposite was produced by functionalization of the clay surfaces with polystyrene, altering the enthalpic interactions. Entropic interactions were also controlled by varying the molecular weight of the surfactant and the grafting density and shows remarkable agreement with the theory proposed by Balazs et al. Due to the increase surface volume ratio of the clay, a flipping transition of the block copolymer morphology was observed during roll-casting producing a near single crystalline parallel/parallel clay/BCP orientation. The modulus was relatively unaffected whereas the toughness increased significantly due to an earlier onset of strain hardening."
}, {
    "id": "oai:dspace.mit.edu:1721.1/38582",
    "title": "GeSi photodetectors and electro-absorption modulators for Si electronic-photonic integrated circuits",
    "abstract": "The silicon electronic-photonic integrated circuit (EPIC) has emerged as a promising technology to break through the interconnect bottlenecks in telecommunications and on-chip interconnects. High performance photonic modulators and photodetectors compatible with Si complimentary metal oxide semiconductor (CMOS) devices are indispensable to achieve this goal. A photonic modulator generates optical \"1\" and \"0\" signals by switching the light on and off, while a photodetector converts the optical signals to electrical ones so that they can be processed by a CMOS circuit. Due to its compatibility with Si CMOS processing and adequate optoelectric properties, epitaxial GeSi material has been considered as a promising candidate to achieve this goal. This thesis investigates epitaxial GeSi photodetectors and electro-absorption (EA) modulators integrated with high index contrast Si(core)/Si02(cladding) waveguides to form an EPIC circuit on a Si platform with CMOS compatibility. Tensile strain is introduced into the GeSi material to enhance its optoelectronic properties. The effect of tensile strain on the band structure of Ge is systematically studied, and the deformation potential constants of Ge are derived from the experimental results with relatively high accuracy.",
    "advisors": ["Lionel C. Kimerling"],
    "text": "GeSi photodetectors and electro-absorption modulators for Si electronic-photonic integrated circuits The silicon electronic-photonic integrated circuit (EPIC) has emerged as a promising technology to break through the interconnect bottlenecks in telecommunications and on-chip interconnects. High performance photonic modulators and photodetectors compatible with Si complimentary metal oxide semiconductor (CMOS) devices are indispensable to achieve this goal. A photonic modulator generates optical \"1\" and \"0\" signals by switching the light on and off, while a photodetector converts the optical signals to electrical ones so that they can be processed by a CMOS circuit. Due to its compatibility with Si CMOS processing and adequate optoelectric properties, epitaxial GeSi material has been considered as a promising candidate to achieve this goal. This thesis investigates epitaxial GeSi photodetectors and electro-absorption (EA) modulators integrated with high index contrast Si(core)/Si02(cladding) waveguides to form an EPIC circuit on a Si platform with CMOS compatibility. Tensile strain is introduced into the GeSi material to enhance its optoelectronic properties. The effect of tensile strain on the band structure of Ge is systematically studied, and the deformation potential constants of Ge are derived from the experimental results with relatively high accuracy."
}, {
    "id": "oai:dspace.mit.edu:1721.1/88276",
    "title": "Helium behavior at fcc-bcc semicoherent interfaces: trapping, clustering, nucleation, and growth of cavities",
    "abstract": "He implanted into metals precipitates into nanoscale bubbles that grow into voids, degrading the properties of engineering alloys in nuclear energy applications. In this thesis, multi-scale modeling techniques and neutron reflectometry measurements are used to study the He trapping, clustering and growth of clusters at fcc-bcc interfaces. Choosing Cu-Nb as a model fcc-bcc interface, a predictive Cu-Nb-He interatomic potential is constructed using density functional theory. These calculations show that two-body radial forces are sucient to describe interactions of He with fcc Cu and bcc Nb. Atomistic simulations reveal that He is initially trapped in the form of stable, sub-nanometer platelet-shaped clusters and not bubbles at the Cu-Nb interface. This behavior occurs due to the spatial heterogeneity of interface energy: He wets high energy, heliophilic regions while avoiding low energy, heliophobic ones. Using these insights, the maximum He concentration that can be stored without forming bubbles at any interface in terms of its location-dependent energy distribution may be predicted. The modeling predictions are validated by neutron reflectometry measurements, which show that interfacial He bubbles form only above a critical He concentration and provide evidence for the presence of stable He platelets below a critical He concentration. This work paves the way for the design of composite structural materials with increased resistance to He-induced degradation by tailoring the types of interfaces they contain.",
    "advisors": ["Michael J. Demkowicz"],
    "text": "Helium behavior at fcc-bcc semicoherent interfaces: trapping, clustering, nucleation, and growth of cavities He implanted into metals precipitates into nanoscale bubbles that grow into voids, degrading the properties of engineering alloys in nuclear energy applications. In this thesis, multi-scale modeling techniques and neutron reflectometry measurements are used to study the He trapping, clustering and growth of clusters at fcc-bcc interfaces. Choosing Cu-Nb as a model fcc-bcc interface, a predictive Cu-Nb-He interatomic potential is constructed using density functional theory. These calculations show that two-body radial forces are sucient to describe interactions of He with fcc Cu and bcc Nb. Atomistic simulations reveal that He is initially trapped in the form of stable, sub-nanometer platelet-shaped clusters and not bubbles at the Cu-Nb interface. This behavior occurs due to the spatial heterogeneity of interface energy: He wets high energy, heliophilic regions while avoiding low energy, heliophobic ones. Using these insights, the maximum He concentration that can be stored without forming bubbles at any interface in terms of its location-dependent energy distribution may be predicted. The modeling predictions are validated by neutron reflectometry measurements, which show that interfacial He bubbles form only above a critical He concentration and provide evidence for the presence of stable He platelets below a critical He concentration. This work paves the way for the design of composite structural materials with increased resistance to He-induced degradation by tailoring the types of interfaces they contain."
}, {
    "id": "oai:dspace.mit.edu:1721.1/36208",
    "title": "Computational studies of cation and anion ordering in cubic yttria stabilized zirconia",
    "abstract": "The investigation of ordering and phase stability in the ZrO2-Y203 system involves two sets of calculations. The first set of calculations uses the cluster expansion method. A guide to the practical implementation of the cluster expansion outlines methods for defining a goal and choosing structures and clusters that best model the system of interest. The cluster expansion of the yttria stabilized zirconia system considers 447 configurations across the ZrO2-Y203 composition range. The effective cluster interaction for pair clusters show electrostatic repulsion between anions and little interaction between cations. Triplet anion terms largely modify the energy contributions of the pair terms. Separate cluster expansions using structures at single compositions show that cation clusters become more important at high yttria composition. The cluster expansion led to the discovery of three previously unidentified ordered ground state structures at 25, 29, and 33 % Y on the cubic fluorite lattice. The ground state with 33 % Y is stable with respect to the calculated energies of monoclinic ZrO2 and the Y4Zr3012 ground state. The ground states have the common ordering feature of yttrium and vacancies in [1 1 2] chains, and Monte Carlo simulations show that vacancy ordering upon cooling is contingent on cation ordering.",
    "advisors": ["Gerbrand Ceder"],
    "text": "Computational studies of cation and anion ordering in cubic yttria stabilized zirconia The investigation of ordering and phase stability in the ZrO2-Y203 system involves two sets of calculations. The first set of calculations uses the cluster expansion method. A guide to the practical implementation of the cluster expansion outlines methods for defining a goal and choosing structures and clusters that best model the system of interest. The cluster expansion of the yttria stabilized zirconia system considers 447 configurations across the ZrO2-Y203 composition range. The effective cluster interaction for pair clusters show electrostatic repulsion between anions and little interaction between cations. Triplet anion terms largely modify the energy contributions of the pair terms. Separate cluster expansions using structures at single compositions show that cation clusters become more important at high yttria composition. The cluster expansion led to the discovery of three previously unidentified ordered ground state structures at 25, 29, and 33 % Y on the cubic fluorite lattice. The ground state with 33 % Y is stable with respect to the calculated energies of monoclinic ZrO2 and the Y4Zr3012 ground state. The ground states have the common ordering feature of yttrium and vacancies in [1 1 2] chains, and Monte Carlo simulations show that vacancy ordering upon cooling is contingent on cation ordering."
}, {
    "id": "oai:dspace.mit.edu:1721.1/88365",
    "title": "Enhancing stability of powder-route nanocrystalline tungsten-titanium via alloy thermodynamics",
    "abstract": "Improvement in material properties as a result of grain size refinement to the nanoscale is often limited by an inherent tendency of these nanostructured materials to coarsen especially at the high temperatures required for processing. The structural instability stems from a large volume fraction of grain boundaries that carry an intrinsic energy penalty, but can be overcome by a consideration of the thermodynamics-based mechanism of interface energy relief via alloying. Suitable alloying conditions can provide a solute segregated grain boundary configuration that enables a nanostructured alloy to become the system's most energetically preferable state. A thermodynamics-based Monte Carlo method that captures the physics of regular solution mixing and grain boundary segregation in nanostructured alloys is developed and used to study the energetics and equilibrium structures of binary alloys. Our simulation is used to identify the alloying elements with preferable interface stabilizing capability appropriate for the high-temperature sintering requirement for powder-route nanocrystalline tungsten. Based on both alloy simulation and consideration of material properties, titanium is selected as a suitable alloying element. Nanocrystalline tungsten alloys with 0-20 atomic percent titanium content are produced by high-energy ball milling and tested at the expected sintering temperature of 1100C. With an addition of 20 atomic percent titanium, nanocrystalline tungsten shows retention of nanoscale grain size after a one-week equilibration at 1100C. Scanning transmission electron microscopy and atom probe tomography techniques reveal a heterogeneous distribution of titanium in the alloy with enhanced grain stability, which contradicts the expectation of a uniform solid solution by conventional bulk thermodynamics but is explicitly predicted by the alloy simulation when grain boundaries are included as possible equilibrium states. The segregation profiles from the experimental characterizations and simulated results show depletion of titanium from tungsten grain centers and enrichment of titanium well above the nominal concentration in the grain boundary vicinity in a form of complex segregation state.",
    "advisors": ["Christopher A. Schuh"],
    "text": "Enhancing stability of powder-route nanocrystalline tungsten-titanium via alloy thermodynamics Improvement in material properties as a result of grain size refinement to the nanoscale is often limited by an inherent tendency of these nanostructured materials to coarsen especially at the high temperatures required for processing. The structural instability stems from a large volume fraction of grain boundaries that carry an intrinsic energy penalty, but can be overcome by a consideration of the thermodynamics-based mechanism of interface energy relief via alloying. Suitable alloying conditions can provide a solute segregated grain boundary configuration that enables a nanostructured alloy to become the system's most energetically preferable state. A thermodynamics-based Monte Carlo method that captures the physics of regular solution mixing and grain boundary segregation in nanostructured alloys is developed and used to study the energetics and equilibrium structures of binary alloys. Our simulation is used to identify the alloying elements with preferable interface stabilizing capability appropriate for the high-temperature sintering requirement for powder-route nanocrystalline tungsten. Based on both alloy simulation and consideration of material properties, titanium is selected as a suitable alloying element. Nanocrystalline tungsten alloys with 0-20 atomic percent titanium content are produced by high-energy ball milling and tested at the expected sintering temperature of 1100C. With an addition of 20 atomic percent titanium, nanocrystalline tungsten shows retention of nanoscale grain size after a one-week equilibration at 1100C. Scanning transmission electron microscopy and atom probe tomography techniques reveal a heterogeneous distribution of titanium in the alloy with enhanced grain stability, which contradicts the expectation of a uniform solid solution by conventional bulk thermodynamics but is explicitly predicted by the alloy simulation when grain boundaries are included as possible equilibrium states. The segregation profiles from the experimental characterizations and simulated results show depletion of titanium from tungsten grain centers and enrichment of titanium well above the nominal concentration in the grain boundary vicinity in a form of complex segregation state."
}, {
    "id": "oai:dspace.mit.edu:1721.1/46677",
    "title": "The role of phase transformation in the rate performance limited Lix V O battery cathode",
    "abstract": "It has recently been reported that the rate performance of Lix VO, a widely studied candidate Li-ion battery cathode material, can be significantly improved through a variety of particle size reduction techniques, (e.g. nano templating). It is widely believed that the microscale mechanism responsible for this improvement in rate performance is a reduction in the Li+ diffusion path length. Yet, the experimentally observed discharge performance of Lix V O cathode films comprised of active material particles of varying sizes (between films) and subject to variable rates of discharge deviates sharply from results predicted by Fickian scaling laws. In a crystalline Li+ insertion host the incorporation of ionic volume commensurate with electrochemical discharge often leads to phase transformation. While the consequent phase coexistence is largely responsible for the high energy densities reported in many crystalline insertion hosts, its effect upon rate performance, (or power density), is not well understood. Recently, researchers identified facilitated phase boundary motion as the mechanism responsible for improved high-rate performance in one nanoscaled insertion compound. The preservation of a coherent phase boundary between differentially lithiated, coexistent end-member phases that would normally relax the interfacial strain associated with biphasic volumetric mismatch by forming incoherent phase boundaries, they reasoned, lead to the observed improvement in high-rate performance. A number of discrete structural and electrochemical signatures have subsequently been identified that are believed to correlate with facilitated phase-boundary-motion in nanoscaled insertion hosts. These equilibrium signatures, which include; enhanced Li+ solubility in end-member phases, decreased volumetric mismatch between coexistent end-member phases, increased interfacial strain between coexistent end-member phases, and reduced cycling hysteresis, have been identified in the dimensionally graded Lix V O system, suggesting that rate performance in this system may, in fact, also be gated by sluggish phase boundary motion.",
    "advisors": ["Donald R. Sadoway"],
    "text": "The role of phase transformation in the rate performance limited Lix V O battery cathode It has recently been reported that the rate performance of Lix VO, a widely studied candidate Li-ion battery cathode material, can be significantly improved through a variety of particle size reduction techniques, (e.g. nano templating). It is widely believed that the microscale mechanism responsible for this improvement in rate performance is a reduction in the Li+ diffusion path length. Yet, the experimentally observed discharge performance of Lix V O cathode films comprised of active material particles of varying sizes (between films) and subject to variable rates of discharge deviates sharply from results predicted by Fickian scaling laws. In a crystalline Li+ insertion host the incorporation of ionic volume commensurate with electrochemical discharge often leads to phase transformation. While the consequent phase coexistence is largely responsible for the high energy densities reported in many crystalline insertion hosts, its effect upon rate performance, (or power density), is not well understood. Recently, researchers identified facilitated phase boundary motion as the mechanism responsible for improved high-rate performance in one nanoscaled insertion compound. The preservation of a coherent phase boundary between differentially lithiated, coexistent end-member phases that would normally relax the interfacial strain associated with biphasic volumetric mismatch by forming incoherent phase boundaries, they reasoned, lead to the observed improvement in high-rate performance. A number of discrete structural and electrochemical signatures have subsequently been identified that are believed to correlate with facilitated phase-boundary-motion in nanoscaled insertion hosts. These equilibrium signatures, which include; enhanced Li+ solubility in end-member phases, decreased volumetric mismatch between coexistent end-member phases, increased interfacial strain between coexistent end-member phases, and reduced cycling hysteresis, have been identified in the dimensionally graded Lix V O system, suggesting that rate performance in this system may, in fact, also be gated by sluggish phase boundary motion."
}, {
    "id": "oai:dspace.mit.edu:1721.1/39499",
    "title": "Nucleation of the isothermal martensitic transformation in iron-nickel-manganese alloys",
    "abstract": "By means of quantitative metallography and electrical resistance measurements, the incubation period (time to form a detectable amount of martensite) and the initial nucleation rate have been determined as a function of subzero reaction temperature and austenitic grain size in three iron-nickel manganese alloys containing 23-25 percent nickel, 2-3 percent manganese and 0.02-0.04 percent carbon. The isothermal martensitic transformation in these alloys has been found to occur over a wide temperature range of at least -1960 to -200 C, and the martensitic structure over the entire range is body centered cubic. The incubation period and nucleation rate as a function of reaction temperature at various austenitic grain sizes show typical C-curve kinetics. The true initial nucleation rate (not the apparent initial nucleation rate during the incubation period) is independent of grain size, and so the effect of grain size on the incubation period arises merely because of the size of the first-formed plates. Accordingly, it may be concluded that grain boundaries do not predominate as preferred nucleation sites for the martensitic transformation in the alloys studied. Calculations of the relevant thermodynamic functions indicate that 1 atomic percent manganese is thermodynamically equivalent to 1.6 atomic percent nickel in the driving force for the martensitic transformation. However, alloys of equivalent thermodynamic driving force exhibit striking differences in kinetic behavior due to slight differences in their manganese and carbon contents, suggesting that these elements reduce the potency of the embryos. The observed activation energies of the isothermal martensitic transformation are in very good agreement with the calculated activation energies based on a nucleation model. Moreover, in these alloys, the potency of the active embryos remains almost constant during the course of the isothermal transformation, notwithstanding the progressive generation of new embryos due to autocatalytic factors. The best fit between theory and experiment is obtained on the assumption that the number of pre-existing embryos in the parent austenite is 10 per cm . The probability of detecting such embryos by transmission electron microscopy is less than 1 in 10. The increase in the rate of isothermal martensitic transformation as a function of time has been shown to be due to the formation of new plates by auto catalysis, while the subsequent retardation is attributable to the partitioning of the austenite by the martensitic plates. The model used provides quantitative agreement with the course of the isothermal transformation up to 12 percent martensite.",
    "advisors": ["Morris Cohen"],
    "text": "Nucleation of the isothermal martensitic transformation in iron-nickel-manganese alloys By means of quantitative metallography and electrical resistance measurements, the incubation period (time to form a detectable amount of martensite) and the initial nucleation rate have been determined as a function of subzero reaction temperature and austenitic grain size in three iron-nickel manganese alloys containing 23-25 percent nickel, 2-3 percent manganese and 0.02-0.04 percent carbon. The isothermal martensitic transformation in these alloys has been found to occur over a wide temperature range of at least -1960 to -200 C, and the martensitic structure over the entire range is body centered cubic. The incubation period and nucleation rate as a function of reaction temperature at various austenitic grain sizes show typical C-curve kinetics. The true initial nucleation rate (not the apparent initial nucleation rate during the incubation period) is independent of grain size, and so the effect of grain size on the incubation period arises merely because of the size of the first-formed plates. Accordingly, it may be concluded that grain boundaries do not predominate as preferred nucleation sites for the martensitic transformation in the alloys studied. Calculations of the relevant thermodynamic functions indicate that 1 atomic percent manganese is thermodynamically equivalent to 1.6 atomic percent nickel in the driving force for the martensitic transformation. However, alloys of equivalent thermodynamic driving force exhibit striking differences in kinetic behavior due to slight differences in their manganese and carbon contents, suggesting that these elements reduce the potency of the embryos. The observed activation energies of the isothermal martensitic transformation are in very good agreement with the calculated activation energies based on a nucleation model. Moreover, in these alloys, the potency of the active embryos remains almost constant during the course of the isothermal transformation, notwithstanding the progressive generation of new embryos due to autocatalytic factors. The best fit between theory and experiment is obtained on the assumption that the number of pre-existing embryos in the parent austenite is 10 per cm . The probability of detecting such embryos by transmission electron microscopy is less than 1 in 10. The increase in the rate of isothermal martensitic transformation as a function of time has been shown to be due to the formation of new plates by auto catalysis, while the subsequent retardation is attributable to the partitioning of the austenite by the martensitic plates. The model used provides quantitative agreement with the course of the isothermal transformation up to 12 percent martensite."
}, {
    "id": "oai:dspace.mit.edu:1721.1/28353",
    "title": "Surface kinetics modeling of silicon oxide etching in fluorocarbon plasmas",
    "abstract": "Fluorocarbon plasma for silicon oxide etching is a complicated system involving many ion and neutral species. Depending on the plasma condition, many difficulties arise such as RIE lag, etch stop, and low selectivity to photoresist. For a better understanding of the process it is necessary to have an appropriate physical model to describe the surface kinetics including simultaneous etching and deposition. A novel surface kinetic model, the translating mixed-layer (TML) model, has been developed. ABACUSS II, a modeling environment and simulator was used for solving differential algebraic equations that describes the surface kinetics. In the modeling, the effect of many variables were investigated including neutral and ion fluxes to the surface, sticking probabilities, surface composition, sputter etching reactions, ion enhanced chemical etching reactions and neutral-to-ion flux ratio. The model has been applied to various systems including silicon etching with chlorine chemistry, silicon oxide etching with fluorine chemistry and silicon oxide etching with fluorocarbon plasma. The verification of the model was done using measured etching yield data determined by quartz crystal microbalance (QCM) in conjunction with plasma neutral and ion concentrations/fluxes determined by mass spectrometry. The etching and deposition rates have been measured as functions of ion impinging angle, sample temperature, which are necessary for profile evolution modeling of silicon oxide etching in inductively coupled plasma. Angular dependence of etching yield of oxide in fluorocarbon plasma shows very unique behavior unlike typical ion-induced chemical etching or physical sputtering. Ion-induced deposition model was suggested and tested.",
    "advisors": ["Herbert H. Sawin"],
    "text": "Surface kinetics modeling of silicon oxide etching in fluorocarbon plasmas Fluorocarbon plasma for silicon oxide etching is a complicated system involving many ion and neutral species. Depending on the plasma condition, many difficulties arise such as RIE lag, etch stop, and low selectivity to photoresist. For a better understanding of the process it is necessary to have an appropriate physical model to describe the surface kinetics including simultaneous etching and deposition. A novel surface kinetic model, the translating mixed-layer (TML) model, has been developed. ABACUSS II, a modeling environment and simulator was used for solving differential algebraic equations that describes the surface kinetics. In the modeling, the effect of many variables were investigated including neutral and ion fluxes to the surface, sticking probabilities, surface composition, sputter etching reactions, ion enhanced chemical etching reactions and neutral-to-ion flux ratio. The model has been applied to various systems including silicon etching with chlorine chemistry, silicon oxide etching with fluorine chemistry and silicon oxide etching with fluorocarbon plasma. The verification of the model was done using measured etching yield data determined by quartz crystal microbalance (QCM) in conjunction with plasma neutral and ion concentrations/fluxes determined by mass spectrometry. The etching and deposition rates have been measured as functions of ion impinging angle, sample temperature, which are necessary for profile evolution modeling of silicon oxide etching in inductively coupled plasma. Angular dependence of etching yield of oxide in fluorocarbon plasma shows very unique behavior unlike typical ion-induced chemical etching or physical sputtering. Ion-induced deposition model was suggested and tested."
}, {
    "id": "oai:dspace.mit.edu:1721.1/82173",
    "title": "Ge-on-Si laser for silicon photonics",
    "abstract": "Ge-on-Si devices are explored for photonic integration. Importance of Ge in photonics has grown and through techniques developed in our group we demonstrated low density of dislocations (<1x109cm-2) and point defects Ge growth for photonic devices. The focus of this document will be exclusively on Ge light emitters. Ge is an indirect band gap material that has shown the ability to act like a pseudo direct band gap material. Through the use of tensile strain and heavy doping, Ge exhibits properties thought exclusive of direct band gap materials. Dependence on temperature suggests strong interaction between indirect bands, [Delta] and L, and the direct band gap at [Gamma]. The behavior is justified through increase in photoluminescence on Ge. The range of efficient emission is to 120 with the first band interaction, and above 400 on the second band interaction. Low defect concentration in Ge is achieved through chemical vapor deposition at high vacuum (~1x10-8 mbar) in a two-step process. The high temperature growth and low concentration of particles permits epitaxial growth with low defect concentration. Chemical selectivity forbids Ge growth on oxide. Oxide trenches permit the growth on Si for a variety of shapes, without detrimentally affecting the strain of the Ge devices. Dopant concentration above intrinsic growth concentration, ~1x1019cm-3 phosphorus, have been achieved through a series of methods non-CMOS, spin-on dopant; and CMOS, implantation and delta doping. All the techniques explored use enhanced dopant diffusion observed in Ge under heavy n-type doping. A dopant source, or well, is used to distribute the dopants in the Ge without increasing the defect concentration. The approach lead to the development of electrically injected devices, LEDs and LDs. Ge pnn double heterostructure diodes were made under low, ~1x1018cm-3, and heavy n-type doping, >1x1019cm-3. Both devices showed improved performance compared to pin Ge LED. Furthermore, heavy doped Ge diodes exhibit evidence of bleaching or transparency. The techniques described permitted the development of Ge-on-Si laser with a concentration ~1-2x1019cm-3. It is the first demonstration of a Ge laser optically pumped working under the direct band gap assumption like other semiconductors. It represents the evidence of carrier inversion on an indirect band gap semiconductor. With 50cm-1 gain, the material shows Fabry-Perot cavity behavior. Finally, we demonstrated a fully functioning laser diode monolithically integrated on Si. Ge pnn lasers were made exhibiting a gain >1000cm-1 and exhibiting a spectrum range of over 200nm, making Ge the ideal candidate for Si photonics.",
    "advisors": ["Lionel C. Kimerling"],
    "text": "Ge-on-Si laser for silicon photonics Ge-on-Si devices are explored for photonic integration. Importance of Ge in photonics has grown and through techniques developed in our group we demonstrated low density of dislocations (<1x109cm-2) and point defects Ge growth for photonic devices. The focus of this document will be exclusively on Ge light emitters. Ge is an indirect band gap material that has shown the ability to act like a pseudo direct band gap material. Through the use of tensile strain and heavy doping, Ge exhibits properties thought exclusive of direct band gap materials. Dependence on temperature suggests strong interaction between indirect bands, [Delta] and L, and the direct band gap at [Gamma]. The behavior is justified through increase in photoluminescence on Ge. The range of efficient emission is to 120 with the first band interaction, and above 400 on the second band interaction. Low defect concentration in Ge is achieved through chemical vapor deposition at high vacuum (~1x10-8 mbar) in a two-step process. The high temperature growth and low concentration of particles permits epitaxial growth with low defect concentration. Chemical selectivity forbids Ge growth on oxide. Oxide trenches permit the growth on Si for a variety of shapes, without detrimentally affecting the strain of the Ge devices. Dopant concentration above intrinsic growth concentration, ~1x1019cm-3 phosphorus, have been achieved through a series of methods non-CMOS, spin-on dopant; and CMOS, implantation and delta doping. All the techniques explored use enhanced dopant diffusion observed in Ge under heavy n-type doping. A dopant source, or well, is used to distribute the dopants in the Ge without increasing the defect concentration. The approach lead to the development of electrically injected devices, LEDs and LDs. Ge pnn double heterostructure diodes were made under low, ~1x1018cm-3, and heavy n-type doping, >1x1019cm-3. Both devices showed improved performance compared to pin Ge LED. Furthermore, heavy doped Ge diodes exhibit evidence of bleaching or transparency. The techniques described permitted the development of Ge-on-Si laser with a concentration ~1-2x1019cm-3. It is the first demonstration of a Ge laser optically pumped working under the direct band gap assumption like other semiconductors. It represents the evidence of carrier inversion on an indirect band gap semiconductor. With 50cm-1 gain, the material shows Fabry-Perot cavity behavior. Finally, we demonstrated a fully functioning laser diode monolithically integrated on Si. Ge pnn lasers were made exhibiting a gain >1000cm-1 and exhibiting a spectrum range of over 200nm, making Ge the ideal candidate for Si photonics."
}, {
    "id": "oai:dspace.mit.edu:1721.1/111334",
    "title": "Repeatable and adjustable on-demand local anesthesia using externally-triggerable liposomes",
    "abstract": "Pain management would be greatly enhanced by a formulation that would provide local anesthesia at the time desired by patients, with the desired intensity and duration. Current treatment for many pain states - including localized pain - often involves systemic medications such as opioids that have significant side effects. Externally triggerable drug delivery systems provide a strategy for the delivery of therapeutic agents preferentially to the target site at the desired timing, dosage, and duration, presenting the ability to enhance therapeutic efficacy while reducing side effects. Here we have developed light- and ultrasound- triggerable liposomes that provide on-demand nerve block at the desired timing, intensity and duration. The responsiveness of the liposomes towards the external triggering was studied in vitro, where the light triggerable system showed 5.6% drug release upon irradiation with 730 nm light and the ultrasound triggerable system showed 5.4% drug release upon insonation 1 MHz, 3W/cm ultrasound. Sciatic nerve blockades for up to a duration of 2h was successfully achieved with safe dosage of light (730 nm, 75 mW/cm, 15 min) or ultrasound (1 MHz, 3W/cm, 10 min). Sciatic nerve block could be triggered repeatedly with light or ultrasound for more than 5 times upon a single injection. The duration of nerve block showed a linear relationship with the energy density of the triggering event, which was controlled by duration and intensity of the external energy source. Tissue reaction was benign. Such on-demand nerve block systems have promising potential to provide personalized and effective local anesthesia that will enhance pain management.",
    "advisors": ["Robert S. Langer", "Daniel S. Kohane"],
    "text": "Repeatable and adjustable on-demand local anesthesia using externally-triggerable liposomes Pain management would be greatly enhanced by a formulation that would provide local anesthesia at the time desired by patients, with the desired intensity and duration. Current treatment for many pain states - including localized pain - often involves systemic medications such as opioids that have significant side effects. Externally triggerable drug delivery systems provide a strategy for the delivery of therapeutic agents preferentially to the target site at the desired timing, dosage, and duration, presenting the ability to enhance therapeutic efficacy while reducing side effects. Here we have developed light- and ultrasound- triggerable liposomes that provide on-demand nerve block at the desired timing, intensity and duration. The responsiveness of the liposomes towards the external triggering was studied in vitro, where the light triggerable system showed 5.6% drug release upon irradiation with 730 nm light and the ultrasound triggerable system showed 5.4% drug release upon insonation 1 MHz, 3W/cm ultrasound. Sciatic nerve blockades for up to a duration of 2h was successfully achieved with safe dosage of light (730 nm, 75 mW/cm, 15 min) or ultrasound (1 MHz, 3W/cm, 10 min). Sciatic nerve block could be triggered repeatedly with light or ultrasound for more than 5 times upon a single injection. The duration of nerve block showed a linear relationship with the energy density of the triggering event, which was controlled by duration and intensity of the external energy source. Tissue reaction was benign. Such on-demand nerve block systems have promising potential to provide personalized and effective local anesthesia that will enhance pain management."
}, {
    "id": "oai:dspace.mit.edu:1721.1/104185",
    "title": "Rational design strategies for oxide oxygen evolution electrocatalysts",
    "abstract": "Understanding and mastering the kinetics of oxygen electrocatalysis is instrumental to enabling solar fuels, fuel cells, electrolyzers, and metal-air batteries. Non-precious transition metal oxides show promise as cost-effective materials in such devices. Leveraging the wealth of solid-state physics understanding developed for this class of materials in the past few decades, new theories and strategies can be explored for designing optimal catalysts. This work presents a framework for the rational design of transition-metal perovskite oxide catalysts that can accelerate the development of highly active catalysts for more efficient energy storage and conversion systems. We describe a method for the synthesis of X-ray emission, absorption, and photoelectron spectroscopy data to experimentally determine the electronic structure of oxides on an absolute energy scale, as well as extract key electronic parameters associated with the material. Using this approach, we show that the charge-transfer energy - a parameter that captures the energy configuration of oxygen and transition-metal valence electrons - is a central descriptor capable of modifying both the oxygen evolution kinetics and mechanism. Its role in determining the absolute band energies of a catalyst can rationalize the differences in the electron-transfer and proton-transfer kinetics across oxide chemistries. Furthermore, we corroborate that the charge-transfer energy is one of the most influential parameters on the oxygen evolution reaction through a statistical analysis of a multitude of structure-activity relationships. The quantitative models generated by this analysis can then be used to rapidly screen oxide materials across a wide chemical space for highthroughput materials discovery.",
    "advisors": ["Yang Shao-Horn"],
    "text": "Rational design strategies for oxide oxygen evolution electrocatalysts Understanding and mastering the kinetics of oxygen electrocatalysis is instrumental to enabling solar fuels, fuel cells, electrolyzers, and metal-air batteries. Non-precious transition metal oxides show promise as cost-effective materials in such devices. Leveraging the wealth of solid-state physics understanding developed for this class of materials in the past few decades, new theories and strategies can be explored for designing optimal catalysts. This work presents a framework for the rational design of transition-metal perovskite oxide catalysts that can accelerate the development of highly active catalysts for more efficient energy storage and conversion systems. We describe a method for the synthesis of X-ray emission, absorption, and photoelectron spectroscopy data to experimentally determine the electronic structure of oxides on an absolute energy scale, as well as extract key electronic parameters associated with the material. Using this approach, we show that the charge-transfer energy - a parameter that captures the energy configuration of oxygen and transition-metal valence electrons - is a central descriptor capable of modifying both the oxygen evolution kinetics and mechanism. Its role in determining the absolute band energies of a catalyst can rationalize the differences in the electron-transfer and proton-transfer kinetics across oxide chemistries. Furthermore, we corroborate that the charge-transfer energy is one of the most influential parameters on the oxygen evolution reaction through a statistical analysis of a multitude of structure-activity relationships. The quantitative models generated by this analysis can then be used to rapidly screen oxide materials across a wide chemical space for highthroughput materials discovery."
}, {
    "id": "oai:dspace.mit.edu:1721.1/98579",
    "title": "Magneto-optical and multiferroic oxide thin films, integrated nonreciprocal photonic devices and multiferroic memory devices",
    "abstract": "Complex oxide thin films offer unique functionalities which can potentially extend the utility of current storage, processing and optical isolator technologies. In this thesis, we present three categories of studies on complex oxide growth using pulsed laser deposition (PLD) and structural, magnetic, magneto-optical and ferroelectric characterization. We first focused on enhancing integrated magneto-optical isolator performance by improving the growth method of magneto-optical Ce1Y2Fe5O12 (Ce:YIG) films. The spectral and substrate orientation dependence of the magneto-optical figure of merit of epitaxial Ce: YIG on GGG substrates show very high magneto-optical figure of merit (379-400 dB-1 at [lambda] = 1550 nm for all substrate orientations). The thermal budgets of Ce: YIG growth on ShN4 (2 high temperature PLD steps and a rapid thermal anneal, RTA), silicon-on-insulator substrates (a high and a low temperature PLD step and a RTA) and optical resonator chips (one PLD step, one RTA, YIG seed layer from the top) were progressively reduced to achieve improved integrated optical isolators with low insertion loss of 7.4  1.8 dB and an isolation ratio of 13.0  2.2 dB. We demonstrated that the ferrimagnetic insulator YIG thin films (Y3Fe5O12) epitaxially grown on GGG substrates achieve ultralow Gilbert damping of spin waves ([alpha] = 2.2-7 x 10-4 ), which enable em-long in-plane propagation of spin waves. This demonstration enables researchers to fabricate near-dissipationless magnon-based logic computers. Finally, we present a substitutionally-doped perovksite, STCo30 (Sr Ti0.70 CO0.30 O3-[delta]) integrated on Si, STO (100), and on Nb:STO substrates. This perovskite oxide has been found to exhibit ferroelectricity and magnetism at room temperature. Experimental results on magnetism, ferroelectricity and structure were reproduced using density functional theory simulations.",
    "advisors": ["Caroline A. Ross"],
    "text": "Magneto-optical and multiferroic oxide thin films, integrated nonreciprocal photonic devices and multiferroic memory devices Complex oxide thin films offer unique functionalities which can potentially extend the utility of current storage, processing and optical isolator technologies. In this thesis, we present three categories of studies on complex oxide growth using pulsed laser deposition (PLD) and structural, magnetic, magneto-optical and ferroelectric characterization. We first focused on enhancing integrated magneto-optical isolator performance by improving the growth method of magneto-optical Ce1Y2Fe5O12 (Ce:YIG) films. The spectral and substrate orientation dependence of the magneto-optical figure of merit of epitaxial Ce: YIG on GGG substrates show very high magneto-optical figure of merit (379-400 dB-1 at [lambda] = 1550 nm for all substrate orientations). The thermal budgets of Ce: YIG growth on ShN4 (2 high temperature PLD steps and a rapid thermal anneal, RTA), silicon-on-insulator substrates (a high and a low temperature PLD step and a RTA) and optical resonator chips (one PLD step, one RTA, YIG seed layer from the top) were progressively reduced to achieve improved integrated optical isolators with low insertion loss of 7.4  1.8 dB and an isolation ratio of 13.0  2.2 dB. We demonstrated that the ferrimagnetic insulator YIG thin films (Y3Fe5O12) epitaxially grown on GGG substrates achieve ultralow Gilbert damping of spin waves ([alpha] = 2.2-7 x 10-4 ), which enable em-long in-plane propagation of spin waves. This demonstration enables researchers to fabricate near-dissipationless magnon-based logic computers. Finally, we present a substitutionally-doped perovksite, STCo30 (Sr Ti0.70 CO0.30 O3-[delta]) integrated on Si, STO (100), and on Nb:STO substrates. This perovskite oxide has been found to exhibit ferroelectricity and magnetism at room temperature. Experimental results on magnetism, ferroelectricity and structure were reproduced using density functional theory simulations."
}, {
    "id": "oai:dspace.mit.edu:1721.1/9690",
    "title": "Electron microscopic investigation of interfaces in materials for orthopedic applications",
    "abstract": "About 250,000 people undergo knee and hip arthroplasty each year in North America alone, with hundreds of thousands more receiving joints over the rest of the world. Two factors are key to the success of these implants: first, the quality of attachment of the prosthetic joint to the patient's bone, and second, the low generation of wear particles as the components of the prosthesis articulate against each other. This thesis is a study of both of these factors. First, the mechanism of bone apposition to hydroxyapatite (HA) coatings on Ti-6Al-4V was investigated via transmission electron microscopy (TEM). In this section of the study, Ti alloy cylinders were coated with HA by two different methods to yield three types of coatings - annealed and unannealed plasma-spray (PSHA) coatings and an annealed ion-beam assisted deposited (IBAD-HA) coating. These cylinders were implanted in trabecular bone in dogs from periods ranging from 3 hours to 14 days. Mechanical testing indicated that the bone/implant interface with the PSHA coated implants was significantly stronger than that with the IBAD-HA coated or uncoated Ti alloy implants. However, there were no differences in the degree of bone apposition to the three HA-coated materials; thus indicating that bone apposition is not a sufficient indicator of mechanical integrity of the bone/HA interface. In the second section of this study, the microstructural factors contributing to the observed wear properties of the oxide on Zr-2.5Nb were investigated via TEM. Zr-2.5Nb barstock which had been rotary-forged to impart an anisotropic microstructure was sectioned and oxidized in dry air at 600C and 635C for a variety of times ranging from 30 minutes to 40 hours. Cross-sections across the oxide/metal interface were observed via TEM. The oxide scale comprises primarily monoclinic zirconia, with small amounts of tetragonal zirconia. Evidence of a mixed oxide phase, 6Zr02.Nb205, was also observed. The microstructure of this oxide is dependent on oxidation temperature, the microstructure of the underlying metal, and oxide depth. Two oxide microstructures originating from beta-Zr grains in the alloy were also identified. A third study concerned the architecture and microstructure of naturally-derived and synthetic bone substitute materials (BSMs). While BSMs are used clinically to promote healing in large bone defects, they were useful to this study as a control for the organization of mineral in mature bone. Low voltage high resolution scanning electron microscopy (LVHRSEM) enabled observations of the three dimensional architecture of these materials which were then correlated with TEM observations. The crystallites in an anorganic bovine-derived BSM were organized in a hierarchical fashion which paralleled the organization of collagen. In contrast, the synthetic materials were organized in an isotropic network. The difference in organization was attributed to the formation of the mineral matrix of bone on an anisotropic collagen template.",
    "advisors": ["Linn W. Hobbs"],
    "text": "Electron microscopic investigation of interfaces in materials for orthopedic applications About 250,000 people undergo knee and hip arthroplasty each year in North America alone, with hundreds of thousands more receiving joints over the rest of the world. Two factors are key to the success of these implants: first, the quality of attachment of the prosthetic joint to the patient's bone, and second, the low generation of wear particles as the components of the prosthesis articulate against each other. This thesis is a study of both of these factors. First, the mechanism of bone apposition to hydroxyapatite (HA) coatings on Ti-6Al-4V was investigated via transmission electron microscopy (TEM). In this section of the study, Ti alloy cylinders were coated with HA by two different methods to yield three types of coatings - annealed and unannealed plasma-spray (PSHA) coatings and an annealed ion-beam assisted deposited (IBAD-HA) coating. These cylinders were implanted in trabecular bone in dogs from periods ranging from 3 hours to 14 days. Mechanical testing indicated that the bone/implant interface with the PSHA coated implants was significantly stronger than that with the IBAD-HA coated or uncoated Ti alloy implants. However, there were no differences in the degree of bone apposition to the three HA-coated materials; thus indicating that bone apposition is not a sufficient indicator of mechanical integrity of the bone/HA interface. In the second section of this study, the microstructural factors contributing to the observed wear properties of the oxide on Zr-2.5Nb were investigated via TEM. Zr-2.5Nb barstock which had been rotary-forged to impart an anisotropic microstructure was sectioned and oxidized in dry air at 600C and 635C for a variety of times ranging from 30 minutes to 40 hours. Cross-sections across the oxide/metal interface were observed via TEM. The oxide scale comprises primarily monoclinic zirconia, with small amounts of tetragonal zirconia. Evidence of a mixed oxide phase, 6Zr02.Nb205, was also observed. The microstructure of this oxide is dependent on oxidation temperature, the microstructure of the underlying metal, and oxide depth. Two oxide microstructures originating from beta-Zr grains in the alloy were also identified. A third study concerned the architecture and microstructure of naturally-derived and synthetic bone substitute materials (BSMs). While BSMs are used clinically to promote healing in large bone defects, they were useful to this study as a control for the organization of mineral in mature bone. Low voltage high resolution scanning electron microscopy (LVHRSEM) enabled observations of the three dimensional architecture of these materials which were then correlated with TEM observations. The crystallites in an anorganic bovine-derived BSM were organized in a hierarchical fashion which paralleled the organization of collagen. In contrast, the synthetic materials were organized in an isotropic network. The difference in organization was attributed to the formation of the mineral matrix of bone on an anisotropic collagen template."
}, {
    "id": "oai:dspace.mit.edu:1721.1/89842",
    "title": "Modeling and theoretical design methods for directed self-assembly of thin film block copolymer systems",
    "abstract": "Block copolymers (BCPs) have become a highly studied material for lithographic applications due to their ability to self-assemble into complex periodic patterns with feature resolutions ranging from a few to 100s nm. BCPs form a wide variety of patterns due the combination of their enthalpic interactions promoting immiscibility between the blocks and the bonding constraint through their chain topology. The morphologies formed can be tailored through a directed self-assembly (DSA) process using chemical or topographical templates to achieve a desired thin film pattern. This method combines the traditional top-down lithographic methods with the bottom-up self-assembly process to obtain greater control over long range order, the local morphology, and overall throughput of the patterns produced. This work looks at key modeling challenges in optimizing BCP DSA to achieve precision morphology control, reproducibility, and defect control. Modeling techniques based on field theoretic simulations are used to both characterize and predict the morphological behavior of a variety of BCPs under a variety of processing conditions including solvent annealing and DSA under topographical boundary conditions. These methods aid experimental studies by saving time in performing experiments over wide parameter spaces as well as elucidating information that may not be available by current experimental techniques. Both forward simulation approaches are studied where parameters are varied over a wide range with phase diagrams of potential morphologies characterized and inverse design approaches where given target patterns are taken as simulation input and required conditions to produce those patterns are outputted from the simulation for experimental testing. The studies ultimately help identify the key control parameters in BCP DSA and enable a vast array of possible utility in the field.",
    "advisors": ["Caroline A. Ross"],
    "text": "Modeling and theoretical design methods for directed self-assembly of thin film block copolymer systems Block copolymers (BCPs) have become a highly studied material for lithographic applications due to their ability to self-assemble into complex periodic patterns with feature resolutions ranging from a few to 100s nm. BCPs form a wide variety of patterns due the combination of their enthalpic interactions promoting immiscibility between the blocks and the bonding constraint through their chain topology. The morphologies formed can be tailored through a directed self-assembly (DSA) process using chemical or topographical templates to achieve a desired thin film pattern. This method combines the traditional top-down lithographic methods with the bottom-up self-assembly process to obtain greater control over long range order, the local morphology, and overall throughput of the patterns produced. This work looks at key modeling challenges in optimizing BCP DSA to achieve precision morphology control, reproducibility, and defect control. Modeling techniques based on field theoretic simulations are used to both characterize and predict the morphological behavior of a variety of BCPs under a variety of processing conditions including solvent annealing and DSA under topographical boundary conditions. These methods aid experimental studies by saving time in performing experiments over wide parameter spaces as well as elucidating information that may not be available by current experimental techniques. Both forward simulation approaches are studied where parameters are varied over a wide range with phase diagrams of potential morphologies characterized and inverse design approaches where given target patterns are taken as simulation input and required conditions to produce those patterns are outputted from the simulation for experimental testing. The studies ultimately help identify the key control parameters in BCP DSA and enable a vast array of possible utility in the field."
}, {
    "id": "oai:dspace.mit.edu:1721.1/75842",
    "title": "Liquid foams of graphene",
    "abstract": "Liquid foams are dispersions of bubbles in a liquid. Bubbles are stabilized by foaming agents that position at the interface between the gas and the liquid. Most foaming agents, such as the commonly used sodium dodecylsulfate, are surfactant molecules with linear or branched chain molecular structures. This thesis presents a new class of liquid foams made with a foaming agent having a sheet molecular structure. In these foams, air bubbles are encapsulated inside graphene shells. The shells have a concentric layered structure made of isophorone diamine modified graphene oxide sheets. The liquid foams of graphene were initially developed as an extractive step in the preparation of graphene-epoxy nanocomposites. Chapter 1 gives an introduction to polymer nanocomposites and graphene. Chapter 2 presents a novel processing method for graphene-epoxy nanocomposites. Chapter 3 deals with the structure, formation mechanism, stability and mechanical properties of the liquid foams of graphene. Chapter 4 reports on materials and methods. Finally, Chapter 5 summarizes the main conclusions of this work and proposes future directions for research.",
    "advisors": ["Edwin L. Thomas"],
    "text": "Liquid foams of graphene Liquid foams are dispersions of bubbles in a liquid. Bubbles are stabilized by foaming agents that position at the interface between the gas and the liquid. Most foaming agents, such as the commonly used sodium dodecylsulfate, are surfactant molecules with linear or branched chain molecular structures. This thesis presents a new class of liquid foams made with a foaming agent having a sheet molecular structure. In these foams, air bubbles are encapsulated inside graphene shells. The shells have a concentric layered structure made of isophorone diamine modified graphene oxide sheets. The liquid foams of graphene were initially developed as an extractive step in the preparation of graphene-epoxy nanocomposites. Chapter 1 gives an introduction to polymer nanocomposites and graphene. Chapter 2 presents a novel processing method for graphene-epoxy nanocomposites. Chapter 3 deals with the structure, formation mechanism, stability and mechanical properties of the liquid foams of graphene. Chapter 4 reports on materials and methods. Finally, Chapter 5 summarizes the main conclusions of this work and proposes future directions for research."
}, {
    "id": "oai:dspace.mit.edu:1721.1/62077",
    "title": "Social networks for lonely objects",
    "abstract": "Visions of ubiquitous computing describe a network of devices that quietly supports human goals, but this may also add complexity to an already frustrating relationship between humans and their electronic objects. As we move from vision to reality, there is an opportunity to rethink how we interact with our objects and networks of objects, and close the communication gap between man and machine. 'This thesis defines social and super-mechanical affordances for products which may consist of many physical and digital objects. These new objects will not look like stripped-down contemporary computers, but augmented ordinary objects that are focused on input and output, exposed on Twitter. Apps in the cloud use Twitter to marshall the appropriate objects to execute human tasks. Using a social network as transport allows apps and their owners to manage a large network of computing objects with the same constructs that we use to manage many human relationships. From this direction, we take a step toward a consumer-amenable implementation of ubiquitous computing.",
    "advisors": ["Henry Holtzman"],
    "text": "Social networks for lonely objects Visions of ubiquitous computing describe a network of devices that quietly supports human goals, but this may also add complexity to an already frustrating relationship between humans and their electronic objects. As we move from vision to reality, there is an opportunity to rethink how we interact with our objects and networks of objects, and close the communication gap between man and machine. 'This thesis defines social and super-mechanical affordances for products which may consist of many physical and digital objects. These new objects will not look like stripped-down contemporary computers, but augmented ordinary objects that are focused on input and output, exposed on Twitter. Apps in the cloud use Twitter to marshall the appropriate objects to execute human tasks. Using a social network as transport allows apps and their owners to manage a large network of computing objects with the same constructs that we use to manage many human relationships. From this direction, we take a step toward a consumer-amenable implementation of ubiquitous computing."
}, {
    "id": "oai:dspace.mit.edu:1721.1/62372",
    "title": "C@t : a language for programming massively distributed embedded systems",
    "abstract": "This thesis presents c@t, a language for programming distributed embedded systems that are composed of thousands (even millions) of interacting computing devices. Due to the improvements in fabricating technologies, it is becoming possible to build tiny single-chip devices equipped with logic circuits, sensors, actuators and communication components. A large number of these devices can be networked together to build Massively Distributed Embedded Systems (MDES). A wide variety of embedded control applications are envisioned for MDES: responsive environments, smart buildings, wildlife monitoring, precision agriculture, inventory tracking, etc. These examples are compelling, however, developing applications for MDES remains complex due to the following issues: MDES consist of large number of resource constrained devices and the number of potential interactions between them can be combinatorially explosive. Systems with the combined issues of such scale complexity, interaction complexity and resource constraints are unprecedented and cannot be programmed using conventional technologies. Accordingly, this thesis presents cut, a language that employs the following techniques to address the issues of MDES: 1. To address the scale complexity, c@t provides tools for programming the system as a unit. 2. c@t offers a declarative style network programming interface so that network interactions can be implemented without writing any low-level networking code. 3. The applications developed using c@t are vertically integrated. That is, the compiler customizes the runtime environment to the suit the application needs. Using this integrated approach, efficient applications can be developed to fit the available resources. This thesis describes the design, features and implementation of c@t in detail. A sample application developed using c@t is also presented.",
    "advisors": ["V. Michael Bove, Jr"],
    "text": "C@t : a language for programming massively distributed embedded systems This thesis presents c@t, a language for programming distributed embedded systems that are composed of thousands (even millions) of interacting computing devices. Due to the improvements in fabricating technologies, it is becoming possible to build tiny single-chip devices equipped with logic circuits, sensors, actuators and communication components. A large number of these devices can be networked together to build Massively Distributed Embedded Systems (MDES). A wide variety of embedded control applications are envisioned for MDES: responsive environments, smart buildings, wildlife monitoring, precision agriculture, inventory tracking, etc. These examples are compelling, however, developing applications for MDES remains complex due to the following issues: MDES consist of large number of resource constrained devices and the number of potential interactions between them can be combinatorially explosive. Systems with the combined issues of such scale complexity, interaction complexity and resource constraints are unprecedented and cannot be programmed using conventional technologies. Accordingly, this thesis presents cut, a language that employs the following techniques to address the issues of MDES: 1. To address the scale complexity, c@t provides tools for programming the system as a unit. 2. c@t offers a declarative style network programming interface so that network interactions can be implemented without writing any low-level networking code. 3. The applications developed using c@t are vertically integrated. That is, the compiler customizes the runtime environment to the suit the application needs. Using this integrated approach, efficient applications can be developed to fit the available resources. This thesis describes the design, features and implementation of c@t in detail. A sample application developed using c@t is also presented."
}, {
    "id": "oai:dspace.mit.edu:1721.1/37388",
    "title": "GlobalMind : bridging the gap between different cultures and languages with common-sense computing",
    "abstract": "The need for more effective communication across different countries has increased as the interactions between them have been growing. Communication is often difficult because of both language differences and cultural differences. Although there have been many attempts to meet the communication need on the level of language with machine translators and dictionaries, many problems related to cultural and conceptual differences still remain. To improve traditional machine translators and cross-cultural communication aids, it is necessary to develop automated mechanisms to analyze cultural differences and similarities. This thesis approaches the problems with automatic computation of cultural differences and similarities. This thesis, GlobalMind, provides common-sense databases of various countries and languages and two inference modules to analyze and compute the cultural differences and similarities from the databases. I describe the design of GlobalMind databases, the implementation of its inference modules, the results of an evaluation of GlobalMind, and available applications.",
    "advisors": ["Walter Bender"],
    "text": "GlobalMind : bridging the gap between different cultures and languages with common-sense computing The need for more effective communication across different countries has increased as the interactions between them have been growing. Communication is often difficult because of both language differences and cultural differences. Although there have been many attempts to meet the communication need on the level of language with machine translators and dictionaries, many problems related to cultural and conceptual differences still remain. To improve traditional machine translators and cross-cultural communication aids, it is necessary to develop automated mechanisms to analyze cultural differences and similarities. This thesis approaches the problems with automatic computation of cultural differences and similarities. This thesis, GlobalMind, provides common-sense databases of various countries and languages and two inference modules to analyze and compute the cultural differences and similarities from the databases. I describe the design of GlobalMind databases, the implementation of its inference modules, the results of an evaluation of GlobalMind, and available applications."
}, {
    "id": "oai:dspace.mit.edu:1721.1/61139",
    "title": "A computational model to connect gestalt perception and natural language",
    "abstract": "We present a computational model that connects gestalt visual perception and language. The model grounds the meaning of natural language words and phrases in terms of the perceptual properties of visually salient groups. We focus on the semantics of a class of words that we call conceptual aggregates e.g., pair, group, stuff, which inherently refer to groups of objects. The model provides an explanation for how the semantics of these natural language terms interact with gestalt processes in order to connect referring expressions to visual groups. Our computational model can be divided into two stages. The first stage performs grouping on visual scenes. It takes a visual scene segmented into block objects as input, and creates a space of possible salient groups arising from the scene. This stage also assigns a saliency score to each group. In the second stage, visual grounding, the space of salient groups, which is the output of the previous stage, is taken as input along with a linguistic scene description. The visual grounding stage comes up with the best match between a linguistic description and a set of objects. Parameters of the model are trained on the basis of observed data from a linguistic description and visual selection task. The proposed model has been implemented in the form of a program that takes as input a synthetic visual scene and linguistic description, and as output identifies likely groups of objects within the scene that correspond to the description. We present an evaluation of the performance of the model on a visual referent identification task. This model may be applied in natural language understanding and generation systems that utilize visual context such as scene description systems for the visually impaired and functionally illiterate.",
    "advisors": ["Deb K. Roy"],
    "text": "A computational model to connect gestalt perception and natural language We present a computational model that connects gestalt visual perception and language. The model grounds the meaning of natural language words and phrases in terms of the perceptual properties of visually salient groups. We focus on the semantics of a class of words that we call conceptual aggregates e.g., pair, group, stuff, which inherently refer to groups of objects. The model provides an explanation for how the semantics of these natural language terms interact with gestalt processes in order to connect referring expressions to visual groups. Our computational model can be divided into two stages. The first stage performs grouping on visual scenes. It takes a visual scene segmented into block objects as input, and creates a space of possible salient groups arising from the scene. This stage also assigns a saliency score to each group. In the second stage, visual grounding, the space of salient groups, which is the output of the previous stage, is taken as input along with a linguistic scene description. The visual grounding stage comes up with the best match between a linguistic description and a set of objects. Parameters of the model are trained on the basis of observed data from a linguistic description and visual selection task. The proposed model has been implemented in the form of a program that takes as input a synthetic visual scene and linguistic description, and as output identifies likely groups of objects within the scene that correspond to the description. We present an evaluation of the performance of the model on a visual referent identification task. This model may be applied in natural language understanding and generation systems that utilize visual context such as scene description systems for the visually impaired and functionally illiterate."
}, {
    "id": "oai:dspace.mit.edu:1721.1/76524",
    "title": "Interest networks : understanding the influence of interesting people in an organization",
    "abstract": "This thesis applies network theory to firms, their employees, and various aspects of the employees to understand diversity within an industry at both the firm-level and employee-level. We hypothesize that the interest diversity of a firms' employee can influence that firm's economic performance and growth. Using the LinkedIn API, we are able to collect ~600 employees (past and present) for 43 companies using a keyword search. This data is used to create a visualization of people's interests, an \"Interest Space\", which is a network graph of how interests are categorized and linked to each other. By analyzing the data from firms associated with the Media Lab, we begin to understand how individual interests affect success among a small network of companies. We research this through case studies of a few companies by analyzing their financial data and interests of their employees.",
    "advisors": ["Andrew B. Lippman"],
    "text": "Interest networks : understanding the influence of interesting people in an organization This thesis applies network theory to firms, their employees, and various aspects of the employees to understand diversity within an industry at both the firm-level and employee-level. We hypothesize that the interest diversity of a firms' employee can influence that firm's economic performance and growth. Using the LinkedIn API, we are able to collect ~600 employees (past and present) for 43 companies using a keyword search. This data is used to create a visualization of people's interests, an \"Interest Space\", which is a network graph of how interests are categorized and linked to each other. By analyzing the data from firms associated with the Media Lab, we begin to understand how individual interests affect success among a small network of companies. We research this through case studies of a few companies by analyzing their financial data and interests of their employees."
}, {
    "id": "oai:dspace.mit.edu:1721.1/28766",
    "title": "Improvisatory music and painting interface",
    "abstract": "(cont.) theoretical section is accompanied by descriptions of historic and contemporary works that have influenced IMPI.",
    "advisors": ["Tod Machover"],
    "text": "Improvisatory music and painting interface (cont.) theoretical section is accompanied by descriptions of historic and contemporary works that have influenced IMPI."
}, {
    "id": "oai:dspace.mit.edu:1721.1/82435",
    "title": "PCB origami : folding circuit boards into electronic products",
    "abstract": "PCB origami is a concept for an alternative manufacturing process of electronic products, in which the electronic material will be manufactured flat and folded into functional 3D graspable products by the user. PCBs will be used both as the functional electronic material and the structural material of the products. This thesis work explores the fabrication, design and interaction aspects of this concept, by demonstrating a series of case studies. The fabrication aspect was examined by creating a straightforward customization workflow of electronic products, and by developing a series of prototyping techniques for PCB lamination. The design aspect was researched through the creation of a new type of electronic products that can be completely flat when not in use, and can be folded once needed. The user interaction aspect of the PCB origami concept was studied in the contexts of foldable surfaces with embedded information on how to fold them and by demonstrating a foldable PCB product that is able to change its shape based on the desired task.",
    "advisors": ["Neri Oxman"],
    "text": "PCB origami : folding circuit boards into electronic products PCB origami is a concept for an alternative manufacturing process of electronic products, in which the electronic material will be manufactured flat and folded into functional 3D graspable products by the user. PCBs will be used both as the functional electronic material and the structural material of the products. This thesis work explores the fabrication, design and interaction aspects of this concept, by demonstrating a series of case studies. The fabrication aspect was examined by creating a straightforward customization workflow of electronic products, and by developing a series of prototyping techniques for PCB lamination. The design aspect was researched through the creation of a new type of electronic products that can be completely flat when not in use, and can be folded once needed. The user interaction aspect of the PCB origami concept was studied in the contexts of foldable surfaces with embedded information on how to fold them and by demonstrating a foldable PCB product that is able to change its shape based on the desired task."
}, {
    "id": "oai:dspace.mit.edu:1721.1/61107",
    "title": "Emergent design and image processing : a case study",
    "abstract": "The digital revolution which has changed so many other aspects of modem life has yet to profoundly affect the working process of visual artists and designers. High-quality digital design tools exist, but they provide the user with an improved traditional design process, not a radically new way of designing. Conventional digital design tools are useful, but when design software emulates a paintbrush or photostudio many powerful possibilities of the computational medium are overlooked. This thesis explores emergent design, a design methodology based on a new process, enhanced interactive genetic programming. The emergent design methodology and tools allow designers to effectively create procedural design solutions (design solutions that take the form of a procedure or program) in a way that requires little or no programming on the part of the designer. The use of preliminary fitness functions in the interactive genetic programming process allows the designer to specify heuristics to guide the search and manage the complexity of the interactive genetic programming task. This document is structured in the form of a case study, in which the enhanced genetic programming process and emergent design methodology are described through their application to the specific problem of developing procedural image filters for still and moving images. Two interactive genetic programming systems for image filter evolution are described, GPI and evolution++, along with the Sol programming language that was used to create them. Results from the implementation and use of GPI and evolution++ are presented, including a number of filtered images and image sequences. These results suggest that fitness-agent enhanced interactive genetic programming and the emergent design methodology may play a useful role in the visual design process, allowing designers to explore a wider range of options with greater ease than is possible through a traditional, procedural, or conventional genetic programming design process.",
    "advisors": ["John Maeda"],
    "text": "Emergent design and image processing : a case study The digital revolution which has changed so many other aspects of modem life has yet to profoundly affect the working process of visual artists and designers. High-quality digital design tools exist, but they provide the user with an improved traditional design process, not a radically new way of designing. Conventional digital design tools are useful, but when design software emulates a paintbrush or photostudio many powerful possibilities of the computational medium are overlooked. This thesis explores emergent design, a design methodology based on a new process, enhanced interactive genetic programming. The emergent design methodology and tools allow designers to effectively create procedural design solutions (design solutions that take the form of a procedure or program) in a way that requires little or no programming on the part of the designer. The use of preliminary fitness functions in the interactive genetic programming process allows the designer to specify heuristics to guide the search and manage the complexity of the interactive genetic programming task. This document is structured in the form of a case study, in which the enhanced genetic programming process and emergent design methodology are described through their application to the specific problem of developing procedural image filters for still and moving images. Two interactive genetic programming systems for image filter evolution are described, GPI and evolution++, along with the Sol programming language that was used to create them. Results from the implementation and use of GPI and evolution++ are presented, including a number of filtered images and image sequences. These results suggest that fitness-agent enhanced interactive genetic programming and the emergent design methodology may play a useful role in the visual design process, allowing designers to explore a wider range of options with greater ease than is possible through a traditional, procedural, or conventional genetic programming design process."
}, {
    "id": "oai:dspace.mit.edu:1721.1/117453",
    "title": "Design and the police : toward a model of citizen intervention and civic imagination",
    "abstract": "The police are designed. Their tools, policies, and human services are all products of deliberation and choice, and therefore open to consideration and re-consideration in an era that has seen widespread abuse of power. This thesis takes up one element of the designed police system in the United States: its material culture-from vehicles, to uniforms and badges, to weapons. The physical tools and devices that the police force use are emblematic of explicit and implicit values. These values make certain conditions and encounters possible, and other scenarios impossible. What is behind these tools, and how might our culture see them anew? How might we re-imagine them in the civic act of designing a future? Oversight, transparency, and accountability are a critical piece of the civic fabric. In order for law enforcement to reflect the needs and expectations of citizens, it is in part, our responsibility to interrogate the designs of the key institutions we rely on. But agency in the design space of the police has not been encouraged. This thesis presents one example of how a dialogue around design is a form of productive civic activity and a check against state violence. In it, I offer a complementary set of tools for imagining possible futures of policing that reconsider scenarios for law enforcement, with a provisional freedom from its current form. Problematizing the physical designs of the police, it focuses on the values, priorities, and politics that are inevitably imbued in these objects. This practice-led research draws from interviews with both citizens and law enforcement, design research, and participatory, critical making. It makes a case for citizen engagement and civic imagination in the proactive design of the police. This speculative design approach fosters understanding and agency, and suggests one way in which the design of the police could be a more inclusive and collaborative project.",
    "advisors": ["Ethan Zuckerman"],
    "text": "Design and the police : toward a model of citizen intervention and civic imagination The police are designed. Their tools, policies, and human services are all products of deliberation and choice, and therefore open to consideration and re-consideration in an era that has seen widespread abuse of power. This thesis takes up one element of the designed police system in the United States: its material culture-from vehicles, to uniforms and badges, to weapons. The physical tools and devices that the police force use are emblematic of explicit and implicit values. These values make certain conditions and encounters possible, and other scenarios impossible. What is behind these tools, and how might our culture see them anew? How might we re-imagine them in the civic act of designing a future? Oversight, transparency, and accountability are a critical piece of the civic fabric. In order for law enforcement to reflect the needs and expectations of citizens, it is in part, our responsibility to interrogate the designs of the key institutions we rely on. But agency in the design space of the police has not been encouraged. This thesis presents one example of how a dialogue around design is a form of productive civic activity and a check against state violence. In it, I offer a complementary set of tools for imagining possible futures of policing that reconsider scenarios for law enforcement, with a provisional freedom from its current form. Problematizing the physical designs of the police, it focuses on the values, priorities, and politics that are inevitably imbued in these objects. This practice-led research draws from interviews with both citizens and law enforcement, design research, and participatory, critical making. It makes a case for citizen engagement and civic imagination in the proactive design of the police. This speculative design approach fosters understanding and agency, and suggests one way in which the design of the police could be a more inclusive and collaborative project."
}, {
    "id": "oai:dspace.mit.edu:1721.1/69522",
    "title": "PROTOTOUCH a system for prototyping ubiquitous computing environments mediated by touch",
    "abstract": "Computers as we know them are fading into the background. However, interaction modalities developed for \"foreground\" computer systems are beginning to seep into the day-to-day interactions that people have with each other and the objects around them. The Prototouch system allows user interface designers to quickly prototype systems of ubiquitous computing devices which utilize touch and gesture based interactions popularized by the recent explosion of multitouch-enabled smartphones, enabling the user to act as container, token, and tool for the manipulation and transportation of abstract digital information between these devices. Two versions of the system utilizing different network topologies have been created, and several example applications utilizing the system have been developed.",
    "advisors": ["V. Michael Bove, Jr"],
    "text": "PROTOTOUCH a system for prototyping ubiquitous computing environments mediated by touch Computers as we know them are fading into the background. However, interaction modalities developed for \"foreground\" computer systems are beginning to seep into the day-to-day interactions that people have with each other and the objects around them. The Prototouch system allows user interface designers to quickly prototype systems of ubiquitous computing devices which utilize touch and gesture based interactions popularized by the recent explosion of multitouch-enabled smartphones, enabling the user to act as container, token, and tool for the manipulation and transportation of abstract digital information between these devices. Two versions of the system utilizing different network topologies have been created, and several example applications utilizing the system have been developed."
}, {
    "id": "oai:dspace.mit.edu:1721.1/95602",
    "title": "Online learning webs : designing support structures for online communities",
    "abstract": "This thesis explores how we can design online learning communities to better support connections to the people and resources beginners need when learning to program. I describe and analyze the design and implementation of the Scripts Workshop, a learning environment that supports members of the Scratch online community who are stuck on a programming problem in a Scratch project. The Scripts Workshop considers the People, Activities and Spaces needed to support these users in getting un-stuck. I conclude by describing a set of design principles for building learning webs within online communities, derived from the Scripts Workshop experiment.",
    "advisors": ["Mitchel Resnick"],
    "text": "Online learning webs : designing support structures for online communities This thesis explores how we can design online learning communities to better support connections to the people and resources beginners need when learning to program. I describe and analyze the design and implementation of the Scripts Workshop, a learning environment that supports members of the Scratch online community who are stuck on a programming problem in a Scratch project. The Scripts Workshop considers the People, Activities and Spaces needed to support these users in getting un-stuck. I conclude by describing a set of design principles for building learning webs within online communities, derived from the Scripts Workshop experiment."
}, {
    "id": "oai:dspace.mit.edu:1721.1/61543",
    "title": "Perceptual synthesis engine : an audio-driven timbre generator",
    "abstract": "A real-time synthesis engine which models and predicts the timbre of acoustic instruments based on perceptual features extracted from an audio stream is presented. The thesis describes the modeling sequence including the analysis of natural sounds, the inference step that finds the mapping between control and output parameters, the timbre prediction step, and the sound synthesis. The system enables applications such as cross-synthesis, pitch shifting or compression of acoustic instruments, and timbre morphing between instrument families. It is fully implemented in the Max/MSP environment. The Perceptual Synthesis Engine was developed for the Hyperviolin as a novel, generic and perceptually meaningful synthesis technique for non-discretely pitched instruments.",
    "advisors": ["Tod Machover"],
    "text": "Perceptual synthesis engine : an audio-driven timbre generator A real-time synthesis engine which models and predicts the timbre of acoustic instruments based on perceptual features extracted from an audio stream is presented. The thesis describes the modeling sequence including the analysis of natural sounds, the inference step that finds the mapping between control and output parameters, the timbre prediction step, and the sound synthesis. The system enables applications such as cross-synthesis, pitch shifting or compression of acoustic instruments, and timbre morphing between instrument families. It is fully implemented in the Max/MSP environment. The Perceptual Synthesis Engine was developed for the Hyperviolin as a novel, generic and perceptually meaningful synthesis technique for non-discretely pitched instruments."
}, {
    "id": "oai:dspace.mit.edu:1721.1/32503",
    "title": "Digital story explication as it relates to emotional needs and learning",
    "abstract": "Too often, efforts toward re-thinking learning environments focus solely on the cognitive aspects of education. By expanding our view to consider other aspects of adolescent development involved in education, we can begin to address the needs of the whole child. This research aims to 1) gain a better understanding of the effects of immediate emotions in middle school academic contexts and 2) create a system geared toward addressing the emotional needs of teenage girls. To support emotional self-awareness and empathy, a proactive emotional health was developed. This is a part of a long-term research plan for understanding the role that digital technology can play in helping address emotions and support learning for teenage girls. The system, G.I.R.L.S (Girls Involved in Real Life Sharing) Talk, allows users to reflect actively upon the emotions related to their situations through the construction of pictorial narratives. Users of this new system were able to gain new knowledge and understanding about themselves and others through the exploration of authentic and personal experiences. The system employs a new technology called common sense reasoning that enables it to infer affective content from the users' stories and support emotional reflection. This system has been evaluated with seventeen subjects; one group used the G.I.R.L.S. Talk system with emotional reflection support, while the control group used the system without the support. Over three weeks, the group supported with common sense reasoning about emotion increased the variety of emotion words used in their writing; the control group showed no such increase.",
    "advisors": ["Rosalind W. Picard"],
    "text": "Digital story explication as it relates to emotional needs and learning Too often, efforts toward re-thinking learning environments focus solely on the cognitive aspects of education. By expanding our view to consider other aspects of adolescent development involved in education, we can begin to address the needs of the whole child. This research aims to 1) gain a better understanding of the effects of immediate emotions in middle school academic contexts and 2) create a system geared toward addressing the emotional needs of teenage girls. To support emotional self-awareness and empathy, a proactive emotional health was developed. This is a part of a long-term research plan for understanding the role that digital technology can play in helping address emotions and support learning for teenage girls. The system, G.I.R.L.S (Girls Involved in Real Life Sharing) Talk, allows users to reflect actively upon the emotions related to their situations through the construction of pictorial narratives. Users of this new system were able to gain new knowledge and understanding about themselves and others through the exploration of authentic and personal experiences. The system employs a new technology called common sense reasoning that enables it to infer affective content from the users' stories and support emotional reflection. This system has been evaluated with seventeen subjects; one group used the G.I.R.L.S. Talk system with emotional reflection support, while the control group used the system without the support. Over three weeks, the group supported with common sense reasoning about emotion increased the variety of emotion words used in their writing; the control group showed no such increase."
}, {
    "id": "oai:dspace.mit.edu:1721.1/33895",
    "title": "Design of intelligent interiors",
    "abstract": "Ubiquitous computing is transforming interior design by allowing utilities, goods and information to be delivered where and when we need them. How will new information technologies impact the design of interior spaces? Intelligent interiors can be more flexible and expressive than traditional spaces. Automation, personal fabrication and augmented reality can be applied to interior spaces with new interaction modes that operate at an architectural scale. Water, light, and other utilities can be automated in a way that empowers users by providing direct feedback, tangible benefit and being fail-soft. Appliances can make it possible to produce and recycle a large number of variable goods locally and on demand. Many of the objects and surfaces of interior spaces can serve as displays to provide information intuitively where and when it is needed. This thesis demonstrates how distributed intelligence can increase productivity and enrich the experience of interior spaces. Experiments with augmentations to the utilities, goods and information of a working kitchen suggest guidelines for interaction with intelligent interior spaces. The perceptual load and quality of interaction needs to be balanced; for example in our experiments projected text was almost always distracting.",
    "advisors": ["Edwin J. Selker"],
    "text": "Design of intelligent interiors Ubiquitous computing is transforming interior design by allowing utilities, goods and information to be delivered where and when we need them. How will new information technologies impact the design of interior spaces? Intelligent interiors can be more flexible and expressive than traditional spaces. Automation, personal fabrication and augmented reality can be applied to interior spaces with new interaction modes that operate at an architectural scale. Water, light, and other utilities can be automated in a way that empowers users by providing direct feedback, tangible benefit and being fail-soft. Appliances can make it possible to produce and recycle a large number of variable goods locally and on demand. Many of the objects and surfaces of interior spaces can serve as displays to provide information intuitively where and when it is needed. This thesis demonstrates how distributed intelligence can increase productivity and enrich the experience of interior spaces. Experiments with augmentations to the utilities, goods and information of a working kitchen suggest guidelines for interaction with intelligent interior spaces. The perceptual load and quality of interaction needs to be balanced; for example in our experiments projected text was almost always distracting."
}, {
    "id": "oai:dspace.mit.edu:1721.1/61859",
    "title": "Pushpin computing : a platform for distributed sensor networks",
    "abstract": "A hardware and software platform has been designed and implemented for modeling, testing, and deploying distributed peer-to-peer sensor networks comprised of many identical nodes. Each node possesses the tangible affordances of a commonplace pushpin to meet ease-of-use and power considerations. The sensing, computational, and communication abilities of a \"Pushpin\", as well as a \"Pushpin\" operating system supporting mobile computational processes are treated in detail. Example applications and future work are discussed.",
    "advisors": ["Joseph A. Paradiso"],
    "text": "Pushpin computing : a platform for distributed sensor networks A hardware and software platform has been designed and implemented for modeling, testing, and deploying distributed peer-to-peer sensor networks comprised of many identical nodes. Each node possesses the tangible affordances of a commonplace pushpin to meet ease-of-use and power considerations. The sensing, computational, and communication abilities of a \"Pushpin\", as well as a \"Pushpin\" operating system supporting mobile computational processes are treated in detail. Example applications and future work are discussed."
}, {
    "id": "oai:dspace.mit.edu:1721.1/61114",
    "title": "Virtual fashion : tracking and analyzing cultural dispersion on the World Wide Web",
    "abstract": "In the real world, people clothe themselves in garments whose cut and design encodes information about their social identity. This encoding changes temporally as the design spreads throughout a population: this is the basis of \"fashion.\" A similar sense of fashion has emerged on the World Wide Web (WWW), as people embellish their homesites with links, pictures, and other objects that exhibit similar patterns of dispersion. I have developed tools and algorithms for tracking and analyzing this \"virtual fashion.\" The initial approach is to examine a set of selected homesites each week and track the spread of links. By developing a system for collecting and analyzing the data, this research provides both macro and micro readings of the phenomenon of virtual fashion. The system shows what is popular, ways that things are related, and what is emerging online. I also use data collected by the system to think about existing social theories of fashion and see how they may help develop models of virtual fashion. This research helps people further understand how the WWW functions as a social environment.",
    "advisors": ["Judith S. Donath"],
    "text": "Virtual fashion : tracking and analyzing cultural dispersion on the World Wide Web In the real world, people clothe themselves in garments whose cut and design encodes information about their social identity. This encoding changes temporally as the design spreads throughout a population: this is the basis of \"fashion.\" A similar sense of fashion has emerged on the World Wide Web (WWW), as people embellish their homesites with links, pictures, and other objects that exhibit similar patterns of dispersion. I have developed tools and algorithms for tracking and analyzing this \"virtual fashion.\" The initial approach is to examine a set of selected homesites each week and track the spread of links. By developing a system for collecting and analyzing the data, this research provides both macro and micro readings of the phenomenon of virtual fashion. The system shows what is popular, ways that things are related, and what is emerging online. I also use data collected by the system to think about existing social theories of fashion and see how they may help develop models of virtual fashion. This research helps people further understand how the WWW functions as a social environment."
}, {
    "id": "oai:dspace.mit.edu:1721.1/36152",
    "title": "<random> search",
    "abstract": "In the past three decades, especially in the aftermath of September 11th, significant effort has been focused on developing technologies for aviation security. Security inspectors have considerable latitude to wave passengers into additional screening, and pat-downs are extensive and thorough. Immigrants, individuals from minority groups, and persons from specific ethnicities are targeted more, and accuse authorities of racial profiling and discrimination in both the \"random\" selection and the actual pat-down procedure, but are often reluctant to resist or file official complaints. Expensive, intrusive technologies at security officials' disposal reinforce an inherent power imbalance between authorities and passengers, and set the space for abuse of power. To date, the only tool at a target's disposal is a verbal or written account of their experience that may or may not be taken seriously. Moreover, existing airport security legislation is flawed and open to interpretation, and official standards used to define a breach are absent or lax. <random> search is an an instrument, a neutral, quantifiable witness to the screening process.",
    "advisors": ["Chris Csikzentmihalyi"],
    "text": "<random> search In the past three decades, especially in the aftermath of September 11th, significant effort has been focused on developing technologies for aviation security. Security inspectors have considerable latitude to wave passengers into additional screening, and pat-downs are extensive and thorough. Immigrants, individuals from minority groups, and persons from specific ethnicities are targeted more, and accuse authorities of racial profiling and discrimination in both the \"random\" selection and the actual pat-down procedure, but are often reluctant to resist or file official complaints. Expensive, intrusive technologies at security officials' disposal reinforce an inherent power imbalance between authorities and passengers, and set the space for abuse of power. To date, the only tool at a target's disposal is a verbal or written account of their experience that may or may not be taken seriously. Moreover, existing airport security legislation is flawed and open to interpretation, and official standards used to define a breach are absent or lax. <random> search is an an instrument, a neutral, quantifiable witness to the screening process."
}, {
    "id": "oai:dspace.mit.edu:1721.1/46587",
    "title": "High-resolution spatial light modulation for holographic video",
    "abstract": "The goal of the proposed research is to further the fabrication of a high-bandwidth two-axis scanning device. The device is intended for use in a holographic video geometry built specifically to take advantage of the new modulator's high-bandwidth and vertical-deflection capabilities, but it could also be used in many developing 3D display systems which currently require high-bandwidth light modulation. The modulator will have a spatial frequency bandwidth one order-of-magnitude greater than current light modulation technologies and be two orders of magnitude less expensive.",
    "advisors": ["V. Michael Bove"],
    "text": "High-resolution spatial light modulation for holographic video The goal of the proposed research is to further the fabrication of a high-bandwidth two-axis scanning device. The device is intended for use in a holographic video geometry built specifically to take advantage of the new modulator's high-bandwidth and vertical-deflection capabilities, but it could also be used in many developing 3D display systems which currently require high-bandwidth light modulation. The modulator will have a spatial frequency bandwidth one order-of-magnitude greater than current light modulation technologies and be two orders of magnitude less expensive."
}, {
    "id": "oai:dspace.mit.edu:1721.1/62631",
    "title": "Enabling, modeling and interconnecting active community publishers",
    "abstract": "Over the past four years the research described in this thesis has enabled community groups to become collaborative content producers on the Internet. These groups use computer-mediated networking to publish their stories and to enhance the interaction among the community members and their peers in other groups. This research has resulted in a community publishing tool called Pluto and its revision called Goofy that is nearing its completion. Further, the growth of these communities has led to the need for another system, called SilverWire, to facilitate interaction among communities. SilverWire is a tool for increasing socialization and augmenting communication among communities that actively publish content on the Internet. SilverWire collects and builds models of communities, which form the basis for customized interconnections among communities. Community models are built implicitly by analyzing the contents of the sites that take part in SilverWire and are collected explicitly from questions asked about community purpose, identity and communication. As a result, SilverWire recommends pointers to related community publications and provides comparisons between communities. The goal of the SilverWire system is to be an intermediary that makes communities more aware of other communities doing similar (or interestingly different) work. To evaluate the project I report in detail the progress of one of the groups called the Silver Stringers, which is a local community consisting of approximately 30 senior citizens. The main impacts of the project for the Silver Stringers have been (1) acquiring a new mindset in becoming media content producers, (2) continuous mental stimulation through learning and creating in a group setting, and (3) increased social interaction.",
    "advisors": ["Walter Bender"],
    "text": "Enabling, modeling and interconnecting active community publishers Over the past four years the research described in this thesis has enabled community groups to become collaborative content producers on the Internet. These groups use computer-mediated networking to publish their stories and to enhance the interaction among the community members and their peers in other groups. This research has resulted in a community publishing tool called Pluto and its revision called Goofy that is nearing its completion. Further, the growth of these communities has led to the need for another system, called SilverWire, to facilitate interaction among communities. SilverWire is a tool for increasing socialization and augmenting communication among communities that actively publish content on the Internet. SilverWire collects and builds models of communities, which form the basis for customized interconnections among communities. Community models are built implicitly by analyzing the contents of the sites that take part in SilverWire and are collected explicitly from questions asked about community purpose, identity and communication. As a result, SilverWire recommends pointers to related community publications and provides comparisons between communities. The goal of the SilverWire system is to be an intermediary that makes communities more aware of other communities doing similar (or interestingly different) work. To evaluate the project I report in detail the progress of one of the groups called the Silver Stringers, which is a local community consisting of approximately 30 senior citizens. The main impacts of the project for the Silver Stringers have been (1) acquiring a new mindset in becoming media content producers, (2) continuous mental stimulation through learning and creating in a group setting, and (3) increased social interaction."
}, {
    "id": "oai:dspace.mit.edu:1721.1/63219",
    "title": "New frontiers of expression through real-time dynamics measurement of violin bows",
    "abstract": "The violin has long been admired as one of the most beautiful, complex, and challenging musical instruments. With its capacity for nuance, richness of tone, and flexibility of range, its expressive qualities have been surpassed by none, despite the fact that its construction has not been changed for hundreds of years. It is the form and function of the traditional violin that inspired the work detailed in this thesis. Here, the design and construction of a new violin interface, the Hyperbow, is discussed. The motivation driving the research of this instrument was the desire to create a violin bow capable of measuring the most intricate aspects of violin technique, the subtle elements of physical gesture that immediately and directly impact the sound of the instrument while playing. In order to provide this insight into the subtleties of bow articulation, a sensing system was implemented to measure changes in position, acceleration, and the downward and lateral strains on the bow stick. These sensors were fashioned using an electromagnetic field sensing technique, commercial MEMS accelerometers, and foil strain gauges. Because the forces and stresses applied to the bow are immediately connected to a violinist's experience while playing, the implementation of a new music controller that utilizes these intimate aspects of physical interaction between a player and an instrument may inspire altogether new methods of expression. The measurement techniques used in this work were found to be quite sensitive and yielded sensors that were easily controllable by a player using traditional right hand bowing technique. In addition, the Hyperbow proved to be helpful in recognizing and analyzing the physical parameters of common bowstrokes",
    "advisors": ["Tod Machover"],
    "text": "New frontiers of expression through real-time dynamics measurement of violin bows The violin has long been admired as one of the most beautiful, complex, and challenging musical instruments. With its capacity for nuance, richness of tone, and flexibility of range, its expressive qualities have been surpassed by none, despite the fact that its construction has not been changed for hundreds of years. It is the form and function of the traditional violin that inspired the work detailed in this thesis. Here, the design and construction of a new violin interface, the Hyperbow, is discussed. The motivation driving the research of this instrument was the desire to create a violin bow capable of measuring the most intricate aspects of violin technique, the subtle elements of physical gesture that immediately and directly impact the sound of the instrument while playing. In order to provide this insight into the subtleties of bow articulation, a sensing system was implemented to measure changes in position, acceleration, and the downward and lateral strains on the bow stick. These sensors were fashioned using an electromagnetic field sensing technique, commercial MEMS accelerometers, and foil strain gauges. Because the forces and stresses applied to the bow are immediately connected to a violinist's experience while playing, the implementation of a new music controller that utilizes these intimate aspects of physical interaction between a player and an instrument may inspire altogether new methods of expression. The measurement techniques used in this work were found to be quite sensitive and yielded sensors that were easily controllable by a player using traditional right hand bowing technique. In addition, the Hyperbow proved to be helpful in recognizing and analyzing the physical parameters of common bowstrokes"
}, {
    "id": "oai:dspace.mit.edu:1721.1/69805",
    "title": "Semantic spaces : behavior, language and word learning in the Human Speechome corpus",
    "abstract": "The Human Speechome Project is an unprecedented attempt to record, analyze and understand the process of language acquisition. It is composed of over 90,000 hours of video and 150,000 hours of audio, capturing roughly 80% of the waking hours of a single child from his birth until age 3. This thesis proposes and develops a method for representing and analyzing a video corpus of this scale that is both compact and efficient, while retaining much of the important information about large scale behaviors of the recorded subjects. This representation is shown to be useful for the unsupervised modeling, clustering and exploration of the data, particularly when it is combined with text transcripts of the speech. Novel methods are introduced to perform Spatial Latent Semantic Analysis - extending the popular framework for topic modeling to cover behavior as well. Finally, the representation is used to analyze the inherent \"spatiality\" of individual words. A surprising connection is demonstrated between the uniqueness of a word's spatial distribution and how early it is learned by the child.",
    "advisors": ["Deb Roy"],
    "text": "Semantic spaces : behavior, language and word learning in the Human Speechome corpus The Human Speechome Project is an unprecedented attempt to record, analyze and understand the process of language acquisition. It is composed of over 90,000 hours of video and 150,000 hours of audio, capturing roughly 80% of the waking hours of a single child from his birth until age 3. This thesis proposes and develops a method for representing and analyzing a video corpus of this scale that is both compact and efficient, while retaining much of the important information about large scale behaviors of the recorded subjects. This representation is shown to be useful for the unsupervised modeling, clustering and exploration of the data, particularly when it is combined with text transcripts of the speech. Novel methods are introduced to perform Spatial Latent Semantic Analysis - extending the popular framework for topic modeling to cover behavior as well. Finally, the representation is used to analyze the inherent \"spatiality\" of individual words. A surprising connection is demonstrated between the uniqueness of a word's spatial distribution and how early it is learned by the child."
}, {
    "id": "oai:dspace.mit.edu:1721.1/62950",
    "title": "A nonlinear dynamic system for spread spectrum code acquisition",
    "abstract": "Nonlinear differential equations and iterated maps can perform any computation. Sometimes, the most difficult part of performing a useful computation, however, is writing the program. Furthermore, in practice, we often need to build special purpose computing hardware suited to run a particular program. Nonlinear dynamics provides a novel and useful language for constructing \"algorithms\" and \"computer architectures.\" We apply the language of nonlinear dynamics to solve a fast coding problem which has previously been implemented by a Digital Signal Processor chip in digital wireless receivers. We eventually hope to produce a novel physical system which exhibits the nonlinear dynamics we require, thereby creating one of the first nonlinear dynamic systems engineered to perform a practical computation. This system, called an Analog Feedback Shift Register (AFSR), should be a faster, more reliable, less expensive, and lower power Spread Spectrum (SS) code acquisition system for wireless receivers. A prohibitive factor in creating ubiquitous short range, digital radio transceivers is the difficulty and expense of creating a mechanism for locking onto the incoming Spread Spectrum code sequence. AFSR is also potentially useful in other applications where low cost, low power channel sharing or addressing is required, for example in wireless auto-identification tags.",
    "advisors": ["Neil Gershenfeld"],
    "text": "A nonlinear dynamic system for spread spectrum code acquisition Nonlinear differential equations and iterated maps can perform any computation. Sometimes, the most difficult part of performing a useful computation, however, is writing the program. Furthermore, in practice, we often need to build special purpose computing hardware suited to run a particular program. Nonlinear dynamics provides a novel and useful language for constructing \"algorithms\" and \"computer architectures.\" We apply the language of nonlinear dynamics to solve a fast coding problem which has previously been implemented by a Digital Signal Processor chip in digital wireless receivers. We eventually hope to produce a novel physical system which exhibits the nonlinear dynamics we require, thereby creating one of the first nonlinear dynamic systems engineered to perform a practical computation. This system, called an Analog Feedback Shift Register (AFSR), should be a faster, more reliable, less expensive, and lower power Spread Spectrum (SS) code acquisition system for wireless receivers. A prohibitive factor in creating ubiquitous short range, digital radio transceivers is the difficulty and expense of creating a mechanism for locking onto the incoming Spread Spectrum code sequence. AFSR is also potentially useful in other applications where low cost, low power channel sharing or addressing is required, for example in wireless auto-identification tags."
}, {
    "id": "oai:dspace.mit.edu:1721.1/101845",
    "title": "Discrete cellular lattice assembly",
    "abstract": "Robotic assembly of discrete cellular lattices at super-hertz (>1Hz) assembly rates is shown to be possible by integrating the design of a modular robotic assembler with the specified lattice topology such that the lattice can itself be removed from the incremental assembly process. Limits to assembly rates are ultimately dependent on allowable error, system stiffness, and damping characteristics. Vibrations due to cyclical motions of the end-effector, locomotion system, and the dynamic response of an incrementally varying lattice must settle to acceptable ranges to enable engagement between end-effectors, discrete elements, and their affixing features to adjacent cells. For given system dynamics, longer settling times enables greater energy dissipation, and less error. With a greater allowable error at the interface, a shorter assembly cycle period can be attained. Passive alignment features designed into the robot end-effectors, locomotion systems, and the discrete lattice elements reduce the precision requirements of the assembly process by opening up the acceptable error range, thereby, enabling higher assembly cycle-rates. An experiment was performed to evaluate how an assembler locally referencing a lattice performed in comparison to a globally referenced assembler. The two assemblers were of similar kinematic form: both gantry-type CNC machines: a ShopBot and a custom built relative robotic assembler. The results showed superior performance by the global coordinate frame system. An error budget analysis of the two systems showed that the locally referenced, lattice based system had a larger more variable structural loop than the global coordinate frame ShopBot. The control experiment, demonstrated 0.1Hz assembly rates, while first order approximations predict a maximum 4Hz cycle for the specified interface geometry. Results show that in order to successfully assemble discrete cellular lattices at super-hertz rates the robot must itself become the local, instantaneous global coordinate frame such that the structural loop is absolutely minimized, while stiffness is maximized; at the instantaneous moment of assembly the structural loop of the robot must reference only itself.",
    "advisors": ["Neil Gershenfeld"],
    "text": "Discrete cellular lattice assembly Robotic assembly of discrete cellular lattices at super-hertz (>1Hz) assembly rates is shown to be possible by integrating the design of a modular robotic assembler with the specified lattice topology such that the lattice can itself be removed from the incremental assembly process. Limits to assembly rates are ultimately dependent on allowable error, system stiffness, and damping characteristics. Vibrations due to cyclical motions of the end-effector, locomotion system, and the dynamic response of an incrementally varying lattice must settle to acceptable ranges to enable engagement between end-effectors, discrete elements, and their affixing features to adjacent cells. For given system dynamics, longer settling times enables greater energy dissipation, and less error. With a greater allowable error at the interface, a shorter assembly cycle period can be attained. Passive alignment features designed into the robot end-effectors, locomotion systems, and the discrete lattice elements reduce the precision requirements of the assembly process by opening up the acceptable error range, thereby, enabling higher assembly cycle-rates. An experiment was performed to evaluate how an assembler locally referencing a lattice performed in comparison to a globally referenced assembler. The two assemblers were of similar kinematic form: both gantry-type CNC machines: a ShopBot and a custom built relative robotic assembler. The results showed superior performance by the global coordinate frame system. An error budget analysis of the two systems showed that the locally referenced, lattice based system had a larger more variable structural loop than the global coordinate frame ShopBot. The control experiment, demonstrated 0.1Hz assembly rates, while first order approximations predict a maximum 4Hz cycle for the specified interface geometry. Results show that in order to successfully assemble discrete cellular lattices at super-hertz rates the robot must itself become the local, instantaneous global coordinate frame such that the structural loop is absolutely minimized, while stiffness is maximized; at the instantaneous moment of assembly the structural loop of the robot must reference only itself."
}, {
    "id": "oai:dspace.mit.edu:1721.1/101792",
    "title": "Holosuite : an exploration into interactive holographic telepresence",
    "abstract": "Research in holographic video technology has made great progress in the past decade, thanks to new advances in hologram computation algorithms and light modulation materials. Due to the niche and inaccessibility of holographic display research, however, literature on applications of holographic display technology remains scarce. In this thesis, we describe a holographic display application that combines remote telepresence with interaction in 3D space. We propose some key concepts on leveraging the strengths of holographic display technology as a medium for interactive telepresence. These concepts are implemented in a real-time, end-to-end 3D telepresence software application titled \"Holosuite.\" Our implementation fosters a novel usage of sharing, collaborating with, and visualizing 3D data between users in a highly immersive and realistic way. In doing so, we have created an experience that connects two remote users in a way that is more engaging and provides more affordances than traditional videoconferencing.",
    "advisors": ["V. Michael Bove, Jr"],
    "text": "Holosuite : an exploration into interactive holographic telepresence Research in holographic video technology has made great progress in the past decade, thanks to new advances in hologram computation algorithms and light modulation materials. Due to the niche and inaccessibility of holographic display research, however, literature on applications of holographic display technology remains scarce. In this thesis, we describe a holographic display application that combines remote telepresence with interaction in 3D space. We propose some key concepts on leveraging the strengths of holographic display technology as a medium for interactive telepresence. These concepts are implemented in a real-time, end-to-end 3D telepresence software application titled \"Holosuite.\" Our implementation fosters a novel usage of sharing, collaborating with, and visualizing 3D data between users in a highly immersive and realistic way. In doing so, we have created an experience that connects two remote users in a way that is more engaging and provides more affordances than traditional videoconferencing."
}, {
    "id": "oai:dspace.mit.edu:1721.1/62378",
    "title": "An approach to bridging atom optics and bulk spin quantum computation",
    "abstract": "This thesis is an exploration in quantum computation and modern physics. Atomic, molecular, and optical (AMO) physics, a centerpiece of modern physics, originated in the 1950's with the discovery of nuclear magnetic resonance (NMR), a field which has mostly been left behind in physics. However, NMR has recently taken yet another leap: quantum computers of up to seven qubits in size, the largest realized to-date, have been implemented by applying NMR to molecules in liquid solution. What new lessons can AMO physics learn from these advances made by NMR into quantum computation? And what can NMR quantum computation learn from the many advances made in recent AMO physics? In this work, I study two specific answers to these twin questions: the use of atom-like quantum systems beyond spin-1/2 for NMR quantum computation, and the demonstration of a modern quantum-optical phenomenon, electromagnetically induced transparency, using NMR quantum computation. Both examples build on theoretical analysis, and include experimental results, showing how atomic physics could be very useful for implementing certain quantum operations and vice versa. These investigations form the basis for an atomic physics test-bed in NMR quantum computation.",
    "advisors": ["Isaac L. Chuang"],
    "text": "An approach to bridging atom optics and bulk spin quantum computation This thesis is an exploration in quantum computation and modern physics. Atomic, molecular, and optical (AMO) physics, a centerpiece of modern physics, originated in the 1950's with the discovery of nuclear magnetic resonance (NMR), a field which has mostly been left behind in physics. However, NMR has recently taken yet another leap: quantum computers of up to seven qubits in size, the largest realized to-date, have been implemented by applying NMR to molecules in liquid solution. What new lessons can AMO physics learn from these advances made by NMR into quantum computation? And what can NMR quantum computation learn from the many advances made in recent AMO physics? In this work, I study two specific answers to these twin questions: the use of atom-like quantum systems beyond spin-1/2 for NMR quantum computation, and the demonstration of a modern quantum-optical phenomenon, electromagnetically induced transparency, using NMR quantum computation. Both examples build on theoretical analysis, and include experimental results, showing how atomic physics could be very useful for implementing certain quantum operations and vice versa. These investigations form the basis for an atomic physics test-bed in NMR quantum computation."
}, {
    "id": "oai:dspace.mit.edu:1721.1/67763",
    "title": "Green mobility Taipei City : with the arrival of mobility-on-demand system with ultra small electric vehicles",
    "abstract": "Urban form always transforms when new transportation technology is deployed. Urban form and transportation technologies always coevolve. Many new technologies have been developed to solve the problems of greenhouse gas emission, air pollution, energy efficiency, high gas prices, traffic congestion, etc. Electric vehicles (EVs) and Mobility-on-Demand systems are two of these technologies. With the advancement of battery technologies, EVs are become the next mainstream product for Automobile industry. Meanwhile, there are many new concepts about various alternative types of car ownership, such as Mobility-on-Demand (MoD) systems, a one-way rental car sharing systems, for which the Smart Cities group of MIT Media Lab is doing research. The regulation and infrastructure of current cities are mainly designed to accommodate gasoline-powered and private owned vehicles. This thesis addresses how will urban fabric and space transform with the arrivals of EVs and MoD systems and what kind of service and urban infrastructure can be integrated when individual vehicles become a node of mobility network. The thesis focuses on Taipei City as a case study city and develops varies scale design strategies, ranging from charging infrastructure, street, sidewalk, curb, parking infrastructure, to building type. The thesis also discusses the benefit of EVs and MoD system may bring to a city.",
    "advisors": ["Kent Larson"],
    "text": "Green mobility Taipei City : with the arrival of mobility-on-demand system with ultra small electric vehicles Urban form always transforms when new transportation technology is deployed. Urban form and transportation technologies always coevolve. Many new technologies have been developed to solve the problems of greenhouse gas emission, air pollution, energy efficiency, high gas prices, traffic congestion, etc. Electric vehicles (EVs) and Mobility-on-Demand systems are two of these technologies. With the advancement of battery technologies, EVs are become the next mainstream product for Automobile industry. Meanwhile, there are many new concepts about various alternative types of car ownership, such as Mobility-on-Demand (MoD) systems, a one-way rental car sharing systems, for which the Smart Cities group of MIT Media Lab is doing research. The regulation and infrastructure of current cities are mainly designed to accommodate gasoline-powered and private owned vehicles. This thesis addresses how will urban fabric and space transform with the arrivals of EVs and MoD systems and what kind of service and urban infrastructure can be integrated when individual vehicles become a node of mobility network. The thesis focuses on Taipei City as a case study city and develops varies scale design strategies, ranging from charging infrastructure, street, sidewalk, curb, parking infrastructure, to building type. The thesis also discusses the benefit of EVs and MoD system may bring to a city."
}, {
    "id": "oai:dspace.mit.edu:1721.1/101827",
    "title": "Quadrasense : immersive UAV-based cross-reality environmental sensor networks",
    "abstract": "Quadrasense explores futuristic applications in environmental sensing by integrating ideas of cross-reality with semi-autonomous sensor-aware vehicles. The cross-reality principals of telepresence, augmented reality, and virtual reality are enabled through an Unnamed-Aerial-Vehicle, a specialized imaging system, a Head-Mounted-Display, a video game engine and a commodity computer. Users may move between any of the three modes of interaction, in real-time, through a singular visual interface. Utilizing an environment built with video game technology, a system was developed that can track and move a UAV in the physical world, towards goals of sensing, exploration and visualization. This application expands on the use of video games engines for simulation by directly joining the virtual and real worlds.",
    "advisors": ["Joseph A. Paradiso"],
    "text": "Quadrasense : immersive UAV-based cross-reality environmental sensor networks Quadrasense explores futuristic applications in environmental sensing by integrating ideas of cross-reality with semi-autonomous sensor-aware vehicles. The cross-reality principals of telepresence, augmented reality, and virtual reality are enabled through an Unnamed-Aerial-Vehicle, a specialized imaging system, a Head-Mounted-Display, a video game engine and a commodity computer. Users may move between any of the three modes of interaction, in real-time, through a singular visual interface. Utilizing an environment built with video game technology, a system was developed that can track and move a UAV in the physical world, towards goals of sensing, exploration and visualization. This application expands on the use of video games engines for simulation by directly joining the virtual and real worlds."
}, {
    "id": "oai:dspace.mit.edu:1721.1/69525",
    "title": "Towards a unified treatment of 3D display using partially coherent light",
    "abstract": "This thesis develops a novel method of decomposing a 3D phase space description of light into multiple partially coherent modes, and applies this decomposition to the creation of a more flexible 3D display format. Any type of light, whether it is completely coherent, partially coherent or incoherent, can be modeled either as a sum of coherent waves or as rays. A set of functions, known as phase space functions, provide an intuitive model for these waves or rays as they pass through a 3D volume to a display viewer's eyes. First, this thesis uses phase space functions to mathematically demonstrate the limitations of two popular 3D display setups: parallax barriers and coherent holograms. Second, this thesis develops a 3D image design algorithm based in phase space. The \"mode-selection\" algorithm can find an optimal holographic display setup to create any desired 3D image. It is based on an iterative algebraic-rank restriction process, and can be extended to model light with an arbitrary degree of partial coherence. Third, insights gained from partially coherent phase space representations lead to the suggestion of a new form of 3D display, implemented with multiple time-sequential diffracting screens. The mode-selection algorithm determines an optimal set of diffracting screens to display within the flicker-fusion rate of a viewer's eye. It is demonstrated both through simulation and experiment that this time-sequential display offers improved performance over a fixed holographic display, creating 3D images with increased intensity variation along depth. Finally, this thesis investigates the tradeoffs involved with multiplexing a holographic display over time with well-known strategies of multiplexing over space, illumination angle and wavelength. The examination of multiplexing tradeoffs is extended into the incoherent realm, where comparisons to ray-based 3D displays can hopefully offer a more unified summary of the limitations of controlling light within a volume.",
    "advisors": ["Ramesh Raskar"],
    "text": "Towards a unified treatment of 3D display using partially coherent light This thesis develops a novel method of decomposing a 3D phase space description of light into multiple partially coherent modes, and applies this decomposition to the creation of a more flexible 3D display format. Any type of light, whether it is completely coherent, partially coherent or incoherent, can be modeled either as a sum of coherent waves or as rays. A set of functions, known as phase space functions, provide an intuitive model for these waves or rays as they pass through a 3D volume to a display viewer's eyes. First, this thesis uses phase space functions to mathematically demonstrate the limitations of two popular 3D display setups: parallax barriers and coherent holograms. Second, this thesis develops a 3D image design algorithm based in phase space. The \"mode-selection\" algorithm can find an optimal holographic display setup to create any desired 3D image. It is based on an iterative algebraic-rank restriction process, and can be extended to model light with an arbitrary degree of partial coherence. Third, insights gained from partially coherent phase space representations lead to the suggestion of a new form of 3D display, implemented with multiple time-sequential diffracting screens. The mode-selection algorithm determines an optimal set of diffracting screens to display within the flicker-fusion rate of a viewer's eye. It is demonstrated both through simulation and experiment that this time-sequential display offers improved performance over a fixed holographic display, creating 3D images with increased intensity variation along depth. Finally, this thesis investigates the tradeoffs involved with multiplexing a holographic display over time with well-known strategies of multiplexing over space, illumination angle and wavelength. The examination of multiplexing tradeoffs is extended into the incoherent realm, where comparisons to ray-based 3D displays can hopefully offer a more unified summary of the limitations of controlling light within a volume."
}, {
    "id": "oai:dspace.mit.edu:1721.1/82427",
    "title": "Use of live video overlay on 3D data for distributed collaborative review",
    "abstract": "Using everyday skills, such as pointing and drawing freehand sketches, facilitates effective communication when reviewing visual information. However, for sharing three-dimensional (3D) data, it is difficult to support such approaches of using hands and real ink in a seamless way. This thesis proposes a new system design called AnnoScape as an approach to performing a remote collaborative review of 3D digital data using a live video overlay of the desktop image on the viewports of the 3D scene. The system's virtual viewports are controlled with tangible handles and can be left spatially in the 3D data space. The viewports can be shared with remote collaborators both asynchronously and in real time. The system allows distributed users to navigate shared 3D space individually or jointly (synchronizing the viewport); generate an overlay of the live video of hand drawings, physical objects, and printed images from the desktop surface with the viewport; and control the legibility of the visual contents. This spatial video overlay technique in the 3D data space allows distributed users to share the live annotations over the synchronized viewports. We report the prototype design and initial experiments to explore AnnoScape's possibilities through the scenario of having remote collaborators review the exterior site and interior reconfiguration of an existing architectural setting.",
    "advisors": ["Hiroshi Ishii"],
    "text": "Use of live video overlay on 3D data for distributed collaborative review Using everyday skills, such as pointing and drawing freehand sketches, facilitates effective communication when reviewing visual information. However, for sharing three-dimensional (3D) data, it is difficult to support such approaches of using hands and real ink in a seamless way. This thesis proposes a new system design called AnnoScape as an approach to performing a remote collaborative review of 3D digital data using a live video overlay of the desktop image on the viewports of the 3D scene. The system's virtual viewports are controlled with tangible handles and can be left spatially in the 3D data space. The viewports can be shared with remote collaborators both asynchronously and in real time. The system allows distributed users to navigate shared 3D space individually or jointly (synchronizing the viewport); generate an overlay of the live video of hand drawings, physical objects, and printed images from the desktop surface with the viewport; and control the legibility of the visual contents. This spatial video overlay technique in the 3D data space allows distributed users to share the live annotations over the synchronized viewports. We report the prototype design and initial experiments to explore AnnoScape's possibilities through the scenario of having remote collaborators review the exterior site and interior reconfiguration of an existing architectural setting."
}, {
    "id": "oai:dspace.mit.edu:1721.1/31103",
    "title": "Aesthetics of computation : unveiling the visual machine",
    "abstract": "This thesis presents a new paradigm for the design of visual programming languages, with the goal of making computation visible and, in turn, more accessible. The Visual Machine Model emphasizes the need for clear visual representations of both machines and materials, and the importance of continuity. Five dynamic visual programming languages were designed and implemented according to the specification of the Visual Machine Model. In addition to individual analysis, a comparative evaluation of all five design experiments is conducted with respect to several ease of use metrics and Visual Machine qualities. While formal user tests have not been conducted, preliminary results from general user experiences indicate that being able to see and interact with computation does enhance the programming process.",
    "advisors": ["John Maeda"],
    "text": "Aesthetics of computation : unveiling the visual machine This thesis presents a new paradigm for the design of visual programming languages, with the goal of making computation visible and, in turn, more accessible. The Visual Machine Model emphasizes the need for clear visual representations of both machines and materials, and the importance of continuity. Five dynamic visual programming languages were designed and implemented according to the specification of the Visual Machine Model. In addition to individual analysis, a comparative evaluation of all five design experiments is conducted with respect to several ease of use metrics and Visual Machine qualities. While formal user tests have not been conducted, preliminary results from general user experiences indicate that being able to see and interact with computation does enhance the programming process."
}, {
    "id": "oai:dspace.mit.edu:1721.1/29170",
    "title": "The sensing and measurement of frustration with computers",
    "abstract": "By giving users a way to vent, we transform their frustration into a valuable source of information for adapting interfaces. Drawing from psychophysiology and tactile sensing, we present frustration sensors as a way of incorporating user feedback into interface design processes. This thesis documents the development of designs for several sensors aimed at detecting user frustration with computers. Additionally the thesis explores the design space between active sensors that facilitate the communication of frustration and passive sensors that detect frustration without demanding the user's attention. During evaluations we learned several things: -- Participants liked having devices to communicate frustration. -- The data that was collected during active and passive user interactions can be used for redesigning and adapting systems (either by hand, or automatically). -- User behaved differently during usability problems. In a comparative study of three active designs (Frustrometer, Squeezemouse, and traditional feedback web page) we found that users prefer the Frustrometer to a web feedback page. Preliminary results suggest that frustration-stimulated behavior can also be detected through passive sensors. When combined with other contextual information, these sensors provide a crucial building block in systems that interact and adapt to human behavior by indicating where and when change is needed.",
    "advisors": ["Rosalind W. Picard"],
    "text": "The sensing and measurement of frustration with computers By giving users a way to vent, we transform their frustration into a valuable source of information for adapting interfaces. Drawing from psychophysiology and tactile sensing, we present frustration sensors as a way of incorporating user feedback into interface design processes. This thesis documents the development of designs for several sensors aimed at detecting user frustration with computers. Additionally the thesis explores the design space between active sensors that facilitate the communication of frustration and passive sensors that detect frustration without demanding the user's attention. During evaluations we learned several things: -- Participants liked having devices to communicate frustration. -- The data that was collected during active and passive user interactions can be used for redesigning and adapting systems (either by hand, or automatically). -- User behaved differently during usability problems. In a comparative study of three active designs (Frustrometer, Squeezemouse, and traditional feedback web page) we found that users prefer the Frustrometer to a web feedback page. Preliminary results suggest that frustration-stimulated behavior can also be detected through passive sensors. When combined with other contextual information, these sensors provide a crucial building block in systems that interact and adapt to human behavior by indicating where and when change is needed."
}, {
    "id": "oai:dspace.mit.edu:1721.1/76530",
    "title": "Truth goggles : automatic incorporation of context and primary source for a critical media experience",
    "abstract": "Falsehoods come in many forms. Politicians and advertisers make false claims, newsrooms and bloggers make mistakes, and the ease of publication and sharing makes it easy for even the most incredible theories to quickly spread. The risk of interpreting fiction as truth is significant and it is only increasing. How can technology protect consumers from falling into the traps set by dubious information? This thesis is an exploration of the challenges and possibilities surrounding the implementation of interfaces that mitigate the effects of misinformation. It contains an analysis of the pitfalls, considerations, and opportunities in the space of digital credibility. Those lessons are then applied to Truth Goggles, a technology prototype that attempts to trigger critical thinking in digital media consumers and codify the process of fair analysis.",
    "advisors": ["Henry Holtzman"],
    "text": "Truth goggles : automatic incorporation of context and primary source for a critical media experience Falsehoods come in many forms. Politicians and advertisers make false claims, newsrooms and bloggers make mistakes, and the ease of publication and sharing makes it easy for even the most incredible theories to quickly spread. The risk of interpreting fiction as truth is significant and it is only increasing. How can technology protect consumers from falling into the traps set by dubious information? This thesis is an exploration of the challenges and possibilities surrounding the implementation of interfaces that mitigate the effects of misinformation. It contains an analysis of the pitfalls, considerations, and opportunities in the space of digital credibility. Those lessons are then applied to Truth Goggles, a technology prototype that attempts to trigger critical thinking in digital media consumers and codify the process of fair analysis."
}, {
    "id": "oai:dspace.mit.edu:1721.1/44805",
    "title": "The role of branding and pricing on health outcomes via the placebo response",
    "abstract": "Marketing factors such as pricing and branding are known to affect people's judgments and expectations regarding a product. When the product in question is a medical treatment, this effect may modify the placebo response. As the placebo response modifies the overall effect of the treatment in an additive manner, this implies that marketing factors may have a direct effect on medical outcomes, particularly in disorders that have large placebo effects. This thesis explores this idea. First a laboratory study is presented that used electric shocks and a placebo pill to demonstrate that a price discount may reduce the efficacy of a placebo analgesic. A second laboratory study, using the cold pressor test and a medication with low non-placebo efficacy, failed to demonstrate superiority of a brand-name over generic medication in terms of cold tolerance or pain ratings, but the brand-name did lead to quicker habituation to the cold pain. In addition, quantitative and qualitative research exploring attitudes held towards generic medications shows conflicted beliefs and a lack of complete trust in generic medications in both general and geriatric populations. \"Non-commercial\" forms of marketing, including the labeling of, and description of, a disorder, are also touched upon. The implications of this research for marketing are discussed, as well as necessary steps for the research development of this field.",
    "advisors": ["Dan Ariely"],
    "text": "The role of branding and pricing on health outcomes via the placebo response Marketing factors such as pricing and branding are known to affect people's judgments and expectations regarding a product. When the product in question is a medical treatment, this effect may modify the placebo response. As the placebo response modifies the overall effect of the treatment in an additive manner, this implies that marketing factors may have a direct effect on medical outcomes, particularly in disorders that have large placebo effects. This thesis explores this idea. First a laboratory study is presented that used electric shocks and a placebo pill to demonstrate that a price discount may reduce the efficacy of a placebo analgesic. A second laboratory study, using the cold pressor test and a medication with low non-placebo efficacy, failed to demonstrate superiority of a brand-name over generic medication in terms of cold tolerance or pain ratings, but the brand-name did lead to quicker habituation to the cold pain. In addition, quantitative and qualitative research exploring attitudes held towards generic medications shows conflicted beliefs and a lack of complete trust in generic medications in both general and geriatric populations. \"Non-commercial\" forms of marketing, including the labeling of, and description of, a disorder, are also touched upon. The implications of this research for marketing are discussed, as well as necessary steps for the research development of this field."
}, {
    "id": "oai:dspace.mit.edu:1721.1/79331",
    "title": "Low dimensionality spectral sensing for low cost material discrimination and identification",
    "abstract": "Spectroscopy is a powerful tool in material identification, characterization and discrimination. Unfortunately industrial and laboratory spectrometers are typically very large, costly, and inconvenient. The aim of this thesis is to broaden the awareness and appeal of spectroscopic sensing modalities by exploring specialized, rather than general purpose instruments. Rather than sensing the entire spectrum, these devices work by observing just the particular spectral features needed to perform identification or discrimination. This approach greatly simplifies the instrument reducing the cost, size, power consumption, and analysis complexity by many orders of magnitude. In this work the anatomy of such specialized sensors is explored by way of a thorough discussion of illuminators, current sources, photodetectors, photodiode amplifiers, control systems and part selection. In the following chapters, instruments are designed and fabricated, and their tradeoffs are enumerated and discussed. Finally, these building-blocks are combined to construct several working prototypes which are informally characterized.",
    "advisors": ["V. Michael Bove, Jr"],
    "text": "Low dimensionality spectral sensing for low cost material discrimination and identification Spectroscopy is a powerful tool in material identification, characterization and discrimination. Unfortunately industrial and laboratory spectrometers are typically very large, costly, and inconvenient. The aim of this thesis is to broaden the awareness and appeal of spectroscopic sensing modalities by exploring specialized, rather than general purpose instruments. Rather than sensing the entire spectrum, these devices work by observing just the particular spectral features needed to perform identification or discrimination. This approach greatly simplifies the instrument reducing the cost, size, power consumption, and analysis complexity by many orders of magnitude. In this work the anatomy of such specialized sensors is explored by way of a thorough discussion of illuminators, current sources, photodetectors, photodiode amplifiers, control systems and part selection. In the following chapters, instruments are designed and fabricated, and their tradeoffs are enumerated and discussed. Finally, these building-blocks are combined to construct several working prototypes which are informally characterized."
}, {
    "id": "oai:dspace.mit.edu:1721.1/61138",
    "title": "A view-sequential 3D display",
    "abstract": "This thesis outlines the various techniques for creating electronic 3D displays and analyzes their commercial potential. The thesis argues for the use of view-sequential techniques in the design of 3D displays based on considerations of state of the art 3D displays and currently available technology. A view-sequential 3D display was built, which is described in detail. The performance of this display is given, as well as an analysis which characterizes its anticipated performance and comparison with measured results.",
    "advisors": ["Stephen A. Benton"],
    "text": "A view-sequential 3D display This thesis outlines the various techniques for creating electronic 3D displays and analyzes their commercial potential. The thesis argues for the use of view-sequential techniques in the design of 3D displays based on considerations of state of the art 3D displays and currently available technology. A view-sequential 3D display was built, which is described in detail. The performance of this display is given, as well as an analysis which characterizes its anticipated performance and comparison with measured results."
}, {
    "id": "oai:dspace.mit.edu:1721.1/98619",
    "title": "Shape synthesis : physical object augmentation and actuation for display and interaction on shape changing interfaces",
    "abstract": "Pin based shape displays can not only give physical form to digital information, but they have the inherent ability to accurately move and manipulate objects that are placed on top of them. This document presents ways and ideas that show how a shape display's dynamic shape changing ability can work in unison with physical objects that are placed on top of it. First, we introduce the idea of shape synthesis, which is the physical augmentation of inert physical objects with the dynamic shape to create a seemingly new object. This synthesized object combines the advantages of the inert object's natural affordances with the computational power of dynamic shape change. In so doing, we can substitute for passive objects, complement passive objects and enable easier interactions with a shape display. We then show that a shape display can be used to assemble, disassemble and reassemble structures from simple passive building blocks through stacking, scaffolding and catapulting. Then, we introduce special unpowered kinematic modules that can be driven and sensed through the underlying pins. These modules can translate the vertical pin movement into other degrees of freedom like rotation or horizontal movement. This suggests that a shape display can be regarded as a versatile physical control platform that can drive and control a variety of useful mechanisms and objects.",
    "advisors": ["Hiroshi Ishii"],
    "text": "Shape synthesis : physical object augmentation and actuation for display and interaction on shape changing interfaces Pin based shape displays can not only give physical form to digital information, but they have the inherent ability to accurately move and manipulate objects that are placed on top of them. This document presents ways and ideas that show how a shape display's dynamic shape changing ability can work in unison with physical objects that are placed on top of it. First, we introduce the idea of shape synthesis, which is the physical augmentation of inert physical objects with the dynamic shape to create a seemingly new object. This synthesized object combines the advantages of the inert object's natural affordances with the computational power of dynamic shape change. In so doing, we can substitute for passive objects, complement passive objects and enable easier interactions with a shape display. We then show that a shape display can be used to assemble, disassemble and reassemble structures from simple passive building blocks through stacking, scaffolding and catapulting. Then, we introduce special unpowered kinematic modules that can be driven and sensed through the underlying pins. These modules can translate the vertical pin movement into other degrees of freedom like rotation or horizontal movement. This suggests that a shape display can be regarded as a versatile physical control platform that can drive and control a variety of useful mechanisms and objects."
}, {
    "id": "oai:dspace.mit.edu:1721.1/46585",
    "title": "Graspables : Grasp recognition as a user interface",
    "abstract": "The Graspables project is an exploration of how measuring the way people hold and manipulate objects can be used as a user interface. As computational power continues to implemented in more and more objects and devices, new interaction methods need to be developed. The Graspables System is embodied by a physical set of sensors combined with pattern recognition software that can determine how users hold a device. The Graspables System has been implemented in two prototypes, the Bar of Soap and the Ball of Soap. User studies have been conducted demostrating the effectiveness of the Graspables System and a variety of applications have been developed to demonstrate its utility.",
    "advisors": ["V. Michael Bove, Jr"],
    "text": "Graspables : Grasp recognition as a user interface The Graspables project is an exploration of how measuring the way people hold and manipulate objects can be used as a user interface. As computational power continues to implemented in more and more objects and devices, new interaction methods need to be developed. The Graspables System is embodied by a physical set of sensors combined with pattern recognition software that can determine how users hold a device. The Graspables System has been implemented in two prototypes, the Bar of Soap and the Ball of Soap. User studies have been conducted demostrating the effectiveness of the Graspables System and a variety of applications have been developed to demonstrate its utility."
}, {
    "id": "oai:dspace.mit.edu:1721.1/98624",
    "title": "Glyph : lightweight and evocative looping images in the news",
    "abstract": "There is an emotional dimension to the informative function of the news. When we read the news we participate in a collective emotional experience- whether that is grief, celebration, worry, or wonder. News video is a crucial vector for these shared emotional experiences, which can propel civic action. But video comes at a high cost in time and attention, and is thus unsuitable for high volume news and social feeds, and mobile and wearable devices. On these interfaces, there is value in presenting video in a way that's immediately evocative, preserving the richness of video while atomizing it to an excerpt as \"glanceable\" as a still image. This thesis proposes Glyph, a tool for creating expressive, seamlessly-looping GIFs from video. The tool integrates opensource software for video/image manipulation and loop detection into a simple, web-based authoring interface. Glyph allows a user to automatically detect perfect loops that occur in a video, create the appearance of seamless motion in a non-looping clip, still some regions of movement in a clip to highlight others, or imbue a still frame with subtle dynamism. The part-automated, part-manual editing tool thus allows users to quickly build nonliteral excerpts from video that can immediately crystalize an affective quality or crucial moment, suspending and intensifying its semantic or emotional content through continuous, seamless looping. This thesis additionally explores applications for this class of image, called glyphs.",
    "advisors": ["Andrew B. Lippman"],
    "text": "Glyph : lightweight and evocative looping images in the news There is an emotional dimension to the informative function of the news. When we read the news we participate in a collective emotional experience- whether that is grief, celebration, worry, or wonder. News video is a crucial vector for these shared emotional experiences, which can propel civic action. But video comes at a high cost in time and attention, and is thus unsuitable for high volume news and social feeds, and mobile and wearable devices. On these interfaces, there is value in presenting video in a way that's immediately evocative, preserving the richness of video while atomizing it to an excerpt as \"glanceable\" as a still image. This thesis proposes Glyph, a tool for creating expressive, seamlessly-looping GIFs from video. The tool integrates opensource software for video/image manipulation and loop detection into a simple, web-based authoring interface. Glyph allows a user to automatically detect perfect loops that occur in a video, create the appearance of seamless motion in a non-looping clip, still some regions of movement in a clip to highlight others, or imbue a still frame with subtle dynamism. The part-automated, part-manual editing tool thus allows users to quickly build nonliteral excerpts from video that can immediately crystalize an affective quality or crucial moment, suspending and intensifying its semantic or emotional content through continuous, seamless looping. This thesis additionally explores applications for this class of image, called glyphs."
}, {
    "id": "oai:dspace.mit.edu:1721.1/79307",
    "title": "SparkInfo : designing a social space for co-creation of multimedia contents",
    "abstract": "People can have more insights and social experiences when they collaborate on collecting, revisiting, and utilizing their contents, such as images and videos; however, designing a social space that offers rich co-creation and exploration of multimedia contents remains a challenge. I propose a new system, SparkInfo, which enables users to create, exchange and augment their multimedia elements in ways that are personally unique and sociable. SparkInfo is designed for a group of people, who have created multimedia elements for the same purpose or at the same event, to collect their elements in one place and have a meaningful experience of their co-created media resources. SparkInfo provides a social space for the co-creation of multimedia resources. In the process of exploring and embellishing their materials, SparkInfo users can create new ideas, stories, and information. By utilizing this process, the users are able to experience how SparkInfo can embody the cycle of knowledge building, re-mixing, and sharing.",
    "advisors": ["Henry Holtzman"],
    "text": "SparkInfo : designing a social space for co-creation of multimedia contents People can have more insights and social experiences when they collaborate on collecting, revisiting, and utilizing their contents, such as images and videos; however, designing a social space that offers rich co-creation and exploration of multimedia contents remains a challenge. I propose a new system, SparkInfo, which enables users to create, exchange and augment their multimedia elements in ways that are personally unique and sociable. SparkInfo is designed for a group of people, who have created multimedia elements for the same purpose or at the same event, to collect their elements in one place and have a meaningful experience of their co-created media resources. SparkInfo provides a social space for the co-creation of multimedia resources. In the process of exploring and embellishing their materials, SparkInfo users can create new ideas, stories, and information. By utilizing this process, the users are able to experience how SparkInfo can embody the cycle of knowledge building, re-mixing, and sharing."
}, {
    "id": "oai:dspace.mit.edu:1721.1/33889",
    "title": "SMPL : a network architecture for collaborative distributed services",
    "abstract": "This thesis proposes a network architecture, called SMPL, for the design and development of collaboration-oriented, distributed applications over the Internet. The goal of SMPL is to enable the development of applications that easily integrate the capabilities of different types of computing resources, software platforms, and data repositories across the Internet transcending the level of a single device. SMPL proposes a new abstraction of the Internet as a network composed of services, resources, and capabilities instead of just machines. The SMPL architecture distributes resources through a peer-to-peer network of service providers. The design of SMPL encourages developers to add value to the system by facilitating the creation of new functionalities based upon compositions of the existing ones.",
    "advisors": ["John Maeda"],
    "text": "SMPL : a network architecture for collaborative distributed services This thesis proposes a network architecture, called SMPL, for the design and development of collaboration-oriented, distributed applications over the Internet. The goal of SMPL is to enable the development of applications that easily integrate the capabilities of different types of computing resources, software platforms, and data repositories across the Internet transcending the level of a single device. SMPL proposes a new abstraction of the Internet as a network composed of services, resources, and capabilities instead of just machines. The SMPL architecture distributes resources through a peer-to-peer network of service providers. The design of SMPL encourages developers to add value to the system by facilitating the creation of new functionalities based upon compositions of the existing ones."
}, {
    "id": "oai:dspace.mit.edu:1721.1/112562",
    "title": "An apparatus enabling easier and more automated dietary pattern tracking",
    "abstract": "Nutritional assessment is an important problem for every American. Studies suggest that as many as 90% of Americans fall short of Vitamins D and E as a result of their regular dietary habits, and up to 50% of Americans do not get enough Vitamin A and Calcium. On the other hand, 68.8% of American adults over 20 were considered overweight or obese (had BMI of over 25), with excessive consumption of added sugars, fats, and carbohydrates being a key factor. There are two potential challenges that, if solved, may help many ordinary Americans manage their diets healthily. The first is recording dietary intake so that we have sufficient information regarding an individual's dietary pattern, and the second is interpreting nutritional profiles from the foods people are eating. It's after these two steps that nutritional intake can be inferred and insights into dietary balance can be gained. This thesis focuses on the first challenge, enabling more convenient tracking of dietary patterns supported by automatic image recognition. Our goal was to provide an improved alternative to current mainstream methods of keeping dietary records such as written records for clinical studies, or text input based digital trackers such as MyFitnessPal. Both current methods are quite tiresome, and we saw opportunities in utilizing computer vision methods to automate the recognition of what a user is eating, therefore hoping to reduce the need for manual input and making the process easier. In practice, we implemented an image classifier based on the Inception architecture of GoogLeNet, and trained it on the Food- 101 dataset. The performance of the classifier on the validation set achieved around 87% for top 5 accuracy. We then deployed our image recognition apparatus in the form of a mobile application, to examine the actual performance of this apparatus in an in-field setting with actual consumer eating patterns. The overall in-field recognition performance was around 28% (top 5), however, since only 30% of our meals observed were actually of foods belonging to the 101 classes we had trained the classifier to recognize, the in-field recognition accuracies for when foods to record were of foods we had trained on was around 92%. Furthermore, in subjective user surveys, 67% of users preferred our computer vision based apparatus to existing text input based digital trackers like MyFitnessPal, with 22% being neutral. Therefore, we believe that this approach to diet tracking is a promising one to explore in the future, as the main cause of low in-field recognition performance seems to be mainly caused by lack of coverage of the training data, and if we can curate a training set that captures the visual food domain appropriately, this approach can yield high in-field results and provide a tangibly more convenient tool for users to log and track their diets.",
    "advisors": ["Deb Roy"],
    "text": "An apparatus enabling easier and more automated dietary pattern tracking Nutritional assessment is an important problem for every American. Studies suggest that as many as 90% of Americans fall short of Vitamins D and E as a result of their regular dietary habits, and up to 50% of Americans do not get enough Vitamin A and Calcium. On the other hand, 68.8% of American adults over 20 were considered overweight or obese (had BMI of over 25), with excessive consumption of added sugars, fats, and carbohydrates being a key factor. There are two potential challenges that, if solved, may help many ordinary Americans manage their diets healthily. The first is recording dietary intake so that we have sufficient information regarding an individual's dietary pattern, and the second is interpreting nutritional profiles from the foods people are eating. It's after these two steps that nutritional intake can be inferred and insights into dietary balance can be gained. This thesis focuses on the first challenge, enabling more convenient tracking of dietary patterns supported by automatic image recognition. Our goal was to provide an improved alternative to current mainstream methods of keeping dietary records such as written records for clinical studies, or text input based digital trackers such as MyFitnessPal. Both current methods are quite tiresome, and we saw opportunities in utilizing computer vision methods to automate the recognition of what a user is eating, therefore hoping to reduce the need for manual input and making the process easier. In practice, we implemented an image classifier based on the Inception architecture of GoogLeNet, and trained it on the Food- 101 dataset. The performance of the classifier on the validation set achieved around 87% for top 5 accuracy. We then deployed our image recognition apparatus in the form of a mobile application, to examine the actual performance of this apparatus in an in-field setting with actual consumer eating patterns. The overall in-field recognition performance was around 28% (top 5), however, since only 30% of our meals observed were actually of foods belonging to the 101 classes we had trained the classifier to recognize, the in-field recognition accuracies for when foods to record were of foods we had trained on was around 92%. Furthermore, in subjective user surveys, 67% of users preferred our computer vision based apparatus to existing text input based digital trackers like MyFitnessPal, with 22% being neutral. Therefore, we believe that this approach to diet tracking is a promising one to explore in the future, as the main cause of low in-field recognition performance seems to be mainly caused by lack of coverage of the training data, and if we can curate a training set that captures the visual food domain appropriately, this approach can yield high in-field results and provide a tangibly more convenient tool for users to log and track their diets."
}, {
    "id": "oai:dspace.mit.edu:1721.1/36111",
    "title": "Thin slices of interest",
    "abstract": "In this thesis we describe an automatic human interest detector that uses speech, physiology, body movement, location and proximity information. The speech features, consisting of activity, stress, empathy and engagement measures are used in three large experimental evaluations; measuring interest in short conversations, attraction in speed dating, and understanding the interactions within a focus group, all within a few minutes. In the conversational interest experiment, the speech features predict about 45% of the variance in self-reported interest ratings for 20 male and female participants. Stress and activity measures play the most important role, and a simple activity-based classifier predicts low or high interest with 74% accuracy (for men). In the speed-dating study, we use the speech features measured from five minutes of conversation to predict attraction between people. The features predict 40% of the variance in outcomes for attraction, friendship and business relationships. Speech features are used in an SVM classifier that is 75%-80% accurate in predicting outcomes based on speaking style. In the context of measuring consumer interest in focus groups, the speech features help to identify a pattern of behavior where subjects changed their opinions after discussion. Finally, we propose a prototype wearable 'interest meter' and various application scenarios. We portray a world where cell phones can automatically measure interest and engagement, and share this information between families and workgroups.",
    "advisors": ["Alex Pentland"],
    "text": "Thin slices of interest In this thesis we describe an automatic human interest detector that uses speech, physiology, body movement, location and proximity information. The speech features, consisting of activity, stress, empathy and engagement measures are used in three large experimental evaluations; measuring interest in short conversations, attraction in speed dating, and understanding the interactions within a focus group, all within a few minutes. In the conversational interest experiment, the speech features predict about 45% of the variance in self-reported interest ratings for 20 male and female participants. Stress and activity measures play the most important role, and a simple activity-based classifier predicts low or high interest with 74% accuracy (for men). In the speed-dating study, we use the speech features measured from five minutes of conversation to predict attraction between people. The features predict 40% of the variance in outcomes for attraction, friendship and business relationships. Speech features are used in an SVM classifier that is 75%-80% accurate in predicting outcomes based on speaking style. In the context of measuring consumer interest in focus groups, the speech features help to identify a pattern of behavior where subjects changed their opinions after discussion. Finally, we propose a prototype wearable 'interest meter' and various application scenarios. We portray a world where cell phones can automatically measure interest and engagement, and share this information between families and workgroups."
}, {
    "id": "oai:dspace.mit.edu:1721.1/39337",
    "title": "A mood-based music classification and exploration system",
    "abstract": "Mood classification of music is an emerging domain of music information retrieval. In the approach presented here features extracted from an audio file are used in combination with the affective value of song lyrics to map a song onto a psychologically based emotion space. The motivation behind this system is the lack of intuitive and contextually aware playlist generation tools available to music listeners. The need for such tools is made obvious by the fact that digital music libraries are constantly expanding, thus making it increasingly difficult to recall a particular song in the library or to create a playlist for a specific event. By combining audio content information with context-aware data, such as song lyrics, this system allows the listener to automatically generate a playlist to suit their current activity or mood.",
    "advisors": ["Barry Vercoe"],
    "text": "A mood-based music classification and exploration system Mood classification of music is an emerging domain of music information retrieval. In the approach presented here features extracted from an audio file are used in combination with the affective value of song lyrics to map a song onto a psychologically based emotion space. The motivation behind this system is the lack of intuitive and contextually aware playlist generation tools available to music listeners. The need for such tools is made obvious by the fact that digital music libraries are constantly expanding, thus making it increasingly difficult to recall a particular song in the library or to create a playlist for a specific event. By combining audio content information with context-aware data, such as song lyrics, this system allows the listener to automatically generate a playlist to suit their current activity or mood."
}, {
    "id": "oai:dspace.mit.edu:1721.1/45756",
    "title": "Exploration of robotic-wheel technology for enhanced urban mobility and city scale omni-directional personal transportation",
    "abstract": "Mobility is traditionally thought of as freedom to access more goods and services. However, in my view, mobility is also largely about personal freedom, i.e., the ability to exceed one's physical limitations, in essence, to become \"more than human\" in physical capabilities. This thesis explores novel designs for omni-directional motion in a mobility scooter, car and bus with the aim of increasing personal mobility and freedom. What links these designs is the use of split active caster wheel robot technology. In the first section, societal and technological impacts of omni-directional motion in the city are examined. The second section of the thesis presents built and rendered prototypes of these three designs. The third and final section, evaluates implementation issues including robotic controls and an algorithm necessary for real world omni-directional mobility.",
    "advisors": ["William J. Mitchell"],
    "text": "Exploration of robotic-wheel technology for enhanced urban mobility and city scale omni-directional personal transportation Mobility is traditionally thought of as freedom to access more goods and services. However, in my view, mobility is also largely about personal freedom, i.e., the ability to exceed one's physical limitations, in essence, to become \"more than human\" in physical capabilities. This thesis explores novel designs for omni-directional motion in a mobility scooter, car and bus with the aim of increasing personal mobility and freedom. What links these designs is the use of split active caster wheel robot technology. In the first section, societal and technological impacts of omni-directional motion in the city are examined. The second section of the thesis presents built and rendered prototypes of these three designs. The third and final section, evaluates implementation issues including robotic controls and an algorithm necessary for real world omni-directional mobility."
}, {
    "id": "oai:dspace.mit.edu:1721.1/37387",
    "title": "Subtle, intimate interfaces for mobile human computer interaction",
    "abstract": "The mobile phone is always carried with the user and is always active: it is a very personal device. It fosters and satisfies a need to be constantly connected to one's significant other, friends or business partners. At the same time, mobile devices are often used in public, where one is surrounded by others not involved in the interaction. This private interaction in public is often a cause of unnecessary disruption and distraction, both for the bystanders and even for the user. Nevertheless, mobile devices do fulfill an important function, informing of important events and urgent communications, so turning them off is often not practical nor possible. This thesis introduces Intimate Interfaces: discreet interfaces that allow subtle private interaction with mobile devices in order to minimize disruption in public and gain social acceptance. Intimate Interfaces are inconspicuous to those around the users, while still allowing them to communicate. The concept is demonstrated through the design, implementation and evaluation of two novel devices: * Intimate Communication Armband - a wearable device, embedded in an armband, that detects motionless gestures through electromyographic (EMG) sensing for subtle input and provides tactile output;",
    "advisors": ["Pattie Maes"],
    "text": "Subtle, intimate interfaces for mobile human computer interaction The mobile phone is always carried with the user and is always active: it is a very personal device. It fosters and satisfies a need to be constantly connected to one's significant other, friends or business partners. At the same time, mobile devices are often used in public, where one is surrounded by others not involved in the interaction. This private interaction in public is often a cause of unnecessary disruption and distraction, both for the bystanders and even for the user. Nevertheless, mobile devices do fulfill an important function, informing of important events and urgent communications, so turning them off is often not practical nor possible. This thesis introduces Intimate Interfaces: discreet interfaces that allow subtle private interaction with mobile devices in order to minimize disruption in public and gain social acceptance. Intimate Interfaces are inconspicuous to those around the users, while still allowing them to communicate. The concept is demonstrated through the design, implementation and evaluation of two novel devices: * Intimate Communication Armband - a wearable device, embedded in an armband, that detects motionless gestures through electromyographic (EMG) sensing for subtle input and provides tactile output;"
}, {
    "id": "oai:dspace.mit.edu:1721.1/91431",
    "title": "Toward accessible evaluation of the electrophysiology of human vision",
    "abstract": "As photoreceptors in our retinas capture discrete photons, that energy is converted into an electrochemical signal which shoots back through the optic nerve and into our visual cortex. We can sample that signal as it's transmitted, by delivering specific stimuli and recording the aggregate response of the photoreceptors, but systems which accomplish this in current practice are out of reach for most ophthalmic clinics and completely unavailable to consumers. With a reimagined signal capture system and an optimized system design, I demonstrate a robust method for capturing the electrical signals emitted from the retina. With the improved accessibility and decreased cost of this technology, there are immediate opportunities for improved ophthalmic care on a broad scale. But beyond the clinical implications, accessible electroretinography presents an unprecedented opportunity for individuals to characterize their specific experience of color, contrast, and movement, making way for a whole new paradigm of tailored display technologies.",
    "advisors": ["Ramesh Raskar"],
    "text": "Toward accessible evaluation of the electrophysiology of human vision As photoreceptors in our retinas capture discrete photons, that energy is converted into an electrochemical signal which shoots back through the optic nerve and into our visual cortex. We can sample that signal as it's transmitted, by delivering specific stimuli and recording the aggregate response of the photoreceptors, but systems which accomplish this in current practice are out of reach for most ophthalmic clinics and completely unavailable to consumers. With a reimagined signal capture system and an optimized system design, I demonstrate a robust method for capturing the electrical signals emitted from the retina. With the improved accessibility and decreased cost of this technology, there are immediate opportunities for improved ophthalmic care on a broad scale. But beyond the clinical implications, accessible electroretinography presents an unprecedented opportunity for individuals to characterize their specific experience of color, contrast, and movement, making way for a whole new paradigm of tailored display technologies."
}, {
    "id": "oai:dspace.mit.edu:1721.1/55197",
    "title": "Jots : cultivating reflective learning in scratch",
    "abstract": "This thesis introduces the Jots system, a new technology designed to engage children in reflective learning as they work on design projects. Jots enables children to create brief updates, or \"jots,\" describing their frustrations, achievements and other thoughts and feelings while creating projects in the Scratch programming environment. Later children can look back at their jots to reflect on their own design and learning processes. This thesis introduces an approach to reflective learning in four facets: cognitive, emotional, social and temporal. The design of the Jots system, as well as its development over time, are discussed. An empirical study with three middle school students who used jots in a facilitated context is analyzed in case studies and categorizations. The results of the study are examined in terms of the four aspects of reflection, and ideas for future work are presented.",
    "advisors": ["Mitchel Resnick"],
    "text": "Jots : cultivating reflective learning in scratch This thesis introduces the Jots system, a new technology designed to engage children in reflective learning as they work on design projects. Jots enables children to create brief updates, or \"jots,\" describing their frustrations, achievements and other thoughts and feelings while creating projects in the Scratch programming environment. Later children can look back at their jots to reflect on their own design and learning processes. This thesis introduces an approach to reflective learning in four facets: cognitive, emotional, social and temporal. The design of the Jots system, as well as its development over time, are discussed. An empirical study with three middle school students who used jots in a facilitated context is analyzed in case studies and categorizations. The results of the study are examined in terms of the four aspects of reflection, and ideas for future work are presented."
}, {
    "id": "oai:dspace.mit.edu:1721.1/114061",
    "title": "From children's play to intentions : a play analytics framework for constructionist learning apps",
    "abstract": "Educational games and digital learning environments provide opportunities to collect fine-grained data on how learners engage with these technologies. The number of technologies targeted at literacy learning for children is increasing. However, the majority of them are structured and reward-based. Therefore, the users' behavior and data collected from them have the same limits. In this thesis, however, we assess children's engagement with a constructionist literacy learning app. The open ended nature of play in such an environment gives us the opportunity to analyze children's play not only through what they made while playing but also how they did it. This thesis provides an analytics pipeline from data acquisition to modeling behavioral patterns. This systematic way of capturing significant events in children's play can be used to inform stakeholders such as parents, peers and teachers and engage them with the learning process. It also gives the learning environment more intelligence on when and what to provide scaffolding on. To collect data, we ran two pilot studies and gathered audio and video recordings of play sessions. In addition, all of the children's interactions within the app were automatically logged. The fine-grained longitudinal data collected during the pilot studies provides a rich yet raw corpus. To reveal the patterns hidden in the data, the analytics pipeline parses logs of low-level interactions into abstract representations for sequences of actions in a word construction process. Next, it visualizes the process for each play session and the entire play history. Using the visualizations, I identified and annotated repeated motifs for more intentional sequences of actions during play and used supervised learning models to capture those patterns. The results of this analytical pipeline are currently being used by literacy experts to provide feedback to parents and suggest activities based on the child's process.",
    "advisors": ["Deb Roy"],
    "text": "From children's play to intentions : a play analytics framework for constructionist learning apps Educational games and digital learning environments provide opportunities to collect fine-grained data on how learners engage with these technologies. The number of technologies targeted at literacy learning for children is increasing. However, the majority of them are structured and reward-based. Therefore, the users' behavior and data collected from them have the same limits. In this thesis, however, we assess children's engagement with a constructionist literacy learning app. The open ended nature of play in such an environment gives us the opportunity to analyze children's play not only through what they made while playing but also how they did it. This thesis provides an analytics pipeline from data acquisition to modeling behavioral patterns. This systematic way of capturing significant events in children's play can be used to inform stakeholders such as parents, peers and teachers and engage them with the learning process. It also gives the learning environment more intelligence on when and what to provide scaffolding on. To collect data, we ran two pilot studies and gathered audio and video recordings of play sessions. In addition, all of the children's interactions within the app were automatically logged. The fine-grained longitudinal data collected during the pilot studies provides a rich yet raw corpus. To reveal the patterns hidden in the data, the analytics pipeline parses logs of low-level interactions into abstract representations for sequences of actions in a word construction process. Next, it visualizes the process for each play session and the entire play history. Using the visualizations, I identified and annotated repeated motifs for more intentional sequences of actions during play and used supervised learning models to capture those patterns. The results of this analytical pipeline are currently being used by literacy experts to provide feedback to parents and suggest activities based on the child's process."
}, {
    "id": "oai:dspace.mit.edu:1721.1/82425",
    "title": "Drawing the electric",
    "abstract": "This thesis explores the intersection of craft and electronics by way of paper and conductive ink, a domain that I'm terming papercraft electronics-a synthesis of electronics, drawing, and painting. I investigate the nature of making a physical electronic artifact, and the ways in which that making informs our relationship with both the artifact, specifically, as well as technology writ large. I examine craft-the manual process-as a means for embedding new kinds of personally-significant meaning in electronics, re-positioning electronics fabrication as the creation of personal, unique, hand-crafted artifacts. I do so through a series of case studies oriented around the papercraft electronics domain. Through a sequence of projects, workshops, and evaluations, I examine the personal connection and pride that comes with making, as well as the handmade artifact's place in technology. In particular, I initiate projects around the making of paper sensors, speakers, synthesizers, and audio-augmented artworks.",
    "advisors": ["Leah Buechley"],
    "text": "Drawing the electric This thesis explores the intersection of craft and electronics by way of paper and conductive ink, a domain that I'm terming papercraft electronics-a synthesis of electronics, drawing, and painting. I investigate the nature of making a physical electronic artifact, and the ways in which that making informs our relationship with both the artifact, specifically, as well as technology writ large. I examine craft-the manual process-as a means for embedding new kinds of personally-significant meaning in electronics, re-positioning electronics fabrication as the creation of personal, unique, hand-crafted artifacts. I do so through a series of case studies oriented around the papercraft electronics domain. Through a sequence of projects, workshops, and evaluations, I examine the personal connection and pride that comes with making, as well as the handmade artifact's place in technology. In particular, I initiate projects around the making of paper sensors, speakers, synthesizers, and audio-augmented artworks."
}, {
    "id": "oai:dspace.mit.edu:1721.1/37385",
    "title": "Searching for commonsense",
    "abstract": "Acquiring and representing the large body of \"common sense\" knowledge underlying ordinary human reasoning and communication is a long standing problem in the field of artificial intelligence. This thesis will address the question whether a significant quantity of this knowledge may be acquired by mining natural language content on the Web. Specifically, this thesis emphasizes the representation of knowledge in the form of binary semantic relationships, such as cause, effect, intent, and time, among natural language phrases. The central hypothesis is that seed knowledge collected from volunteers enables automated acquisition of this knowledge from a large, unannotated, general corpus like the Web. A text mining system, ConceptMiner, was developed to evaluate this hypothesis. ConceptMiner leverages web search engines, Information Extraction techniques and the ConceptNet toolkit to analyze Web content for textual evidence indicating common sense relationships.",
    "advisors": ["Walter Bender, Hugh Herr", "Rada Mihalcea"],
    "text": "Searching for commonsense Acquiring and representing the large body of \"common sense\" knowledge underlying ordinary human reasoning and communication is a long standing problem in the field of artificial intelligence. This thesis will address the question whether a significant quantity of this knowledge may be acquired by mining natural language content on the Web. Specifically, this thesis emphasizes the representation of knowledge in the form of binary semantic relationships, such as cause, effect, intent, and time, among natural language phrases. The central hypothesis is that seed knowledge collected from volunteers enables automated acquisition of this knowledge from a large, unannotated, general corpus like the Web. A text mining system, ConceptMiner, was developed to evaluate this hypothesis. ConceptMiner leverages web search engines, Information Extraction techniques and the ConceptNet toolkit to analyze Web content for textual evidence indicating common sense relationships."
}, {
    "id": "oai:dspace.mit.edu:1721.1/61845",
    "title": "Talking in circles : representing place and situation in an online social environment",
    "abstract": "This thesis presents work focused on the creation of a sociable space for communication online. Sociable communication requires the ability to converse with others using simple and meaningful mechanisms, supporting flexibility and expressiveness. Equally important is the ability for people to read the space they inhabit and make sense of it in socially significant ways, such as people watching to observe others' interests and interaction styles. A third key to sociable communication is emphasis on identity and embodiment, giving participants a strong sense of themselves and others through their online representations. These issues are approached through research in areas ranging from sociology to urban architecture, directed at finding bases for the design of capabilities that are useful and engaging in the context of computer support for distributed multiparty communication. The result of this research is Talking in Circles, a graphical audio conferencing environment that employs abstract graphics for representation and provides lightweight access to multiple expressive modes. This thesis discusses foundations for work towards sociable communication online as well as the design and implementation processes involved in the creation of the Talking in Circles system. User experiences with the system, lessons learned and directions for further research into sociable communication are then detailed.",
    "advisors": ["Judith Donath"],
    "text": "Talking in circles : representing place and situation in an online social environment This thesis presents work focused on the creation of a sociable space for communication online. Sociable communication requires the ability to converse with others using simple and meaningful mechanisms, supporting flexibility and expressiveness. Equally important is the ability for people to read the space they inhabit and make sense of it in socially significant ways, such as people watching to observe others' interests and interaction styles. A third key to sociable communication is emphasis on identity and embodiment, giving participants a strong sense of themselves and others through their online representations. These issues are approached through research in areas ranging from sociology to urban architecture, directed at finding bases for the design of capabilities that are useful and engaging in the context of computer support for distributed multiparty communication. The result of this research is Talking in Circles, a graphical audio conferencing environment that employs abstract graphics for representation and provides lightweight access to multiple expressive modes. This thesis discusses foundations for work towards sociable communication online as well as the design and implementation processes involved in the creation of the Talking in Circles system. User experiences with the system, lessons learned and directions for further research into sociable communication are then detailed."
}, {
    "id": "oai:dspace.mit.edu:1721.1/93019",
    "title": "SproutslO Urban Microfarm : interactive indoor farming system for urban use",
    "abstract": "SproutslO Urban Microfarm is an interactive farming system that enables people to reliably grow and access healthy produce in urban areas. The introduction of a localized system with the capacity to induce scalar impact through citywide collective participation has the potential to redefine our current opaque food system. SproutsIO incorporates modular components augmented by technology such as monitoring sensors, automated systems, and smart mobile applications with the goal to facilitate aeroponic growing of organic produce in cities. A database and monitoring network has been established to assist users in determining the growing needs and profiles of plant species in order to provide real-time feedback in assisting with plant care. This is the first system to incorporate aeroponic growing technology with sensors, automation, and mobile applications into a fully integrated, networked, and responsive system for ease of use in an indoor urban context. User studies were conducted to test primary aspects of the system: user interaction with the mobile application, ease of assembly and initial planting of the system, and user experience growing in the system over a weeklong period. The analysis of this testing provides valuable information toward future optimization of the SproutslO system.",
    "advisors": ["Kent Larson"],
    "text": "SproutslO Urban Microfarm : interactive indoor farming system for urban use SproutslO Urban Microfarm is an interactive farming system that enables people to reliably grow and access healthy produce in urban areas. The introduction of a localized system with the capacity to induce scalar impact through citywide collective participation has the potential to redefine our current opaque food system. SproutsIO incorporates modular components augmented by technology such as monitoring sensors, automated systems, and smart mobile applications with the goal to facilitate aeroponic growing of organic produce in cities. A database and monitoring network has been established to assist users in determining the growing needs and profiles of plant species in order to provide real-time feedback in assisting with plant care. This is the first system to incorporate aeroponic growing technology with sensors, automation, and mobile applications into a fully integrated, networked, and responsive system for ease of use in an indoor urban context. User studies were conducted to test primary aspects of the system: user interaction with the mobile application, ease of assembly and initial planting of the system, and user experience growing in the system over a weeklong period. The analysis of this testing provides valuable information toward future optimization of the SproutslO system."
}, {
    "id": "oai:dspace.mit.edu:1721.1/55193",
    "title": "FlickerThis : a mobile service to facilitate grounding in communication through viewable media content",
    "abstract": "Remote communication has become part of our daily lives. Technology plays a decisive role in filling the gap caused by discrepancies in time and space between us and the people we want to reach. However, the level of immediacy and interactivity are always lacking when compared to face-to-face communication. In most cases, the desire to communicate is postponed or abridged to nonreciprocal sharing, especially when time differences are taken into account. Limited bandwidth also increases the likelihood of a misunderstanding occurring during a remote conversation. Confronting the limitations of remote communication, FlickerThis is a mobile service that facilitates grounding in communication through viewable content. Different from most communication services that rely on isolated channels to push information out, FlickerThis adapts the essential perceptions, seeing and hearing, in face-to-face communication to mediate remote communication. With photos taken by a camera phone and audio narration as the intuitive input, FlickerThis messages are composed to become stories whenever one has an intention to communicate with others. Users can talk over the phone while flipping through a virtual photo album together, or they can record and send voicemails embedded with pictures. Therefore, FlickerThis enables remote communication to switch between synchronous and asynchronous modes, and uses viewable content to enhance mutual understanding.",
    "advisors": ["Christopher Schmandt"],
    "text": "FlickerThis : a mobile service to facilitate grounding in communication through viewable media content Remote communication has become part of our daily lives. Technology plays a decisive role in filling the gap caused by discrepancies in time and space between us and the people we want to reach. However, the level of immediacy and interactivity are always lacking when compared to face-to-face communication. In most cases, the desire to communicate is postponed or abridged to nonreciprocal sharing, especially when time differences are taken into account. Limited bandwidth also increases the likelihood of a misunderstanding occurring during a remote conversation. Confronting the limitations of remote communication, FlickerThis is a mobile service that facilitates grounding in communication through viewable content. Different from most communication services that rely on isolated channels to push information out, FlickerThis adapts the essential perceptions, seeing and hearing, in face-to-face communication to mediate remote communication. With photos taken by a camera phone and audio narration as the intuitive input, FlickerThis messages are composed to become stories whenever one has an intention to communicate with others. Users can talk over the phone while flipping through a virtual photo album together, or they can record and send voicemails embedded with pictures. Therefore, FlickerThis enables remote communication to switch between synchronous and asynchronous modes, and uses viewable content to enhance mutual understanding."
}, {
    "id": "oai:dspace.mit.edu:1721.1/26917",
    "title": "Scalable multi-view stereo camera array for real world real-time image capture and three-dimensional displays",
    "abstract": "The number of three-dimensional displays available is escalating and yet the capturing devices for multiple view content are focused on either single camera precision rigs that are limited to stationary objects or the use of synthetically created animations. In this work we will use the existence of inexpensive digital CMOS cameras to explore a multi- image capture paradigm and the gathering of real world real-time data of active and static scenes. The capturing system can be developed and employed for a wide range of applications such as portrait-based images for multi-view facial recognition systems, hypostereo surgical training systems, and stereo surveillance by unmanned aerial vehicles. The system will be adaptable to capturing the correct stereo views based on the environmental scene and the desired three-dimensional display. Several issues explored by the system will include image calibration, geometric correction, the possibility of object tracking, and transfer of the array technology into other image capturing systems. These features provide the user more freedom to interact with their specific 3-D content while allowing the computer to take on the difficult role of stereoscopic cinematographer.",
    "advisors": ["V. Michael Bove, Jr"],
    "text": "Scalable multi-view stereo camera array for real world real-time image capture and three-dimensional displays The number of three-dimensional displays available is escalating and yet the capturing devices for multiple view content are focused on either single camera precision rigs that are limited to stationary objects or the use of synthetically created animations. In this work we will use the existence of inexpensive digital CMOS cameras to explore a multi- image capture paradigm and the gathering of real world real-time data of active and static scenes. The capturing system can be developed and employed for a wide range of applications such as portrait-based images for multi-view facial recognition systems, hypostereo surgical training systems, and stereo surveillance by unmanned aerial vehicles. The system will be adaptable to capturing the correct stereo views based on the environmental scene and the desired three-dimensional display. Several issues explored by the system will include image calibration, geometric correction, the possibility of object tracking, and transfer of the array technology into other image capturing systems. These features provide the user more freedom to interact with their specific 3-D content while allowing the computer to take on the difficult role of stereoscopic cinematographer."
}, {
    "id": "oai:dspace.mit.edu:1721.1/41755",
    "title": "The Chandelier : towards a digitally conceived physical performance object",
    "abstract": "In the performing arts, the relationship that is established between what is seen and what is heard must be experienced to fully appreciate and understand the aesthetics of performance. Actual physical objects such as musical instruments, lights, elements of the set, props, and people provide the visual associations and a tangible reality which can enhance the musical elements in a performance. This thesis proposes that new and artistic physical objects can, in themselves, be designed to perform. It introduces the Chandelier, a kinetic sculpture, a central set piece for a new opera, a new kind of musical instrument, and an object that performs. The piece moves and changes shape through mechanical action and the designed interplay between surfaces and light. It is intended to be interacted with by musicians and players of the opera. This thesis also explores the design process and evolution of the Chandelier with a primary objective of realizing a constructible, physical performance object through an authentic and abstruse digital conception. It is a conception not of a static nature, but incorporates a dynamic sense of changeable form through coordinated elements of light, mechanics, and sculpture.",
    "advisors": ["Tod Machover"],
    "text": "The Chandelier : towards a digitally conceived physical performance object In the performing arts, the relationship that is established between what is seen and what is heard must be experienced to fully appreciate and understand the aesthetics of performance. Actual physical objects such as musical instruments, lights, elements of the set, props, and people provide the visual associations and a tangible reality which can enhance the musical elements in a performance. This thesis proposes that new and artistic physical objects can, in themselves, be designed to perform. It introduces the Chandelier, a kinetic sculpture, a central set piece for a new opera, a new kind of musical instrument, and an object that performs. The piece moves and changes shape through mechanical action and the designed interplay between surfaces and light. It is intended to be interacted with by musicians and players of the opera. This thesis also explores the design process and evolution of the Chandelier with a primary objective of realizing a constructible, physical performance object through an authentic and abstruse digital conception. It is a conception not of a static nature, but incorporates a dynamic sense of changeable form through coordinated elements of light, mechanics, and sculpture."
}, {
    "id": "oai:dspace.mit.edu:1721.1/120673",
    "title": "OCA : a device interconnection platform for context-aware transformable environments",
    "abstract": "It is inevitable that personal architectural environments in the future will contain dozens, if not hundreds, of connected devices that will require human control. Currently, most interactions involve standalone devices, each with a unique and often non-intuitive interface, which places an unacceptable cognitive load on the occupants. To address this challenge, I propose to develop a framework that enables the devices to understand and react to the user's intentions towards the environment using pattern recognition coupled with a single user interface that is consistent across output modes. The system, designed to work within complex transformable environments, will allow the user to control furniture configuration, lighting, transparency, audio, fragrance, health systems, etc. Using machine learning, the system will correlate the user's preferences to those of others who have exhibited similar patterns of behavior in order to predict appropriate settings for novel situations. Overall, the system is expected to reduce the amount of time and cognitive load required to configure an environment for optimal comfort and utility. The proposed framework will be tested and validated in a small, re-configurable workplace environment designed to accommodate private work, phone calls, conversations, napping, and meditating. Occupants will be able to tune many parameters of the environment in response to context-aware transformations triggered by their ongoing activities. The proposed system will have an architecture based on a database infrastructure, sensor data collection, and algorithms for activity and pattern recognition.",
    "advisors": ["Kent Larson"],
    "text": "OCA : a device interconnection platform for context-aware transformable environments It is inevitable that personal architectural environments in the future will contain dozens, if not hundreds, of connected devices that will require human control. Currently, most interactions involve standalone devices, each with a unique and often non-intuitive interface, which places an unacceptable cognitive load on the occupants. To address this challenge, I propose to develop a framework that enables the devices to understand and react to the user's intentions towards the environment using pattern recognition coupled with a single user interface that is consistent across output modes. The system, designed to work within complex transformable environments, will allow the user to control furniture configuration, lighting, transparency, audio, fragrance, health systems, etc. Using machine learning, the system will correlate the user's preferences to those of others who have exhibited similar patterns of behavior in order to predict appropriate settings for novel situations. Overall, the system is expected to reduce the amount of time and cognitive load required to configure an environment for optimal comfort and utility. The proposed framework will be tested and validated in a small, re-configurable workplace environment designed to accommodate private work, phone calls, conversations, napping, and meditating. Occupants will be able to tune many parameters of the environment in response to context-aware transformations triggered by their ongoing activities. The proposed system will have an architecture based on a database infrastructure, sensor data collection, and algorithms for activity and pattern recognition."
}, {
    "id": "oai:dspace.mit.edu:1721.1/76519",
    "title": "Reflective Interfaces : assisting teens with stressful situations online",
    "abstract": "This thesis presents the concept of Reflective Interfaces, a novel approach to user experience design that promotes positive behavioral norms. Traditional interface design methodologies such as User Centered Design are oriented towards efficient satisfaction of short-term interface goals, but may not serve the best interests of the user in the long term. Reflective Interfaces encourage the user to think about the space of possible choices they can make, reasons for making those choices, and consequences of their interactions for themselves and others. The problem of Cyberbullying is a serious problem, threatening the viability of social networks for youth today, as spam once threatened email in the early days of the Internet. We explore the design of several Reflective Interfaces for helping teens in distress over social network interactions. First, we implemented a fictitious, but fully functional social network, Fakebook, that provides just-in-time and just-in-place help when potentially bullying interactions are detected. Laboratory tests of the Fakebook interface showed encouraging results. Second, we collaborated with MTV on their site, A Thin Line, which finds stories analogous to a users' particular situation and helps reduce feelings of isolation. We are also working on TimeOut, a dashboard for social network providers that alerts them to situations where outbreaks of bullying might escalate in a community. By putting users in a reflective state, Reflective Interfaces can help them self-correct toward an implicit goal of the community, the interface, the application, or reaffirm the user's own stated goals. These principles can be applied across a wide variety of interfaces for social interaction and other domains.",
    "advisors": ["Henry Lieberman"],
    "text": "Reflective Interfaces : assisting teens with stressful situations online This thesis presents the concept of Reflective Interfaces, a novel approach to user experience design that promotes positive behavioral norms. Traditional interface design methodologies such as User Centered Design are oriented towards efficient satisfaction of short-term interface goals, but may not serve the best interests of the user in the long term. Reflective Interfaces encourage the user to think about the space of possible choices they can make, reasons for making those choices, and consequences of their interactions for themselves and others. The problem of Cyberbullying is a serious problem, threatening the viability of social networks for youth today, as spam once threatened email in the early days of the Internet. We explore the design of several Reflective Interfaces for helping teens in distress over social network interactions. First, we implemented a fictitious, but fully functional social network, Fakebook, that provides just-in-time and just-in-place help when potentially bullying interactions are detected. Laboratory tests of the Fakebook interface showed encouraging results. Second, we collaborated with MTV on their site, A Thin Line, which finds stories analogous to a users' particular situation and helps reduce feelings of isolation. We are also working on TimeOut, a dashboard for social network providers that alerts them to situations where outbreaks of bullying might escalate in a community. By putting users in a reflective state, Reflective Interfaces can help them self-correct toward an implicit goal of the community, the interface, the application, or reaffirm the user's own stated goals. These principles can be applied across a wide variety of interfaces for social interaction and other domains."
}, {
    "id": "oai:dspace.mit.edu:1721.1/106046",
    "title": "Extending the reach of anterior segment ophthalmic imaging",
    "abstract": "Eye exams via a slit lamp are critical in screening for conditions such as cataracts, corneal opacities and pterygia early on to avert vision loss. The slit lamp, however, is a purely qualitative optical device that is bulky, expensive, can cause eye discomfort due to light sensitivity. It also requires a trained physician to operate, making it unsuitable for large-scale screening in resource-constrained settings. In this thesis, we propose a spectrum of portable anterior segment imaging solutions that can be operated by minimally trained health workers. On one end, we present a smartphone attachment with minimal optics and no electronic components beyond what is present in the smartphone itself to examine and image the anterior segment of the eye. This cost-effective, easily scalable solution would help extend the reach of anterior segment examination to extremely resource constrained settings, such as mass-screening camps, mobile ophthalmology clinics, war zones etc. On the other end, we propose purely solid-state instrumentation that employs programmable illumination and light steering optics to simulate the motion of a slit on the eye, thereby exhibiting functionality similar to that of a slit lamp with no moving parts. Finally, we discuss potential deployment strategies for two implementations of this technology in the specific cases of two contrasting healthcare systems in India.",
    "advisors": ["Ramesh Raskar"],
    "text": "Extending the reach of anterior segment ophthalmic imaging Eye exams via a slit lamp are critical in screening for conditions such as cataracts, corneal opacities and pterygia early on to avert vision loss. The slit lamp, however, is a purely qualitative optical device that is bulky, expensive, can cause eye discomfort due to light sensitivity. It also requires a trained physician to operate, making it unsuitable for large-scale screening in resource-constrained settings. In this thesis, we propose a spectrum of portable anterior segment imaging solutions that can be operated by minimally trained health workers. On one end, we present a smartphone attachment with minimal optics and no electronic components beyond what is present in the smartphone itself to examine and image the anterior segment of the eye. This cost-effective, easily scalable solution would help extend the reach of anterior segment examination to extremely resource constrained settings, such as mass-screening camps, mobile ophthalmology clinics, war zones etc. On the other end, we propose purely solid-state instrumentation that employs programmable illumination and light steering optics to simulate the motion of a slit on the eye, thereby exhibiting functionality similar to that of a slit lamp with no moving parts. Finally, we discuss potential deployment strategies for two implementations of this technology in the specific cases of two contrasting healthcare systems in India."
}, {
    "id": "oai:dspace.mit.edu:1721.1/41743",
    "title": "Embodied emergence : distributed computing manipulatives",
    "abstract": "Distributed systems and the emergent properties that can arise out of simple localized interactions have fascinated scientists and artists alike for the last century. They challenge the notions of control and creativity, producing outcomes that can be beautiful, engaging and surprising at the same time. While extensive work has been done using computer simulations of such systems in fields like artificial life and generative art, their physically embodied counterparts are still in their infancy, in part due to the complexity of building and deploying such systems. In this thesis, I will discuss how simple tangible nodes can enable playful and creative experimentation with the concept of emergent behavior. Specifically, I will address how embodied interaction scenarios involving parallel systems can be implemented and how a range of sensing and actuating possibilities can be leveraged to generate novel and engaging experiences for the end users. In particular, the use of sound will be explored as a medium for representation. Finally, I will argue that there is value in making the transition from software simulations to a situated and manipulable instantiation of these concepts, both for the designer of a system and its users.",
    "advisors": ["Patricia Maes"],
    "text": "Embodied emergence : distributed computing manipulatives Distributed systems and the emergent properties that can arise out of simple localized interactions have fascinated scientists and artists alike for the last century. They challenge the notions of control and creativity, producing outcomes that can be beautiful, engaging and surprising at the same time. While extensive work has been done using computer simulations of such systems in fields like artificial life and generative art, their physically embodied counterparts are still in their infancy, in part due to the complexity of building and deploying such systems. In this thesis, I will discuss how simple tangible nodes can enable playful and creative experimentation with the concept of emergent behavior. Specifically, I will address how embodied interaction scenarios involving parallel systems can be implemented and how a range of sensing and actuating possibilities can be leveraged to generate novel and engaging experiences for the end users. In particular, the use of sound will be explored as a medium for representation. Finally, I will argue that there is value in making the transition from software simulations to a situated and manipulable instantiation of these concepts, both for the designer of a system and its users."
}, {
    "id": "oai:dspace.mit.edu:1721.1/58466",
    "title": "Syncwalk : a framework for locative audio composition",
    "abstract": "The way we perceive everyday space-a room, a building, a city-is informed not just by our immediate sensory input: culture, history, and other contextual cues complete our experience. With the advent of sensor-rich, highly-connected objects, our ability to interpret and refine these contextual elements, and therefore our experience of space, grows ever sharper. Location-aware sound art has the potential to apply this new technology in groundbreaking ways, but at present such work is hampered by the lack of a widely-accessible composition platform. In this work, I survey prominent works in the locative-sound art field and propose a scale-independent software framework for composing sound in space. As a proof-of-concept and to encourage further dialogue, I use this framework to create a large-scale participatory project that will allow anyone to sound-design his or her neighborhood space.",
    "advisors": ["Tod Machover"],
    "text": "Syncwalk : a framework for locative audio composition The way we perceive everyday space-a room, a building, a city-is informed not just by our immediate sensory input: culture, history, and other contextual cues complete our experience. With the advent of sensor-rich, highly-connected objects, our ability to interpret and refine these contextual elements, and therefore our experience of space, grows ever sharper. Location-aware sound art has the potential to apply this new technology in groundbreaking ways, but at present such work is hampered by the lack of a widely-accessible composition platform. In this work, I survey prominent works in the locative-sound art field and propose a scale-independent software framework for composing sound in space. As a proof-of-concept and to encourage further dialogue, I use this framework to create a large-scale participatory project that will allow anyone to sound-design his or her neighborhood space."
}, {
    "id": "oai:dspace.mit.edu:1721.1/119086",
    "title": "Receptive Skins : towards a somatosensitive architecture",
    "abstract": "In architecture, the building skin is the primary interface for mediating the environment of the external with the internal. But today, this mediation is mechanical, deterministic, and static-often seeing the human as a generalizable and problematic input. With advances in material science however, there is great potential to disrupt these traditional manufactured environments of architecture and turn them into responsive mediated environments. What this thesis aims to explore is this idea of the receptive skin-a sensate and dynamic multimaterial interface for environmental mediation. This suggests that by departing from the view that buildings are static artifacts, we may instead begin to see buildings as organic, living entities. Through the development of a working prototype, this thesis explores how such an interface may manifest itself, through dynamic material composites, instead of mechanical and electronic means. The final prototype is a \"proof of concept,\" a built example of this novel design methodology, which unites material performance with sensate technologies, as a way to enable new interactions between building and environment.",
    "advisors": ["Kent Larson"],
    "text": "Receptive Skins : towards a somatosensitive architecture In architecture, the building skin is the primary interface for mediating the environment of the external with the internal. But today, this mediation is mechanical, deterministic, and static-often seeing the human as a generalizable and problematic input. With advances in material science however, there is great potential to disrupt these traditional manufactured environments of architecture and turn them into responsive mediated environments. What this thesis aims to explore is this idea of the receptive skin-a sensate and dynamic multimaterial interface for environmental mediation. This suggests that by departing from the view that buildings are static artifacts, we may instead begin to see buildings as organic, living entities. Through the development of a working prototype, this thesis explores how such an interface may manifest itself, through dynamic material composites, instead of mechanical and electronic means. The final prototype is a \"proof of concept,\" a built example of this novel design methodology, which unites material performance with sensate technologies, as a way to enable new interactions between building and environment."
}, {
    "id": "oai:dspace.mit.edu:1721.1/62093",
    "title": "MediaJacket : an integrated clothing based personal communications system",
    "abstract": "Recent developments in fabric based conductive embroidered input devices has created the\r\nopportunity for the next generation of wearable computing. This thesis presents a preliminary\r\nattempt to develop a wearable, multi-purpose, extensible, IP device that uses flexible fabric based\r\ncircuitry for its user interface.\r\nIt integrates a suite of advanced communications devices into a standard PolarTecTMjacket using\r\nan embedded personal computer for its controller. Users wear the MediaJacket similarly to\r\nnormal clothing, and can use a diverse set of applications that include IP telephony, two-way\r\npager-like email, an MP3 audio player, and a contactless \"interface pocket\" for handling input\r\ndata streams from external devices.\r\nBy embedding advanced electronics into clothing using an RF connection for tetherless internet\r\nconnectivity, this research aims to reduce the stigma of using technology by creating a more\r\npersonalized user experience. It is our hope, that as the size and cost of the MediaJacket's\r\ncomponents come down, this research will help people better to better integrate technology into\r\ntheir lives.",
    "advisors": ["Andrew Lippman"],
    "text": "MediaJacket : an integrated clothing based personal communications system Recent developments in fabric based conductive embroidered input devices has created the\r\nopportunity for the next generation of wearable computing. This thesis presents a preliminary\r\nattempt to develop a wearable, multi-purpose, extensible, IP device that uses flexible fabric based\r\ncircuitry for its user interface.\r\nIt integrates a suite of advanced communications devices into a standard PolarTecTMjacket using\r\nan embedded personal computer for its controller. Users wear the MediaJacket similarly to\r\nnormal clothing, and can use a diverse set of applications that include IP telephony, two-way\r\npager-like email, an MP3 audio player, and a contactless \"interface pocket\" for handling input\r\ndata streams from external devices.\r\nBy embedding advanced electronics into clothing using an RF connection for tetherless internet\r\nconnectivity, this research aims to reduce the stigma of using technology by creating a more\r\npersonalized user experience. It is our hope, that as the size and cost of the MediaJacket's\r\ncomponents come down, this research will help people better to better integrate technology into\r\ntheir lives."
}, {
    "id": "oai:dspace.mit.edu:1721.1/62351",
    "title": "E-broidery : an infrastructure for washable computing",
    "abstract": "Wash-and-wear multilayer electronic circuitry can be constructed on fabric substrates, using conductive textiles and suitably packaged components. Fabrics are perhaps the first composite materials engineered by humanity; their evolution led to the development of the Jacquard loom, which itself led to the development of the modern computer. The development of fabric circuitry is a compelling closure of the cycle that points to a new class of textiles which interact with their users and their environments, while retaining the properties that made them the first ubiquitous \"smart material\". Fabrics are in several respects superior to existing flexible substrates in terms of their durability, conformability, and breathability. The present work adopts a modular approach to circuit fabrication, from which follow circuit design techniques and component packages optimized for use in fabric-based circuitry, flexible all-fabric interconnects, and multilayer circuits. While maintaining close compatibility with existing components, tools, and techniques, the present work demonstrates all steps of a process to create multilayer printed circuits on fabric substrates using conductive textiles.",
    "advisors": ["Neil Gershenfeld"],
    "text": "E-broidery : an infrastructure for washable computing Wash-and-wear multilayer electronic circuitry can be constructed on fabric substrates, using conductive textiles and suitably packaged components. Fabrics are perhaps the first composite materials engineered by humanity; their evolution led to the development of the Jacquard loom, which itself led to the development of the modern computer. The development of fabric circuitry is a compelling closure of the cycle that points to a new class of textiles which interact with their users and their environments, while retaining the properties that made them the first ubiquitous \"smart material\". Fabrics are in several respects superior to existing flexible substrates in terms of their durability, conformability, and breathability. The present work adopts a modular approach to circuit fabrication, from which follow circuit design techniques and component packages optimized for use in fabric-based circuitry, flexible all-fabric interconnects, and multilayer circuits. While maintaining close compatibility with existing components, tools, and techniques, the present work demonstrates all steps of a process to create multilayer printed circuits on fabric substrates using conductive textiles."
}, {
    "id": "oai:dspace.mit.edu:1721.1/46589",
    "title": "FEMINIZM4ALL : a framework for feminist technology intervention",
    "abstract": "This thesis describes a feminist framework for technological interventions. I first define the problem by contrasting studies from psychology with research from other social sciences to determine that the primary reason for the gender imbalance in technological spaces is based in hostile work environments and not in the fact that women are disinterested as recent psychological research claims. This lack of diversity affects how technology products are shaped and how consumers interact with these artefacts. I outline a techno-feminist approach to intervention by looking at legislative and technological interventions into tech workspaces. Because this thesis is concerned with creating a framework for interventions rather than an individual technology, I describe different collaboration and production models typical to contemporary technology. These models are Web 2.0, open source software production, and collaborative platforms for distributing physical technology objects. In order to find out how to build a technological framework for making technology spaces more equitable for women, I created two projects. The first one is a Web 2.0 platform that provides data about gender and the technology workspace as well as instructions for visualizing it. The second one is a collaboration on a feminist technology for the workplace. The conclusion of the thesis is a description of future work based on these two projects.",
    "advisors": ["Chris Csikszentmihlyi"],
    "text": "FEMINIZM4ALL : a framework for feminist technology intervention This thesis describes a feminist framework for technological interventions. I first define the problem by contrasting studies from psychology with research from other social sciences to determine that the primary reason for the gender imbalance in technological spaces is based in hostile work environments and not in the fact that women are disinterested as recent psychological research claims. This lack of diversity affects how technology products are shaped and how consumers interact with these artefacts. I outline a techno-feminist approach to intervention by looking at legislative and technological interventions into tech workspaces. Because this thesis is concerned with creating a framework for interventions rather than an individual technology, I describe different collaboration and production models typical to contemporary technology. These models are Web 2.0, open source software production, and collaborative platforms for distributing physical technology objects. In order to find out how to build a technological framework for making technology spaces more equitable for women, I created two projects. The first one is a Web 2.0 platform that provides data about gender and the technology workspace as well as instructions for visualizing it. The second one is a collaboration on a feminist technology for the workplace. The conclusion of the thesis is a description of future work based on these two projects."
}, {
    "id": "oai:dspace.mit.edu:1721.1/69807",
    "title": "Expressive re-performance",
    "abstract": "Many music enthusiasts abandon music studies because they are frustrated by the amount of time and effort it takes to learn to play interesting songs. There are two major components to performance: the technical requirement of correctly playing the notes, and the emotional content conveyed through expressivity. While technical details like pitch and note order are largely set, expression, which is accomplished through timing, dynamics, vibrato, and timbre, is more personal. This thesis develops expressive re-performance, which entails the simplification of technical requirements of music-making to allow a user to experience music beyond his technical level, with particular focus on expression. Expressive re-performance aims to capture the fantasy and sound of a favorite recording by using audio extraction to split the original target solo and giving expressive control over that solo to a user. The re-performance experience starts with an electronic mimic of a traditional instrument with which the user steps-through a recording. Data generated from the users actions is parsed to determine note changes and expressive intent. Pitch is innate to the recording, allowing the user to concentrate on expressive gesture. Two pre-processing systems, analysis to discover note starts and extraction, are necessary. Extraction of the solo is done through user provided mimicry of the target combined with Probabalistic Latent Component Analysis with Dirichlet Hyperparameters. Audio elongation to match the users performance is performed using time-stretch. Instrument interfaces used were Akais Electronic Wind Controller (EWI), Fender's Squier Stratocaster Guitar and Controller, and a Wii-mote. Tests of the system and concept were performed using the EWI and Wii-mote for re-performance of two songs. User response indicated that while the interaction was fun, it did not succeed at enabling significant expression. Users expressed difficulty learning to use the EWI during the short test window and had insufficient interest in the offered songs. Both problems should be possible to overcome with further test time and system development. Users expressed interest in the concept of a real instrument mimic and found the audio extractions to be sufficient. Follow-on work to address issues discovered during the testing phase is needed to further validate the concept and explore means of developing expressive re-performance as a learning tool.",
    "advisors": ["Joseph A. Paradiso"],
    "text": "Expressive re-performance Many music enthusiasts abandon music studies because they are frustrated by the amount of time and effort it takes to learn to play interesting songs. There are two major components to performance: the technical requirement of correctly playing the notes, and the emotional content conveyed through expressivity. While technical details like pitch and note order are largely set, expression, which is accomplished through timing, dynamics, vibrato, and timbre, is more personal. This thesis develops expressive re-performance, which entails the simplification of technical requirements of music-making to allow a user to experience music beyond his technical level, with particular focus on expression. Expressive re-performance aims to capture the fantasy and sound of a favorite recording by using audio extraction to split the original target solo and giving expressive control over that solo to a user. The re-performance experience starts with an electronic mimic of a traditional instrument with which the user steps-through a recording. Data generated from the users actions is parsed to determine note changes and expressive intent. Pitch is innate to the recording, allowing the user to concentrate on expressive gesture. Two pre-processing systems, analysis to discover note starts and extraction, are necessary. Extraction of the solo is done through user provided mimicry of the target combined with Probabalistic Latent Component Analysis with Dirichlet Hyperparameters. Audio elongation to match the users performance is performed using time-stretch. Instrument interfaces used were Akais Electronic Wind Controller (EWI), Fender's Squier Stratocaster Guitar and Controller, and a Wii-mote. Tests of the system and concept were performed using the EWI and Wii-mote for re-performance of two songs. User response indicated that while the interaction was fun, it did not succeed at enabling significant expression. Users expressed difficulty learning to use the EWI during the short test window and had insufficient interest in the offered songs. Both problems should be possible to overcome with further test time and system development. Users expressed interest in the concept of a real instrument mimic and found the audio extractions to be sufficient. Follow-on work to address issues discovered during the testing phase is needed to further validate the concept and explore means of developing expressive re-performance as a learning tool."
}, {
    "id": "oai:dspace.mit.edu:1721.1/101790",
    "title": "Hacking innovation - group dynamics in innovation teams",
    "abstract": "Innovative work is gradually shifting towards collaborative efforts by larger, multidisciplinary teams, making team efficacy an increasingly important field of study. Researchers in this field have mainly focused on laboratory experiments, which may not fully capture the complex situations that teams encounter in real life. The alternative, field studies, are difficult to maintain and often require significant time to produce results. In this thesis we propose a methodology that bridges the gap between these two settings -- the laboratory and the field. By combining a new, affordable electronic badge that captures vocalization data with an innovative setting -- the Hackathon -- we create a new environment for studying team performance. This methodology reduces the duration and maintenance burden of such studies, and offers new opportunities for examining the effects of interventions on teamwork. The preliminary results from our studies show a variety of individual and team behaviors that can be captured in Hackathons using badges, such as participation, the parity of contribution to group discussions, the level of turn taking, and the frequency and duration of meetings. In a Hackathon, we measure these behaviors throughout the entire life cycle of each team, observe how they change in response to different shocks, and study how well the team members collaborate and perform as a team.",
    "advisors": ["Alex \"Sandy\" P. Pentland"],
    "text": "Hacking innovation - group dynamics in innovation teams Innovative work is gradually shifting towards collaborative efforts by larger, multidisciplinary teams, making team efficacy an increasingly important field of study. Researchers in this field have mainly focused on laboratory experiments, which may not fully capture the complex situations that teams encounter in real life. The alternative, field studies, are difficult to maintain and often require significant time to produce results. In this thesis we propose a methodology that bridges the gap between these two settings -- the laboratory and the field. By combining a new, affordable electronic badge that captures vocalization data with an innovative setting -- the Hackathon -- we create a new environment for studying team performance. This methodology reduces the duration and maintenance burden of such studies, and offers new opportunities for examining the effects of interventions on teamwork. The preliminary results from our studies show a variety of individual and team behaviors that can be captured in Hackathons using badges, such as participation, the parity of contribution to group discussions, the level of turn taking, and the frequency and duration of meetings. In a Hackathon, we measure these behaviors throughout the entire life cycle of each team, observe how they change in response to different shocks, and study how well the team members collaborate and perform as a team."
}, {
    "id": "oai:dspace.mit.edu:1721.1/106053",
    "title": "How communication technologies impact the size and composition of human collective memory",
    "abstract": "The ability of humans to accumulate knowledge and information across generations is a defining feature of our species. This ability depends on factors that range from the psychological biases that predispose us to learn from skillful and prestigious people, to the development of technologies for recording and communicating information: from clay tablets to the Internet. How do these communication technologies affect the size and composition of our human collective memory? Here we use two datasets on historical characters to present empirical evidence documenting how communication technologies have shaped human collective memory. We show that changes in communication technologies, including the introduction of movable type printing and shorter forms of printed media-such as newspapers, journals, and pamphlets-were accompanied by sharp changes (or breaks) in the per-capita number of memorable biographies from a given time period found in 'current online and offline sources. Moreover, changes in technology, such as the introduction of.printing, film and radio, and television, coincide with sharp changes in the occupations of the individuals present in these biographical records. These two empirical facts provide evidence in support of theories arguing that communication technologies are more consequential to society than the messages transmitted through them. Finally, this thesis contributes an update to the Pantheon dataset that includes historical geocoded data. We hope this updated version of the Pantheon dataset will enable future work documenting the effect of new communication technologies in ancient and modern civilizations.",
    "advisors": ["Cesar Hidalgo"],
    "text": "How communication technologies impact the size and composition of human collective memory The ability of humans to accumulate knowledge and information across generations is a defining feature of our species. This ability depends on factors that range from the psychological biases that predispose us to learn from skillful and prestigious people, to the development of technologies for recording and communicating information: from clay tablets to the Internet. How do these communication technologies affect the size and composition of our human collective memory? Here we use two datasets on historical characters to present empirical evidence documenting how communication technologies have shaped human collective memory. We show that changes in communication technologies, including the introduction of movable type printing and shorter forms of printed media-such as newspapers, journals, and pamphlets-were accompanied by sharp changes (or breaks) in the per-capita number of memorable biographies from a given time period found in 'current online and offline sources. Moreover, changes in technology, such as the introduction of.printing, film and radio, and television, coincide with sharp changes in the occupations of the individuals present in these biographical records. These two empirical facts provide evidence in support of theories arguing that communication technologies are more consequential to society than the messages transmitted through them. Finally, this thesis contributes an update to the Pantheon dataset that includes historical geocoded data. We hope this updated version of the Pantheon dataset will enable future work documenting the effect of new communication technologies in ancient and modern civilizations."
}, {
    "id": "oai:dspace.mit.edu:1721.1/36161",
    "title": "Ambient addition : how to turn urban noise into music",
    "abstract": "As human civilization devises ever more powerful machines, living among them may become more difficult. We may find ourselves surrounded by incidentally created sounds and noises which are out of synchronization with our momentary needs and discordant. Currently, legislating noise pollution is the only articulated solution and clearly it is not very effective. Our impression of sound, however, may be mediated and manipulated, transformed into something less jarring. So far, Walkmans and sound canceling headphones have done this, isolating us from noise but also from one another. In their place, a next generation headphone system is proposed which integrates environmental sound into a personal soundscape. It allows one to synthesize music from environmental sound using a number of digital signal processing (DSP) algorithms to create a sonic space in which the listener remains connected with his or her surroundings, is also cushioned from the most harsh and arrhythmic incursions and may also be drawn to appreciate the more subtle and elegant ones.",
    "advisors": ["Chris P. Csikszentmihlyi"],
    "text": "Ambient addition : how to turn urban noise into music As human civilization devises ever more powerful machines, living among them may become more difficult. We may find ourselves surrounded by incidentally created sounds and noises which are out of synchronization with our momentary needs and discordant. Currently, legislating noise pollution is the only articulated solution and clearly it is not very effective. Our impression of sound, however, may be mediated and manipulated, transformed into something less jarring. So far, Walkmans and sound canceling headphones have done this, isolating us from noise but also from one another. In their place, a next generation headphone system is proposed which integrates environmental sound into a personal soundscape. It allows one to synthesize music from environmental sound using a number of digital signal processing (DSP) algorithms to create a sonic space in which the listener remains connected with his or her surroundings, is also cushioned from the most harsh and arrhythmic incursions and may also be drawn to appreciate the more subtle and elegant ones."
}, {
    "id": "oai:dspace.mit.edu:1721.1/62083",
    "title": "Subjectified : personification as a design strategy in visual communication",
    "abstract": "When we encounter statistics too far removed from our personal experience, we sometimes find it difficult to imagine the real implications of that data. While we might understand the information logically, it can be hard to relate it to our immediate personal lives. In this thesis, I investigate a novel visual representation for such data, which I call Personification of Information. This alternative form of data visualization incorporates real people within the viewer's immediate physical or social environment as part of the representation. The goal of this visualization technique is to bring information that is otherwise perceived as distant and detached closer to the viewer. This design strategy is explored in three artistic projects, \"What If the World were your n Facebook friends?\", \"Unification-A Case Study?\" And \"What Was the Media Lab Thinking About In The Year _ _ _ ?\" They are complemented by two projects from other areas that investigate Personification as a design strategy to bring the abstract closer to the individual: \"Omnivisu\" uses Personification as an interface to architecture; \"Giving Character to Characters\" applies the strategy to augment digital typography with human expression. Additionally I formalize the findings of these projects as a set of generalized design parameters for Personification of Information.",
    "advisors": ["David Small"],
    "text": "Subjectified : personification as a design strategy in visual communication When we encounter statistics too far removed from our personal experience, we sometimes find it difficult to imagine the real implications of that data. While we might understand the information logically, it can be hard to relate it to our immediate personal lives. In this thesis, I investigate a novel visual representation for such data, which I call Personification of Information. This alternative form of data visualization incorporates real people within the viewer's immediate physical or social environment as part of the representation. The goal of this visualization technique is to bring information that is otherwise perceived as distant and detached closer to the viewer. This design strategy is explored in three artistic projects, \"What If the World were your n Facebook friends?\", \"Unification-A Case Study?\" And \"What Was the Media Lab Thinking About In The Year _ _ _ ?\" They are complemented by two projects from other areas that investigate Personification as a design strategy to bring the abstract closer to the individual: \"Omnivisu\" uses Personification as an interface to architecture; \"Giving Character to Characters\" applies the strategy to augment digital typography with human expression. Additionally I formalize the findings of these projects as a set of generalized design parameters for Personification of Information."
}, {
    "id": "oai:dspace.mit.edu:1721.1/61113",
    "title": "Curlybot : designing a new class of computational toys",
    "abstract": "I introduce an educational toy, called curlybot, as the basis for a new class of toys aimed at children in their early stages of development - ages four and up. curlybot is an autonomous two-wheeled vehicle with embedded electronics that can records how it has been moved on any flat surface and then plays back that motion accurately and repeatedly. Children can use curlybot to gain a strong intuition for advanced mathematical and computational concepts, like differential geometry, through play outside a traditional computer. Preliminary studies show that children can create gestures quickly, allowing them to iterate on the patterns that emerge, and successfully understanding and solving problems with curlybot. Programming by demonstration in this context makes the educational ideas implicit in the design of curlybot accessible to young children. curlybot can also act as an expressive tool because of its ability to remember the intricacies of the original gestures: every pause, acceleration, and even the shaking in the hand is recorded and played.",
    "advisors": ["Hiroshi Ishii"],
    "text": "Curlybot : designing a new class of computational toys I introduce an educational toy, called curlybot, as the basis for a new class of toys aimed at children in their early stages of development - ages four and up. curlybot is an autonomous two-wheeled vehicle with embedded electronics that can records how it has been moved on any flat surface and then plays back that motion accurately and repeatedly. Children can use curlybot to gain a strong intuition for advanced mathematical and computational concepts, like differential geometry, through play outside a traditional computer. Preliminary studies show that children can create gestures quickly, allowing them to iterate on the patterns that emerge, and successfully understanding and solving problems with curlybot. Programming by demonstration in this context makes the educational ideas implicit in the design of curlybot accessible to young children. curlybot can also act as an expressive tool because of its ability to remember the intricacies of the original gestures: every pause, acceleration, and even the shaking in the hand is recorded and played."
}, {
    "id": "oai:dspace.mit.edu:1721.1/82428",
    "title": "Beyond visualization : designing interfaces to contextualize geospatial data",
    "abstract": "The growing sensor data collections about our environment have the potential to drastically change our perception of the fragile world we live in. To make sense of such data, we commonly use visualization techniques, enabling public discourse and analysis. This thesis describes the design and implementation of a series of interactive systems that integrate geospatial sensor data visualization and terrain models with various user interface modalities in an educational context to support data analysis and knowledge building using part-digital, part-physical rendering. The main contribution of this thesis is a concrete application scenario and initial prototype of a \"Designed Environment\" where we can explore the relationship between the surface of Japan's islands, the tension that originates in the fault lines along the seafloor beneath its east coast, and the resulting natural disasters. The system is able to import geospatial data from a multitude of sources on the \"Spatial Web\", bringing us one step closer to a tangible \"dashboard of the Earth.\"",
    "advisors": ["Hiroshi Ishii"],
    "text": "Beyond visualization : designing interfaces to contextualize geospatial data The growing sensor data collections about our environment have the potential to drastically change our perception of the fragile world we live in. To make sense of such data, we commonly use visualization techniques, enabling public discourse and analysis. This thesis describes the design and implementation of a series of interactive systems that integrate geospatial sensor data visualization and terrain models with various user interface modalities in an educational context to support data analysis and knowledge building using part-digital, part-physical rendering. The main contribution of this thesis is a concrete application scenario and initial prototype of a \"Designed Environment\" where we can explore the relationship between the surface of Japan's islands, the tension that originates in the fault lines along the seafloor beneath its east coast, and the resulting natural disasters. The system is able to import geospatial data from a multitude of sources on the \"Spatial Web\", bringing us one step closer to a tangible \"dashboard of the Earth.\""
}, {
    "id": "oai:dspace.mit.edu:1721.1/95597",
    "title": "Engineering serendipity : Terra Incognita and other strange encounters with global news",
    "abstract": "There is a significant body of research that shows that people tend to congregate with others like them and favor information that confirms their existing views. With declining global news coverage and the rise of personalized news feeds and social media, there is concern that our forms of information consumption do not support encountering sufficient information about other cultures and places to make us effective citizens of the world. This thesis reviews these arguments and proposes a design intervention called \"Terra Incognita: 1000 Cities of the World\" to help address the geographic dimension of information diversity. Terra Incognita brings together aspects of serendipitous information discovery, personal informatics and \"nudge\" applications to provide users with multiple daily opportunities to explore faraway cities by reading global news recommendations. This study shows that while Terra Incognita did not shift user behavior in aggregate towards reading about more diverse places, it did make them curious about new places, prompted them to reflect and broadened their horizons. The final chapter offers guidance for designers who might aspire to create applications at the intersection of personal behavior change and news media.",
    "advisors": ["Ethan Zuckerman"],
    "text": "Engineering serendipity : Terra Incognita and other strange encounters with global news There is a significant body of research that shows that people tend to congregate with others like them and favor information that confirms their existing views. With declining global news coverage and the rise of personalized news feeds and social media, there is concern that our forms of information consumption do not support encountering sufficient information about other cultures and places to make us effective citizens of the world. This thesis reviews these arguments and proposes a design intervention called \"Terra Incognita: 1000 Cities of the World\" to help address the geographic dimension of information diversity. Terra Incognita brings together aspects of serendipitous information discovery, personal informatics and \"nudge\" applications to provide users with multiple daily opportunities to explore faraway cities by reading global news recommendations. This study shows that while Terra Incognita did not shift user behavior in aggregate towards reading about more diverse places, it did make them curious about new places, prompted them to reflect and broadened their horizons. The final chapter offers guidance for designers who might aspire to create applications at the intersection of personal behavior change and news media."
}, {
    "id": "oai:dspace.mit.edu:1721.1/107564",
    "title": "(Im)possible baby : how to stimulate discussions about possibilities of two-mum and two-dad children",
    "abstract": "(lm)possible Baby is a speculative design project which aims to stimulate discussions about the social, cultural and ethical implications of emerging biotechnologies that could enable same-sex couples to have their own, genetically related children. Delivering a baby from same-sex parents is starting to not look like a sci-fi dream anymore recent developments in genetics and stem cell research, such as the achievements of scientists from Cambridge University in England and Israel's Weizmann Institute of Science 1 have made this dream much closer to reality. Jacob Hanna, the specialist leading the project's Israeli arm, said it may be possible to use the technique to create a baby in just two years. \"It has already caused interest from gay groups because of the possibility of making egg and sperm cells from parents of the same sex,\" he said.\"2 Is creating a baby from same-sex parents the ethical thing to do? Who has the right to decide this, and how? This project aims to design and inspire debate about the bioethics of producing babies from same-sex couples. In this project, the DNA data of a lesbian couple was analyzed using 23andMe to simulate and visualize their potential children, and then we created a set of fictional, \"what if' future family photos using this information to produce a hardcover album which was presented to the couple as a gift. To achieve more public outreach, we worked with the Japanese national television service, NHK, to create a 30-minute documentary film following the whole process, which aired in October 2015.",
    "advisors": ["Hiromi Ozaki"],
    "text": "(Im)possible baby : how to stimulate discussions about possibilities of two-mum and two-dad children (lm)possible Baby is a speculative design project which aims to stimulate discussions about the social, cultural and ethical implications of emerging biotechnologies that could enable same-sex couples to have their own, genetically related children. Delivering a baby from same-sex parents is starting to not look like a sci-fi dream anymore recent developments in genetics and stem cell research, such as the achievements of scientists from Cambridge University in England and Israel's Weizmann Institute of Science 1 have made this dream much closer to reality. Jacob Hanna, the specialist leading the project's Israeli arm, said it may be possible to use the technique to create a baby in just two years. \"It has already caused interest from gay groups because of the possibility of making egg and sperm cells from parents of the same sex,\" he said.\"2 Is creating a baby from same-sex parents the ethical thing to do? Who has the right to decide this, and how? This project aims to design and inspire debate about the bioethics of producing babies from same-sex couples. In this project, the DNA data of a lesbian couple was analyzed using 23andMe to simulate and visualize their potential children, and then we created a set of fictional, \"what if' future family photos using this information to produce a hardcover album which was presented to the couple as a gift. To achieve more public outreach, we worked with the Japanese national television service, NHK, to create a 30-minute documentary film following the whole process, which aired in October 2015."
}, {
    "id": "oai:dspace.mit.edu:1721.1/41752",
    "title": "Exploiting object dynamics for recognition and control",
    "abstract": "This thesis explores how state-of-the-art object recognition methods can benefit from integrating information across multiple observations of an object. Considered are active vision systems that allow to steer the camera along predetermined trajectories, resulting in sweeps of ordered views of an object. For systems of this kind, a solution is presented that exploits the order relationship between successive frames to derive a classifier based on the characteristic motion of local features across the sweep. It is shown that this motion model reveals structural information about the object that can be exploited for recognition. The main contribution of this thesis is a recognition system that extends invariant local features (shape context) into the time domain by adding the mentioned feature motion model into a joint classifier. Second, an entropy-based view selection scheme is presented that allows the vision system to skip ahead to highly discriminative viewing positions. Using two datasets, one standard (ETH-80) and one collected from our robot head, both feature motion and active view selection extensions are shown to achieve a higher-quality hypothesis about the presented object quicker than a baseline system treating object views as an unordered stream of images.",
    "advisors": ["Deb Roy"],
    "text": "Exploiting object dynamics for recognition and control This thesis explores how state-of-the-art object recognition methods can benefit from integrating information across multiple observations of an object. Considered are active vision systems that allow to steer the camera along predetermined trajectories, resulting in sweeps of ordered views of an object. For systems of this kind, a solution is presented that exploits the order relationship between successive frames to derive a classifier based on the characteristic motion of local features across the sweep. It is shown that this motion model reveals structural information about the object that can be exploited for recognition. The main contribution of this thesis is a recognition system that extends invariant local features (shape context) into the time domain by adding the mentioned feature motion model into a joint classifier. Second, an entropy-based view selection scheme is presented that allows the vision system to skip ahead to highly discriminative viewing positions. Using two datasets, one standard (ETH-80) and one collected from our robot head, both feature motion and active view selection extensions are shown to achieve a higher-quality hypothesis about the presented object quicker than a baseline system treating object views as an unordered stream of images."
}, {
    "id": "oai:dspace.mit.edu:1721.1/61839",
    "title": "Using high-bandwidth input/output in interactive art",
    "abstract": "Are we making the best use of commonly available high-bandwidth input/output ( I/O) devices on our computers? How would research on this subject be affected if it were driven by a purely artistic mandate? The bandwidth in question refers specifically to video input and output devices, the only high-bandwidth devices that are on found on common, conventional computers. Under normal circumstances, these devices transmit moving two dimensional images at rapid refresh rates; this high-bandwidth is a prerequisite for the capturing and viewing of motion images. A great potential exists in using this high-throughput capacity in applications that do not simply convey continuous moving images. In the burgeoning field of highly technological interactive art, a large number of works suffer from poorly adapted interface mechanisms. New high-bandwidth I/O configurations can serve to derive improved interfaces for the creation of interactive art. This course of research is not driven solely by the desire to create new technology and improved modes of interaction. As the the infusion of rapid-changing technology in art reaches popular levels, the role of the artist in society is equally in flux. The definition of such a role is sought as part of this thesis. These goals are accomplished through the study of the nature and history of interactivity in art, the development of new prototypes, the creation and exhibition of interactive art works in public spaces, and through a close analysis of the role of the artist-scientist in contemporary society.",
    "advisors": ["John Maeda"],
    "text": "Using high-bandwidth input/output in interactive art Are we making the best use of commonly available high-bandwidth input/output ( I/O) devices on our computers? How would research on this subject be affected if it were driven by a purely artistic mandate? The bandwidth in question refers specifically to video input and output devices, the only high-bandwidth devices that are on found on common, conventional computers. Under normal circumstances, these devices transmit moving two dimensional images at rapid refresh rates; this high-bandwidth is a prerequisite for the capturing and viewing of motion images. A great potential exists in using this high-throughput capacity in applications that do not simply convey continuous moving images. In the burgeoning field of highly technological interactive art, a large number of works suffer from poorly adapted interface mechanisms. New high-bandwidth I/O configurations can serve to derive improved interfaces for the creation of interactive art. This course of research is not driven solely by the desire to create new technology and improved modes of interaction. As the the infusion of rapid-changing technology in art reaches popular levels, the role of the artist in society is equally in flux. The definition of such a role is sought as part of this thesis. These goals are accomplished through the study of the nature and history of interactivity in art, the development of new prototypes, the creation and exhibition of interactive art works in public spaces, and through a close analysis of the role of the artist-scientist in contemporary society."
}, {
    "id": "oai:dspace.mit.edu:1721.1/55195",
    "title": "Affect reflection technology in face-to-face service encounters",
    "abstract": "This thesis examines the role of facial expressions in dyadic interactions between a banking service provider and customer. We conduct experiments in which service providers manipulate their facial expressions while interacting with customers in one of three conditions: In the neutral condition the banker tried to maintain a neutral facial expression; in the smiling condition the banker tried to smile throughout the interaction; in the empathetic condition the banker tried to respond with the same or complementary facial expressions. Results show that the customers (n=46) were more satisfied with the interaction when they perceived the service provider was empathetic. More significantly, the service provider and customer shared synchronized facial expressions with many prolonged smiles, when customers said the service provider was empathetic. We suggested three different criteria to investigate customer satisfaction as follows; according to what the service provider tried to convey, what the customer perceived and what was actually detected in their interactions. According to the analysis of the interactions, smiling bankers who shared smiles were evaluated as the best while smiling bankers who did not share smiles with customers were appraised similar to non-smiling bankers.",
    "advisors": ["Rosalind W. Picard"],
    "text": "Affect reflection technology in face-to-face service encounters This thesis examines the role of facial expressions in dyadic interactions between a banking service provider and customer. We conduct experiments in which service providers manipulate their facial expressions while interacting with customers in one of three conditions: In the neutral condition the banker tried to maintain a neutral facial expression; in the smiling condition the banker tried to smile throughout the interaction; in the empathetic condition the banker tried to respond with the same or complementary facial expressions. Results show that the customers (n=46) were more satisfied with the interaction when they perceived the service provider was empathetic. More significantly, the service provider and customer shared synchronized facial expressions with many prolonged smiles, when customers said the service provider was empathetic. We suggested three different criteria to investigate customer satisfaction as follows; according to what the service provider tried to convey, what the customer perceived and what was actually detected in their interactions. According to the analysis of the interactions, smiling bankers who shared smiles were evaluated as the best while smiling bankers who did not share smiles with customers were appraised similar to non-smiling bankers."
}, {
    "id": "oai:dspace.mit.edu:1721.1/76573",
    "title": "Advanced prototyping of variable impedance prosthetic sockets for trans-tibial amputees : polyjet matrix 3D printing of comfortable prosthetic sockets using digital anatomical data",
    "abstract": "This work, supported by the Media Lab Consortium, evaluates the design of a Variable Impedance Prosthetic (VIPr) socket for a transtibial amputee using computer-aided design and manufacturing (CAD/CAM). Compliant features are seamlessly integrated into a 3D printed socket to achieve lower interface peak pressures over bony protuberances by using anthropomorphic data acquired through surface scanning and magnetic resonance imaging techniques. An inverse linear mathematical transformation spatially maps quantitative measurements (bone tissue depth) of the human residual limb to the corresponding socket shape and impedance characteristics. The CAD/CAM VIPr socket is compared to a state-of-the-art prosthetic socket of similar internal geometry and shape, designed by a prosthetist using conventional methods. An active, bilateral transtibial male amputee of weight 70 kg walks on a force plate loaded 5-meter walkway, at self-selected speeds while synchronized ground reaction forces, motion capture data and socket residual limb interface pressures are measured for the evaluated sockets. We anticipated a decreased average interface pressure (measured using the Teksan F-SocketTM pressure sensors) in the VIPr socket, especially over stiff anatomical landmarks including the fibula head, the tibia, lateral and medial femoral condyles and medial tibial flare. Contact interface pressure recorded during stance of a complete gait cycle indicated a 15% and 17% reduction at toe-off and heel-strike respectively at the fibula head while the subject uses a VIPr socket in comparison to a conventional socket of similar internal shape. A corresponding 7% and 8% reduction in pressure is observed along the tibia. Similar trends of high-pressure reductions are observed during stair ascent trials with the VIPr socket.",
    "advisors": ["Hugh Herr"],
    "text": "Advanced prototyping of variable impedance prosthetic sockets for trans-tibial amputees : polyjet matrix 3D printing of comfortable prosthetic sockets using digital anatomical data This work, supported by the Media Lab Consortium, evaluates the design of a Variable Impedance Prosthetic (VIPr) socket for a transtibial amputee using computer-aided design and manufacturing (CAD/CAM). Compliant features are seamlessly integrated into a 3D printed socket to achieve lower interface peak pressures over bony protuberances by using anthropomorphic data acquired through surface scanning and magnetic resonance imaging techniques. An inverse linear mathematical transformation spatially maps quantitative measurements (bone tissue depth) of the human residual limb to the corresponding socket shape and impedance characteristics. The CAD/CAM VIPr socket is compared to a state-of-the-art prosthetic socket of similar internal geometry and shape, designed by a prosthetist using conventional methods. An active, bilateral transtibial male amputee of weight 70 kg walks on a force plate loaded 5-meter walkway, at self-selected speeds while synchronized ground reaction forces, motion capture data and socket residual limb interface pressures are measured for the evaluated sockets. We anticipated a decreased average interface pressure (measured using the Teksan F-SocketTM pressure sensors) in the VIPr socket, especially over stiff anatomical landmarks including the fibula head, the tibia, lateral and medial femoral condyles and medial tibial flare. Contact interface pressure recorded during stance of a complete gait cycle indicated a 15% and 17% reduction at toe-off and heel-strike respectively at the fibula head while the subject uses a VIPr socket in comparison to a conventional socket of similar internal shape. A corresponding 7% and 8% reduction in pressure is observed along the tibia. Similar trends of high-pressure reductions are observed during stair ascent trials with the VIPr socket."
}, {
    "id": "oai:dspace.mit.edu:1721.1/62358",
    "title": "Interactive cinema : collaborative expression with digital video",
    "abstract": "Advances in technologies for digital video editing and streaming have lowered the barrier to entry for aspiring videomakers, and they provide an opportunity to expand the vocabulary for using and sharing video. Custom interfaces for editing and sharing video can suggest and support novel methods of collaborative production, cinematic narration, and casual dialogue with media. This thesis research presents Individeo, an online application with for video browsing and editing, and explores how interface design can enable closer collaboration among online videographers. The thesis evaluates Individeo's custom interfaces through Honeymoon, an experimental collaborative video production, in which geographically separated videomakers attempt to build a cinematic narrative together through online collaboration.",
    "advisors": ["Glorianna Davenport"],
    "text": "Interactive cinema : collaborative expression with digital video Advances in technologies for digital video editing and streaming have lowered the barrier to entry for aspiring videomakers, and they provide an opportunity to expand the vocabulary for using and sharing video. Custom interfaces for editing and sharing video can suggest and support novel methods of collaborative production, cinematic narration, and casual dialogue with media. This thesis research presents Individeo, an online application with for video browsing and editing, and explores how interface design can enable closer collaboration among online videographers. The thesis evaluates Individeo's custom interfaces through Honeymoon, an experimental collaborative video production, in which geographically separated videomakers attempt to build a cinematic narrative together through online collaboration."
}, {
    "id": "oai:dspace.mit.edu:1721.1/37386",
    "title": "Influence modeling of complex stochastic processes",
    "abstract": "A complex stochastic process involving human behaviors or human group behaviors is computationally hard to model with a hidden Markov process. This is because the state space of such behaviors is often a Cartesian product of a large number of constituent probability spaces, and is exponentially large. A sample for those stochastic processes is normally composed of a large collection of heterogeneous constituent samples. How to combine those heterogeneous constituent samples in a consistent and stable way is another difficulty for the hidden Markov process modeling. A latent structure influence process models human behaviors and human group behaviors by emulating the work of a team of experts. In such a team, each expert concentrates on one constituent probability space, investigates one type of constituent samples, and/or employ one type of technique. An expert improves his work by considering the results from the other experts, instead of the raw data for them. Compared with the hidden Markov process, the latent structure influence process is more expressive, more stable to outliers, and less likely to overfit. It can be used to study the interaction of over 100 persons and get good results.",
    "advisors": ["Alex (Sandy) Pentland"],
    "text": "Influence modeling of complex stochastic processes A complex stochastic process involving human behaviors or human group behaviors is computationally hard to model with a hidden Markov process. This is because the state space of such behaviors is often a Cartesian product of a large number of constituent probability spaces, and is exponentially large. A sample for those stochastic processes is normally composed of a large collection of heterogeneous constituent samples. How to combine those heterogeneous constituent samples in a consistent and stable way is another difficulty for the hidden Markov process modeling. A latent structure influence process models human behaviors and human group behaviors by emulating the work of a team of experts. In such a team, each expert concentrates on one constituent probability space, investigates one type of constituent samples, and/or employ one type of technique. An expert improves his work by considering the results from the other experts, instead of the raw data for them. Compared with the hidden Markov process, the latent structure influence process is more expressive, more stable to outliers, and less likely to overfit. It can be used to study the interaction of over 100 persons and get good results."
}, {
    "id": "oai:dspace.mit.edu:1721.1/61547",
    "title": "In form",
    "abstract": "Spatial computing is human interaction with a machine in which the machine retains and manipulates referents to real objects and spaces. It is an essential component for making our machines fuller partners in our work and play. This thesis presents a series of experiments in the discipline and analysis of its fundamental properties.",
    "advisors": ["John Maeda"],
    "text": "In form Spatial computing is human interaction with a machine in which the machine retains and manipulates referents to real objects and spaces. It is an essential component for making our machines fuller partners in our work and play. This thesis presents a series of experiments in the discipline and analysis of its fundamental properties."
}, {
    "id": "oai:dspace.mit.edu:1721.1/55190",
    "title": "Configurable dynamic privacy for pervasive sensor networks",
    "abstract": "Ubiquitous computing sensor networks have greatly augmented the functionality of interactive media systems by adding the ability to capture and store activity-related information. Analyzing the information recorded from pervasive sensor networks can provide insight about human behavior for better personalized system services, as well as richer media content and social communication. With these increased capabilities, serious concerns which create great obstacles to the deployment of such network are raised with regard to privacy and boundaries. However, there exist no real data currently about privacy in pervasive media networks and most studies that have been made so far are speculative. This thesis presents the design and implementation of a configurable infrastructure that can protect users' dynamic levels of privacy in a pervasive sensor network. Through an active badge system, users have different options to disable each type of data transmission. This work evaluates approaches for privacy protection through conducting an extensive user study in an actual ubiquitous invasive sensing environment to obtain feedback via sensor system data and questionnaires and correlates that information for future reference in the design of privacy-protected ubiquitous sensor networks. Results from the user study indicated that an active badge for on-site control, especially periodically broadcast RF beacon for privacy control, is the most effective and acceptable method.",
    "advisors": ["Joseph A. Paradiso"],
    "text": "Configurable dynamic privacy for pervasive sensor networks Ubiquitous computing sensor networks have greatly augmented the functionality of interactive media systems by adding the ability to capture and store activity-related information. Analyzing the information recorded from pervasive sensor networks can provide insight about human behavior for better personalized system services, as well as richer media content and social communication. With these increased capabilities, serious concerns which create great obstacles to the deployment of such network are raised with regard to privacy and boundaries. However, there exist no real data currently about privacy in pervasive media networks and most studies that have been made so far are speculative. This thesis presents the design and implementation of a configurable infrastructure that can protect users' dynamic levels of privacy in a pervasive sensor network. Through an active badge system, users have different options to disable each type of data transmission. This work evaluates approaches for privacy protection through conducting an extensive user study in an actual ubiquitous invasive sensing environment to obtain feedback via sensor system data and questionnaires and correlates that information for future reference in the design of privacy-protected ubiquitous sensor networks. Results from the user study indicated that an active badge for on-site control, especially periodically broadcast RF beacon for privacy control, is the most effective and acceptable method."
}, {
    "id": "oai:dspace.mit.edu:1721.1/32509",
    "title": "Self-* properties of multi sensing entities in smart environments",
    "abstract": "Computers and sensors are more and more often embedded into everyday objects, woven into garments, \"painted\" on architecture or deployed directly into the environment. They monitor the environment, process the information and extract knowledge that their designed and programmers hope will be interesting. As the number and variety of these sensors and their connections increase, so does the complexity of the networks in which they operate. Deployment, management, and repair become difficult to perform manually. It is, then, particularly appealing to design a software architecture that can achieve the necessary organizational structures without requiring human intervention. Focusing on image sensing and machine vision techniques, we propose to investigate how small, unspecialized, low-processing sensing entities can self-organize to create a scalable, fault tolerant, decentralized, and easily reconfigurable system for smart environments and how these entities self-adapt to optimize their contribution in the presence of constraints inherent to sensor networks.",
    "advisors": ["V. Michael Bove, Jr"],
    "text": "Self-* properties of multi sensing entities in smart environments Computers and sensors are more and more often embedded into everyday objects, woven into garments, \"painted\" on architecture or deployed directly into the environment. They monitor the environment, process the information and extract knowledge that their designed and programmers hope will be interesting. As the number and variety of these sensors and their connections increase, so does the complexity of the networks in which they operate. Deployment, management, and repair become difficult to perform manually. It is, then, particularly appealing to design a software architecture that can achieve the necessary organizational structures without requiring human intervention. Focusing on image sensing and machine vision techniques, we propose to investigate how small, unspecialized, low-processing sensing entities can self-organize to create a scalable, fault tolerant, decentralized, and easily reconfigurable system for smart environments and how these entities self-adapt to optimize their contribution in the presence of constraints inherent to sensor networks."
}, {
    "id": "oai:dspace.mit.edu:1721.1/120697",
    "title": "PictureBlocks : constructing and deconstructing picture-driven literacy development",
    "abstract": "Pictures play an important role in aiding literacy development amongst children. Present day educational apps for children take advantage of pictures in an instructionst manner - such as a flashcard, drag-and-drop, or fill-in-the-blanks approach. However, research indicates that following a constructionist approach rather than instructionst, where children actively construct meaningful projects playfully, leads to better engagement and learning. It is also universally established that children across the world enjoy creating and drawing pictures as a means of self-expression. Despite the evidence from the literature and the data, there is a lack of constructionist approaches towards picture-based learning apps for children. The goals of this thesis are two-fold: 1. Successfully design and evaluate a picture-based, constructionist literacy learning app in order to address this gap. 2. Explore the unique affordances/implications that this exploratory approach has on children's self-expression and learning. This app is called PictureBlocks, and it is designed for children between the ages of 5-9 years. The design of PictureBlocks is refined through several rounds of playtesting. Finally, a 15-day pilot study conducted in children's homes helps evaluate the app's design. Data analysis and findings also establish unique affordances and future implications for picture-based, constructionist learning apps.",
    "advisors": ["Deb Roy"],
    "text": "PictureBlocks : constructing and deconstructing picture-driven literacy development Pictures play an important role in aiding literacy development amongst children. Present day educational apps for children take advantage of pictures in an instructionst manner - such as a flashcard, drag-and-drop, or fill-in-the-blanks approach. However, research indicates that following a constructionist approach rather than instructionst, where children actively construct meaningful projects playfully, leads to better engagement and learning. It is also universally established that children across the world enjoy creating and drawing pictures as a means of self-expression. Despite the evidence from the literature and the data, there is a lack of constructionist approaches towards picture-based learning apps for children. The goals of this thesis are two-fold: 1. Successfully design and evaluate a picture-based, constructionist literacy learning app in order to address this gap. 2. Explore the unique affordances/implications that this exploratory approach has on children's self-expression and learning. This app is called PictureBlocks, and it is designed for children between the ages of 5-9 years. The design of PictureBlocks is refined through several rounds of playtesting. Finally, a 15-day pilot study conducted in children's homes helps evaluate the app's design. Data analysis and findings also establish unique affordances and future implications for picture-based, constructionist learning apps."
}, {
    "id": "oai:dspace.mit.edu:1721.1/37398",
    "title": "An ad-hoc wireless communication system",
    "abstract": "This thesis studies the challenges of providing load balancing and fault-tolerant external links between ad-hoc multicast mesh networks. The work is the gateway component of a research platform called FluidVoice, a wireless audio communication system. This system consists of nodes forming a broadcast mesh based on 802.11. Some of these nodes called Stargates have the capability to communicate to the external world. The problem is that these gateways can fail or lose capacity unexpectedly. In this work we explore the ways to provide communications to the external world under unexpected gateway node failures, and variance of load. We propose and evaluate a distributed algorithm designed to form this robust and balanced interconnection. The algorithm is designed with robustness in mind, and takes into account failures in the outbound links as well as between the gateways, and it is focused to support real-time applications running over it. In this thesis we show that by adopting this algorithm, we can provide a reliable connection to the end-user even as gateways presence or capacity varies. The prototype version has about 20ms of additional transmission time in average, with an overhead of about 5% to 35% depending on the packet size, and a recovery time of 1 to 3 seconds. The redundant traffic generated in intermediate steps of the optimization problem can grow up proportionally to the number of participating gateway nodes, and reduces quickly to only the required amount of traffic.",
    "advisors": ["Andrew B. Lippman"],
    "text": "An ad-hoc wireless communication system This thesis studies the challenges of providing load balancing and fault-tolerant external links between ad-hoc multicast mesh networks. The work is the gateway component of a research platform called FluidVoice, a wireless audio communication system. This system consists of nodes forming a broadcast mesh based on 802.11. Some of these nodes called Stargates have the capability to communicate to the external world. The problem is that these gateways can fail or lose capacity unexpectedly. In this work we explore the ways to provide communications to the external world under unexpected gateway node failures, and variance of load. We propose and evaluate a distributed algorithm designed to form this robust and balanced interconnection. The algorithm is designed with robustness in mind, and takes into account failures in the outbound links as well as between the gateways, and it is focused to support real-time applications running over it. In this thesis we show that by adopting this algorithm, we can provide a reliable connection to the end-user even as gateways presence or capacity varies. The prototype version has about 20ms of additional transmission time in average, with an overhead of about 5% to 35% depending on the packet size, and a recovery time of 1 to 3 seconds. The redundant traffic generated in intermediate steps of the optimization problem can grow up proportionally to the number of participating gateway nodes, and reduces quickly to only the required amount of traffic."
}, {
    "id": "oai:dspace.mit.edu:1721.1/37397",
    "title": "RadioActive : enabling large-scale asynchronous audio discussions on mobile devices",
    "abstract": "Current mobile technology works well to connect individuals together at any time or place. However, general focus on one-to-one conversations has overlooked the potential of always-on group and community links. I hypothesize that asynchronous persistent audio is a superior medium to support scalable always-on group communication for mobile devices. To evaluate this claim, one must first have an adequate interaction design before its possible to investigate the qualities and usage patterns over the long-term. This design does not exist for mobile devices. This thesis takes the first step in this direction by creating and evaluating an initial design called RadioActive. RadioActive is a technological and interaction design for persistent mobile audio chat spaces, focusing on the key issue of navigating asynchronous audio. If RadioActive is shown to be a good design in the long-term, I hope to prove with additional studies the value of asynchronous persistent audio. In this thesis I examine related work, describe RadioActive from a methodologically constrained bottom-up approach, discuss the theoretical rationale behind the design, what seems to work, what doesn't, and suggestions for the future.",
    "advisors": ["Judith S. Donath"],
    "text": "RadioActive : enabling large-scale asynchronous audio discussions on mobile devices Current mobile technology works well to connect individuals together at any time or place. However, general focus on one-to-one conversations has overlooked the potential of always-on group and community links. I hypothesize that asynchronous persistent audio is a superior medium to support scalable always-on group communication for mobile devices. To evaluate this claim, one must first have an adequate interaction design before its possible to investigate the qualities and usage patterns over the long-term. This design does not exist for mobile devices. This thesis takes the first step in this direction by creating and evaluating an initial design called RadioActive. RadioActive is a technological and interaction design for persistent mobile audio chat spaces, focusing on the key issue of navigating asynchronous audio. If RadioActive is shown to be a good design in the long-term, I hope to prove with additional studies the value of asynchronous persistent audio. In this thesis I examine related work, describe RadioActive from a methodologically constrained bottom-up approach, discuss the theoretical rationale behind the design, what seems to work, what doesn't, and suggestions for the future."
}, {
    "id": "oai:dspace.mit.edu:1721.1/107574",
    "title": "Emotive Materials : towards a shared language of the meaning of materials",
    "abstract": "Due to advances in design generation and digital fabrication, novice designers are able to access more and more tools to bring their visions to life. As materials begin to evolve and change shape, having a set of rules with which to evaluate, interpret, and design them will become increasingly important. In moving towards tools that allow us to design and create our own materials these the two worlds of creation and curation must be (re)connected: in this work I strive to quantify and understand the emotive aspects of materials, such as haptic responses to, cognitive evaluation of, and emotive perception of materials; in order to understand how materials communicate meaning. My aim is to produce a set of guidelines that enable designers and scientists to communicate and help creators understand the implications of emerging material combinations. For those without the resources to conduct time intensive user studies for every project and without the intuitive knowledge of a professional, it can be very difficult to predict the implications of materials and their impact on the interaction. In this thesis, a repeatable methodology for exploring these impacts was implemented and evaluated. As a result, it will be possible to create a holistic material selection process. By combining materials to maximize properties, I plan to go beyond existing databases and fabricate objects designed to evoke specific reactions. Developing an effective methodology would enable fabrication of more engaging objects. Through this research, I plan to establish guidelines and provide a common language that enables designers to influence materials development and connect designers and researchers in a more effective way than is currently possible. This will promote unique research of materials and expand their range of use. Such a tool will enable new design practices by adding emotive factors that are not rigorously understood to the material selection and fabrication process. At its core, materials science is the study of how the structure and processing of materials impact the properties of compounds. I plan to help designers and scientists go one step further, and use material combinations to connect directly with the end user.",
    "advisors": ["V. Michael Bove, Jr"],
    "text": "Emotive Materials : towards a shared language of the meaning of materials Due to advances in design generation and digital fabrication, novice designers are able to access more and more tools to bring their visions to life. As materials begin to evolve and change shape, having a set of rules with which to evaluate, interpret, and design them will become increasingly important. In moving towards tools that allow us to design and create our own materials these the two worlds of creation and curation must be (re)connected: in this work I strive to quantify and understand the emotive aspects of materials, such as haptic responses to, cognitive evaluation of, and emotive perception of materials; in order to understand how materials communicate meaning. My aim is to produce a set of guidelines that enable designers and scientists to communicate and help creators understand the implications of emerging material combinations. For those without the resources to conduct time intensive user studies for every project and without the intuitive knowledge of a professional, it can be very difficult to predict the implications of materials and their impact on the interaction. In this thesis, a repeatable methodology for exploring these impacts was implemented and evaluated. As a result, it will be possible to create a holistic material selection process. By combining materials to maximize properties, I plan to go beyond existing databases and fabricate objects designed to evoke specific reactions. Developing an effective methodology would enable fabrication of more engaging objects. Through this research, I plan to establish guidelines and provide a common language that enables designers to influence materials development and connect designers and researchers in a more effective way than is currently possible. This will promote unique research of materials and expand their range of use. Such a tool will enable new design practices by adding emotive factors that are not rigorously understood to the material selection and fabrication process. At its core, materials science is the study of how the structure and processing of materials impact the properties of compounds. I plan to help designers and scientists go one step further, and use material combinations to connect directly with the end user."
}, {
    "id": "oai:dspace.mit.edu:1721.1/78213",
    "title": "Designing complementary communication systems",
    "abstract": "We have long assumed that being face to face is the best environment for social interaction. But is \"being there\" the best we can aspire to? One common approach to improving face-to-face contexts is to add new communication channels - a strategy often described as creating \"backchannels.\" In my work, I use a series of novel complementary communication systems to show how adding communication platforms to collaborative situations can be useful while also arguing for a new conceptual model of a main stage and a side stage (in the Goffman sense) that contrasts with the traditional model of backchannels. I describe a series of projects that embody this approach and explore its limits. My work covers virtual world meetings and presentations, an audience interaction tool for large groups (backchan.nl), a tablet-based system for small group discussions (Tin Can), and a platform for connecting huge distributed audiences (ROAR). In each of these projects I trace my three major research themes: understanding how conversational grounding operates in these environments, how non-verbal actions complement text-based interaction, and how people make decisions about how to manage their attention in environments with multiple simultaneous communication channels.",
    "advisors": ["Chris Schmandt"],
    "text": "Designing complementary communication systems We have long assumed that being face to face is the best environment for social interaction. But is \"being there\" the best we can aspire to? One common approach to improving face-to-face contexts is to add new communication channels - a strategy often described as creating \"backchannels.\" In my work, I use a series of novel complementary communication systems to show how adding communication platforms to collaborative situations can be useful while also arguing for a new conceptual model of a main stage and a side stage (in the Goffman sense) that contrasts with the traditional model of backchannels. I describe a series of projects that embody this approach and explore its limits. My work covers virtual world meetings and presentations, an audience interaction tool for large groups (backchan.nl), a tablet-based system for small group discussions (Tin Can), and a platform for connecting huge distributed audiences (ROAR). In each of these projects I trace my three major research themes: understanding how conversational grounding operates in these environments, how non-verbal actions complement text-based interaction, and how people make decisions about how to manage their attention in environments with multiple simultaneous communication channels."
}, {
    "id": "oai:dspace.mit.edu:1721.1/107578",
    "title": "Making machines that make : object-oriented hardware meets object-oriented software",
    "abstract": "Rapid prototyping has been in the limelight for the past decade. 3D printers have an evocative name that promises production of complex parts on demand. Yet current practice doesn't quite deliver on these promises of advanced manufacturing. Existing digital fabrication tools enable repeatability and precision by using codes to describe machine actions. But the infrastructure used for digital fabrication machines is difficult to extend, modify, and customize. It is very difficult for the end-user to incorporate more forms of control into the workflow. Machine design today is largely the same as it was 50 years ago, despite decades of progress in other fields such as computer science and network engineering. I argue that we need to transition from rapid prototyping to rapid prototyping of rapid prototyping. To make diverse goods, we need diverse tools. To develop diversity in digital fabrication tools, we need reconfigurable and extensible infrastructure for machine building. Using insights from object-oriented programming, end-to-end principles in network design, and the open system interconnection model, I propose a new paradigm for machine building called object-oriented hardware. In this paradigm, software objects and hardware objects are peers that have procedures, methods, ports, and presentations. Machine building modules are available as software libraries are to programmers. A machine instantiation is an assembly of objects situated in a particular context. Using this approach, a thing together with the machine that makes it becomes an application. This method transcends the additive versus subtractive manufacturing comparisons by considering both types of rapid automation. Development work is divided into infrastructural engineering, which develop modules for use in any machine, and application development, which develop specific machine instantiations. Here I present technical implementations of machine building infrastructure first. These include distributed networked controls, reconfigurable software interfaces, and modular mechanical machine components. Then I present machine instantiations that use this infrastructure to demonstrate its capability. Finally to evaluate the object-oriented hardware paradigm in the wild, I observe machine building novices using these tools in both a workshop format and in the Fab Lab network for machine building. To make the modular components for machine building accessible in this context, I developed an extensible toolkit for machine building-the Cardboard Machine Kit. Using this toolkit, novices were able to make a wide range of machines, demonstrating the power of this method.",
    "advisors": ["Neil Gershenfeld"],
    "text": "Making machines that make : object-oriented hardware meets object-oriented software Rapid prototyping has been in the limelight for the past decade. 3D printers have an evocative name that promises production of complex parts on demand. Yet current practice doesn't quite deliver on these promises of advanced manufacturing. Existing digital fabrication tools enable repeatability and precision by using codes to describe machine actions. But the infrastructure used for digital fabrication machines is difficult to extend, modify, and customize. It is very difficult for the end-user to incorporate more forms of control into the workflow. Machine design today is largely the same as it was 50 years ago, despite decades of progress in other fields such as computer science and network engineering. I argue that we need to transition from rapid prototyping to rapid prototyping of rapid prototyping. To make diverse goods, we need diverse tools. To develop diversity in digital fabrication tools, we need reconfigurable and extensible infrastructure for machine building. Using insights from object-oriented programming, end-to-end principles in network design, and the open system interconnection model, I propose a new paradigm for machine building called object-oriented hardware. In this paradigm, software objects and hardware objects are peers that have procedures, methods, ports, and presentations. Machine building modules are available as software libraries are to programmers. A machine instantiation is an assembly of objects situated in a particular context. Using this approach, a thing together with the machine that makes it becomes an application. This method transcends the additive versus subtractive manufacturing comparisons by considering both types of rapid automation. Development work is divided into infrastructural engineering, which develop modules for use in any machine, and application development, which develop specific machine instantiations. Here I present technical implementations of machine building infrastructure first. These include distributed networked controls, reconfigurable software interfaces, and modular mechanical machine components. Then I present machine instantiations that use this infrastructure to demonstrate its capability. Finally to evaluate the object-oriented hardware paradigm in the wild, I observe machine building novices using these tools in both a workshop format and in the Fab Lab network for machine building. To make the modular components for machine building accessible in this context, I developed an extensible toolkit for machine building-the Cardboard Machine Kit. Using this toolkit, novices were able to make a wide range of machines, demonstrating the power of this method."
}, {
    "id": "oai:dspace.mit.edu:1721.1/33677",
    "title": "Microslots : scalable electromagnetic instrumentation",
    "abstract": "This thesis explores spin manipulation, fabrication techniques and boundary conditions of electromagnetism to bridge the macroscopic and microscopic worlds of biology, chemistry and electronics. This work is centered around the design of a novel electromagnetic device scalable from centimeters to micrometers called a microslot. By creating a small slot in a planarized waveguide called a microstrip, the boundary conditions of the system force an electromagnetic wave to create a concentrated magnetic field around the slot that can be used to detect or produce magnetic fields. By constructing suitable boundary conditions, a detector of electric fields can be produced as well. One of the most important applications of this technology is for Nuclear Magnetic Resonance (NMR). As demonstrated experimentally in this thesis, microslots improves the mass-limited detectability of NMR by orders of magnitude over conventional technology and may move us closer to the dream of NMR on a chip.",
    "advisors": ["Neil A. Gershenfeld"],
    "text": "Microslots : scalable electromagnetic instrumentation This thesis explores spin manipulation, fabrication techniques and boundary conditions of electromagnetism to bridge the macroscopic and microscopic worlds of biology, chemistry and electronics. This work is centered around the design of a novel electromagnetic device scalable from centimeters to micrometers called a microslot. By creating a small slot in a planarized waveguide called a microstrip, the boundary conditions of the system force an electromagnetic wave to create a concentrated magnetic field around the slot that can be used to detect or produce magnetic fields. By constructing suitable boundary conditions, a detector of electric fields can be produced as well. One of the most important applications of this technology is for Nuclear Magnetic Resonance (NMR). As demonstrated experimentally in this thesis, microslots improves the mass-limited detectability of NMR by orders of magnitude over conventional technology and may move us closer to the dream of NMR on a chip."
}, {
    "id": "oai:dspace.mit.edu:1721.1/82421",
    "title": "3-D optical waveguide arrays for in-vivo optogenetics : development and application",
    "abstract": "A key feature of neural circuits in the mammalian brain is their 3-dimensional geometric complexity. The ability to optically drive or silence sets of neurons distributed throughout complexly shaped brain circuits, in a temporally precise fashion, would enable analysis of how sets of neurons in different parts of the circuit work together to achieve specific neural codes, circuit dynamics, and behaviors. It could also enable new prototype neural control prosthetics capable of entering information into the brain in a high-bandwidth, cell-specific fashion. This dissertation work involves the development, characterization, and initial utilization of a technology capable of delivering patterned light to 3D targets in neural tissue. Silicon oxynitride waveguide fabrication was optimized for integration onto insertable silicon probes. The waveguides have a propagation loss of-0.4 dB/cm. Right-angle corner mirrors were fabricated at the outputs of the waveguides with losses measured to be 1.5  0.4 dB. Silicon MEMS techniques were developed to fabricate both single- and multi-shank probe geometries with integrated waveguides. Methods were developed to assemble the multi-shank probes into a 3D format using discrete monolithic silicon pieces. Three coupling schemes were developed to couple light to both single- and multi-shank probes. For individual probes not assembled in a 3D format, ribbon cables were used. Modular connection schemes were developed based on ribbon cable connector technologies. Input coupling losses were measured to be 3.4  2.2 dB. For probes which were assembled in a 3D format, two coupling methods were developed: projector-based and scanning-mirror-based. The losses associated with the projector-based system are 17.3  1.8 dB. With a 1.5W 473 nm laser source, 100 pW is capable of being delivered from 300 separate waveguides. The losses associated with the scanning-mirrorbased system are 11.9  2.5 dB. With a 1.6 mW 473 nm laser source, 100 pW is capable of being delivered from an individual waveguide. These fabrication, assembly, and coupling methods demonstrate a successful development of a technology capable of delivering patterned light to 3D targets in neural tissue. Initial biological experiments being performed on microbial-opsin expressing mice is presented. 3D patterned light is delivered to targets in the primary somatosensory cortex while electrical activity is recorded from the primary motor cortex.",
    "advisors": ["Edward S. Boyden"],
    "text": "3-D optical waveguide arrays for in-vivo optogenetics : development and application A key feature of neural circuits in the mammalian brain is their 3-dimensional geometric complexity. The ability to optically drive or silence sets of neurons distributed throughout complexly shaped brain circuits, in a temporally precise fashion, would enable analysis of how sets of neurons in different parts of the circuit work together to achieve specific neural codes, circuit dynamics, and behaviors. It could also enable new prototype neural control prosthetics capable of entering information into the brain in a high-bandwidth, cell-specific fashion. This dissertation work involves the development, characterization, and initial utilization of a technology capable of delivering patterned light to 3D targets in neural tissue. Silicon oxynitride waveguide fabrication was optimized for integration onto insertable silicon probes. The waveguides have a propagation loss of-0.4 dB/cm. Right-angle corner mirrors were fabricated at the outputs of the waveguides with losses measured to be 1.5  0.4 dB. Silicon MEMS techniques were developed to fabricate both single- and multi-shank probe geometries with integrated waveguides. Methods were developed to assemble the multi-shank probes into a 3D format using discrete monolithic silicon pieces. Three coupling schemes were developed to couple light to both single- and multi-shank probes. For individual probes not assembled in a 3D format, ribbon cables were used. Modular connection schemes were developed based on ribbon cable connector technologies. Input coupling losses were measured to be 3.4  2.2 dB. For probes which were assembled in a 3D format, two coupling methods were developed: projector-based and scanning-mirror-based. The losses associated with the projector-based system are 17.3  1.8 dB. With a 1.5W 473 nm laser source, 100 pW is capable of being delivered from 300 separate waveguides. The losses associated with the scanning-mirrorbased system are 11.9  2.5 dB. With a 1.6 mW 473 nm laser source, 100 pW is capable of being delivered from an individual waveguide. These fabrication, assembly, and coupling methods demonstrate a successful development of a technology capable of delivering patterned light to 3D targets in neural tissue. Initial biological experiments being performed on microbial-opsin expressing mice is presented. 3D patterned light is delivered to targets in the primary somatosensory cortex while electrical activity is recorded from the primary motor cortex."
}, {
    "id": "oai:dspace.mit.edu:1721.1/55062",
    "title": "Molecular design of ordering transitions in block copolymers",
    "abstract": "The tendency of block copolymers (BCP's) to microphase separate at the molecular level, producing a wide array of ordered nanostructures, is of particular interest from an engineering standpoint due to the unique mechanical, optical or electrical properties that ensue. Upon considering the potential applications of these materials, however, one limitation arises from the lack of control over bulk thermodynamics and the appearance of order/disorder (solid-like/liquid-like) transitions in these materials. To address this problem, this thesis aims to, firstly, develop a more quantifiable understanding of the molecular factors governing BCP phase behavior, and, secondly, use that knowledge to molecularly engineer new BCP's with enhanced processibility. While most BCP's microphase separate upon cooling through an upper disorder-to-order transition (UDOT), polystyrene-block-poly n-butyl methacrylate, PS-b-PBMA, undergoes ordering upon heating through a lower disorder-to-order transition (LDOT). Preliminary studies on this material revealed a unique pressure sensitivity of this ordering transition. By applying pressure, this material could be forced into the segmentally mixed liquid state, implying \"baroplasticity\", a highly attractive property from a processing standpoint. To better understand the molecular origin of this behavior, the bulk thermodynamics of a family of BCPs formed from styrene and a homologous series of n-alkyl methacrylates (PS-b-PnAMA, n ranging from 1 to 12) was investigated, both as a function of pressure and temperature. The results of this study reveal an unexpected, though systematic, dependence of the phase behavior of these BCP's on monomer architecture. In short, over a certain range of alkyl side chain length, PS-b-PnAMA block copolymers are marginally compatible and exhibit unexpectedly large pressure coefficients for the ordering transition, ranging from 60 to 150C/kbar. In an attempt to identify molecular parameters responsible for these thermodynamic trends, as well as those displayed by other systems reported in the literature, combined group contribution/lattice fluid model calculations of the cohesive properties of the corresponding homopolymers are performed. Based on this analysis, the homopolymer mass density is proposed as a macroscopic parameter that appears to govern phase behavior in weakly interacting block copolymers or polymer blends. Using this new criterion, a simple tool for the molecular design of phase behavior into weakly interacting BCP's is identified, which is successfully used to engineer \"baroplastic\" behavior into several new systems of commercial relevance, including elastomers and adhesives based on styrene and low Tg acrylates. In light of the improved understanding of BCP phase behavior emerging from these studies, a simple phenomenological free energy expression is proposed for compressible polymer mixtures, that can be extended to block copolymers. Its ability to predict qualitative phase diagrams for the systems investigated in this thesis as well as many other polymer pairs is demonstrated. Using this expression, basic principles regarding polymer thermodynamics are outlined.",
    "advisors": ["Anne M. Mayes"],
    "text": "Molecular design of ordering transitions in block copolymers The tendency of block copolymers (BCP's) to microphase separate at the molecular level, producing a wide array of ordered nanostructures, is of particular interest from an engineering standpoint due to the unique mechanical, optical or electrical properties that ensue. Upon considering the potential applications of these materials, however, one limitation arises from the lack of control over bulk thermodynamics and the appearance of order/disorder (solid-like/liquid-like) transitions in these materials. To address this problem, this thesis aims to, firstly, develop a more quantifiable understanding of the molecular factors governing BCP phase behavior, and, secondly, use that knowledge to molecularly engineer new BCP's with enhanced processibility. While most BCP's microphase separate upon cooling through an upper disorder-to-order transition (UDOT), polystyrene-block-poly n-butyl methacrylate, PS-b-PBMA, undergoes ordering upon heating through a lower disorder-to-order transition (LDOT). Preliminary studies on this material revealed a unique pressure sensitivity of this ordering transition. By applying pressure, this material could be forced into the segmentally mixed liquid state, implying \"baroplasticity\", a highly attractive property from a processing standpoint. To better understand the molecular origin of this behavior, the bulk thermodynamics of a family of BCPs formed from styrene and a homologous series of n-alkyl methacrylates (PS-b-PnAMA, n ranging from 1 to 12) was investigated, both as a function of pressure and temperature. The results of this study reveal an unexpected, though systematic, dependence of the phase behavior of these BCP's on monomer architecture. In short, over a certain range of alkyl side chain length, PS-b-PnAMA block copolymers are marginally compatible and exhibit unexpectedly large pressure coefficients for the ordering transition, ranging from 60 to 150C/kbar. In an attempt to identify molecular parameters responsible for these thermodynamic trends, as well as those displayed by other systems reported in the literature, combined group contribution/lattice fluid model calculations of the cohesive properties of the corresponding homopolymers are performed. Based on this analysis, the homopolymer mass density is proposed as a macroscopic parameter that appears to govern phase behavior in weakly interacting block copolymers or polymer blends. Using this new criterion, a simple tool for the molecular design of phase behavior into weakly interacting BCP's is identified, which is successfully used to engineer \"baroplastic\" behavior into several new systems of commercial relevance, including elastomers and adhesives based on styrene and low Tg acrylates. In light of the improved understanding of BCP phase behavior emerging from these studies, a simple phenomenological free energy expression is proposed for compressible polymer mixtures, that can be extended to block copolymers. Its ability to predict qualitative phase diagrams for the systems investigated in this thesis as well as many other polymer pairs is demonstrated. Using this expression, basic principles regarding polymer thermodynamics are outlined."
}, {
    "id": "oai:dspace.mit.edu:1721.1/33007",
    "title": "Changing small group interaction through visual reflections of social behavior",
    "abstract": "People collaborating in groups have potential to produce higher-quality output than individuals working alone, due to the pooling of resources, information, and skills. Yet social psychologists have determined that groups rarely harness this potential. This thesis proposes that technology in face-to-face settings can be used to address the social factors that have damaging influence on group decision-making processes. While there is much work in the area of collaborative software and groupware, this work differentiates itself with its specific aim to influence the way a group shares information without mediating the group's communication. By presenting visualizations to the group of individual levels of participation and turn-taking behavior, the technology aims to augment the group's communication ability, by making it more aware of imbalances. A series of dynamic displays positioned peripherally to a discussion were developed and used by a variety of groups during face-to-face meetings. Both observational and experimental results indicate that these displays influence individual participation levels and the process of information sharing used during a decision-making discussion. A display revealing real-time participation levels caused those at the highest levels of participation to decrease the amount they spoke. Viewing a visualization of previous turn-taking patterns caused those who spoke the least to increase the amount they spoke in a subsequent discussion; real-time feedback did not produce this change. Additionally, after reviewing their turn-taking patterns, groups altered their information-sharing strategies.",
    "advisors": ["Walter Bender"],
    "text": "Changing small group interaction through visual reflections of social behavior People collaborating in groups have potential to produce higher-quality output than individuals working alone, due to the pooling of resources, information, and skills. Yet social psychologists have determined that groups rarely harness this potential. This thesis proposes that technology in face-to-face settings can be used to address the social factors that have damaging influence on group decision-making processes. While there is much work in the area of collaborative software and groupware, this work differentiates itself with its specific aim to influence the way a group shares information without mediating the group's communication. By presenting visualizations to the group of individual levels of participation and turn-taking behavior, the technology aims to augment the group's communication ability, by making it more aware of imbalances. A series of dynamic displays positioned peripherally to a discussion were developed and used by a variety of groups during face-to-face meetings. Both observational and experimental results indicate that these displays influence individual participation levels and the process of information sharing used during a decision-making discussion. A display revealing real-time participation levels caused those at the highest levels of participation to decrease the amount they spoke. Viewing a visualization of previous turn-taking patterns caused those who spoke the least to increase the amount they spoke in a subsequent discussion; real-time feedback did not produce this change. Additionally, after reviewing their turn-taking patterns, groups altered their information-sharing strategies."
}, {
    "id": "oai:dspace.mit.edu:1721.1/62951",
    "title": "Adaptive models for the recognition of human gesture",
    "abstract": "Tomorrow's ubiquitous computing environments will go beyond the keyboard, mouse and monitor paradigm of interaction and will require the automatic interpretation of human motion using a variety of sensors including video cameras. I present several techniques for human motion recognition that are inspired by observations on human gesture, the class of communicative human movement. Typically, gesture recognition systems are unable to handle systematic variation in the input signal, and so are too brittle to be applied successfully in many real-world situations. To address this problem, I present modeling and recognition techniques to adapt gesture models to the situation at hand. A number of systems and frameworks that use adaptive gesture models are presented. First, the parametric hidden Markov model (PHMM) addresses the representation and recognition of gesture families, to extract how a gesture is executed. Second, strong temporal models drawn from natural gesture theory are exploited to segment two kinds of natural gestures from video sequences. Third, a realtime computer vision system learns gesture models online from time-varying context. Fourth, a realtime computer vision system employs hybrid Bayesian networks to unify and extend the previous approaches, as well as point the way for future work.",
    "advisors": ["Aaron F. Bobick", "Bruce M. Blumberg"],
    "text": "Adaptive models for the recognition of human gesture Tomorrow's ubiquitous computing environments will go beyond the keyboard, mouse and monitor paradigm of interaction and will require the automatic interpretation of human motion using a variety of sensors including video cameras. I present several techniques for human motion recognition that are inspired by observations on human gesture, the class of communicative human movement. Typically, gesture recognition systems are unable to handle systematic variation in the input signal, and so are too brittle to be applied successfully in many real-world situations. To address this problem, I present modeling and recognition techniques to adapt gesture models to the situation at hand. A number of systems and frameworks that use adaptive gesture models are presented. First, the parametric hidden Markov model (PHMM) addresses the representation and recognition of gesture families, to extract how a gesture is executed. Second, strong temporal models drawn from natural gesture theory are exploited to segment two kinds of natural gestures from video sequences. Third, a realtime computer vision system learns gesture models online from time-varying context. Fourth, a realtime computer vision system employs hybrid Bayesian networks to unify and extend the previous approaches, as well as point the way for future work."
}, {
    "id": "oai:dspace.mit.edu:1721.1/34182",
    "title": "A quantitative, parametric model of musical tension",
    "abstract": "This thesis presents a quantitative, parametric model for describing musical tension. While the phenomenon of tension is evident to listeners, it is difficult to formalize due to its subjective and multi-dimensional nature. The model is therefore derived from empirical data. Two experiments with contrasting approaches are described. The first experiment is an online test with short musical excerpts and multiple choice answers. The format of the test makes it possible to gather large amounts of data. The second study requires fewer subjects and collects real-time responses to musical stimuli. Both studies present test subjects with examples that take into account a number of musical parameters including harmony, pitch height, melodic expectation, dynamics, onset frequency, tempo, and rhythmic regularity. The goal of the first experiment is to confirm that the individual musical parameters contribute directly to the listener's overall perception of tension. The goal of the second experiment is to explore linear and nonlinear models for predicting tension given descriptions of the musical parameters for each excerpt. The resulting model is considered for potential incorporation into computer-based applications. Specifically, it could be used as part of a computer-assisted composition environment. One such application, Hyperscore, is described and presented as a possible platform for integration.",
    "advisors": ["Tod Machover"],
    "text": "A quantitative, parametric model of musical tension This thesis presents a quantitative, parametric model for describing musical tension. While the phenomenon of tension is evident to listeners, it is difficult to formalize due to its subjective and multi-dimensional nature. The model is therefore derived from empirical data. Two experiments with contrasting approaches are described. The first experiment is an online test with short musical excerpts and multiple choice answers. The format of the test makes it possible to gather large amounts of data. The second study requires fewer subjects and collects real-time responses to musical stimuli. Both studies present test subjects with examples that take into account a number of musical parameters including harmony, pitch height, melodic expectation, dynamics, onset frequency, tempo, and rhythmic regularity. The goal of the first experiment is to confirm that the individual musical parameters contribute directly to the listener's overall perception of tension. The goal of the second experiment is to explore linear and nonlinear models for predicting tension given descriptions of the musical parameters for each excerpt. The resulting model is considered for potential incorporation into computer-based applications. Specifically, it could be used as part of a computer-assisted composition environment. One such application, Hyperscore, is described and presented as a possible platform for integration."
}, {
    "id": "oai:dspace.mit.edu:1721.1/41705",
    "title": "Ensemble : fluency and embodiment for robots acting with humans",
    "abstract": "This thesis is concerned with the notion of fluency in human-robot interaction (HRI), exploring cognitive mechanisms for robotic agents that would enable them to overcome the stop-and-go rigidity present in much of HRI to date. We define fluency as the ethereal yet manifest quality existent when two agents perform together at high level of coordination and adaptation, in particular when they are well-accustomed to the task and to each other. Based on mounting psychological and neurological evidence, we argue that one of the keys to this goal is the adaptation of an embodied approach to robot cognition. We show how central ideas from this psychological school are applicable to robot cognition and present a cognitive architecture making use of perceptual symbols, simulation, and perception-action networks. In addition, we demonstrate that anticipation of perceptual input, and in particular of the actions of others, are an important ingredient of fluent joint action. To that end, we show results from an experiment studying the effects of anticipatory action on fluency and teamwork, and use these results to suggest benchmark metrics for fluency. We also show the relationship between anticipatory action and a simulator approach to perception, through a comparative human subject study of an implemented cognitive architecture on the robot AUR, a robotic desk lamp, designed for this thesis. A result of this work is modeling the effect of practice on human-robot joint action, arguing that mechanisms that govern the passage of cognitive capabilities from a deliberate yet slower system to a faster, sub-intentional, and more rigid one, are crucial to fluent joint action in well-rehearsed ensembles. Theatrical acting theory serves as an inspiration for this work, as we argue that lessons from acting method can be applied to human-robot interaction.",
    "advisors": ["Cynthia Breazeal"],
    "text": "Ensemble : fluency and embodiment for robots acting with humans This thesis is concerned with the notion of fluency in human-robot interaction (HRI), exploring cognitive mechanisms for robotic agents that would enable them to overcome the stop-and-go rigidity present in much of HRI to date. We define fluency as the ethereal yet manifest quality existent when two agents perform together at high level of coordination and adaptation, in particular when they are well-accustomed to the task and to each other. Based on mounting psychological and neurological evidence, we argue that one of the keys to this goal is the adaptation of an embodied approach to robot cognition. We show how central ideas from this psychological school are applicable to robot cognition and present a cognitive architecture making use of perceptual symbols, simulation, and perception-action networks. In addition, we demonstrate that anticipation of perceptual input, and in particular of the actions of others, are an important ingredient of fluent joint action. To that end, we show results from an experiment studying the effects of anticipatory action on fluency and teamwork, and use these results to suggest benchmark metrics for fluency. We also show the relationship between anticipatory action and a simulator approach to perception, through a comparative human subject study of an implemented cognitive architecture on the robot AUR, a robotic desk lamp, designed for this thesis. A result of this work is modeling the effect of practice on human-robot joint action, arguing that mechanisms that govern the passage of cognitive capabilities from a deliberate yet slower system to a faster, sub-intentional, and more rigid one, are crucial to fluent joint action in well-rehearsed ensembles. Theatrical acting theory serves as an inspiration for this work, as we argue that lessons from acting method can be applied to human-robot interaction."
}, {
    "id": "oai:dspace.mit.edu:1721.1/76513",
    "title": "Design and evaluation of a biomimetic agonist-antagonist active knee prosthesis",
    "abstract": "The loss of a limb is extremely debilitating. Unfortunately, today's assistive technologies are still far from providing fully functional artificial limb replacements. Although lower extremity prostheses are currently better able to give assistance than their upper-extremity counterparts, important locomotion problems still remain for leg amputees. Instability, gait asymmetry, decreased walking speeds and high metabolic energy costs are some of the main challenges requiring the development of a new kind of prosthetic device. These challenges point to the need for highly versatile, fully integrated lower-extremity powered prostheses that can replicate the biological behavior of the intact human leg. This thesis presents the design and evaluation of a novel biomimetic active knee prosthesis capable of emulating intact knee biomechanics during level-ground walking. The knee design is motivated by a mono-articular prosthetic knee model comprised of a variable damper and two series elastic clutch units spanning the knee joint. The powered knee system is comprised of two series-elastic actuators positioned in parallel in an agonist-antagonist configuration. This investigation hypothesizes that the biomimetic active-knee prosthesis, with a variable impedance control, can improve unilateral transfemoral amputee locomotion in level-ground walking, reducing the metabolic cost of walking at selfselected speeds. To evaluate this hypothesis, a preliminary study investigated the clinical impact of the active knee prosthesis on the metabolic cost of walking of four unilateral above-knee amputees. This preliminary study compared the antagonistic active knee prosthesis with subjects' prescribed knee prostheses. The subjects' prescribed prostheses encompass four of the leading prosthetic knee technologies commercially available, including passive and electronically controlled variable-damping prosthetic systems. Use of the novel biomimetic active knee prosthesis resulted in a metabolic cost reduction for all four subjects by an average of 5.8%. Kinematic and kinetic analyses indicate that the active knee can increase self-selected walking speed in addition to reducing upper body vertical displacement during walking by an average of 16%. The results of this investigation report for the first time a metabolic cost reduction when walking with a prosthetic system comprised of an electrically powered active knee and passive foot-ankle prostheses, as compared to walking with a conventional transfemoral prosthesis. With this work I aim to advance the field of biomechatronics, contributing to the development of integral assistive technologies that adapt to the needs of the physically challenged.",
    "advisors": ["Hugh Herr"],
    "text": "Design and evaluation of a biomimetic agonist-antagonist active knee prosthesis The loss of a limb is extremely debilitating. Unfortunately, today's assistive technologies are still far from providing fully functional artificial limb replacements. Although lower extremity prostheses are currently better able to give assistance than their upper-extremity counterparts, important locomotion problems still remain for leg amputees. Instability, gait asymmetry, decreased walking speeds and high metabolic energy costs are some of the main challenges requiring the development of a new kind of prosthetic device. These challenges point to the need for highly versatile, fully integrated lower-extremity powered prostheses that can replicate the biological behavior of the intact human leg. This thesis presents the design and evaluation of a novel biomimetic active knee prosthesis capable of emulating intact knee biomechanics during level-ground walking. The knee design is motivated by a mono-articular prosthetic knee model comprised of a variable damper and two series elastic clutch units spanning the knee joint. The powered knee system is comprised of two series-elastic actuators positioned in parallel in an agonist-antagonist configuration. This investigation hypothesizes that the biomimetic active-knee prosthesis, with a variable impedance control, can improve unilateral transfemoral amputee locomotion in level-ground walking, reducing the metabolic cost of walking at selfselected speeds. To evaluate this hypothesis, a preliminary study investigated the clinical impact of the active knee prosthesis on the metabolic cost of walking of four unilateral above-knee amputees. This preliminary study compared the antagonistic active knee prosthesis with subjects' prescribed knee prostheses. The subjects' prescribed prostheses encompass four of the leading prosthetic knee technologies commercially available, including passive and electronically controlled variable-damping prosthetic systems. Use of the novel biomimetic active knee prosthesis resulted in a metabolic cost reduction for all four subjects by an average of 5.8%. Kinematic and kinetic analyses indicate that the active knee can increase self-selected walking speed in addition to reducing upper body vertical displacement during walking by an average of 16%. The results of this investigation report for the first time a metabolic cost reduction when walking with a prosthetic system comprised of an electrically powered active knee and passive foot-ankle prostheses, as compared to walking with a conventional transfemoral prosthesis. With this work I aim to advance the field of biomechatronics, contributing to the development of integral assistive technologies that adapt to the needs of the physically challenged."
}, {
    "id": "oai:dspace.mit.edu:1721.1/114071",
    "title": "Mediated atmospheres : context-aware adaptive lighting and multimodal media environments",
    "abstract": "This dissertation introduces a controller for a context-aware office space that seamlessly and continuously transforms itself to support the activities of its occupants. Properties of the workspace, such as light, sound, textures and objects, have a remarkable influence on the human body, with measurable effects on physiology, cognition, and emotion. New control capabilities, e.g. wireless-controlled lighting and digital media displays, offer an opportunity to dynamically enhance the workspace for recreation, creativity, and productivity, in unison with the occupant's activities. Besides benefits for work engagement, personalized control could optimize for energy efficiency and foster Loose Fit planning to maximize flexible use of limited space. Accordingly, this controller incorporates sensing and computation to mediate between occupants' actions and the adaptive environment in a closed-loop fashion, producing what I call Mediated Atmospheres - seamless transitions of harmonious compositions of (1) light and (2) multimodal media. Through a series of studies, I evaluated this controller and demonstrated its potential to improve the workspace. Improvements were observed in energy savings (52% estimated energy savings compared to static illumination), significant (p < 0.05) increases of perceived fitness for focus and stress restoration, and significant (p < 0.05) physiological changes towards preferred conditions, e.g. in heart rate variability and facial expression. Furthermore, this work discusses the effects of adaptation as a form of behavioral feedback and raises broadly applicable questions about how to best balance automatic control and manual preference setting. The controller, in its core, builds on two contextual control dimensions, Focus and Restoration, which were experimentally discovered. They establish a lower dimensional representation (control map) that facilitates continuous mapping of the sensing and adaptive capabilities. Each unique space produces a new control map, which I computed using either subjective ratings, image analysis or physiological monitoring of the occupant. I performed an in-depth comparison of the image and rating approach for lighting and found that image analysis of either photographs or 3D renderings can accelerate the mapping process by reducing the amount of required human input. This finding allows us to generalize the subjective rating approach, and for the first time enables practicing designers to quickly link the positioning of lighting instruments to perceptual models of space.",
    "advisors": ["Joseph A. Paradiso."],
    "text": "Mediated atmospheres : context-aware adaptive lighting and multimodal media environments This dissertation introduces a controller for a context-aware office space that seamlessly and continuously transforms itself to support the activities of its occupants. Properties of the workspace, such as light, sound, textures and objects, have a remarkable influence on the human body, with measurable effects on physiology, cognition, and emotion. New control capabilities, e.g. wireless-controlled lighting and digital media displays, offer an opportunity to dynamically enhance the workspace for recreation, creativity, and productivity, in unison with the occupant's activities. Besides benefits for work engagement, personalized control could optimize for energy efficiency and foster Loose Fit planning to maximize flexible use of limited space. Accordingly, this controller incorporates sensing and computation to mediate between occupants' actions and the adaptive environment in a closed-loop fashion, producing what I call Mediated Atmospheres - seamless transitions of harmonious compositions of (1) light and (2) multimodal media. Through a series of studies, I evaluated this controller and demonstrated its potential to improve the workspace. Improvements were observed in energy savings (52% estimated energy savings compared to static illumination), significant (p < 0.05) increases of perceived fitness for focus and stress restoration, and significant (p < 0.05) physiological changes towards preferred conditions, e.g. in heart rate variability and facial expression. Furthermore, this work discusses the effects of adaptation as a form of behavioral feedback and raises broadly applicable questions about how to best balance automatic control and manual preference setting. The controller, in its core, builds on two contextual control dimensions, Focus and Restoration, which were experimentally discovered. They establish a lower dimensional representation (control map) that facilitates continuous mapping of the sensing and adaptive capabilities. Each unique space produces a new control map, which I computed using either subjective ratings, image analysis or physiological monitoring of the occupant. I performed an in-depth comparison of the image and rating approach for lighting and found that image analysis of either photographs or 3D renderings can accelerate the mapping process by reducing the amount of required human input. This finding allows us to generalize the subjective rating approach, and for the first time enables practicing designers to quickly link the positioning of lighting instruments to perceptual models of space."
}, {
    "id": "oai:dspace.mit.edu:1721.1/107581",
    "title": "Development of extracellular electrophysiology methods for scalable neural recording",
    "abstract": "In order to map the dynamics of neural circuits in mammalian brains, there is a need for tools that can record activity over large volumes of tissue and correctly attribute the recorded signals to the individual neurons that generated them. High-resolution neural activity maps will be critical for the discovery of new principles of neural coding and neural computation, and to test computational models of neural circuits. Extracellular electrophysiology is a neural recording method that has been developed to record from large populations of neurons, but well-known problems with signal attribution pose an existential threat to the viability of further system scaling, as analyses of network function become more sensitive to errors in attribution. A key insight is that blind-source separation algorithms such as Independent Component Analysis may ameliorate problems with signal attribution. These algorithms require recording signals at much finer spatial resolutions than existing probes have accomplished, which places demands on recording system bandwidth. We present several advances to technologies in neural recording systems, and a complete neural recording system designed to investigate the challenges of scaling electrophysiology to whole brain recording. We have developed close-packed microelectrode arrays with the highest density of recording sites yet achieved, for which we built our own data acquisition hardware, developed with a computational architecture specifically designed to scale to over several orders of magnitude. We also present results from validation experiments using colocalized patch clamp recording to obtain ground-truth activity data. This dataset provides immediate insight into the nature of electrophysiological signals and the interpretation of data collected from any electrophysiology recording system. This data is also essential in order to optimize probe development and data analysis algorithms which will one day enable whole-brain activity mapping.",
    "advisors": ["Edward S. Boyden, III"],
    "text": "Development of extracellular electrophysiology methods for scalable neural recording In order to map the dynamics of neural circuits in mammalian brains, there is a need for tools that can record activity over large volumes of tissue and correctly attribute the recorded signals to the individual neurons that generated them. High-resolution neural activity maps will be critical for the discovery of new principles of neural coding and neural computation, and to test computational models of neural circuits. Extracellular electrophysiology is a neural recording method that has been developed to record from large populations of neurons, but well-known problems with signal attribution pose an existential threat to the viability of further system scaling, as analyses of network function become more sensitive to errors in attribution. A key insight is that blind-source separation algorithms such as Independent Component Analysis may ameliorate problems with signal attribution. These algorithms require recording signals at much finer spatial resolutions than existing probes have accomplished, which places demands on recording system bandwidth. We present several advances to technologies in neural recording systems, and a complete neural recording system designed to investigate the challenges of scaling electrophysiology to whole brain recording. We have developed close-packed microelectrode arrays with the highest density of recording sites yet achieved, for which we built our own data acquisition hardware, developed with a computational architecture specifically designed to scale to over several orders of magnitude. We also present results from validation experiments using colocalized patch clamp recording to obtain ground-truth activity data. This dataset provides immediate insight into the nature of electrophysiological signals and the interpretation of data collected from any electrophysiology recording system. This data is also essential in order to optimize probe development and data analysis algorithms which will one day enable whole-brain activity mapping."
}, {
    "id": "oai:dspace.mit.edu:1721.1/61538",
    "title": "Characterization of unstructured video",
    "abstract": "In this work, we examine video retrieval from a synthesis perspective in co-operation with the more common analysis perspective. Specifically, we target our algorithms for one particular domain- unstructured video material. The goal is to make this unstructured video available for manipulation in interesting ways. I.e, take video that may have been shot with no specific intent and use it in different settings. For example, we build a set of interfaces that will enable taking a collection of home videos and making Christmas cards, Refrigerator magnets, family dramas etc out of them. The work is divided into three parts. First, we study features and models for characterization of video. Examples are VideoBook with its extensions and Hidden Markov Models for video analysis. Secondly, we examine clustering as an approach for characterization of unstructured video. Clustering alleviates some of the common problems with \"query-by- example\" and presents groupings that rely on the user's abilities to make relevant connections. The clustering techniques we employ operate in the probability density space. One of our goals is to employ these techniques with sophisticated models such as Bayesian Networks and HMMs, which give similar descriptions. The clustering techniques we employ are shown to be optimal in an information theoretic and Gibbs Free Energy sense. Finally, we present a set of interfaces that use these features and groupings to enable browsing and editing of unstructured video content.",
    "advisors": ["Andrew B. Lippman"],
    "text": "Characterization of unstructured video In this work, we examine video retrieval from a synthesis perspective in co-operation with the more common analysis perspective. Specifically, we target our algorithms for one particular domain- unstructured video material. The goal is to make this unstructured video available for manipulation in interesting ways. I.e, take video that may have been shot with no specific intent and use it in different settings. For example, we build a set of interfaces that will enable taking a collection of home videos and making Christmas cards, Refrigerator magnets, family dramas etc out of them. The work is divided into three parts. First, we study features and models for characterization of video. Examples are VideoBook with its extensions and Hidden Markov Models for video analysis. Secondly, we examine clustering as an approach for characterization of unstructured video. Clustering alleviates some of the common problems with \"query-by- example\" and presents groupings that rely on the user's abilities to make relevant connections. The clustering techniques we employ operate in the probability density space. One of our goals is to employ these techniques with sophisticated models such as Bayesian Networks and HMMs, which give similar descriptions. The clustering techniques we employ are shown to be optimal in an information theoretic and Gibbs Free Energy sense. Finally, we present a set of interfaces that use these features and groupings to enable browsing and editing of unstructured video content."
}, {
    "id": "oai:dspace.mit.edu:1721.1/115738",
    "title": "The equipped explorer : virtual reality as a medium for learning",
    "abstract": "What opportunities does virtual reality offer to improve the way we learn? In this thesis, I investigate the ways that constructivist approaches, in particular exploratory and experiential learning, can be uniquely supported by immersive virtual worlds. Against the background of these learning theories, I introduce a design framework that centers around defining a medium of virtuality that is fundamentally social, and uses capture of movement and interaction as a key means for creating interactive scenarios and narrative. Within the world conjured by this medium, the Equipped Explorer learns, reviews, creates and communicates using tools that I propose and classify according to a taxonomy. A series of prototypes and design explorations are used as proofs of concept for aspects of the design framework. Experimental studies are used to investigate foundational questions concerning the learning benefits of using VR over 2D interactive media, and the viability of social interaction and collaboration in VR. I reflect on the implications of this framework and my experimental results to extrapolate how they might impact the future classroom and the practice of learning and discovery more broadly. Finally, I discuss what kinds of research might be needed to maximize that impact moving forward.",
    "advisors": ["Pattie Maes"],
    "text": "The equipped explorer : virtual reality as a medium for learning What opportunities does virtual reality offer to improve the way we learn? In this thesis, I investigate the ways that constructivist approaches, in particular exploratory and experiential learning, can be uniquely supported by immersive virtual worlds. Against the background of these learning theories, I introduce a design framework that centers around defining a medium of virtuality that is fundamentally social, and uses capture of movement and interaction as a key means for creating interactive scenarios and narrative. Within the world conjured by this medium, the Equipped Explorer learns, reviews, creates and communicates using tools that I propose and classify according to a taxonomy. A series of prototypes and design explorations are used as proofs of concept for aspects of the design framework. Experimental studies are used to investigate foundational questions concerning the learning benefits of using VR over 2D interactive media, and the viability of social interaction and collaboration in VR. I reflect on the implications of this framework and my experimental results to extrapolate how they might impact the future classroom and the practice of learning and discovery more broadly. Finally, I discuss what kinds of research might be needed to maximize that impact moving forward."
}, {
    "id": "oai:dspace.mit.edu:1721.1/101846",
    "title": "Digital digits : designing assistive finger augmentation devices",
    "abstract": "Wearable computers are becoming a widespread reality. Driven by a quest for sensorial ultrability (ultra-ability) and control of our environment and bodies, we search for ever more intimate solutions to increase our innate physical capacities using technology. Finger-wearable devices for augmentation are nowadays part of the mainstream wearable fashion and research agenda, because of their uniquely convenient placement on the human body and proximity to the most expressive of limbs - the fingers. This thesis proposes a consideration of finger augmenting devices as a new class of instruments, rather than an opportunistic approach for positioning sensors and actuators. Out of a comprehensive survey of the work on finger augmentation, I put forward a definition for finger augmentation, a classification framework, and design guidelines for creating new finger-worn devices. I present four designs of finger-augmenters, their technical underpinnings, evaluation methods and theoretical contributions. Assistance is ubiquitous throughout the spectrum of technological benefit, advancing those with specific needs for recovery or rehabilitation, as well as those looking to go beyond human ability. This cross-cutting design principle for human-computer interfaces is uncontested yet underutilized. This thesis conceptualizes the Assistive Augmentation spectrum as a metaphor for the flexible interpretability of technology to simultaneously help many communities. The concrete prototypes I hereby present: EyeRing, FingerReader, Mobile-FingerReader and MusicReader, exemplify this idea and suggest an inclusive path of technology development.",
    "advisors": ["Pattie Maes"],
    "text": "Digital digits : designing assistive finger augmentation devices Wearable computers are becoming a widespread reality. Driven by a quest for sensorial ultrability (ultra-ability) and control of our environment and bodies, we search for ever more intimate solutions to increase our innate physical capacities using technology. Finger-wearable devices for augmentation are nowadays part of the mainstream wearable fashion and research agenda, because of their uniquely convenient placement on the human body and proximity to the most expressive of limbs - the fingers. This thesis proposes a consideration of finger augmenting devices as a new class of instruments, rather than an opportunistic approach for positioning sensors and actuators. Out of a comprehensive survey of the work on finger augmentation, I put forward a definition for finger augmentation, a classification framework, and design guidelines for creating new finger-worn devices. I present four designs of finger-augmenters, their technical underpinnings, evaluation methods and theoretical contributions. Assistance is ubiquitous throughout the spectrum of technological benefit, advancing those with specific needs for recovery or rehabilitation, as well as those looking to go beyond human ability. This cross-cutting design principle for human-computer interfaces is uncontested yet underutilized. This thesis conceptualizes the Assistive Augmentation spectrum as a metaphor for the flexible interpretability of technology to simultaneously help many communities. The concrete prototypes I hereby present: EyeRing, FingerReader, Mobile-FingerReader and MusicReader, exemplify this idea and suggest an inclusive path of technology development."
}, {
    "id": "oai:dspace.mit.edu:1721.1/119070",
    "title": "Sensor(y) Landscapes : technologies for new perceptual sensibilities",
    "abstract": "When we listen closely, there is a pervading sense that we could hear more if we could only focus a little more intently. Our own perceptual limits are a moving target that we cannot delineate and rarely reach. This dissertation introduces technologies that operate at that mysterious boundary. I envision sensor(y) landscapes, physical sites that meld distributed sensing and sensory perception to afford new perceptual sensibilities. Today's mainstream technologies are well designed for rapid consumption of information and linear, sequential action. A side effect of their effectiveness to task, however, is a loss of undirected, curiosity-driven exploration in the world. I propose alternative technologies that would extend perceptual presence, amplify attention, and leverage intuitions. My focus is on turning rich sensor data into compelling sensory input, and as such, a substantial component of my work involved deploying sensor infrastructure in beautiful places. My projects center on a wetland restoration site, called Tidmarsh, where environmental data are densely and continuously collected and streamed. Using sound and vibration as the medium and nature as the setting, I undertook this work in two steps. The first constructs environments suffused with sensing and built for being present in. My projects in this space comprise sensor-driven virtual worlds, glass elevator sound installations, and vibrating forests that give oral histories. Building on lessons and infrastructure from the first approach, my culminating work uses non-occluding spatial audio to create situated perceptions of data. I developed a bone-conduction headphone device, called HearThere, that renders a live soundscape from distributed microphones and sensors, fully merged with the user's natural hearing. HearThere combines its wearer's inferred listening state with classification output from an Al engine to adjust the mix and spatial parameters of virtual audio sources. The device was developed based on findings from lab studies into spatial hearing and attention, and evaluated in a human subjects study with a panel of experts. Through these projects, I found that deriving meaning in the medium is a matter of possessing or developing perceptual sensibilities, intuitions for how the permeated data can be teased out and contemplated. Carefully composed perceptual confusion-a blurring of place and distributed media-becomes an opportunity for the development of new transpresence sensibilities. How do users make sense of these new dimensions of perception, and how can technologies be designed to facilitate perceptual sense-making?",
    "advisors": ["Joseph A. Paradiso"],
    "text": "Sensor(y) Landscapes : technologies for new perceptual sensibilities When we listen closely, there is a pervading sense that we could hear more if we could only focus a little more intently. Our own perceptual limits are a moving target that we cannot delineate and rarely reach. This dissertation introduces technologies that operate at that mysterious boundary. I envision sensor(y) landscapes, physical sites that meld distributed sensing and sensory perception to afford new perceptual sensibilities. Today's mainstream technologies are well designed for rapid consumption of information and linear, sequential action. A side effect of their effectiveness to task, however, is a loss of undirected, curiosity-driven exploration in the world. I propose alternative technologies that would extend perceptual presence, amplify attention, and leverage intuitions. My focus is on turning rich sensor data into compelling sensory input, and as such, a substantial component of my work involved deploying sensor infrastructure in beautiful places. My projects center on a wetland restoration site, called Tidmarsh, where environmental data are densely and continuously collected and streamed. Using sound and vibration as the medium and nature as the setting, I undertook this work in two steps. The first constructs environments suffused with sensing and built for being present in. My projects in this space comprise sensor-driven virtual worlds, glass elevator sound installations, and vibrating forests that give oral histories. Building on lessons and infrastructure from the first approach, my culminating work uses non-occluding spatial audio to create situated perceptions of data. I developed a bone-conduction headphone device, called HearThere, that renders a live soundscape from distributed microphones and sensors, fully merged with the user's natural hearing. HearThere combines its wearer's inferred listening state with classification output from an Al engine to adjust the mix and spatial parameters of virtual audio sources. The device was developed based on findings from lab studies into spatial hearing and attention, and evaluated in a human subjects study with a panel of experts. Through these projects, I found that deriving meaning in the medium is a matter of possessing or developing perceptual sensibilities, intuitions for how the permeated data can be teased out and contemplated. Carefully composed perceptual confusion-a blurring of place and distributed media-becomes an opportunity for the development of new transpresence sensibilities. How do users make sense of these new dimensions of perception, and how can technologies be designed to facilitate perceptual sense-making?"
}, {
    "id": "oai:dspace.mit.edu:1721.1/8531",
    "title": "Synthetic social relationships for computational entities",
    "abstract": "Humans and many other animals form long term social relationships with each other. These relationships confer a variety of benefits upon us, both as individuals and as groups. Computational systems that can form social relationships like those formed by animals could reap many of the benefits of sociality, both within their own groups and in their interactions with people. This dissertation explores two main questions: *What kinds of internal and external representations are necessary for computational entities to form social relationships like those formed by animals? *How can people participate in and direct the relationships of these entities? To explore these questions, I designed and implemented a system by which computational entities may form simple social relationships. In particular, these synthetic social relationships are modeled after the social behavior of the gray wolf (Canis lupus). The system comprises a novel combination of simple models of emotion, perception and learning in an emotional memory-based mechanism for social relationship formation. The system also includes supporting technologies through which people may participate in and direct the relationships. The system was presented as an interactive installation entitled AlphaWolf in the Emerging Technologies program at SIGGRAPH 2001. This installation featured a pack of six virtual wolves - three fully autonomous adults and three semi-autonomous pups whom people could direct by howling, growling, whining or barking into microphones.",
    "advisors": ["Bruce M. Blumberg"],
    "text": "Synthetic social relationships for computational entities Humans and many other animals form long term social relationships with each other. These relationships confer a variety of benefits upon us, both as individuals and as groups. Computational systems that can form social relationships like those formed by animals could reap many of the benefits of sociality, both within their own groups and in their interactions with people. This dissertation explores two main questions: *What kinds of internal and external representations are necessary for computational entities to form social relationships like those formed by animals? *How can people participate in and direct the relationships of these entities? To explore these questions, I designed and implemented a system by which computational entities may form simple social relationships. In particular, these synthetic social relationships are modeled after the social behavior of the gray wolf (Canis lupus). The system comprises a novel combination of simple models of emotion, perception and learning in an emotional memory-based mechanism for social relationship formation. The system also includes supporting technologies through which people may participate in and direct the relationships. The system was presented as an interactive installation entitled AlphaWolf in the Emerging Technologies program at SIGGRAPH 2001. This installation featured a pack of six virtual wolves - three fully autonomous adults and three semi-autonomous pups whom people could direct by howling, growling, whining or barking into microphones."
}, {
    "id": "oai:dspace.mit.edu:1721.1/78201",
    "title": "The anatomy of an urban modular electric vehicle : how the architecture of the CityCar enhances personal mobility and supporting industries",
    "abstract": "Growing populations, increasing middle-class, and rapid urbanization - for today's urban dweller, all of these escalating factors continue to contribute to problems of excessive energy use, road congestion, pollution due to carbon emissions, and inefficient personal transit. Considering that the average vehicle in a city weighs thousands of pounds, usually caries only one person per trip, and expends significant proportions of its gasoline simply searching for resources such as parking, new efficient and intelligent modes of transportation are in need of exploration. This dissertation presents the design and development of an electric vehicle called the \"CityCar\" that confronts the aforementioned problems of urban mobility with a novel vehicle architecture. The assembly of the CityCar derives from a subset of \"urban modular electric vehicle\" (uMEV) components in which five core units are combined to create a variety of solutions for urban personal mobility. Drastically decreasing the granularity of the vehicle's subcomponents into larger interchangeable modules, the uMEV platform expands options for fleet customization while simultaneously addressing the complex rapport between automotive manufacturers and their suppliers through a responsibility shift among their respective subcomponents. Transforming its anatomy from complex mechanically-dominant entities to electrically-dominant modular components enables unique design features within the uMEV fleet. The CityCar for example exploits technologies such as a folding chassis to reduce its footprint by 40% and Robot Wheels that each are allotted between 72 to 120-degrees of rotation to together enable a seven-foot turning circle. Just over 1,000 pounds, its lightweight zero-emitting electric platform, comprised of significantly fewer parts, curbs negative externalities that today's automobiles create in city environments. Additionally, the vehicle platform developed from the assembly of several core units empowers a consortium of suppliers to self-coordinate through a unique modular business model. Lastly, the CityCar specific uMEV confronts problems within urban transit by providing a nimble folding mobility solution tailored specifically to crowded cities. Benefits, such as a 5:1 parking density and its reduced maintenance demands, are especially reinforced in the context of shared personal transportation services like Mobility-on-Demand.",
    "advisors": ["Kent Larson"],
    "text": "The anatomy of an urban modular electric vehicle : how the architecture of the CityCar enhances personal mobility and supporting industries Growing populations, increasing middle-class, and rapid urbanization - for today's urban dweller, all of these escalating factors continue to contribute to problems of excessive energy use, road congestion, pollution due to carbon emissions, and inefficient personal transit. Considering that the average vehicle in a city weighs thousands of pounds, usually caries only one person per trip, and expends significant proportions of its gasoline simply searching for resources such as parking, new efficient and intelligent modes of transportation are in need of exploration. This dissertation presents the design and development of an electric vehicle called the \"CityCar\" that confronts the aforementioned problems of urban mobility with a novel vehicle architecture. The assembly of the CityCar derives from a subset of \"urban modular electric vehicle\" (uMEV) components in which five core units are combined to create a variety of solutions for urban personal mobility. Drastically decreasing the granularity of the vehicle's subcomponents into larger interchangeable modules, the uMEV platform expands options for fleet customization while simultaneously addressing the complex rapport between automotive manufacturers and their suppliers through a responsibility shift among their respective subcomponents. Transforming its anatomy from complex mechanically-dominant entities to electrically-dominant modular components enables unique design features within the uMEV fleet. The CityCar for example exploits technologies such as a folding chassis to reduce its footprint by 40% and Robot Wheels that each are allotted between 72 to 120-degrees of rotation to together enable a seven-foot turning circle. Just over 1,000 pounds, its lightweight zero-emitting electric platform, comprised of significantly fewer parts, curbs negative externalities that today's automobiles create in city environments. Additionally, the vehicle platform developed from the assembly of several core units empowers a consortium of suppliers to self-coordinate through a unique modular business model. Lastly, the CityCar specific uMEV confronts problems within urban transit by providing a nimble folding mobility solution tailored specifically to crowded cities. Benefits, such as a 5:1 parking density and its reduced maintenance demands, are especially reinforced in the context of shared personal transportation services like Mobility-on-Demand."
}, {
    "id": "oai:dspace.mit.edu:1721.1/9543",
    "title": "Wearable computing and contextual awareness",
    "abstract": "Computer hardware continues to shrink in size and increase in capability. This trend has allowed the prevailing concept of a computer to evolve from the mainframe to the minicomputer to the desktop. Just as the physical hardware changes, so does the use of the technology, tending towards more interactive and personal systems. Currently, another physical change is underway, placing computational power on the user's body. These wearable machines encourage new applications that were formerly infeasible and, correspondingly, will result in new usage patterns. This thesis suggests that the fundamental improvement offered by wearable computing is an increased sense of user context. I hypothesize that on-body systems can sense the user's context with little or no assistance from environmental infrastructure. These body-centered systems that \"see\" as the user sees and \"hear\" as the user hears, provide a unique \"first-person\" viewpoint of the user's environment. By exploiting models recovered by these systems, interfaces are created which require minimal directed action or attention by the user. In addition, more traditional applications are augmented by the contextual information recovered by these systems. To investigate these issues, I provide perceptually sensible tools for recovering and modeling user context in a mobile, everyday environment. These tools include a downward-facing, camera-based system for establishing the location of the user; a tag-based object recognition system for augmented reality; and several on-body gesture recognition systems to identify various user tasks in constrained environments. To address the practicality of contextually-aware wearable computers, issues of power recovery, heat dissipation, and weight distribution are examined. In addition, I have encouraged a community of wearable computer users at the Media Lab through design, management, and support of hardware and software infrastructure. This unique community provides a heightened awareness of the use and social issues of wearable computing. As much as possible, the lessons from this experience will be conveyed in the thesis.",
    "advisors": ["Alex Pentland"],
    "text": "Wearable computing and contextual awareness Computer hardware continues to shrink in size and increase in capability. This trend has allowed the prevailing concept of a computer to evolve from the mainframe to the minicomputer to the desktop. Just as the physical hardware changes, so does the use of the technology, tending towards more interactive and personal systems. Currently, another physical change is underway, placing computational power on the user's body. These wearable machines encourage new applications that were formerly infeasible and, correspondingly, will result in new usage patterns. This thesis suggests that the fundamental improvement offered by wearable computing is an increased sense of user context. I hypothesize that on-body systems can sense the user's context with little or no assistance from environmental infrastructure. These body-centered systems that \"see\" as the user sees and \"hear\" as the user hears, provide a unique \"first-person\" viewpoint of the user's environment. By exploiting models recovered by these systems, interfaces are created which require minimal directed action or attention by the user. In addition, more traditional applications are augmented by the contextual information recovered by these systems. To investigate these issues, I provide perceptually sensible tools for recovering and modeling user context in a mobile, everyday environment. These tools include a downward-facing, camera-based system for establishing the location of the user; a tag-based object recognition system for augmented reality; and several on-body gesture recognition systems to identify various user tasks in constrained environments. To address the practicality of contextually-aware wearable computers, issues of power recovery, heat dissipation, and weight distribution are examined. In addition, I have encouraged a community of wearable computer users at the Media Lab through design, management, and support of hardware and software infrastructure. This unique community provides a heightened awareness of the use and social issues of wearable computing. As much as possible, the lessons from this experience will be conveyed in the thesis."
}, {
    "id": "oai:dspace.mit.edu:1721.1/9165",
    "title": "Inside the conductor's jacket : analysis, interpretation and musical synthesis of expressive gesture",
    "abstract": "We present the design and implementation of the Conductor's Jacket, a unique wearable device that measures physiological and gestural signals, together with the Gesture Construction, a musical software system that interprets these signals and applies them expressively in a musical context. Sixteen sensors have been incorporated into the Conductor's Jacket in such a way as to not encumber or interfere with the gestures of a working orchestra conductor. The Conductor's Jacket system gathers up to sixteen data channels reliably at rates of 3 kHz per channel, and also provides mcal-time graphical feedback. Unlike many gesture-sensing systems it not only gathers positional and accelerational data but also senses muscle tension from several locations on each arm. The Conductor's Jacket was used to gather conducting data from six subjects, three professional conductors and three students, during twelve hours of rehearsals and performances. Analyses of the data yielded thirty-five significant features that seem to reflect intuitive and natural gestural tendencies, including context-based hand switching, anticipatory 'flatlining' effects, and correlations between respiration and phrasing. The results indicate that muscle tension and respiration signals reflect several significant and expressive characteristics of a conductor's gestures. From these results we present nine hypotheses about human musical expression, including ideas about efficiency, intentionality, polyphony, signal-to-noise ratios, and musical flow state. Finally, this thesis describes the Gesture Construction, a musical software system that analyzes and performs music in real-time based on the performer's gestures and breathing signals. A bank of software filters extracts several of the features that were found in the conductor study, including beat intensities and the alternation between arms. These features are then used to generate real-time expressive effects by shaping the beats, tempos, articulations, dynamics, and note lengths in a musical score.",
    "advisors": ["Tod Machover", "Rosalind W. Picard"],
    "text": "Inside the conductor's jacket : analysis, interpretation and musical synthesis of expressive gesture We present the design and implementation of the Conductor's Jacket, a unique wearable device that measures physiological and gestural signals, together with the Gesture Construction, a musical software system that interprets these signals and applies them expressively in a musical context. Sixteen sensors have been incorporated into the Conductor's Jacket in such a way as to not encumber or interfere with the gestures of a working orchestra conductor. The Conductor's Jacket system gathers up to sixteen data channels reliably at rates of 3 kHz per channel, and also provides mcal-time graphical feedback. Unlike many gesture-sensing systems it not only gathers positional and accelerational data but also senses muscle tension from several locations on each arm. The Conductor's Jacket was used to gather conducting data from six subjects, three professional conductors and three students, during twelve hours of rehearsals and performances. Analyses of the data yielded thirty-five significant features that seem to reflect intuitive and natural gestural tendencies, including context-based hand switching, anticipatory 'flatlining' effects, and correlations between respiration and phrasing. The results indicate that muscle tension and respiration signals reflect several significant and expressive characteristics of a conductor's gestures. From these results we present nine hypotheses about human musical expression, including ideas about efficiency, intentionality, polyphony, signal-to-noise ratios, and musical flow state. Finally, this thesis describes the Gesture Construction, a musical software system that analyzes and performs music in real-time based on the performer's gestures and breathing signals. A bank of software filters extracts several of the features that were found in the conductor study, including beat intensities and the alternation between arms. These features are then used to generate real-time expressive effects by shaping the beats, tempos, articulations, dynamics, and note lengths in a musical score."
}, {
    "id": "oai:dspace.mit.edu:1721.1/91863",
    "title": "Holographic television : measuring visual performance with holographic and other 3D television technologies",
    "abstract": "We are surrounded by visual reproductions: computer screens, photographs, televisions, and countless other technologies allow us to perceive objects and scenes that are not physically in-front of us. All existing technologies that reproduce images perform engineering tradeoffs that provide the viewer with some subset of the visual information that would be available in person, in exchange for cost, convenience, or practicality. For many viewing tasks, incomplete reproductions go unnoticed. This dissertation provides a set of findings that illuminate the value of binocular disparity, and ocular focus information that can be provided by some three-dimensional display technologies. These findings include new experimental methods, as well as results, for conducting evaluations of current and future display technologies. Methodologies were validated on an implementation of digital holographic television, an image capture and reproduction system for visual telepresence. The holographic television system, allows viewers to observe, in real-time, a remote 3D scene, through a display that preserves focus (individual objects can be brought into optical focus at the expense of others), and horizontal motion parallax (depth and other geometry of objects appears natural over a range of head movement). Holographic television can also emulate other precursor 2D and 3D display technologies. This capability was used to validate the evaluation methodologies (meta-evaluation) by comparing visual performance on simulations of conventional displays to results of past studies by other researchers.",
    "advisors": ["V. Michael Bove, Jr"],
    "text": "Holographic television : measuring visual performance with holographic and other 3D television technologies We are surrounded by visual reproductions: computer screens, photographs, televisions, and countless other technologies allow us to perceive objects and scenes that are not physically in-front of us. All existing technologies that reproduce images perform engineering tradeoffs that provide the viewer with some subset of the visual information that would be available in person, in exchange for cost, convenience, or practicality. For many viewing tasks, incomplete reproductions go unnoticed. This dissertation provides a set of findings that illuminate the value of binocular disparity, and ocular focus information that can be provided by some three-dimensional display technologies. These findings include new experimental methods, as well as results, for conducting evaluations of current and future display technologies. Methodologies were validated on an implementation of digital holographic television, an image capture and reproduction system for visual telepresence. The holographic television system, allows viewers to observe, in real-time, a remote 3D scene, through a display that preserves focus (individual objects can be brought into optical focus at the expense of others), and horizontal motion parallax (depth and other geometry of objects appears natural over a range of head movement). Holographic television can also emulate other precursor 2D and 3D display technologies. This capability was used to validate the evaluation methodologies (meta-evaluation) by comparing visual performance on simulations of conventional displays to results of past studies by other researchers."
}, {
    "id": "oai:dspace.mit.edu:1721.1/112523",
    "title": "Lensing Machines : representing perspective in machine learning",
    "abstract": "Generative models are venerated as full probabilistic models that randomly generate observable data given a set of latent variables that cannot be directly observed. They can be used to simulate values for variables in the model, allowing analysis by synthesis or model criticism, towards an iterative cycle of model specification, estimation, and critique. However, many datasets represent a combination of several viewpoints - different ways of looking at the same data that leads to various generalizations. For example, a corpus that has data generated by multiple people may be mixtures of several perspectives and can be viewed with different opinions by others. It isn't always possible to represent the viewpoints by clean separation, in advance, of examples representing each perspective and train a separate model for each point of view. In this thesis, we introduce lensing, a mixed-initiative technique to (i) extract lenses or mappings between machine-learned representations and perspectives of human experts, and (2) generate lensed models that afford multiple perspectives of the same dataset. We explore lensing of latent variable model in their configuration, parameter and evidential spaces. We apply lensing to three health applications, namely imbuing the perspectives of experts into latent variable models that analyze adolescent distress and crisis counseling.",
    "advisors": ["Rosalind Wright Picard"],
    "text": "Lensing Machines : representing perspective in machine learning Generative models are venerated as full probabilistic models that randomly generate observable data given a set of latent variables that cannot be directly observed. They can be used to simulate values for variables in the model, allowing analysis by synthesis or model criticism, towards an iterative cycle of model specification, estimation, and critique. However, many datasets represent a combination of several viewpoints - different ways of looking at the same data that leads to various generalizations. For example, a corpus that has data generated by multiple people may be mixtures of several perspectives and can be viewed with different opinions by others. It isn't always possible to represent the viewpoints by clean separation, in advance, of examples representing each perspective and train a separate model for each point of view. In this thesis, we introduce lensing, a mixed-initiative technique to (i) extract lenses or mappings between machine-learned representations and perspectives of human experts, and (2) generate lensed models that afford multiple perspectives of the same dataset. We explore lensing of latent variable model in their configuration, parameter and evidential spaces. We apply lensing to three health applications, namely imbuing the perspectives of experts into latent variable models that analyze adolescent distress and crisis counseling."
}, {
    "id": "oai:dspace.mit.edu:1721.1/29142",
    "title": "A computational memory and processing model for prosody",
    "abstract": "This thesis links processing in working memory to prosody in speech, and links different working memory capacities to different prosodic styles. It provides a causal account of prosodic differences and an architecture for reproducing them in synthesized speech. The implemented system mediates text-based information through a model of attention and working memory. The main simulation parameter of the memory model quantifies recall. Changing its value changes what counts as given and new information in a text, and therefore determines the intonation with which the text is uttered. Other aspects of search and storage in the memory model are mapped to the remainder of the continuous and categorical features of pitch and timing, producing prosody in three different styles: for small recall values, the exaggerated and sing-song melodies of children's speech; for mid-range values, an adult expressive style; for the largest values, the prosody of a speaker who is familiar with the text, and at times sounds bored or irritated. In addition, because the storage procedure is stochastic, the prosody from simulation to simulation varies, even for identical control parameters. As with with human speech, no two renditions are alike. Informal feedback indicates that the stylistic differences are recognizable and that the prosody is improved over current offerings. A comparison with natural data shows clear and predictable trends although not at significance. However, a comparison within the natural data also did not produce results at significance. One practical contribution of this work is a text mark-up schema consisting of relational annotations to grammatical structures. Another is the product - varied and plausible prosody in synthesized speech. The main theoretical contribution is to show that resource-bound cognitive activity has prosodic correlates, thus providing a rationale for the individual and stylistic differences in melody and rhythm that are ubiquitous in human speech.",
    "advisors": ["Kenneth Haase"],
    "text": "A computational memory and processing model for prosody This thesis links processing in working memory to prosody in speech, and links different working memory capacities to different prosodic styles. It provides a causal account of prosodic differences and an architecture for reproducing them in synthesized speech. The implemented system mediates text-based information through a model of attention and working memory. The main simulation parameter of the memory model quantifies recall. Changing its value changes what counts as given and new information in a text, and therefore determines the intonation with which the text is uttered. Other aspects of search and storage in the memory model are mapped to the remainder of the continuous and categorical features of pitch and timing, producing prosody in three different styles: for small recall values, the exaggerated and sing-song melodies of children's speech; for mid-range values, an adult expressive style; for the largest values, the prosody of a speaker who is familiar with the text, and at times sounds bored or irritated. In addition, because the storage procedure is stochastic, the prosody from simulation to simulation varies, even for identical control parameters. As with with human speech, no two renditions are alike. Informal feedback indicates that the stylistic differences are recognizable and that the prosody is improved over current offerings. A comparison with natural data shows clear and predictable trends although not at significance. However, a comparison within the natural data also did not produce results at significance. One practical contribution of this work is a text mark-up schema consisting of relational annotations to grammatical structures. Another is the product - varied and plausible prosody in synthesized speech. The main theoretical contribution is to show that resource-bound cognitive activity has prosodic correlates, thus providing a rationale for the individual and stylistic differences in melody and rhythm that are ubiquitous in human speech."
}, {
    "id": "oai:dspace.mit.edu:1721.1/91853",
    "title": "Technology-supported apprenticeship in the management of chronic disease",
    "abstract": "Chronic disease is the most important cause of morbidity and mortality worldwide, but the current standard of care is woefully ineffective. It is paternalistic, episodic, and perversely incentivized based on volume, resulting in poor outcomes at extraordinary cost. Technology-supported apprenticeship is a model of chronic disease management that embraces the contribution of the patient. It is collaborative, continuous, and designed to achieve value through improvement in the experience, clinical outcomes, and cost of care. In this model, patients are the novice apprentices of master clinicians. A software platform called CollaboRhythm provides applications on mobile phones and tablets as scaffolding for collaboration. Tracking tools document progress, visualizations highlight associations between actions and outcomes, and personalized decision support encourages self-efficacy. Powerful virtual visits and instant messaging allow master clinicians to provide adaptive coaching within the context of daily life rather than in the artificial environment of the office. Apprentice patients have the potential to become master coaches themselves; thus producing an exponentially scaling health ecosystem at minimal cost. Two randomized, controlled trials were conducted to evaluate if technology-supported apprenticeship could augment the \"best of the best\" in office-based care and scale it via virtual deployment. Apprentice patients for basal insulin titration at the Joslin Diabetes Center were more satisfied with their care than controls, achieved better outcomes (-3.1% vs. -2.5% HbA1C), and did so with minimal increase in cost ($206). Those for hypertension management at the Massachusetts General Hospital were also more satisfied with their care, achieved better outcomes (-26.3 vs. -15.9 mmHg SBP), and did so with negligible increase in cost ($14). Over a longer period of time, apprenticeship is projected to produce better outcomes at decreased cost. Technology-supported apprenticeship has extraordinary potential, but the paternalistic culture of medicine and its volume-based economic model present significant impediments. Future work needs to address longer durations of coaching, greater numbers of apprentices per coach, patients as coaches, other chronic diseases, and patients with comorbidities.",
    "advisors": ["Franklin H. Moss"],
    "text": "Technology-supported apprenticeship in the management of chronic disease Chronic disease is the most important cause of morbidity and mortality worldwide, but the current standard of care is woefully ineffective. It is paternalistic, episodic, and perversely incentivized based on volume, resulting in poor outcomes at extraordinary cost. Technology-supported apprenticeship is a model of chronic disease management that embraces the contribution of the patient. It is collaborative, continuous, and designed to achieve value through improvement in the experience, clinical outcomes, and cost of care. In this model, patients are the novice apprentices of master clinicians. A software platform called CollaboRhythm provides applications on mobile phones and tablets as scaffolding for collaboration. Tracking tools document progress, visualizations highlight associations between actions and outcomes, and personalized decision support encourages self-efficacy. Powerful virtual visits and instant messaging allow master clinicians to provide adaptive coaching within the context of daily life rather than in the artificial environment of the office. Apprentice patients have the potential to become master coaches themselves; thus producing an exponentially scaling health ecosystem at minimal cost. Two randomized, controlled trials were conducted to evaluate if technology-supported apprenticeship could augment the \"best of the best\" in office-based care and scale it via virtual deployment. Apprentice patients for basal insulin titration at the Joslin Diabetes Center were more satisfied with their care than controls, achieved better outcomes (-3.1% vs. -2.5% HbA1C), and did so with minimal increase in cost ($206). Those for hypertension management at the Massachusetts General Hospital were also more satisfied with their care, achieved better outcomes (-26.3 vs. -15.9 mmHg SBP), and did so with negligible increase in cost ($14). Over a longer period of time, apprenticeship is projected to produce better outcomes at decreased cost. Technology-supported apprenticeship has extraordinary potential, but the paternalistic culture of medicine and its volume-based economic model present significant impediments. Future work needs to address longer durations of coaching, greater numbers of apprentices per coach, patients as coaches, other chronic diseases, and patients with comorbidities."
}, {
    "id": "oai:dspace.mit.edu:1721.1/61931",
    "title": "Beyond transparency : collective engagement in sustainable design",
    "abstract": "For a timely answer to the question of sustainability, or how to provide for future generations, there needs to be shared accounting of our social and physical resources. Supply chain transparency makes it possible to map resource flows and ensure dependable production while avoiding social and environmental problems. Open channels of communications can support a collective effort to account for the impacts of supply chains and engage more people in the invention of long-term solutions, or sustainable design. This thesis proposes a crowd-sourced approach to resource accounting through the democratization of sustainable design. A web-based social network called Sourcemap was built to link diverse stakeholders through an open forum for supply chain transparency and environmental assessment. The scalable system points the way towards comprehensive sustainability accounting through the distributed verification of industrial practices. Sourcemap was developed over a two-year period in partnership with regional organizations, large businesses and SME's. Small business case studies show that an open social media platform can motivate sustainable practices at an enterprise level and on a regional scale. The public-facing supply chain publishing platform actively engages communities of producers, experts, consumers and oversight groups. Thousands of user-generated contributions point towards the need to improve the quality of transparency to form a broadly accessible resource for sustainability accounting.",
    "advisors": ["Hiroshi Ishii"],
    "text": "Beyond transparency : collective engagement in sustainable design For a timely answer to the question of sustainability, or how to provide for future generations, there needs to be shared accounting of our social and physical resources. Supply chain transparency makes it possible to map resource flows and ensure dependable production while avoiding social and environmental problems. Open channels of communications can support a collective effort to account for the impacts of supply chains and engage more people in the invention of long-term solutions, or sustainable design. This thesis proposes a crowd-sourced approach to resource accounting through the democratization of sustainable design. A web-based social network called Sourcemap was built to link diverse stakeholders through an open forum for supply chain transparency and environmental assessment. The scalable system points the way towards comprehensive sustainability accounting through the distributed verification of industrial practices. Sourcemap was developed over a two-year period in partnership with regional organizations, large businesses and SME's. Small business case studies show that an open social media platform can motivate sustainable practices at an enterprise level and on a regional scale. The public-facing supply chain publishing platform actively engages communities of producers, experts, consumers and oversight groups. Thousands of user-generated contributions point towards the need to improve the quality of transparency to form a broadly accessible resource for sustainability accounting."
}, {
    "id": "oai:dspace.mit.edu:1721.1/112527",
    "title": "Governing human and machine behavior in an experimenting society",
    "abstract": "We live in a culture that depends on technologies to record our behavior and coordinate our actions with billions of other connected people. In this computational culture, humans and machines continue to perpetuate deep-seated injustices. Our abilities to observe and intervene in other people's lives also allow us to govern, forcing us to ask how to govern wisely and who should be responsible. In this dissertation, I argue that to govern wisely, we need to remake large-scale social experiments to follow values of democracy. Using qualitative and quantitative methods, I spent time with hundreds of communities on the social news platform reddit and learned how they govern themselves. I designed CivilServant, novel experimentation software that communities have used to evaluate how they govern harassment and misinformation. Finally, I examined the uses of this evidence in community policy deliberation. As we develop ways to govern behavior through technology platforms, we have an opportunity to ensure that that the benefits will be enjoyed, questioned, and validated widely in an open society. Despite common views of social experiments as scarce knowledge that consolidates the power of experts, I show how community experiments can scale policy evaluation and expand public influence on the governance of human and machine behavior.",
    "advisors": ["Ethan Zuckerman"],
    "text": "Governing human and machine behavior in an experimenting society We live in a culture that depends on technologies to record our behavior and coordinate our actions with billions of other connected people. In this computational culture, humans and machines continue to perpetuate deep-seated injustices. Our abilities to observe and intervene in other people's lives also allow us to govern, forcing us to ask how to govern wisely and who should be responsible. In this dissertation, I argue that to govern wisely, we need to remake large-scale social experiments to follow values of democracy. Using qualitative and quantitative methods, I spent time with hundreds of communities on the social news platform reddit and learned how they govern themselves. I designed CivilServant, novel experimentation software that communities have used to evaluate how they govern harassment and misinformation. Finally, I examined the uses of this evidence in community policy deliberation. As we develop ways to govern behavior through technology platforms, we have an opportunity to ensure that that the benefits will be enjoyed, questioned, and validated widely in an open society. Despite common views of social experiments as scarce knowledge that consolidates the power of experts, I show how community experiments can scale policy evaluation and expand public influence on the governance of human and machine behavior."
}, {
    "id": "oai:dspace.mit.edu:1721.1/70808",
    "title": "Mindful navigation with guiding light : design considerations for projector based indoor navigation assistance system",
    "abstract": "People can easily become mindless in their decision-making and become disengaged from their surroundings when their actions depend on information and guidance from an assistive technology. Research has shown how automated navigation assistance systems lead users to be disengaged from the space through which they are traveling, resulting in poor recollection of the environment and poorer situational decision-making. This disengagement and mindlessness can potentially increase the risk of accidents and lower the quality of user experience. If we can help people become mindfully attentive to the environment and surroundings while carrying out navigation tasks using assistive technologies, I hypothesize that we will have better memory of the space, improved cognitive reconstruction of environment, and better understanding of the immediate situation, all of which will lead to better decision making and more efficient navigation. In this work, I present a new approach for analyzing the problem of navigation assistance for pedestrians, which considers both the physical and psychological constraints of users focused on navigation. I address the physical constraint that eyes should remain \"on the street\" by providing a new visual interface, named Guiding Light, that offers a mixed reality presentation of guidance information in the environment itself, instead of on a screen. We address the psychological constraint that minds should remain engaged with the environment by applying a framework based on mindfulness and mindlessness theory (Langer 1989) in the design of the system. The theory explains how mindsets affect engagement levels and decision-making in daily activities. In addition, this thesis describes an indoor positioning technology that provides relatively high accuracy localization and heading orientation of a user in indoor environments. The innovation not only involved developing a new sensor but also a software system to collect fingerprint maps and tracking location with the fingerprint maps. This new technology opens up a new area in the field to explore other possibilities of using a magnetic field based positioning system.",
    "advisors": ["Christopher Schmandt"],
    "text": "Mindful navigation with guiding light : design considerations for projector based indoor navigation assistance system People can easily become mindless in their decision-making and become disengaged from their surroundings when their actions depend on information and guidance from an assistive technology. Research has shown how automated navigation assistance systems lead users to be disengaged from the space through which they are traveling, resulting in poor recollection of the environment and poorer situational decision-making. This disengagement and mindlessness can potentially increase the risk of accidents and lower the quality of user experience. If we can help people become mindfully attentive to the environment and surroundings while carrying out navigation tasks using assistive technologies, I hypothesize that we will have better memory of the space, improved cognitive reconstruction of environment, and better understanding of the immediate situation, all of which will lead to better decision making and more efficient navigation. In this work, I present a new approach for analyzing the problem of navigation assistance for pedestrians, which considers both the physical and psychological constraints of users focused on navigation. I address the physical constraint that eyes should remain \"on the street\" by providing a new visual interface, named Guiding Light, that offers a mixed reality presentation of guidance information in the environment itself, instead of on a screen. We address the psychological constraint that minds should remain engaged with the environment by applying a framework based on mindfulness and mindlessness theory (Langer 1989) in the design of the system. The theory explains how mindsets affect engagement levels and decision-making in daily activities. In addition, this thesis describes an indoor positioning technology that provides relatively high accuracy localization and heading orientation of a user in indoor environments. The innovation not only involved developing a new sensor but also a software system to collect fingerprint maps and tracking location with the fingerprint maps. This new technology opens up a new area in the field to explore other possibilities of using a magnetic field based positioning system."
}, {
    "id": "oai:dspace.mit.edu:1721.1/42407",
    "title": "Designing for long-term human-robot interaction and application to weight loss",
    "abstract": "Human-robot interaction is now well enough understood to allow us to build useful systems that can function outside of the laboratory. This thesis defines sociable robot system in the context of long-term interaction, proposes guidelines for creating and evaluating such systems, and describes the implementation of a robot that has been designed to help individuals effect behavior change while dieting. The implemented system is a robotic weight loss coach, which is compared to a standalone computer and to a traditional paper log in a controlled study. A current challenge in weight loss is in getting individuals to keep off weight that is lost. The results of our study show that participants track their calorie consumption and exercise for nearly twice as long when using the robot than with the other methods and develop a closer relationship with the robot. Both of these are indicators of longer-term success at weight loss and maintenance.",
    "advisors": ["Cynthia Breazeal"],
    "text": "Designing for long-term human-robot interaction and application to weight loss Human-robot interaction is now well enough understood to allow us to build useful systems that can function outside of the laboratory. This thesis defines sociable robot system in the context of long-term interaction, proposes guidelines for creating and evaluating such systems, and describes the implementation of a robot that has been designed to help individuals effect behavior change while dieting. The implemented system is a robotic weight loss coach, which is compared to a standalone computer and to a traditional paper log in a controlled study. A current challenge in weight loss is in getting individuals to keep off weight that is lost. The results of our study show that participants track their calorie consumption and exercise for nearly twice as long when using the robot than with the other methods and develop a closer relationship with the robot. Both of these are indicators of longer-term success at weight loss and maintenance."
}, {
    "id": "oai:dspace.mit.edu:1721.1/28776",
    "title": "Every sign of life",
    "abstract": "Every Sign of Life introduces an approach to and motivational schema for personal health monitoring. It is an exploration of how to make information collected by personal health-monitoring devices fun and engaging, and consequently more useful to the non-specialist. In contrast to the common methodology of adding game elements to established biofeedback systems, the Every Sign of Life approach is to design and build games that use biosensor information to effect the game environment. This work tests the hypothesis that fun (the joy of learning, achieving, competing, etc.) is a way to achieve the goal of self-efficacy; to induce people to take care of their own health by altering their habits and lifestyles. One result is a basic architecture for personal health-monitoring systems that has led to an approach to the design of sensor peripherals and wearable computer components called \"Extremity Computing.\" This approach is used to redefine biosensor monitoring from periodic to continuous (ultimately saving data over a lifetime). Another result is an approach to adding implicit biofeedback to computer games. This has led to a new genre of games called \"Bio-Analytical Games\" that straddles the boundary between sports and computer games. A series of studies of how to present health information to children and adults have demonstrated the ability of consumers to use bioinformatics without involving professionals.",
    "advisors": ["Walter Bender"],
    "text": "Every sign of life Every Sign of Life introduces an approach to and motivational schema for personal health monitoring. It is an exploration of how to make information collected by personal health-monitoring devices fun and engaging, and consequently more useful to the non-specialist. In contrast to the common methodology of adding game elements to established biofeedback systems, the Every Sign of Life approach is to design and build games that use biosensor information to effect the game environment. This work tests the hypothesis that fun (the joy of learning, achieving, competing, etc.) is a way to achieve the goal of self-efficacy; to induce people to take care of their own health by altering their habits and lifestyles. One result is a basic architecture for personal health-monitoring systems that has led to an approach to the design of sensor peripherals and wearable computer components called \"Extremity Computing.\" This approach is used to redefine biosensor monitoring from periodic to continuous (ultimately saving data over a lifetime). Another result is an approach to adding implicit biofeedback to computer games. This has led to a new genre of games called \"Bio-Analytical Games\" that straddles the boundary between sports and computer games. A series of studies of how to present health information to children and adults have demonstrated the ability of consumers to use bioinformatics without involving professionals."
}, {
    "id": "oai:dspace.mit.edu:1721.1/107576",
    "title": "Reflecting music through movement : a body-syntonic approach to playing [with] the piano",
    "abstract": "This thesis introduces and examines methods for the capture and reproduction of music on the piano that maintain a tight coupling between the body, the sound, and the physical instrument. For expert musicians, the body plays an indispensable role in the physical act of playing and the understanding of both musical structure and expressivity. However, many music learning technologies mistakenly assume that playing music is \"playing the notes\" and neglect the role of the body in the development of the musical mind. Drawing from research in telepresence, tangible interfaces, and augmented reality, I propose to bring the body back into the picture, literally and metaphorically, by augmenting a digital player piano with projection mapping. My platform synchronizes dynamic imagery with the piano's moving keys and acoustic sounds. I here focus on two main projects: MirrorFugue and Andante. Inspired by reflections on the lacquered surfaces of a grand piano, MirrorFugue simulates the presence of a virtual pianist whose reflection is actually playing the physically moving keys. It encourages anyone to take the seat left empty at the piano, to feel in his or her own body how music is expressed through the body of the performer, and to play along. Andante presents music as miniature figures that appear to walk and dance on the piano keyboard, physically striking a key with each stop. It conveys the expressivity in rhythms and phrases as well as musical structures through the bodies and movements of the figures. Both installations are designed as immersive \"sandboxes\" for the playful exploration of musical ideas. Beyond my projects, this thesis explores the parallels between music learning and learning and large. I discuss the connections between theories of music learning (particularly Dalcroze Eurhythmics) with theories of general mental development (Jean Piaget, Jerome Bruner, Seymour Papert, and Marvin Minsky), as well as how strategies from music learning could inform the art of learning in general.",
    "advisors": ["Hiroshi Ishii"],
    "text": "Reflecting music through movement : a body-syntonic approach to playing [with] the piano This thesis introduces and examines methods for the capture and reproduction of music on the piano that maintain a tight coupling between the body, the sound, and the physical instrument. For expert musicians, the body plays an indispensable role in the physical act of playing and the understanding of both musical structure and expressivity. However, many music learning technologies mistakenly assume that playing music is \"playing the notes\" and neglect the role of the body in the development of the musical mind. Drawing from research in telepresence, tangible interfaces, and augmented reality, I propose to bring the body back into the picture, literally and metaphorically, by augmenting a digital player piano with projection mapping. My platform synchronizes dynamic imagery with the piano's moving keys and acoustic sounds. I here focus on two main projects: MirrorFugue and Andante. Inspired by reflections on the lacquered surfaces of a grand piano, MirrorFugue simulates the presence of a virtual pianist whose reflection is actually playing the physically moving keys. It encourages anyone to take the seat left empty at the piano, to feel in his or her own body how music is expressed through the body of the performer, and to play along. Andante presents music as miniature figures that appear to walk and dance on the piano keyboard, physically striking a key with each stop. It conveys the expressivity in rhythms and phrases as well as musical structures through the bodies and movements of the figures. Both installations are designed as immersive \"sandboxes\" for the playful exploration of musical ideas. Beyond my projects, this thesis explores the parallels between music learning and learning and large. I discuss the connections between theories of music learning (particularly Dalcroze Eurhythmics) with theories of general mental development (Jean Piaget, Jerome Bruner, Seymour Papert, and Marvin Minsky), as well as how strategies from music learning could inform the art of learning in general."
}, {
    "id": "oai:dspace.mit.edu:1721.1/8708",
    "title": "Identity construction environments : the design of computational tools for exploring a sense of self and moral values",
    "abstract": "We live in a society where concepts of self, community and what is right and wrong are constantly changing. This makes it particularly challenging for young people to construct a sense of self and to identify and develop their most cherished personal and moral values. It also puts pressure on schools and society to help them do so. This thesis explores how new technologies can be used to create environments explicitly designed to help young people explore their inner worlds. I coined the term identity construction environments (ICE) to refer to computational tools purposefully designed with the goal of helping young people explore different aspects of the self, in particular personal and moral values. My contribution in this thesis involves three dimensions: theory, design and empirical research. At the theoretical level, I propose a framework through which people can think and learn about identity as a complex entity embracing multiple and contradictory values. At the design level, I describe an evolutionary process of building and investigating the use of three identity construction environments which are precursors to the one that is at the center of the empirical investigation described in this thesis.",
    "advisors": ["Seymour Papert"],
    "text": "Identity construction environments : the design of computational tools for exploring a sense of self and moral values We live in a society where concepts of self, community and what is right and wrong are constantly changing. This makes it particularly challenging for young people to construct a sense of self and to identify and develop their most cherished personal and moral values. It also puts pressure on schools and society to help them do so. This thesis explores how new technologies can be used to create environments explicitly designed to help young people explore their inner worlds. I coined the term identity construction environments (ICE) to refer to computational tools purposefully designed with the goal of helping young people explore different aspects of the self, in particular personal and moral values. My contribution in this thesis involves three dimensions: theory, design and empirical research. At the theoretical level, I propose a framework through which people can think and learn about identity as a complex entity embracing multiple and contradictory values. At the design level, I describe an evolutionary process of building and investigating the use of three identity construction environments which are precursors to the one that is at the center of the empirical investigation described in this thesis."
}, {
    "id": "oai:dspace.mit.edu:1721.1/70807",
    "title": "Social fMRI : measuring and designing social mechanisms using mobile phones",
    "abstract": "A key challenge of data-driven social science is the gathering of high quality multi-dimensional datasets. A second challenge relates to the design and execution of social experiments in the real world that are as reliable as those within a controlled laboratory, yet yield more practical results. We introduce the Social Functional Mechanism-design and Relationship Imaging, or \"SocialfMRI\" - an approach that enhances existing computational social science methodologies by bridging rich data collection strategies with experimental interventions. In this thesis, we demonstrate the value of the Social fMRI approach in our Friends and Family study. We transformed a young-family residential community into a living laboratory for 15 months, through a very fine-grained and longitudinal data collection process combined with targeted experimental interventions. Through the derived dataset of unprecedented quality, the Social fMRI approach allows us to gain insights into intricate social mechanisms and interpersonal relationships within the community in ways not previously possible. This thesis delivers the following contributions: (1) A methodology combining a rich-data experimental approach together with carefully designed interventions, (2) a system supporting the methodology - implemented, field-tested, and released to the world as an open-source framework with a growing community of users, (3) a dataset collected using the system, comprising what is, to date, the richest real-world dataset of its genre, (4) a very large set of experimental findings that contribute to our understanding of important research questions in computational social science in addition to demonstrating the methodology's potential. Among the results described in this thesis are the design and evaluation of a novel mechanism for social support in a health-related context, the observation that the diffusion of mobile applications relies more on the face-to-face interaction ties than on self-perceived friendship ties, and a gained understanding of the evolution of modeling and prediction processes over time and varying sample sizes.",
    "advisors": ["Alex P. Pentland"],
    "text": "Social fMRI : measuring and designing social mechanisms using mobile phones A key challenge of data-driven social science is the gathering of high quality multi-dimensional datasets. A second challenge relates to the design and execution of social experiments in the real world that are as reliable as those within a controlled laboratory, yet yield more practical results. We introduce the Social Functional Mechanism-design and Relationship Imaging, or \"SocialfMRI\" - an approach that enhances existing computational social science methodologies by bridging rich data collection strategies with experimental interventions. In this thesis, we demonstrate the value of the Social fMRI approach in our Friends and Family study. We transformed a young-family residential community into a living laboratory for 15 months, through a very fine-grained and longitudinal data collection process combined with targeted experimental interventions. Through the derived dataset of unprecedented quality, the Social fMRI approach allows us to gain insights into intricate social mechanisms and interpersonal relationships within the community in ways not previously possible. This thesis delivers the following contributions: (1) A methodology combining a rich-data experimental approach together with carefully designed interventions, (2) a system supporting the methodology - implemented, field-tested, and released to the world as an open-source framework with a growing community of users, (3) a dataset collected using the system, comprising what is, to date, the richest real-world dataset of its genre, (4) a very large set of experimental findings that contribute to our understanding of important research questions in computational social science in addition to demonstrating the methodology's potential. Among the results described in this thesis are the design and evaluation of a novel mechanism for social support in a health-related context, the observation that the diffusion of mobile applications relies more on the face-to-face interaction ties than on self-perceived friendship ties, and a gained understanding of the evolution of modeling and prediction processes over time and varying sample sizes."
}, {
    "id": "oai:dspace.mit.edu:1721.1/42406",
    "title": "Understanding the embodied teacher : nonverbal cues for sociable robot learning",
    "abstract": "As robots enter the social environments of our workplaces and homes, it will be important for them to be able to learn from natural human teaching behavior. My research seeks to identify simple, non-verbal cues that human teachers naturally provide that are useful for directing the attention of robot learners. I conducted two novel studies that examined the use of embodied cues in human task learning and teaching behavior. These studies motivated the creation of a novel data-gathering system for capturing teaching and learning interactions at very high spatial and temporal resolutions. Through the studies, I observed a number of salient attention-direction cues, the most promising of which were visual perspective, action timing, and spatial scaffolding. In particular, this thesis argues that spatial scaffolding, in which teachers use their bodies to spatially structure the learning environment to direct the attention of the learner, is a highly valuable cue for robotic learning systems. I constructed a number of learning algorithms to evaluate the utility of the identified cues. I situated these learning algorithms within a large architecture for robot cognition, augmented with novel mechanisms for social attention and visual perspective taking. Finally, I evaluated the performance of these learning algorithms in comparison to human learning data, providing quantitative evidence for the utility of the identified cues. As a secondary contribution, this evaluation process supported the construction of a number of demonstrations of the humanoid robot Leonardo learning in novel ways from natural human teaching behavior.",
    "advisors": ["Cynthia Breazeal"],
    "text": "Understanding the embodied teacher : nonverbal cues for sociable robot learning As robots enter the social environments of our workplaces and homes, it will be important for them to be able to learn from natural human teaching behavior. My research seeks to identify simple, non-verbal cues that human teachers naturally provide that are useful for directing the attention of robot learners. I conducted two novel studies that examined the use of embodied cues in human task learning and teaching behavior. These studies motivated the creation of a novel data-gathering system for capturing teaching and learning interactions at very high spatial and temporal resolutions. Through the studies, I observed a number of salient attention-direction cues, the most promising of which were visual perspective, action timing, and spatial scaffolding. In particular, this thesis argues that spatial scaffolding, in which teachers use their bodies to spatially structure the learning environment to direct the attention of the learner, is a highly valuable cue for robotic learning systems. I constructed a number of learning algorithms to evaluate the utility of the identified cues. I situated these learning algorithms within a large architecture for robot cognition, augmented with novel mechanisms for social attention and visual perspective taking. Finally, I evaluated the performance of these learning algorithms in comparison to human learning data, providing quantitative evidence for the utility of the identified cues. As a secondary contribution, this evaluation process supported the construction of a number of demonstrations of the humanoid robot Leonardo learning in novel ways from natural human teaching behavior."
}, {
    "id": "oai:dspace.mit.edu:1721.1/91438",
    "title": "Mind-theoretic planning for social robots",
    "abstract": "As robots move out of factory floors and into human environments, out from safe barricaded workstations to operating in close proximity with people, they will increasingly be expected to understand and coordinate with basic aspects of human behavior. If they are to become useful and productive participants in human-robot teams, they will require effective methods of modeling their human counterparts in order to better coordinate and cooperate with them. Theory of Mind (ToM) is defined as people's ability to reason about others' behavior in terms of their internal states, such as beliefs and desires. Having a ToM allows an individual to understand the observed behavior of others, based not only on directly observable perceptual features but also an understanding of underlying mental states; this understanding allows the individual to anticipate and better react to future actions. In this thesis a Mind-Theoretic Planning (MTP) system is presented which attempts to provide robots with some of the basic ToM abilities that people rely on for coordinating and interacting with others. The MTP system frames the problem of mind-theoretic reasoning as a planning problem with mixed observability. A predictive forward model of others' behavior is computed by creating a set of mental state situations (MSS), each composed of stacks of Markov Decision Process (MDP) models whose solutions provide approximations of anticipated rational actions and reactions of that agent. This forward model, in addition to a perceptual-range limiting observation function, is combined into a Partially Observable MDP (POMDP). The presented MTP approach increases computational efficiency by taking advantage of approximation methods offered by a novel POMDP solver B3RTDP as well as leveraging value functions at various levels of the MSS as heuristics for value functions at higher levels. For the purpose of creating an efficient MTP system, a novel general-purpose online POMDP solver B3RTDP was developed. This planner extends the Real- Time Dynamic Programming (RTDP) approach to solving POMDPs. By using a bounded value function representation, we are able to apply a novel approach to pruning the belief-action search graph and maintain a Convergence Frontier, a novel mechanism for taking advantage of early action convergence, which can greatly improve RTDP's search time. Lastly, an online video game was developed for the purpose of evaluating the MTP system by having people complete tasks in a virtual environment with a simulated robotic assistant. A human subject study was performed to assess both the objective behavioral differences in performance of the human-robot teams, as well as the subjective attitudinal differences in how people perceived agents with varying MTP capabilities. We demonstrate that providing agents with mind-theoretic capabilities can significantly improve the efficiency of human-robot teamwork in certain domains and suggest that it may also positively influence humans' subjective perception of their robotic teammates.",
    "advisors": ["Cynthia Breazeal"],
    "text": "Mind-theoretic planning for social robots As robots move out of factory floors and into human environments, out from safe barricaded workstations to operating in close proximity with people, they will increasingly be expected to understand and coordinate with basic aspects of human behavior. If they are to become useful and productive participants in human-robot teams, they will require effective methods of modeling their human counterparts in order to better coordinate and cooperate with them. Theory of Mind (ToM) is defined as people's ability to reason about others' behavior in terms of their internal states, such as beliefs and desires. Having a ToM allows an individual to understand the observed behavior of others, based not only on directly observable perceptual features but also an understanding of underlying mental states; this understanding allows the individual to anticipate and better react to future actions. In this thesis a Mind-Theoretic Planning (MTP) system is presented which attempts to provide robots with some of the basic ToM abilities that people rely on for coordinating and interacting with others. The MTP system frames the problem of mind-theoretic reasoning as a planning problem with mixed observability. A predictive forward model of others' behavior is computed by creating a set of mental state situations (MSS), each composed of stacks of Markov Decision Process (MDP) models whose solutions provide approximations of anticipated rational actions and reactions of that agent. This forward model, in addition to a perceptual-range limiting observation function, is combined into a Partially Observable MDP (POMDP). The presented MTP approach increases computational efficiency by taking advantage of approximation methods offered by a novel POMDP solver B3RTDP as well as leveraging value functions at various levels of the MSS as heuristics for value functions at higher levels. For the purpose of creating an efficient MTP system, a novel general-purpose online POMDP solver B3RTDP was developed. This planner extends the Real- Time Dynamic Programming (RTDP) approach to solving POMDPs. By using a bounded value function representation, we are able to apply a novel approach to pruning the belief-action search graph and maintain a Convergence Frontier, a novel mechanism for taking advantage of early action convergence, which can greatly improve RTDP's search time. Lastly, an online video game was developed for the purpose of evaluating the MTP system by having people complete tasks in a virtual environment with a simulated robotic assistant. A human subject study was performed to assess both the objective behavioral differences in performance of the human-robot teams, as well as the subjective attitudinal differences in how people perceived agents with varying MTP capabilities. We demonstrate that providing agents with mind-theoretic capabilities can significantly improve the efficiency of human-robot teamwork in certain domains and suggest that it may also positively influence humans' subjective perception of their robotic teammates."
}, {
    "id": "oai:dspace.mit.edu:1721.1/62044",
    "title": "Singing voice analysis/synthesis",
    "abstract": "The singing voice is the oldest and most variable of musical instruments. By combining music, lyrics, and expression, the voice is able to affect us in ways that no other instrument can. As listeners, we are innately drawn to the sound of the human voice, and when present it is almost always the focal point of a musical piece. But the acoustic flexibility of the voice in intimating words, shaping phrases, and conveying emotion also makes it the most difficult instrument to model computationally. Moreover, while all voices are capable of producing the common sounds necessary for language understanding and communication, each voice possesses distinctive features independent of phonemes and words. These unique acoustic qualities are the result of a combination of innate physical factors and expressive characteristics of performance, reflecting an individual's vocal identity. A great deal of prior research has focused on speech recognition and speaker identification, but relatively little work has been performed specifically on singing. There are significant differences between speech and singing in terms of both production and perception. Traditional computational models of speech have focused on the intelligibility of language, often sacrificing sound quality for model simplicity. Such models, however, are detrimental to the goal of singing, which relies on acoustic authenticity for the non-linguistic communication of expression and emotion. These differences between speech and singing dictate that a different and specialized representation is needed to capture the sound quality and musicality most valued in singing.",
    "advisors": ["Barry L. Vercoe"],
    "text": "Singing voice analysis/synthesis The singing voice is the oldest and most variable of musical instruments. By combining music, lyrics, and expression, the voice is able to affect us in ways that no other instrument can. As listeners, we are innately drawn to the sound of the human voice, and when present it is almost always the focal point of a musical piece. But the acoustic flexibility of the voice in intimating words, shaping phrases, and conveying emotion also makes it the most difficult instrument to model computationally. Moreover, while all voices are capable of producing the common sounds necessary for language understanding and communication, each voice possesses distinctive features independent of phonemes and words. These unique acoustic qualities are the result of a combination of innate physical factors and expressive characteristics of performance, reflecting an individual's vocal identity. A great deal of prior research has focused on speech recognition and speaker identification, but relatively little work has been performed specifically on singing. There are significant differences between speech and singing in terms of both production and perception. Traditional computational models of speech have focused on the intelligibility of language, often sacrificing sound quality for model simplicity. Such models, however, are detrimental to the goal of singing, which relies on acoustic authenticity for the non-linguistic communication of expression and emotion. These differences between speech and singing dictate that a different and specialized representation is needed to capture the sound quality and musicality most valued in singing."
}, {
    "id": "oai:dspace.mit.edu:1721.1/107580",
    "title": "Children as data scientists : explorations in creating, thinking, and learning with data",
    "abstract": "Data is a powerful lens for learning about the world. Driven by advances in computational technologies and methods that make it easier to collect, store, and analyze vast amounts of data about our world, data science has emerged as a new discipline with immense possibilities for discovery and learning. However, these possibilities are primarily accessible for adult experts - in this thesis, I examine pathways to support children as data scientists. In the first part of this thesis, I study children's use of variables and lists in the Scratch programming environment. I quantitatively study the ways in which children use variables and lists in Scratch (e.g., to keep score in games), as well as factors that foster this engagement. I find support for the theory that children learn to use data-structures through remixing their peers' works, as well through looking at source code of projects created by their peers. I also find evidence to suggest that providing more powerful uses of data-structures (such as data-persistence) leads to children using more data structures overall. In the second part of the thesis, I introduce a new system, Scratch Community Blocks, that enables children to create projects that access and analyze data from the Scratch online community (e.g., creating visualizations that show which programming blocks they used in their projects or analyzing trends in the popularity of their projects within the community). Through artifact-based case studies, interviews, and survey responses collected from a group of children using the system, I show how children use data and programming to answer their own questions about learning and social behaviour within the Scratch community. I find that children use Scratch Community Blocks not only to create with data through stories and games, but also to think with data by engaging in self-reflection about their own learning and social participation, and through critical conversations about the role of data within the culture of the Scratch community.",
    "advisors": ["Mitchel Resnick"],
    "text": "Children as data scientists : explorations in creating, thinking, and learning with data Data is a powerful lens for learning about the world. Driven by advances in computational technologies and methods that make it easier to collect, store, and analyze vast amounts of data about our world, data science has emerged as a new discipline with immense possibilities for discovery and learning. However, these possibilities are primarily accessible for adult experts - in this thesis, I examine pathways to support children as data scientists. In the first part of this thesis, I study children's use of variables and lists in the Scratch programming environment. I quantitatively study the ways in which children use variables and lists in Scratch (e.g., to keep score in games), as well as factors that foster this engagement. I find support for the theory that children learn to use data-structures through remixing their peers' works, as well through looking at source code of projects created by their peers. I also find evidence to suggest that providing more powerful uses of data-structures (such as data-persistence) leads to children using more data structures overall. In the second part of the thesis, I introduce a new system, Scratch Community Blocks, that enables children to create projects that access and analyze data from the Scratch online community (e.g., creating visualizations that show which programming blocks they used in their projects or analyzing trends in the popularity of their projects within the community). Through artifact-based case studies, interviews, and survey responses collected from a group of children using the system, I show how children use data and programming to answer their own questions about learning and social behaviour within the Scratch community. I find that children use Scratch Community Blocks not only to create with data through stories and games, but also to think with data by engaging in self-reflection about their own learning and social participation, and through critical conversations about the role of data within the culture of the Scratch community."
}, {
    "id": "oai:dspace.mit.edu:1721.1/91434",
    "title": "Design and applications of inkjet-printed flexible sensate surfaces",
    "abstract": "We live in a world where everyday artifacts begin to be designed and augmented as media interfaces. New technologies based on this mission enable us to more easily sense, interact, and communicate with objects. However, the world is highly variable in physical forms. To achieve the vision of ubiquitous computing, common manmade objects need to be designed from the ground up to incorporate computers and sensors. Often, we find ourselves confined by existing sensing infrastructures that are not designed to adapt the complexity of the physical world. This dissertation presents a research platform to investigate design principles and applications for flexible sensate surfaces. This platform utilizes recent advancements in low-cost, roll-to-roll conductive inkjet printing technology as an enabler for creating a scalable, physically and functionally adaptive and customizable sensing system. This collection of work demonstrates design principles and examples in the following four areas: manufacturing, customizable computer aided design, fabrication with physical manipulation and multi-modal sensing techniques. Two types of manufacturing methods are used and characterized. The first approach customizes the sensing design in a digital environment, where users define the geometry, shape and sensing inputs in a computer and print out customized functional patterns. The second approach is sensor fabrication via physical manipulation, where the sensate surface is premanufactured and through an additive method (paneling linear sensor tape stripes), or a subtractive method (cutting a sensor sheet), and the shape and sensing targets are processed post-manufacturing. Lastly, I demonstrate three techniques for multimodal sensing - designing \"target specific shapes\" for different sensing targets, multiplexing single input electrodes with various analog circuits for near surface sensing (pressure, touch, folding, proximity sensing), and adding extra layers of chemical for the designed ad-hoc sensing target alteration. The outcome of this exploration combines emerging technologies to realize a new way of designing sensate surfaces for smart environments and objects and helps us rethink sensing as both a graphical design and a physical manipulation process. In the course of this thesis, I demonstrate these principals by designing, testing, and evaluating a variety of flexible sensate surfaces.",
    "advisors": ["Joseph A. Paradiso"],
    "text": "Design and applications of inkjet-printed flexible sensate surfaces We live in a world where everyday artifacts begin to be designed and augmented as media interfaces. New technologies based on this mission enable us to more easily sense, interact, and communicate with objects. However, the world is highly variable in physical forms. To achieve the vision of ubiquitous computing, common manmade objects need to be designed from the ground up to incorporate computers and sensors. Often, we find ourselves confined by existing sensing infrastructures that are not designed to adapt the complexity of the physical world. This dissertation presents a research platform to investigate design principles and applications for flexible sensate surfaces. This platform utilizes recent advancements in low-cost, roll-to-roll conductive inkjet printing technology as an enabler for creating a scalable, physically and functionally adaptive and customizable sensing system. This collection of work demonstrates design principles and examples in the following four areas: manufacturing, customizable computer aided design, fabrication with physical manipulation and multi-modal sensing techniques. Two types of manufacturing methods are used and characterized. The first approach customizes the sensing design in a digital environment, where users define the geometry, shape and sensing inputs in a computer and print out customized functional patterns. The second approach is sensor fabrication via physical manipulation, where the sensate surface is premanufactured and through an additive method (paneling linear sensor tape stripes), or a subtractive method (cutting a sensor sheet), and the shape and sensing targets are processed post-manufacturing. Lastly, I demonstrate three techniques for multimodal sensing - designing \"target specific shapes\" for different sensing targets, multiplexing single input electrodes with various analog circuits for near surface sensing (pressure, touch, folding, proximity sensing), and adding extra layers of chemical for the designed ad-hoc sensing target alteration. The outcome of this exploration combines emerging technologies to realize a new way of designing sensate surfaces for smart environments and objects and helps us rethink sensing as both a graphical design and a physical manipulation process. In the course of this thesis, I demonstrate these principals by designing, testing, and evaluating a variety of flexible sensate surfaces."
}, {
    "id": "oai:dspace.mit.edu:1721.1/67762",
    "title": "Understanding the link between changes in social support and changes in outcomes with the sociometric badge",
    "abstract": "The goal of this thesis is to show that social support created through face-to-face interaction is a driving factor in a number of important outcomes. Through a series of studies we show that social support, operationalized using face-to-face network constraint (information clearing), is positively related to important outcomes such as productivity and job satisfaction and that changes in social support are positively related to changes in these outcomes. We then discuss a two-phase study where we experimentally modify break structure to increase network constraint and demonstrate a corresponding positive change in outcomes. Finally, we show that network constraint is also qualitatively related to outcomes and is an effective proxy for social support. To conclude we situate this research under a larger framework that provides direction for future research.",
    "advisors": ["Alex (Sandy) Pentland"],
    "text": "Understanding the link between changes in social support and changes in outcomes with the sociometric badge The goal of this thesis is to show that social support created through face-to-face interaction is a driving factor in a number of important outcomes. Through a series of studies we show that social support, operationalized using face-to-face network constraint (information clearing), is positively related to important outcomes such as productivity and job satisfaction and that changes in social support are positively related to changes in these outcomes. We then discuss a two-phase study where we experimentally modify break structure to increase network constraint and demonstrate a corresponding positive change in outcomes. Finally, we show that network constraint is also qualitatively related to outcomes and is an effective proxy for social support. To conclude we situate this research under a larger framework that provides direction for future research."
}, {
    "id": "oai:dspace.mit.edu:1721.1/32498",
    "title": "Machine perception and learning of complex social systems",
    "abstract": "The study of complex social systems has traditionally been an arduous process, involving extensive surveys, interviews, ethnographic studies, or analysis of online behavior. Today, however, it is possible to use the unprecedented amount of information generated by pervasive mobile phones to provide insights into the dynamics of both individual and group behavior. Information such as continuous proximity, location, communication and activity data, has been gathered from the phones of 100 human subjects at MIT. Systematic measurements from these 100 people over the course of eight months has generated one of the largest datasets of continuous human behavior ever collected, representing over 300,000 hours of daily activity. In this thesis we describe how this data can be used to uncover regular rules and structure in behavior of both individuals and organizations, infer relationships between subjects, verify self- report survey data, and study social network dynamics. By combining theoretical models with rich and systematic measurements, we show it is possible to gain insight into the underlying behavior of complex social systems.",
    "advisors": ["Alex P. Pentland"],
    "text": "Machine perception and learning of complex social systems The study of complex social systems has traditionally been an arduous process, involving extensive surveys, interviews, ethnographic studies, or analysis of online behavior. Today, however, it is possible to use the unprecedented amount of information generated by pervasive mobile phones to provide insights into the dynamics of both individual and group behavior. Information such as continuous proximity, location, communication and activity data, has been gathered from the phones of 100 human subjects at MIT. Systematic measurements from these 100 people over the course of eight months has generated one of the largest datasets of continuous human behavior ever collected, representing over 300,000 hours of daily activity. In this thesis we describe how this data can be used to uncover regular rules and structure in behavior of both individuals and organizations, infer relationships between subjects, verify self- report survey data, and study social network dynamics. By combining theoretical models with rich and systematic measurements, we show it is possible to gain insight into the underlying behavior of complex social systems."
}, {
    "id": "oai:dspace.mit.edu:1721.1/34180",
    "title": "The role of groups in smart camera networks",
    "abstract": "Recent research in sensor networks has made it possible to deploy networks of sensors with significant local processing. These sensor networks are revolutionising information collection and processing in many different environments. Often the amount of local data produced by these devices, and their sheer number, makes centralised data processing infeasible. Smart camera networks represent a particular challenge in this regard, partly because of the amount of data produced by each camera, but also because many high level vision algorithms require data from more than one camera. Many distributed algorithms exist that work locally to produce results from a collection of nodes, but as this number grows the algorithm's performance is quickly crippled by the resulting exponential increase in communication overhead. This thesis examines the limits this puts on peer-to-peer cooperation between nodes, and demonstrates how for large networks these can only be circumvented by locally formed organisations of nodes. A local group forming protocol is described that provides a method for nodes to create a bottom-up organisation based purely on local conditions. This allows the formation of a dynamic information network of cooperating nodes, in which a distributed algorithm can organise the communications of its nodes using purely local knowledge to maintain its global network performance.",
    "advisors": ["V. Michael Bove, Jr"],
    "text": "The role of groups in smart camera networks Recent research in sensor networks has made it possible to deploy networks of sensors with significant local processing. These sensor networks are revolutionising information collection and processing in many different environments. Often the amount of local data produced by these devices, and their sheer number, makes centralised data processing infeasible. Smart camera networks represent a particular challenge in this regard, partly because of the amount of data produced by each camera, but also because many high level vision algorithms require data from more than one camera. Many distributed algorithms exist that work locally to produce results from a collection of nodes, but as this number grows the algorithm's performance is quickly crippled by the resulting exponential increase in communication overhead. This thesis examines the limits this puts on peer-to-peer cooperation between nodes, and demonstrates how for large networks these can only be circumvented by locally formed organisations of nodes. A local group forming protocol is described that provides a method for nodes to create a bottom-up organisation based purely on local conditions. This allows the formation of a dynamic information network of cooperating nodes, in which a distributed algorithm can organise the communications of its nodes using purely local knowledge to maintain its global network performance."
}, {
    "id": "oai:dspace.mit.edu:1721.1/30242",
    "title": "Personal long-term memory aids",
    "abstract": "The prevalence and affordability of personal and environmental recording apparatuses are leading to increased documentation of our daily lives. This trend is bound to continue and it follows that academic, industry, and government groups are showing an increased interest in such endeavors for various purposes. In the present case, I assert that such documentation can be used to help remedy common memory problems. Assuming a long-term personal archive exists, when confronted with a memory problem, one faces a new challenge, that of finding relevant memory triggers. This dissertation examines the use of information-retrieval technologies on long-term archives of personal experiences towards remedying certain types of long-term forgetting. The approach focuses on capturing audio for the content. Research on Spoken Document Retrieval examines the pitfalls of information-retrieval techniques on error-prone speech- recognizer-generated transcripts and these challenges carry over to the present task. However, \"memory retrieval\" can benefit from the person's familiarity of the recorded data and the context in which it was recorded to help guide their effort. To study this, I constructed memory-retrieval tools designed to leverage a person's familiarity of their past to optimize their search task. To evaluate the utility of these towards solving long-term memory problems, I (1) recorded public events and evaluated witnesses' memory-retrieval approaches using these tools; and (2) conducted a longer- term memory-retrieval study based on recordings of several years of my personal and research-related conversations. Subjects succeeded with memory-retrieval tasks in both studies, typically finding answers within minutes.",
    "advisors": ["Walter Bender"],
    "text": "Personal long-term memory aids The prevalence and affordability of personal and environmental recording apparatuses are leading to increased documentation of our daily lives. This trend is bound to continue and it follows that academic, industry, and government groups are showing an increased interest in such endeavors for various purposes. In the present case, I assert that such documentation can be used to help remedy common memory problems. Assuming a long-term personal archive exists, when confronted with a memory problem, one faces a new challenge, that of finding relevant memory triggers. This dissertation examines the use of information-retrieval technologies on long-term archives of personal experiences towards remedying certain types of long-term forgetting. The approach focuses on capturing audio for the content. Research on Spoken Document Retrieval examines the pitfalls of information-retrieval techniques on error-prone speech- recognizer-generated transcripts and these challenges carry over to the present task. However, \"memory retrieval\" can benefit from the person's familiarity of the recorded data and the context in which it was recorded to help guide their effort. To study this, I constructed memory-retrieval tools designed to leverage a person's familiarity of their past to optimize their search task. To evaluate the utility of these towards solving long-term memory problems, I (1) recorded public events and evaluated witnesses' memory-retrieval approaches using these tools; and (2) conducted a longer- term memory-retrieval study based on recordings of several years of my personal and research-related conversations. Subjects succeeded with memory-retrieval tasks in both studies, typically finding answers within minutes."
}, {
    "id": "oai:dspace.mit.edu:1721.1/78199",
    "title": "Digital cellular solids : reconfigurable composite materials",
    "abstract": "Digital materials are comprised of a small number of types of discrete physical building blocks, which assemble to form constructions that meet the versatility and scalability of digital computation and communication systems. This work seeks to demonstrate the applicability of a digital material approach in designing new cellular materials and methods for assembly of structures with static reconfigurability. The science of cellular solids has enabled the widespread use of lightweight materials to meet important engineering needs, such as passive energy absorption, but they are not in widespread use for structural applications, perhaps due to a large gap between the strength and stiffness to weight ratios of popular classical solids, and the performance of known lightweight cellular materials that are produced from the same constituent material. The engineering of fiber reinforced composite materials has enabled structures with large reductions in weight for given strength and stiffness targets, but at very high design and processing costs, and many challenges producing mechanical interfaces (joints). Digital materials promise scalable methods of producing functional things with reconfigurable sets of discrete and compatible parts, but the presence of many reversible connections raises questions about the performance of the end result. Digital Cellular Solids are cellular solids that exhibit improvements in relative stiffness and strength compared to relative density, over current practices for producing lightweight materials. This is accomplished by assembling lattice geometries that perform better than any that we know how to make with traditional methods. When implemented with fiber composites, the result is not only stiffer and stronger than any previously known ultra-light material, but it presents a new scalable and flexible workflow for applying fiber composites to engineering problems.",
    "advisors": ["Neil Gershenfeld"],
    "text": "Digital cellular solids : reconfigurable composite materials Digital materials are comprised of a small number of types of discrete physical building blocks, which assemble to form constructions that meet the versatility and scalability of digital computation and communication systems. This work seeks to demonstrate the applicability of a digital material approach in designing new cellular materials and methods for assembly of structures with static reconfigurability. The science of cellular solids has enabled the widespread use of lightweight materials to meet important engineering needs, such as passive energy absorption, but they are not in widespread use for structural applications, perhaps due to a large gap between the strength and stiffness to weight ratios of popular classical solids, and the performance of known lightweight cellular materials that are produced from the same constituent material. The engineering of fiber reinforced composite materials has enabled structures with large reductions in weight for given strength and stiffness targets, but at very high design and processing costs, and many challenges producing mechanical interfaces (joints). Digital materials promise scalable methods of producing functional things with reconfigurable sets of discrete and compatible parts, but the presence of many reversible connections raises questions about the performance of the end result. Digital Cellular Solids are cellular solids that exhibit improvements in relative stiffness and strength compared to relative density, over current practices for producing lightweight materials. This is accomplished by assembling lattice geometries that perform better than any that we know how to make with traditional methods. When implemented with fiber composites, the result is not only stiffer and stronger than any previously known ultra-light material, but it presents a new scalable and flexible workflow for applying fiber composites to engineering problems."
}, {
    "id": "oai:dspace.mit.edu:1721.1/28779",
    "title": "Social catalysts : embracing communication in mediated spaces",
    "abstract": "Mediated communication between public spaces is a relatively new concept. One current example of this interaction is video conferencing among people within the same organization. Large scale video-conferencing walls have begun to appear in public or semi-public areas, such as workplace lobbies and kitchens. These connections provide a link via audio and/or video to another public space within the organization. When placed in public or semi-public work spaces, they are often designed for casual encounters among people within that community. Thus far, communicating via these systems has not met expectations. Some drawbacks to such systems have been lack of privacy, gaze ambiguity, spatial incongruity, and fear of appearing too social in a work environment. In this thesis we explore a different goal and approach to linking public spaces. We are not creating a substitute for face-to-face interaction, but rather new modes of conversational and physical interaction within this blended space. This is accomplished through the introduction of what we are defining as a social catalyst. We address the need for designs best suited for linking public spaces and present a series of design criteria for incorporating mediated communication between public and semi-public spaces.",
    "advisors": ["Judith S. Donath"],
    "text": "Social catalysts : embracing communication in mediated spaces Mediated communication between public spaces is a relatively new concept. One current example of this interaction is video conferencing among people within the same organization. Large scale video-conferencing walls have begun to appear in public or semi-public areas, such as workplace lobbies and kitchens. These connections provide a link via audio and/or video to another public space within the organization. When placed in public or semi-public work spaces, they are often designed for casual encounters among people within that community. Thus far, communicating via these systems has not met expectations. Some drawbacks to such systems have been lack of privacy, gaze ambiguity, spatial incongruity, and fear of appearing too social in a work environment. In this thesis we explore a different goal and approach to linking public spaces. We are not creating a substitute for face-to-face interaction, but rather new modes of conversational and physical interaction within this blended space. This is accomplished through the introduction of what we are defining as a social catalyst. We address the need for designs best suited for linking public spaces and present a series of design criteria for incorporating mediated communication between public and semi-public spaces."
}, {
    "id": "oai:dspace.mit.edu:1721.1/28777",
    "title": "Mobile cinema",
    "abstract": "This thesis develops techniques and methods that extend the art and craft of storytelling, and in particular enable the creation of mobile cinema. Stories are always constrained by the medium in which they are told and the mode by which they are delivered to an audience. This dissertation addresses the design of content, systems, and tools that facilitate the emerging type of computational audio-visual narrative that we call mobile cinema. Storytelling in this medium requires temporally and spatially encoded narrative segments that are delivered over a wireless channel to mobile devices such as PDAs and mobile phones. These devices belong to \"the audience,\" individuals who are navigating physical space and interact with local circumstances in the environment. This thesis examines the underlying requirements for coherent mobile narrative and explores two particular challenges which must be solved in order to make a reliable and scalable stream of content for mobile cinema: technology uncertainty (the fact that what the mobile cinema system presents may not be what the creator intends) and participation uncertainty (the fact that what the audience does may not be what the creator expects). The exploration and analysis of these problems involved prototyping two versions of the M-Views system for mobile cinema and three prototype cinematic narratives. Small user studies accompanied each production. The iterative process enabled the author to explore both aspects of uncertainty and to introduce innovations in four key areas to help address these uncertainties: practical location detection, authoring tools designed for mobile channels, responsive story presentation mechanisms, and creative story production strategies.",
    "advisors": ["Glorianna Davenport"],
    "text": "Mobile cinema This thesis develops techniques and methods that extend the art and craft of storytelling, and in particular enable the creation of mobile cinema. Stories are always constrained by the medium in which they are told and the mode by which they are delivered to an audience. This dissertation addresses the design of content, systems, and tools that facilitate the emerging type of computational audio-visual narrative that we call mobile cinema. Storytelling in this medium requires temporally and spatially encoded narrative segments that are delivered over a wireless channel to mobile devices such as PDAs and mobile phones. These devices belong to \"the audience,\" individuals who are navigating physical space and interact with local circumstances in the environment. This thesis examines the underlying requirements for coherent mobile narrative and explores two particular challenges which must be solved in order to make a reliable and scalable stream of content for mobile cinema: technology uncertainty (the fact that what the mobile cinema system presents may not be what the creator intends) and participation uncertainty (the fact that what the audience does may not be what the creator expects). The exploration and analysis of these problems involved prototyping two versions of the M-Views system for mobile cinema and three prototype cinematic narratives. Small user studies accompanied each production. The iterative process enabled the author to explore both aspects of uncertainty and to introduce innovations in four key areas to help address these uncertainties: practical location detection, authoring tools designed for mobile channels, responsive story presentation mechanisms, and creative story production strategies."
}, {
    "id": "oai:dspace.mit.edu:1721.1/61934",
    "title": "Computer as chalk : cultivating and sustaining communities of youth as designers of tangible user interfaces",
    "abstract": "My research efforts focus primarily on two areas: (1) developing engaging technological tools that promote learning and creative expression and (2) designing supportive environments that invite broad participation with these technologies. In this dissertation, I argue that the ways in which people use chalk (e.g., drawing hopscotch grids) can serve as an inspiration for rethinking how people can harness the expressive power of computational technologies. Today's computing devices have the potential to enhance expressive activities for diverse groups in similar ways that chalk does, but that potential has yet to be realized. At the core of my research is the Hook-ups System, a set of technologies and activities designed to enable young people to create interactive experiences by programming connections between physical and digital media. With it, young people integrate sensors with various materials to create tangible interfaces for controlling images and sounds in computer programs that they themselves create. For example, a 10-year-old created a paper-plate-based flying saucer, added a sensor, then wrote a program to control an animated flying saucer image on the computer screen. A framework called the Constellation of Connected Creators emerged from my work with the Hook-ups System. It provides facilitators with strategies for introducing technological tools and activities to communities of learners. It identifies several roles that both facilitators and participants adopt over time to sustain youth engagement in technology-rich learning activities: creator, co-learner, collaborator, coach, and colleague. This dissertation reports on my investigation that took place in two after-school technology centers over a five-year period. Two sets of questions guided my inquiry. The first set probed how attributes of the Hook-ups System enabled diverse audiences to engage in building personally meaningful projects, express themselves, and transform how they approached design. The second set examined which strategies were successful for using the Constellation of Connected Creators to establish a culture in which facilitators engaged groups of newcomers, cultivated future facilitators and supported their successors.",
    "advisors": ["Mitchel Resnick"],
    "text": "Computer as chalk : cultivating and sustaining communities of youth as designers of tangible user interfaces My research efforts focus primarily on two areas: (1) developing engaging technological tools that promote learning and creative expression and (2) designing supportive environments that invite broad participation with these technologies. In this dissertation, I argue that the ways in which people use chalk (e.g., drawing hopscotch grids) can serve as an inspiration for rethinking how people can harness the expressive power of computational technologies. Today's computing devices have the potential to enhance expressive activities for diverse groups in similar ways that chalk does, but that potential has yet to be realized. At the core of my research is the Hook-ups System, a set of technologies and activities designed to enable young people to create interactive experiences by programming connections between physical and digital media. With it, young people integrate sensors with various materials to create tangible interfaces for controlling images and sounds in computer programs that they themselves create. For example, a 10-year-old created a paper-plate-based flying saucer, added a sensor, then wrote a program to control an animated flying saucer image on the computer screen. A framework called the Constellation of Connected Creators emerged from my work with the Hook-ups System. It provides facilitators with strategies for introducing technological tools and activities to communities of learners. It identifies several roles that both facilitators and participants adopt over time to sustain youth engagement in technology-rich learning activities: creator, co-learner, collaborator, coach, and colleague. This dissertation reports on my investigation that took place in two after-school technology centers over a five-year period. Two sets of questions guided my inquiry. The first set probed how attributes of the Hook-ups System enabled diverse audiences to engage in building personally meaningful projects, express themselves, and transform how they approached design. The second set examined which strategies were successful for using the Constellation of Connected Creators to establish a culture in which facilitators engaged groups of newcomers, cultivated future facilitators and supported their successors."
}, {
    "id": "oai:dspace.mit.edu:1721.1/115736",
    "title": "Networked Playscapes : redefining the playground",
    "abstract": "In recent years the world became mostly urban, communication untethered and objects surpassed humans connected to the Internet. We are being shaped by the intersection of urbanization and ubiquitous computing. \"Smart Cities\" offer an efficiency-driven solution by \"programming\" the city, but this centralized approach forgets that it is the people that make the city and that playing is central to being human. Digital or physical, play is an act of creation and appropriation, a respite in a world geared towards consumption, efficiency and technological determinism. Simultaneously, playgrounds are suffering abandonment. Poorly designed, they are deemed childish and boring, the streets insecure and parents too busy. Portable computing devices have taken over most of the playtime and confined it to human-screen interaction. With less time spent outdoors, social networks and video games have become important hubs where we converge to play-mediated, across distance, with people we might never meet. This dissertation proposes that the advantages of connected play need not be exclusive to the indoors, and that playgrounds today need no real estate. Additionally, it hypothesizes that connected play in the public space enhances the social integration function that playgrounds as architectural constructs have previously served. Drawing from research in play, cognitive development, ubiquitous computing, architecture, telepresence and urban planning, this dissertation posits the redesign of playgrounds into Networked Playscapes. Grounded in the public space, they take existing urban affordances and add largely invisible technological underpinnings so as to support connected play. Deployed in Mexico City, Networked Playscapes is illustrated through three experiments: Triciclo, Andamio and ListenTree. Placed at highly marginalized areas and designed with a broad definition of play, they provide infrastructure for connection at different scales while centering on ludic interaction as the purpose to come together across social and geographic divisions. Space informs play as much as play can inform space. This thesis will discuss design guidelines driven by local idiosyncrasies and physical affordances for grounding and place making, and proposes taking the telepresent quality of imaginative play as the parameter to make congruous use of physical computing embedded in architectural constructs and nature itself.",
    "advisors": ["V. Michael Bove"],
    "text": "Networked Playscapes : redefining the playground In recent years the world became mostly urban, communication untethered and objects surpassed humans connected to the Internet. We are being shaped by the intersection of urbanization and ubiquitous computing. \"Smart Cities\" offer an efficiency-driven solution by \"programming\" the city, but this centralized approach forgets that it is the people that make the city and that playing is central to being human. Digital or physical, play is an act of creation and appropriation, a respite in a world geared towards consumption, efficiency and technological determinism. Simultaneously, playgrounds are suffering abandonment. Poorly designed, they are deemed childish and boring, the streets insecure and parents too busy. Portable computing devices have taken over most of the playtime and confined it to human-screen interaction. With less time spent outdoors, social networks and video games have become important hubs where we converge to play-mediated, across distance, with people we might never meet. This dissertation proposes that the advantages of connected play need not be exclusive to the indoors, and that playgrounds today need no real estate. Additionally, it hypothesizes that connected play in the public space enhances the social integration function that playgrounds as architectural constructs have previously served. Drawing from research in play, cognitive development, ubiquitous computing, architecture, telepresence and urban planning, this dissertation posits the redesign of playgrounds into Networked Playscapes. Grounded in the public space, they take existing urban affordances and add largely invisible technological underpinnings so as to support connected play. Deployed in Mexico City, Networked Playscapes is illustrated through three experiments: Triciclo, Andamio and ListenTree. Placed at highly marginalized areas and designed with a broad definition of play, they provide infrastructure for connection at different scales while centering on ludic interaction as the purpose to come together across social and geographic divisions. Space informs play as much as play can inform space. This thesis will discuss design guidelines driven by local idiosyncrasies and physical affordances for grounding and place making, and proposes taking the telepresent quality of imaginative play as the parameter to make congruous use of physical computing embedded in architectural constructs and nature itself."
}, {
    "id": "oai:dspace.mit.edu:1721.1/95588",
    "title": "Computational visual reality",
    "abstract": "It is not so far-fetched to envision a future student working through a difficult physics problem by using their hands to manipulate a 3D visualization that floats above the desk. A doctor preparing for heart surgery will rehearse on a photo-real replica of his patient's organ. A visitor to the British Museum in London will sketch a golden Pharaoh's headdress, illuminated by a ray of sunlight pouring in the window, never aware that the physical artifact is still in Egypt. Though such scenarios may seem cut from the pages of science fiction, this thesis illuminates a path to making them possible. To create more realistic and interactive visual information, displays must show high quality 3D images that respond to environmental lighting conditions and user input. The availability of displays capable of addressing the full range of visual experience will improve our ability to interact with computation, the world, and one another. Two of the many problems that have impeded previous efforts to design high-dimensional displays are the need to: 1. process large amounts of information in realtime; and 2. fabricate hardware capable of conveying that information. Light field capture and display is enormously data-intensive, but by applying compressive techniques that take advantage of multiple data redundancies in light transport, it is possible to overcome these challenges and make use of hardware available in the near-term. This thesis proposes display and capture frameworks that use non-negative tensor factorization and dictionary-based sparse reconstruction, respectively, in conjunction with the co-design of algorithms, optics, and electronics to allow compressive, simultaneous, light field display and capture.",
    "advisors": ["Henry Holtzman", "Ramesh Raskar"],
    "text": "Computational visual reality It is not so far-fetched to envision a future student working through a difficult physics problem by using their hands to manipulate a 3D visualization that floats above the desk. A doctor preparing for heart surgery will rehearse on a photo-real replica of his patient's organ. A visitor to the British Museum in London will sketch a golden Pharaoh's headdress, illuminated by a ray of sunlight pouring in the window, never aware that the physical artifact is still in Egypt. Though such scenarios may seem cut from the pages of science fiction, this thesis illuminates a path to making them possible. To create more realistic and interactive visual information, displays must show high quality 3D images that respond to environmental lighting conditions and user input. The availability of displays capable of addressing the full range of visual experience will improve our ability to interact with computation, the world, and one another. Two of the many problems that have impeded previous efforts to design high-dimensional displays are the need to: 1. process large amounts of information in realtime; and 2. fabricate hardware capable of conveying that information. Light field capture and display is enormously data-intensive, but by applying compressive techniques that take advantage of multiple data redundancies in light transport, it is possible to overcome these challenges and make use of hardware available in the near-term. This thesis proposes display and capture frameworks that use non-negative tensor factorization and dictionary-based sparse reconstruction, respectively, in conjunction with the co-design of algorithms, optics, and electronics to allow compressive, simultaneous, light field display and capture."
}, {
    "id": "oai:dspace.mit.edu:1721.1/61929",
    "title": "Modeling and analysis of affective influences on human experience, prediction, decision making, and behavior",
    "abstract": "Subjective and affective elements are well-known to influence human decision making. This dissertation presents a theoretical and empirical framework on how human decision makers' subjective experience and affective prediction influence their choice behavior under uncertainty, frames and emotions. The framework extends and integrates existing theories of prospect theory (PT) and reinforcement learning (RL), drawing on a growing literature offering the role of affect in decision making and the neural underpinnings of human decision behavior. The proposed Affective-Cognitive (AC) model extends Prospect Theory (PT)- based subjective value functions to model human experienced-utility and predicted-utility functions. The AC model assumes that the shapes (or parameters) of these subjective value functions dynamically vary with the decision makers affective states in sequential decision making. Human decision-making experiments were conducted to empirically infer how people adjust the parameters (i.e., shape and reference point) of their experienced-utility and predicted-utility functions in sequential decision-making situations involving incidental affective states (e.g., anger, fear, economic fear) and task-related confidence. I constructed a new model combining measures to evaluate risk preferences: behavioral choices, selfreported experience self-reported experience, self-reported predicted utility, self-reported confidence. The analysis results show how domain uncertainty, framing, and emotion state of decision makers influence their subjective experience and discriminability, affective prediction, optimal decisions and exploratory regulation. I found empirically that there were significant interaction effects of framing and emotion on risk preferences: negative emotions made people more risk-averse in face of gains. When it comes to losses, anger made people more risk-averse and fear more risk seeking. I also characterized how gender and emotion influence confidence and exploratory choice behavior. The theoretical analysis nicely supports empirical findings from human experiments. The new model provides a theory that better explain and simulate human behavior under uncertainty, frames and emotions.",
    "advisors": ["Rosalind W. Picard"],
    "text": "Modeling and analysis of affective influences on human experience, prediction, decision making, and behavior Subjective and affective elements are well-known to influence human decision making. This dissertation presents a theoretical and empirical framework on how human decision makers' subjective experience and affective prediction influence their choice behavior under uncertainty, frames and emotions. The framework extends and integrates existing theories of prospect theory (PT) and reinforcement learning (RL), drawing on a growing literature offering the role of affect in decision making and the neural underpinnings of human decision behavior. The proposed Affective-Cognitive (AC) model extends Prospect Theory (PT)- based subjective value functions to model human experienced-utility and predicted-utility functions. The AC model assumes that the shapes (or parameters) of these subjective value functions dynamically vary with the decision makers affective states in sequential decision making. Human decision-making experiments were conducted to empirically infer how people adjust the parameters (i.e., shape and reference point) of their experienced-utility and predicted-utility functions in sequential decision-making situations involving incidental affective states (e.g., anger, fear, economic fear) and task-related confidence. I constructed a new model combining measures to evaluate risk preferences: behavioral choices, selfreported experience self-reported experience, self-reported predicted utility, self-reported confidence. The analysis results show how domain uncertainty, framing, and emotion state of decision makers influence their subjective experience and discriminability, affective prediction, optimal decisions and exploratory regulation. I found empirically that there were significant interaction effects of framing and emotion on risk preferences: negative emotions made people more risk-averse in face of gains. When it comes to losses, anger made people more risk-averse and fear more risk seeking. I also characterized how gender and emotion influence confidence and exploratory choice behavior. The theoretical analysis nicely supports empirical findings from human experiments. The new model provides a theory that better explain and simulate human behavior under uncertainty, frames and emotions."
}, {
    "id": "oai:dspace.mit.edu:1721.1/91873",
    "title": "WaaZam! : supporting creative play at a distance in customized video environments",
    "abstract": "This thesis presents the design, implementation and evaluation of WaaZam, a telepresence system for families that supports social engagement through creative play in a unified video space. The goal of the project is to design an environment that facilitates more engaging shared experiences for geographically separated adults and children by placing them together in fantastic worlds and providing tools for them to customize the environment. Standard video mediated technologies provide a live window between remote spaces but do not support users who want to design and interact together in those virtual spaces. We outline the design an implementation of a merged telepresence environment with assets layered in 3D space. The software includes a scene maker and a scene renderer in which users can quickly design digital sets and then play in them together. It supports both physical and digital customization through gestural tools that allow users to seamlessly layer digital and physical content with their bodies. Within the environment users can also transform their appearance and record video. We present the result of pilot studies and a formal evaluation of families using WaaZam in four conditions: separate windows, magic mirror, provided fantastic sets, and self-made sets. Across these conditions we measure dramatic and narrative play patterns, mutuality, social engagement, and physical engagement. Adults and children are interviewed to determine if personalization increases the richness and depth of the user experience. A longitudinal design plan outlines a framework for how this platform could scale beyond the laboratory. We provide an outline for implementing scalable web technologies, a video protocol that supports depth and metadata, and design guidelines for installation in the homes of geographically separated families to support shared experiences.",
    "advisors": ["Pattie Maes"],
    "text": "WaaZam! : supporting creative play at a distance in customized video environments This thesis presents the design, implementation and evaluation of WaaZam, a telepresence system for families that supports social engagement through creative play in a unified video space. The goal of the project is to design an environment that facilitates more engaging shared experiences for geographically separated adults and children by placing them together in fantastic worlds and providing tools for them to customize the environment. Standard video mediated technologies provide a live window between remote spaces but do not support users who want to design and interact together in those virtual spaces. We outline the design an implementation of a merged telepresence environment with assets layered in 3D space. The software includes a scene maker and a scene renderer in which users can quickly design digital sets and then play in them together. It supports both physical and digital customization through gestural tools that allow users to seamlessly layer digital and physical content with their bodies. Within the environment users can also transform their appearance and record video. We present the result of pilot studies and a formal evaluation of families using WaaZam in four conditions: separate windows, magic mirror, provided fantastic sets, and self-made sets. Across these conditions we measure dramatic and narrative play patterns, mutuality, social engagement, and physical engagement. Adults and children are interviewed to determine if personalization increases the richness and depth of the user experience. A longitudinal design plan outlines a framework for how this platform could scale beyond the laboratory. We provide an outline for implementing scalable web technologies, a video protocol that supports depth and metadata, and design guidelines for installation in the homes of geographically separated families to support shared experiences."
}, {
    "id": "oai:dspace.mit.edu:1721.1/8674",
    "title": "Sculptured computational objects with smart and active computing materials",
    "abstract": "This thesis presents the creative, technological, and philosophical means and methodology, by which technology artists and researchers can materially and sculpturally transform physical computing technology from hard, remotely-designed, plastic shells, into intimately created, sensual computing objects and artifacts. It asserts that the rigid, square, and prefabricated physical materials of computing technology are a fundamental technological and artistic limitation to anyone who wishes to sensually transform physical computing technology, or develop a rich artistic vocabulary for it. Smart and active sculptural computing materials are presented as a solution to this problem. Practically, smart computing materials reduce the number of separate, rigid, and square prefabricated parts required to create physical computing objects. Artistically, active sculptural computing materials give artists and designers the ability to directly manipulate, shape, experiment with, and therefore aesthetically understand the real, physical materials of computing technology. Such active design materials will also enable creative people to develop a meaningful artistic relationship between physical form and computation. The total contributions of this thesis include a proposal for a future three-dimensional design/technology practice, a portfolio of sensually transformed expressive computational objects (including new physical interfaces, electronic fashions, and embroidered musical instruments), and the smart and active sculptural computing materials and processes (in this case smart textiles), which make that transformation possible. Projects from the design portfolio include: The Triangles, and its applications; Electronic Fashions, including the Firefly Dress and Necklace, New Year's Eve Ball Gown, and Serial Suit; The Musical Jacket; Electronic Tablecloths; and a series of Embroidered Musical Instruments with embroidered pressure sensors. Contributions from the supporting technical area include: the first fabric keypad (a row and column switch matrix), a new conductive yarn capable of tying and electrical/mechanical knot, an advanced process for machine embroidering highly conductive, flexible and visually diverse electrodes, an empirical model of complex impedance sensing, and a definition of and test for the machine sewability and flexibility of yarns. These contributions are presented in three sections: 1) the supporting arguments, and philosophy of materiality and computation behind this work, 2) the design portfolio, and 3) the supporting technical story.",
    "advisors": ["Tod Machover"],
    "text": "Sculptured computational objects with smart and active computing materials This thesis presents the creative, technological, and philosophical means and methodology, by which technology artists and researchers can materially and sculpturally transform physical computing technology from hard, remotely-designed, plastic shells, into intimately created, sensual computing objects and artifacts. It asserts that the rigid, square, and prefabricated physical materials of computing technology are a fundamental technological and artistic limitation to anyone who wishes to sensually transform physical computing technology, or develop a rich artistic vocabulary for it. Smart and active sculptural computing materials are presented as a solution to this problem. Practically, smart computing materials reduce the number of separate, rigid, and square prefabricated parts required to create physical computing objects. Artistically, active sculptural computing materials give artists and designers the ability to directly manipulate, shape, experiment with, and therefore aesthetically understand the real, physical materials of computing technology. Such active design materials will also enable creative people to develop a meaningful artistic relationship between physical form and computation. The total contributions of this thesis include a proposal for a future three-dimensional design/technology practice, a portfolio of sensually transformed expressive computational objects (including new physical interfaces, electronic fashions, and embroidered musical instruments), and the smart and active sculptural computing materials and processes (in this case smart textiles), which make that transformation possible. Projects from the design portfolio include: The Triangles, and its applications; Electronic Fashions, including the Firefly Dress and Necklace, New Year's Eve Ball Gown, and Serial Suit; The Musical Jacket; Electronic Tablecloths; and a series of Embroidered Musical Instruments with embroidered pressure sensors. Contributions from the supporting technical area include: the first fabric keypad (a row and column switch matrix), a new conductive yarn capable of tying and electrical/mechanical knot, an advanced process for machine embroidering highly conductive, flexible and visually diverse electrodes, an empirical model of complex impedance sensing, and a definition of and test for the machine sewability and flexibility of yarns. These contributions are presented in three sections: 1) the supporting arguments, and philosophy of materiality and computation behind this work, 2) the design portfolio, and 3) the supporting technical story."
}, {
    "id": "oai:dspace.mit.edu:1721.1/44913",
    "title": "Using machine learning for real-time activity recognition and estimation of energy expenditure",
    "abstract": "Obesity is now considered a global epidemic and is predicted to become the number one preventive health threat in the industrialized world. Presently, over 60% of the U.S. adult population is overweight and 30% is obese. This is of concern because obesity is linked to leading causes of death, such as heart and pulmonary diseases, stroke, and type 2 diabetes. The dramatic rise in obesity rates is attributed to an environment that provides easy access to high caloric food and drink and promotes low levels of physical activity. Unfortunately, many people have a poor understanding of their own daily energy (im)balance: the number of calories they consume from food compared with what they expend through physical activity. Accelerometers offer promise as an objective measure of physical activity. In prior work they have been used to estimate energy expenditure and activity type. This work further demonstrates how wireless accelerometers can be used for real-time automatic recognition of physical activity type, intensity, and duration and estimation of energy expenditure. The parameters of the algorithms such as type of classifier/regressor, feature set, window length, signal preprocessing, sensor set utilized and their placement on the human body are selected by performing a set of incremental experiments designed to identify sets of parameters that may balance system usability with robust, real-time performance in low processing power devices such as mobile phones. The algorithms implemented are evaluated using a dataset of examples of 52 activities collected from 20 participants at a gymnasium and a residential home. The algorithms presented here may ultimately allow for the development of mobile phone-based just-in-time interventions to increase self-awareness of physical activity patterns and increases in physical activity levels in real-time during free-living that scale to large populations.",
    "advisors": ["Kent Larson"],
    "text": "Using machine learning for real-time activity recognition and estimation of energy expenditure Obesity is now considered a global epidemic and is predicted to become the number one preventive health threat in the industrialized world. Presently, over 60% of the U.S. adult population is overweight and 30% is obese. This is of concern because obesity is linked to leading causes of death, such as heart and pulmonary diseases, stroke, and type 2 diabetes. The dramatic rise in obesity rates is attributed to an environment that provides easy access to high caloric food and drink and promotes low levels of physical activity. Unfortunately, many people have a poor understanding of their own daily energy (im)balance: the number of calories they consume from food compared with what they expend through physical activity. Accelerometers offer promise as an objective measure of physical activity. In prior work they have been used to estimate energy expenditure and activity type. This work further demonstrates how wireless accelerometers can be used for real-time automatic recognition of physical activity type, intensity, and duration and estimation of energy expenditure. The parameters of the algorithms such as type of classifier/regressor, feature set, window length, signal preprocessing, sensor set utilized and their placement on the human body are selected by performing a set of incremental experiments designed to identify sets of parameters that may balance system usability with robust, real-time performance in low processing power devices such as mobile phones. The algorithms implemented are evaluated using a dataset of examples of 52 activities collected from 20 participants at a gymnasium and a residential home. The algorithms presented here may ultimately allow for the development of mobile phone-based just-in-time interventions to increase self-awareness of physical activity patterns and increases in physical activity levels in real-time during free-living that scale to large populations."
}, {
    "id": "oai:dspace.mit.edu:1721.1/91432",
    "title": "InterTwinkles : online tools for non-hierarchical, consensus-oriented decision making",
    "abstract": "Non-hierarchical, participatory, consensus-based decision making has seen an explosion in popularity in recent years. The traditional techniques of formal consensus, however, are limited to face-to-face meetings, which can limit organizations' capacity due to their time and cost. InterTwinkles is a set of integrated but composable online tools designed to assist small and medium-sized groups in engaging in formal group decision making processes online. In this thesis, I present a thorough investigation of the ethical and practical motivations for consensus decision making, and relate these to concerns of control and autonomy in the design of online systems. I describe the participatory and iterative design process for building an online platform for consensus, with particular attention to the practical constraints of real-world groups with mixed technical aptitude. I present the results of a three month field trial with six cooperative groups in the Boston area, and evaluate the results through the lens of adaptive structuration theory, with particular attention on the fit between the ethical motivations and performance outcomes.",
    "advisors": ["Christopher Schmandt"],
    "text": "InterTwinkles : online tools for non-hierarchical, consensus-oriented decision making Non-hierarchical, participatory, consensus-based decision making has seen an explosion in popularity in recent years. The traditional techniques of formal consensus, however, are limited to face-to-face meetings, which can limit organizations' capacity due to their time and cost. InterTwinkles is a set of integrated but composable online tools designed to assist small and medium-sized groups in engaging in formal group decision making processes online. In this thesis, I present a thorough investigation of the ethical and practical motivations for consensus decision making, and relate these to concerns of control and autonomy in the design of online systems. I describe the participatory and iterative design process for building an online platform for consensus, with particular attention to the practical constraints of real-world groups with mixed technical aptitude. I present the results of a three month field trial with six cooperative groups in the Boston area, and evaluate the results through the lens of adaptive structuration theory, with particular attention on the fit between the ethical motivations and performance outcomes."
}, {
    "id": "oai:dspace.mit.edu:1721.1/28287",
    "title": "Interconnected musical networks : bringing expression and thoughtfulness to collaborative group playing",
    "abstract": "(cont.) In order to addressee the latter challenge I have decided to employ the digital network--a promising candidate for bringing a unique added value to the musical experience of collaborative group playing. I have chosen to address both challenges by embedding cognitive and educational concepts in newly designed interconnect instruments and applications, which led to the development of a number of such Interconnected Musical Networks (IMNs)--live performance systems that allow players to influence, share, and shape each other's music in real-time. In my thesis I discuss the concepts, motivations, and aesthetics of IMNs and review a number of historical and current technological landmarks that led the way to the development of the field. I then suggest a comprehensive theoretical framework for artistic interdependency, based on which I developed a set of instruments and activities in an effort to turn IMNs into an expressive and intuitive art form that provides meaningful learning experiences, engaging collaborative interactions, and worthy music.",
    "advisors": ["Tod Machover"],
    "text": "Interconnected musical networks : bringing expression and thoughtfulness to collaborative group playing (cont.) In order to addressee the latter challenge I have decided to employ the digital network--a promising candidate for bringing a unique added value to the musical experience of collaborative group playing. I have chosen to address both challenges by embedding cognitive and educational concepts in newly designed interconnect instruments and applications, which led to the development of a number of such Interconnected Musical Networks (IMNs)--live performance systems that allow players to influence, share, and shape each other's music in real-time. In my thesis I discuss the concepts, motivations, and aesthetics of IMNs and review a number of historical and current technological landmarks that led the way to the development of the field. I then suggest a comprehensive theoretical framework for artistic interdependency, based on which I developed a set of instruments and activities in an effort to turn IMNs into an expressive and intuitive art form that provides meaningful learning experiences, engaging collaborative interactions, and worthy music."
}, {
    "id": "oai:dspace.mit.edu:1721.1/29264",
    "title": "Tangible interfaces for manipulating aggregates of digital information",
    "abstract": "This thesis develops new approaches for people to physically represent and interact with aggregates of digital information. These support the concept of Tangible User Interfaces (TUIs), a genre of human-computer interaction that uses spatially reconfigurable physical objects as representations and controls for digital information. The thesis supports the manipulation of information aggregates through systems of physical tokens and constraints. In these interfaces, physical tokens act as containers and parameters for referencing digital information elements and aggregates. Physical constraints are then used to map structured compositions of tokens onto a variety of computational interpretations. This approach is supported through the design and implementation of several systems. The mediaBlocks system enables people to use physical blocks to \"copy and paste\" digital media between specialized devices and general-purpose computers, and to physically compose and edit this content (e.g., to build multimedia presentations). This system also contributes new tangible interface techniques for binding, aggregating, and disaggregating sequences of digital information into physical objects.",
    "advisors": ["Hiroshi Ishii"],
    "text": "Tangible interfaces for manipulating aggregates of digital information This thesis develops new approaches for people to physically represent and interact with aggregates of digital information. These support the concept of Tangible User Interfaces (TUIs), a genre of human-computer interaction that uses spatially reconfigurable physical objects as representations and controls for digital information. The thesis supports the manipulation of information aggregates through systems of physical tokens and constraints. In these interfaces, physical tokens act as containers and parameters for referencing digital information elements and aggregates. Physical constraints are then used to map structured compositions of tokens onto a variety of computational interpretations. This approach is supported through the design and implementation of several systems. The mediaBlocks system enables people to use physical blocks to \"copy and paste\" digital media between specialized devices and general-purpose computers, and to physically compose and edit this content (e.g., to build multimedia presentations). This system also contributes new tangible interface techniques for binding, aggregating, and disaggregating sequences of digital information into physical objects."
}, {
    "id": "oai:dspace.mit.edu:1721.1/77809",
    "title": "Design of currency, markets, and economy for knowledge",
    "abstract": "Information markets benefit the communities they serve by facilitating electronic distributed exchange of information. Further benefits include enhancing knowledge sharing, innovation, and productivity. This research explores innovative market mechanisms to build longterm sustainable incentives that many existing platforms fail to provide, while encouraging pro-social behavior. A key advantage of this research is direct application of established information economic and macroeconomic theories to the design of social software and knowledge platforms. The research contribution is the design of a complete framework for information economy, which consists of several distinct components: 1) a market engine for exchanging information products that are non-rivalrous and non-excludable; 2) a serialized currency system that enables monetary acceleration; 3) \"monetary policies\" that ensure a healthy growth of currency supply; 4) \"fiscal policies\" that reward information reuse and good behavior such as tagging, voting, tipping, and fraud reporting. We built a web-based software platform called Barter, and have deployed it at several universities. Analysis of user data helps test information market effectiveness and illustrates effects of various market interventions. We present our key findings learned in the process of system deployment, such as the impacts of social connections on market interactions and fraud, effects of bounty on information quality, market fraud and intervention of fraud prevention mechanism.",
    "advisors": ["Andrew B. Lippman"],
    "text": "Design of currency, markets, and economy for knowledge Information markets benefit the communities they serve by facilitating electronic distributed exchange of information. Further benefits include enhancing knowledge sharing, innovation, and productivity. This research explores innovative market mechanisms to build longterm sustainable incentives that many existing platforms fail to provide, while encouraging pro-social behavior. A key advantage of this research is direct application of established information economic and macroeconomic theories to the design of social software and knowledge platforms. The research contribution is the design of a complete framework for information economy, which consists of several distinct components: 1) a market engine for exchanging information products that are non-rivalrous and non-excludable; 2) a serialized currency system that enables monetary acceleration; 3) \"monetary policies\" that ensure a healthy growth of currency supply; 4) \"fiscal policies\" that reward information reuse and good behavior such as tagging, voting, tipping, and fraud reporting. We built a web-based software platform called Barter, and have deployed it at several universities. Analysis of user data helps test information market effectiveness and illustrates effects of various market interventions. We present our key findings learned in the process of system deployment, such as the impacts of social connections on market interactions and fraud, effects of bounty on information quality, market fraud and intervention of fraud prevention mechanism."
}, {
    "id": "oai:dspace.mit.edu:1721.1/91860",
    "title": "Hybrid re Assembage : bridging traditional craft and digital design",
    "abstract": "Hybrid reAssemblage is a design gestalt that lies at the cross-section of digital design practice and the tactile qualities of traditional craft. It spans a territory in which the value of artifacts is produced through automated production as well as human subjectivity. This work is an exploration of two divergent realms: that of emerging computational technologies, and traditional hand-hewn practice. Hybrid reAssemblage proposes a new way of thinking about the machine, as generator of control and efficiency, and the unpredictable and singular nature of the raw and the manual. I illustrate Hybrid reAssemblage through three diverse projects: FreeD is a digital handheld milling device for carving, guided and monitored by a computer while preserving the maker's freedom to manipulate the work in many creative ways. It reintroduces craft techniques to digital fabrication, proposing a hybrid human-computer interaction experience. In addition to the technology, I present a user study, demonstrating how FreeD enables personalization and expression as an inherent part of the fabrication process. Chameleon Guitar exploits a selection of acoustic properties via a set of replaceable resonators and by a simulated shape, merging real-wood acoustic qualities with a simulated guitar body. It marries digital freedom with the uniqueness of acoustic instruments, and demonstrates a hybrid functionality platform. Focusing on the production of sonic qualities, this project is evaluated acoustically, pointing to the significance of attention to detail such as craft and wood qualities. Finally, Fused Crafts is a collection of artifacts that are part handcrafted and part 3D printed, visually demonstrating the potential of combining these practices to create hybrid aesthetics. I illustrate this visual concept with two examples: intentionally broken ceramic artifacts with 3D printed restoration, and 3D printed structure that is designed to allow the application of hand-woven patterns. This project is a search for an approach where both technologies can benefit from each other aesthetically, enriching the final product with new qualities. This dissertation begins with a contextual background, leading to the presentation of the projects. In the last part, I evaluate the work through feedback received from a panel of design, craft, and HCI experts.",
    "advisors": ["Joseph A. Paradiso"],
    "text": "Hybrid re Assembage : bridging traditional craft and digital design Hybrid reAssemblage is a design gestalt that lies at the cross-section of digital design practice and the tactile qualities of traditional craft. It spans a territory in which the value of artifacts is produced through automated production as well as human subjectivity. This work is an exploration of two divergent realms: that of emerging computational technologies, and traditional hand-hewn practice. Hybrid reAssemblage proposes a new way of thinking about the machine, as generator of control and efficiency, and the unpredictable and singular nature of the raw and the manual. I illustrate Hybrid reAssemblage through three diverse projects: FreeD is a digital handheld milling device for carving, guided and monitored by a computer while preserving the maker's freedom to manipulate the work in many creative ways. It reintroduces craft techniques to digital fabrication, proposing a hybrid human-computer interaction experience. In addition to the technology, I present a user study, demonstrating how FreeD enables personalization and expression as an inherent part of the fabrication process. Chameleon Guitar exploits a selection of acoustic properties via a set of replaceable resonators and by a simulated shape, merging real-wood acoustic qualities with a simulated guitar body. It marries digital freedom with the uniqueness of acoustic instruments, and demonstrates a hybrid functionality platform. Focusing on the production of sonic qualities, this project is evaluated acoustically, pointing to the significance of attention to detail such as craft and wood qualities. Finally, Fused Crafts is a collection of artifacts that are part handcrafted and part 3D printed, visually demonstrating the potential of combining these practices to create hybrid aesthetics. I illustrate this visual concept with two examples: intentionally broken ceramic artifacts with 3D printed restoration, and 3D printed structure that is designed to allow the application of hand-woven patterns. This project is a search for an approach where both technologies can benefit from each other aesthetically, enriching the final product with new qualities. This dissertation begins with a contextual background, leading to the presentation of the projects. In the last part, I evaluate the work through feedback received from a panel of design, craft, and HCI experts."
}, {
    "id": "oai:dspace.mit.edu:1721.1/57570",
    "title": "Reusing a robot's behavioral mechanisms to model and manipulate human mental states",
    "abstract": "In a task domain characterized by physical actions and where information has value, competing teams gain advantage by spying on and deceiving an opposing team while cooperating teammates can help the team by secretly communicating new information. For a robot to thrive in this environment it must be able to perform actions in a manner to deceive opposing agents as well as to be able to secretly communicate with friendly agents. It must further be able to extract information from observing the actions of other agents. The goal of this research is to expand on current human robot interaction by creating a robot that can operate in the above scenario. To enable these behaviors, an architecture is created which provides the robot with mechanisms to work with hidden human mental states. The robot attempts to infer these hidden states from observable factors and use them to better understand and predict behavior. It also takes steps to alter them in order to change the future behavior of the other agent. It utilizes the knowledge that the human is performing analogous inferences about the robot's own internal states to predict the effect of its actions on the human's knowledge and perceptions of the robot. The research focuses on the implicit communication that is made possible by two embodied agents interacting in a shared space through nonverbal interaction. While the processes used by a robot differ significantly from the cognitive mechanisms employed by humans, each face the similar challenge of completing the loop from sensing to acting. This architecture employs a self-as-simulator strategy, reusing the robot's behavioral mechanisms to model aspects of the human's mental states. This reuse allows the robot to model human actions and the mental states behind them using the grammar of its own representations and actions.",
    "advisors": ["Cynthia Breazeal"],
    "text": "Reusing a robot's behavioral mechanisms to model and manipulate human mental states In a task domain characterized by physical actions and where information has value, competing teams gain advantage by spying on and deceiving an opposing team while cooperating teammates can help the team by secretly communicating new information. For a robot to thrive in this environment it must be able to perform actions in a manner to deceive opposing agents as well as to be able to secretly communicate with friendly agents. It must further be able to extract information from observing the actions of other agents. The goal of this research is to expand on current human robot interaction by creating a robot that can operate in the above scenario. To enable these behaviors, an architecture is created which provides the robot with mechanisms to work with hidden human mental states. The robot attempts to infer these hidden states from observable factors and use them to better understand and predict behavior. It also takes steps to alter them in order to change the future behavior of the other agent. It utilizes the knowledge that the human is performing analogous inferences about the robot's own internal states to predict the effect of its actions on the human's knowledge and perceptions of the robot. The research focuses on the implicit communication that is made possible by two embodied agents interacting in a shared space through nonverbal interaction. While the processes used by a robot differ significantly from the cognitive mechanisms employed by humans, each face the similar challenge of completing the loop from sensing to acting. This architecture employs a self-as-simulator strategy, reusing the robot's behavioral mechanisms to model aspects of the human's mental states. This reuse allows the robot to model human actions and the mental states behind them using the grammar of its own representations and actions."
}, {
    "id": "oai:dspace.mit.edu:1721.1/120685",
    "title": "Robotic symbionts : exploring integrated human-machine action and expression",
    "abstract": "Throughout history we have augmented our physical abilities with machines. Concepts for flying machines and the ideas behind today's exoskeletons were recorded as early as the 13th century. Today, as technology permeates every aspect of our lives, it is easy to imagine a much closer integration of machines into the tasks we carry out. This thesis explores a vision of humans and machines symbiotically working together on a task through co-action and coagency. This vision opens up many opportunities in between the extremes of autonomous robots and master-slave systems, through more complex systems in which human and machine collaborate to perform actions and manipulate robotic extensions. This dissertation also reports on three extensive experiments, each consisting of multiple iterations of actual, tested designs: a series of robotic extra-numerary finger robots for increasing manual dexterity, a series of collaborative human-drone drawing systems enabling novel expressive capability, and a series of semi-automated guitar systems enabling extended musical expression as well as new instrument-learning opportunities. The studies performed with these prototypes give insight into the impact of such robotic integration on the human user: the user is nudged to adapt to the new condition and re-calibrate the expectations associated with certain input actions; the division of roles allows the user to explore and understand experiences outside their given skills or physical limits; and the robotic extension inspires activity outside of the user's regular practice. Finally, the thesis also defines a design space and corresponding terminology to situate different technical and design choices for these new forms of human-robot integration. I categorize some of the existing approaches based on how human and robotic actions are coordinated, and how the robotic movements are controlled. I also propose ways to qualitatively describe the interaction between human and machine, in terms of how the robotic extension may affect the cognition and behaviors of its user. The experiments with the prototypes support and are analyzed through these definitions, and discuss how we could achieve novel or synergistic outcomes with robotic augmentations.",
    "advisors": ["Pattie Maes"],
    "text": "Robotic symbionts : exploring integrated human-machine action and expression Throughout history we have augmented our physical abilities with machines. Concepts for flying machines and the ideas behind today's exoskeletons were recorded as early as the 13th century. Today, as technology permeates every aspect of our lives, it is easy to imagine a much closer integration of machines into the tasks we carry out. This thesis explores a vision of humans and machines symbiotically working together on a task through co-action and coagency. This vision opens up many opportunities in between the extremes of autonomous robots and master-slave systems, through more complex systems in which human and machine collaborate to perform actions and manipulate robotic extensions. This dissertation also reports on three extensive experiments, each consisting of multiple iterations of actual, tested designs: a series of robotic extra-numerary finger robots for increasing manual dexterity, a series of collaborative human-drone drawing systems enabling novel expressive capability, and a series of semi-automated guitar systems enabling extended musical expression as well as new instrument-learning opportunities. The studies performed with these prototypes give insight into the impact of such robotic integration on the human user: the user is nudged to adapt to the new condition and re-calibrate the expectations associated with certain input actions; the division of roles allows the user to explore and understand experiences outside their given skills or physical limits; and the robotic extension inspires activity outside of the user's regular practice. Finally, the thesis also defines a design space and corresponding terminology to situate different technical and design choices for these new forms of human-robot integration. I categorize some of the existing approaches based on how human and robotic actions are coordinated, and how the robotic movements are controlled. I also propose ways to qualitatively describe the interaction between human and machine, in terms of how the robotic extension may affect the cognition and behaviors of its user. The experiments with the prototypes support and are analyzed through these definitions, and discuss how we could achieve novel or synergistic outcomes with robotic augmentations."
}, {
    "id": "oai:dspace.mit.edu:1721.1/106069",
    "title": "The use of a novel residuum model to design a variable-impedance transtibial prosthetic socket",
    "abstract": "For people living with limb amputation, the prosthetic socket - the interface between the residuum and prosthesis - is the most critical component. When a socket is uncomfortable, especially due to poor fit, the quality of life for a patient is greatly hindered. However, conventional design of sockets is largely artisan, with limited input of quantitative data. Current computer-aided and manufacturing (CAD/CAM) designs are still not clinically applicable solutions. Due to model identification procedures that employ non patient-specific and incomplete data sets, today's finite element (FE) models of the residuum are not predictive, leading to suboptimal socket designs. As such, there exists a need for a comprehensive biomechanical model of the residuum for the quantitative design and computational evaluation of patient-specific prosthetic sockets. This thesis presents a combined experimental-numerical approach to evaluate and validate a transtibial residuum biomechanical model. The central hypothesis of the work is that a single biomechanical model can predict the large non-linear response at various sites on a residuum under load. To evaluate this hypothesis, a non-linear, two-tissue model was formulated where tissue geometries were defined using MRI data of the residuum. The non-linear viscoelastic material parameters of the model were identified through inverse FEA-based optimization using in-vivo indentation experimental data at four locations. Using optimized model tissue parameters, the mean percentage error (mean absolute error/ maximum experimental force) between the experimental and simulation force-time curves at 14 other locations across the evaluated transtibial residuum was 7  3%. Using this same modeling methodology and a single set of material constants to describe the bulk soft tissue biomechanical response of seven distinct transtibial residual limb models, the average percentage error for indentations at multiple locations across all seven limbs was 7  1%. From these predictive models of residuum limbs, one rigid novel socket and two multimaterial transtibial sockets were designed, fabricated and evaluated through an entirely quantitative, automated and repeatable methodology. In a preliminary clinical investigation, the novel sockets were shown to reduce peak contact pressures at the tibia and fibular head regions on the residuum by significant amounts during standing compared to a conventional socket interface designed and fabricated by a trained prosthetist.",
    "advisors": ["Hugh Herr"],
    "text": "The use of a novel residuum model to design a variable-impedance transtibial prosthetic socket For people living with limb amputation, the prosthetic socket - the interface between the residuum and prosthesis - is the most critical component. When a socket is uncomfortable, especially due to poor fit, the quality of life for a patient is greatly hindered. However, conventional design of sockets is largely artisan, with limited input of quantitative data. Current computer-aided and manufacturing (CAD/CAM) designs are still not clinically applicable solutions. Due to model identification procedures that employ non patient-specific and incomplete data sets, today's finite element (FE) models of the residuum are not predictive, leading to suboptimal socket designs. As such, there exists a need for a comprehensive biomechanical model of the residuum for the quantitative design and computational evaluation of patient-specific prosthetic sockets. This thesis presents a combined experimental-numerical approach to evaluate and validate a transtibial residuum biomechanical model. The central hypothesis of the work is that a single biomechanical model can predict the large non-linear response at various sites on a residuum under load. To evaluate this hypothesis, a non-linear, two-tissue model was formulated where tissue geometries were defined using MRI data of the residuum. The non-linear viscoelastic material parameters of the model were identified through inverse FEA-based optimization using in-vivo indentation experimental data at four locations. Using optimized model tissue parameters, the mean percentage error (mean absolute error/ maximum experimental force) between the experimental and simulation force-time curves at 14 other locations across the evaluated transtibial residuum was 7  3%. Using this same modeling methodology and a single set of material constants to describe the bulk soft tissue biomechanical response of seven distinct transtibial residual limb models, the average percentage error for indentations at multiple locations across all seven limbs was 7  1%. From these predictive models of residuum limbs, one rigid novel socket and two multimaterial transtibial sockets were designed, fabricated and evaluated through an entirely quantitative, automated and repeatable methodology. In a preliminary clinical investigation, the novel sockets were shown to reduce peak contact pressures at the tibia and fibular head regions on the residuum by significant amounts during standing compared to a conventional socket interface designed and fabricated by a trained prosthetist."
}, {
    "id": "oai:dspace.mit.edu:1721.1/78200",
    "title": "Smart customization : making evidence-based environmental decisions",
    "abstract": "This thesis examines the environmental benefits created by the manufacture, distribution, and consumer use of products that are mass customized (MC) or produced \"on-demand\" and tailored to individual end-user preferences. Traditional mass production (MP) models take advantage of economies of scale by efficiently producing multiple copies of the same standard product. However, this also creates waste throughout the product life cycle. The waste of stocks, transportation, overproduction, and non-actuality (markdowns and disposal due to inability to move products in time) pose a problem for manufacturers to achieve financial and environmental sustainability. Studies have found that the textile industry can lose approximately one-third of total revenue ($300B) a year due to waste alone. The men's dress shirt industry serves as a comparative case study in this research, demonstrating the trade-offs between MC and MP methods and enabling evidence-based environmental decisions by manufacturers and consumers. In addition to an examination of the carbon footprint created by the manufacture and distribution of MC vs. MP men's dress shirts, this study includes experiments to understand, in detail, the environmental consequences of shirt acquisition and consumer use. Experiment participants are provided coupons to \"purchase\" two new dress shirts (one MC, one MP), which are embedded with washable and dry-clean proof RFID tags. A RFID tracking system deployed at the entrance and exit of the participants' offices collects data over a period of 60 working days to determine overall utilization patterns. Armed with this \"post-transaction\" information gathered by this tracking methodology and ethnographic findings (information that manufacturers often lack), this thesis provides an evidence-based guide that takes into account the environmental benefits of both MC and MP models to enable manufacturers to produce more sustainable products and consumers to practice \"Responsible Consumerism.\"",
    "advisors": ["Kent Larson"],
    "text": "Smart customization : making evidence-based environmental decisions This thesis examines the environmental benefits created by the manufacture, distribution, and consumer use of products that are mass customized (MC) or produced \"on-demand\" and tailored to individual end-user preferences. Traditional mass production (MP) models take advantage of economies of scale by efficiently producing multiple copies of the same standard product. However, this also creates waste throughout the product life cycle. The waste of stocks, transportation, overproduction, and non-actuality (markdowns and disposal due to inability to move products in time) pose a problem for manufacturers to achieve financial and environmental sustainability. Studies have found that the textile industry can lose approximately one-third of total revenue ($300B) a year due to waste alone. The men's dress shirt industry serves as a comparative case study in this research, demonstrating the trade-offs between MC and MP methods and enabling evidence-based environmental decisions by manufacturers and consumers. In addition to an examination of the carbon footprint created by the manufacture and distribution of MC vs. MP men's dress shirts, this study includes experiments to understand, in detail, the environmental consequences of shirt acquisition and consumer use. Experiment participants are provided coupons to \"purchase\" two new dress shirts (one MC, one MP), which are embedded with washable and dry-clean proof RFID tags. A RFID tracking system deployed at the entrance and exit of the participants' offices collects data over a period of 60 working days to determine overall utilization patterns. Armed with this \"post-transaction\" information gathered by this tracking methodology and ethnographic findings (information that manufacturers often lack), this thesis provides an evidence-based guide that takes into account the environmental benefits of both MC and MP models to enable manufacturers to produce more sustainable products and consumers to practice \"Responsible Consumerism.\""
}, {
    "id": "oai:dspace.mit.edu:1721.1/120882",
    "title": "Score instruments : a new paradigm of musical instruments to guide musical wonderers",
    "abstract": "Advancements in technology have made musical instruments, especially electronic instruments, accessible to the masses. As a result, music-making has become more widespread and convenient. However, the blackboxing practices of commercial Digital Musical Instruments (DMIs) have conditioned many users to produce only specific styles of music. Furthermore, as many of these commercial instruments produce sound through loudspeakers, rather than the body of the instrument, players lose the physical and tactile connection to sound and music. Consequently, these DMIs inhibit understanding of the relationship between musicality and our everyday physical world, and cut players off from exploring a more extensive range of musical possibilities. Despite the multiplication of music-making tools, music-making practices still operate on the same principles. The production of music requires instruments to generate organized physical sound energies that follow the schema of a score. This dissertation studies a new class of Interactive Music Systems (IMSs) called Score Instruments that embed both instrument and score into a single unified interface. Score Instruments reopen the range of possibilities offered by everyday sounds and objects as musical bricolage tools to bring players into a personalized, guided, and open-ended use of the instrument. Players of Score Instruments are called Musical Wonderers, as the instruments encourage them to focus on exploration to build their own musical language, rather than on the technically correct realization of music. The dissertation describes the concept of Score Instruments. Two instances of Score Instruments demonstrate how the techniques and criteria translate into specific IMSs. City Symphonies is a massive musical collaboration platform that encourages players to listen to their cities and create music with environmental sounds. MM-RT is a tabletop tangible musical instrument that employs electromagnetic actuators and small permanent magnets to physically induce sounds with found objects. Both projects exemplify how Score Instruments can simultaneously stimulate open creativity and provide meaningful direction and constraints that guide users to learn underlying principles about music and the physical world. The design investigations and historical perspective of this dissertation offer a future of music-making practice that is based on exploration and designed to broaden the definition and variety of music.",
    "advisors": ["Tod Machover"],
    "text": "Score instruments : a new paradigm of musical instruments to guide musical wonderers Advancements in technology have made musical instruments, especially electronic instruments, accessible to the masses. As a result, music-making has become more widespread and convenient. However, the blackboxing practices of commercial Digital Musical Instruments (DMIs) have conditioned many users to produce only specific styles of music. Furthermore, as many of these commercial instruments produce sound through loudspeakers, rather than the body of the instrument, players lose the physical and tactile connection to sound and music. Consequently, these DMIs inhibit understanding of the relationship between musicality and our everyday physical world, and cut players off from exploring a more extensive range of musical possibilities. Despite the multiplication of music-making tools, music-making practices still operate on the same principles. The production of music requires instruments to generate organized physical sound energies that follow the schema of a score. This dissertation studies a new class of Interactive Music Systems (IMSs) called Score Instruments that embed both instrument and score into a single unified interface. Score Instruments reopen the range of possibilities offered by everyday sounds and objects as musical bricolage tools to bring players into a personalized, guided, and open-ended use of the instrument. Players of Score Instruments are called Musical Wonderers, as the instruments encourage them to focus on exploration to build their own musical language, rather than on the technically correct realization of music. The dissertation describes the concept of Score Instruments. Two instances of Score Instruments demonstrate how the techniques and criteria translate into specific IMSs. City Symphonies is a massive musical collaboration platform that encourages players to listen to their cities and create music with environmental sounds. MM-RT is a tabletop tangible musical instrument that employs electromagnetic actuators and small permanent magnets to physically induce sounds with found objects. Both projects exemplify how Score Instruments can simultaneously stimulate open creativity and provide meaningful direction and constraints that guide users to learn underlying principles about music and the physical world. The design investigations and historical perspective of this dissertation offer a future of music-making practice that is based on exploration and designed to broaden the definition and variety of music."
}, {
    "id": "oai:dspace.mit.edu:1721.1/91854",
    "title": "A substrate for accountable layered systems",
    "abstract": "A system built on a layered reflective cognitive architecture presents many novel and difficult software engineering problems. Some of these problems can be ameliorated by erecting the system on a substrate that implicitly supports tracing the behavior of the system to the data and through the procedures that produced that behavior. Good traces make the system accountable; it enables the analysis of success and failure, and thus enhances the ability to learn from mistakes. This constructed substrate provides for general parallelism and concurrency, while supporting the automatic collection of audit trails for all processes, including the processes that analyze audit trails. My system natively supports a Lisp-like language. In such a language, as in machine language, a program is data that can be easily manipulated by a program, making it easier for a user or an automatic procedure to read, edit, and write programs as they are debugged. Constructed within this substrate is an implementation of the bottom four layers of an Emotion Machine cognitive architecture, including built-in reactive, learned reactive, deliberative, and reflective layers. A simple natural language planning language is presented for the deliberative control of a problem domain. Also, a number of deliberative planning algorithms are implemented in this natural planning language, allowing a recursive application of reflectively planned control. This recursion is demonstrated in a fifth super-reflective layer of planned control of the reflective planning layer, implying N reflective layers of planned control. Here, I build and demonstrate an example of reflective problem solving through the use of English plans in a block building problem domain. In my demonstration an AI model can learn from experience of success or failure. The Al not only learns about physical activities but also reflectively learns about thinking activities, refining and learning the utility of built-in knowledge. Procedurally traced memory can be used to assign credit to those thinking processes that are responsible for the failure, facilitating learning how to better plan for these types of problems in the future.",
    "advisors": ["Joseph Paradiso"],
    "text": "A substrate for accountable layered systems A system built on a layered reflective cognitive architecture presents many novel and difficult software engineering problems. Some of these problems can be ameliorated by erecting the system on a substrate that implicitly supports tracing the behavior of the system to the data and through the procedures that produced that behavior. Good traces make the system accountable; it enables the analysis of success and failure, and thus enhances the ability to learn from mistakes. This constructed substrate provides for general parallelism and concurrency, while supporting the automatic collection of audit trails for all processes, including the processes that analyze audit trails. My system natively supports a Lisp-like language. In such a language, as in machine language, a program is data that can be easily manipulated by a program, making it easier for a user or an automatic procedure to read, edit, and write programs as they are debugged. Constructed within this substrate is an implementation of the bottom four layers of an Emotion Machine cognitive architecture, including built-in reactive, learned reactive, deliberative, and reflective layers. A simple natural language planning language is presented for the deliberative control of a problem domain. Also, a number of deliberative planning algorithms are implemented in this natural planning language, allowing a recursive application of reflectively planned control. This recursion is demonstrated in a fifth super-reflective layer of planned control of the reflective planning layer, implying N reflective layers of planned control. Here, I build and demonstrate an example of reflective problem solving through the use of English plans in a block building problem domain. In my demonstration an AI model can learn from experience of success or failure. The Al not only learns about physical activities but also reflectively learns about thinking activities, refining and learning the utility of built-in knowledge. Procedurally traced memory can be used to assign credit to those thinking processes that are responsible for the failure, facilitating learning how to better plan for these types of problems in the future."
}, {
    "id": "oai:dspace.mit.edu:1721.1/61936",
    "title": "Play it by eye, frame it by hand! : Gesture Object Interfaces to enable a world of multiple projections",
    "abstract": "Tangible Media as an area has not explored how the tangible handle is more than a marker or place-holder for digital data. Tangible Media can do more. It has the power to materialize and redefine our conception of space and content during the creative process. It can vary from an abstract token that represents a movie to an anthropomorphic plush that reflects the behavior of a sibling during play. My work begins by extending tangible concepts of representation and token-based interactions into movie editing and play scenarios. Through several design iterations and research studies, I establish tangible technologies to drive visual and oral perspectives along with finalized creative works, all during a child's play and exploration. I define the framework, Gesture Object Interfaces, expanding on the fields of Tangible User Interaction and Gesture Recognition. Gesture is a mechanism that can reinforce or create the anthropomorphism of an object. It can give the object life. A Gesture Object is an object in hand while doing anthropomorphized gestures. Gesture Object Interfaces engender new visual and narrative perspectives as part of automatic film assembly during children's play. I generated a suite of automatic film assembly tools accessible to diverse users. The tools that I designed allow for capture, editing and performing to be completely indistinguishable from one another. Gestures integrated with objects become a coherent interface on top of natural play. I built a distributed, modular camera environment and gesture interaction to control that environment. The goal of these new technologies is to motivate children to take new visual and narrative perspectives. In this dissertation I present four tangible platforms that I created as alternatives to the usual fragmented and sequential capturing, editing and performing of narratives available to users of current storytelling tools. I developed Play it by Eye, Frame it by hand, a new generation of narrative tools that shift the frame of reference from the eye to the hand, from the viewpoint (where the eye is) to the standpoint (where the hand is). In Play it by Eye, Frame it by Hand environments, children discover atypical perspectives through the lens of everyday objects. When using Picture This!, children imagine how an object would appear relative to the viewpoint of the toy. They iterate between trying and correcting in a world of multiple perspectives. The results are entirely new genres of child-created films, where children finally capture the cherished visual idioms of action and drama. I report my design process over the course of four tangible research projects that I evaluate during qualitative observations with over one hundred 4- to 14-year-old users. Based on these research findings, I propose a class of moviemaking tools that transform the way users interpret the world visually, and through storytelling.",
    "advisors": ["Hiroshi Ishii"],
    "text": "Play it by eye, frame it by hand! : Gesture Object Interfaces to enable a world of multiple projections Tangible Media as an area has not explored how the tangible handle is more than a marker or place-holder for digital data. Tangible Media can do more. It has the power to materialize and redefine our conception of space and content during the creative process. It can vary from an abstract token that represents a movie to an anthropomorphic plush that reflects the behavior of a sibling during play. My work begins by extending tangible concepts of representation and token-based interactions into movie editing and play scenarios. Through several design iterations and research studies, I establish tangible technologies to drive visual and oral perspectives along with finalized creative works, all during a child's play and exploration. I define the framework, Gesture Object Interfaces, expanding on the fields of Tangible User Interaction and Gesture Recognition. Gesture is a mechanism that can reinforce or create the anthropomorphism of an object. It can give the object life. A Gesture Object is an object in hand while doing anthropomorphized gestures. Gesture Object Interfaces engender new visual and narrative perspectives as part of automatic film assembly during children's play. I generated a suite of automatic film assembly tools accessible to diverse users. The tools that I designed allow for capture, editing and performing to be completely indistinguishable from one another. Gestures integrated with objects become a coherent interface on top of natural play. I built a distributed, modular camera environment and gesture interaction to control that environment. The goal of these new technologies is to motivate children to take new visual and narrative perspectives. In this dissertation I present four tangible platforms that I created as alternatives to the usual fragmented and sequential capturing, editing and performing of narratives available to users of current storytelling tools. I developed Play it by Eye, Frame it by hand, a new generation of narrative tools that shift the frame of reference from the eye to the hand, from the viewpoint (where the eye is) to the standpoint (where the hand is). In Play it by Eye, Frame it by Hand environments, children discover atypical perspectives through the lens of everyday objects. When using Picture This!, children imagine how an object would appear relative to the viewpoint of the toy. They iterate between trying and correcting in a world of multiple perspectives. The results are entirely new genres of child-created films, where children finally capture the cherished visual idioms of action and drama. I report my design process over the course of four tangible research projects that I evaluate during qualitative observations with over one hundred 4- to 14-year-old users. Based on these research findings, I propose a class of moviemaking tools that transform the way users interpret the world visually, and through storytelling."
}, {
    "id": "oai:dspace.mit.edu:1721.1/29991",
    "title": "Inertial measurement via dynamics of trapped particles",
    "abstract": "We describe theoretical and practical aspects of the particle trap as an inertial sensor. The insight motivating this approach is that a trapped particle acts like a mass on a spring, but the restoring forces are provided by electrostatic fields. Exquisitely machined physical mechanisms can be replaced by carefully tuned mechanical physics. Such inertial sensors could be simpler to build yet exhibit superior performance because their operating parameters can be dynamically controlled. Most currently available inertial sensors are inherently planar devices that obtain no more than two degrees of motional sensitivity from a given proof mass. The availability of an accurate, inexpensive, integrated six-degree-of-freedom inertial sensor would enable new applications of inertial sensing that are presently either infeasible or unconsidered. By adding inertial terms to the Paul trap dynamics we derive classical observables that depend on the local acceleration field. We also confirm that these observables appear in practice, in what we believe to be the first electrodynamic particle trap accelerometer. An important (and unusual) aspect of our accelerometer is its dynamic tunability: its effective spring constant depends on the trap drive parameters. Our roughly constructed trap also exhibits a large region of linear response to acceleration, and we present evidence suggesting that our accelerometer has performance comparable to commercially available sensors.",
    "advisors": ["Neil A. Gershenfeld"],
    "text": "Inertial measurement via dynamics of trapped particles We describe theoretical and practical aspects of the particle trap as an inertial sensor. The insight motivating this approach is that a trapped particle acts like a mass on a spring, but the restoring forces are provided by electrostatic fields. Exquisitely machined physical mechanisms can be replaced by carefully tuned mechanical physics. Such inertial sensors could be simpler to build yet exhibit superior performance because their operating parameters can be dynamically controlled. Most currently available inertial sensors are inherently planar devices that obtain no more than two degrees of motional sensitivity from a given proof mass. The availability of an accurate, inexpensive, integrated six-degree-of-freedom inertial sensor would enable new applications of inertial sensing that are presently either infeasible or unconsidered. By adding inertial terms to the Paul trap dynamics we derive classical observables that depend on the local acceleration field. We also confirm that these observables appear in practice, in what we believe to be the first electrodynamic particle trap accelerometer. An important (and unusual) aspect of our accelerometer is its dynamic tunability: its effective spring constant depends on the trap drive parameters. Our roughly constructed trap also exhibits a large region of linear response to acceleration, and we present evidence suggesting that our accelerometer has performance comparable to commercially available sensors."
}, {
    "id": "oai:dspace.mit.edu:1721.1/8597",
    "title": "Creating a modeling culture : supporting the development of scientific practice among teachers",
    "abstract": "This thesis describes the processes of teacher learning and explores the associated changes that take place in classrooms. It describes the Adventures in Modeling Workshops, which we designed and created to introduce teachers to the process of conceptualizing, building, and analyzing their own models of complex, dynamic systems. The Workshops facilitate the growth of a modeling culture among teachers by giving them the tools and the ability to pose, investigate, and answer their own questions. This research examines the development, sustainability, and impact of that culture. It describes how participation in a modeling culture can contribute to a scientific way of thinking, for both teachers and their students, and can help teachers bring authentic science practice into high school classrooms. Employing technological tools developed at the Media Lab, we crafted an introduction to scientific modeling for teachers. These tools, used in concert with a constructionist pedagogy of design and creation, enable teachers to become full-fledged practitioners of modeling. Our workshop structure supports teachers as they learn to act as scientists, creating and exploring models of phenomena in the world around them, evaluating and critiquing those models, refining and validating their own mental models, and improving their understandings. This work serves as a proof of concept for a structure and methodology that increases teachers' individual capacities and helps them integrate aspects of their learning into their own classes. It examines the role that new media plays in supporting new ways of thinking and enabling explorations of new domains of knowledge. It also serves as a platform for examining the details of three components of educational change: 1) the development of technology-enabled materials and activities for teacher and student learning, 2) the construction of a scientific culture among teachers through learning about, gaining fluency with, and exploring modeling technologies, and 3) the paths toward implementation of new content and educational approaches in teachers' classrooms. The results of this project provide one benchmark for evaluating the potential that new ideas and technologies hold for facilitating lasting change in America's classrooms.",
    "advisors": ["Mitchel Resnick"],
    "text": "Creating a modeling culture : supporting the development of scientific practice among teachers This thesis describes the processes of teacher learning and explores the associated changes that take place in classrooms. It describes the Adventures in Modeling Workshops, which we designed and created to introduce teachers to the process of conceptualizing, building, and analyzing their own models of complex, dynamic systems. The Workshops facilitate the growth of a modeling culture among teachers by giving them the tools and the ability to pose, investigate, and answer their own questions. This research examines the development, sustainability, and impact of that culture. It describes how participation in a modeling culture can contribute to a scientific way of thinking, for both teachers and their students, and can help teachers bring authentic science practice into high school classrooms. Employing technological tools developed at the Media Lab, we crafted an introduction to scientific modeling for teachers. These tools, used in concert with a constructionist pedagogy of design and creation, enable teachers to become full-fledged practitioners of modeling. Our workshop structure supports teachers as they learn to act as scientists, creating and exploring models of phenomena in the world around them, evaluating and critiquing those models, refining and validating their own mental models, and improving their understandings. This work serves as a proof of concept for a structure and methodology that increases teachers' individual capacities and helps them integrate aspects of their learning into their own classes. It examines the role that new media plays in supporting new ways of thinking and enabling explorations of new domains of knowledge. It also serves as a platform for examining the details of three components of educational change: 1) the development of technology-enabled materials and activities for teacher and student learning, 2) the construction of a scientific culture among teachers through learning about, gaining fluency with, and exploring modeling technologies, and 3) the paths toward implementation of new content and educational approaches in teachers' classrooms. The results of this project provide one benchmark for evaluating the potential that new ideas and technologies hold for facilitating lasting change in America's classrooms."
}, {
    "id": "oai:dspace.mit.edu:1721.1/112522",
    "title": "Bidirectional gaze guiding and indexing in human-robot interaction through a situated architecture",
    "abstract": "In this body of work, I present a situated and interactive agent perception system that can index into its world and, through a bidirectional exchange of referential gesture, direct its internal indexing system toward both well-known objects as well as simple visuo-spatial indexing in the world. The architecture presented incorporates a novel method for synthetic human-robot joint attention, an internal and automatic crowdsourcing system that provides opportunistic and lifelong robotic socio-visual learning, supports the bidirectional process of following referential behavior; and generates referential behavior useful for directing the gaze of human peers. This document critically probes questions in human-robot interaction around our understanding of gaze manipulation and memory imprinting on human partners in similar architectures and makes recommendations that may improve human-robot peer-to-peer learning.",
    "advisors": ["Cynthia Breazeal"],
    "text": "Bidirectional gaze guiding and indexing in human-robot interaction through a situated architecture In this body of work, I present a situated and interactive agent perception system that can index into its world and, through a bidirectional exchange of referential gesture, direct its internal indexing system toward both well-known objects as well as simple visuo-spatial indexing in the world. The architecture presented incorporates a novel method for synthetic human-robot joint attention, an internal and automatic crowdsourcing system that provides opportunistic and lifelong robotic socio-visual learning, supports the bidirectional process of following referential behavior; and generates referential behavior useful for directing the gaze of human peers. This document critically probes questions in human-robot interaction around our understanding of gaze manipulation and memory imprinting on human partners in similar architectures and makes recommendations that may improve human-robot peer-to-peer learning."
}, {
    "id": "oai:dspace.mit.edu:1721.1/79306",
    "title": "ReflectOns : mental prostheses for self-reflection",
    "abstract": "Since the time of the first philosophers, logic and observed human behavior have stood somewhat in contradiction. More recently, scientist have started to delve into decision making to understand why the way we act differs from rational choice, and indeed from our own desires. We believe that it is possible to use just-in-time feedback drawn from machine-observable behavior to help align behavior with personal goals. This dissertation presents mental prosthetics, a model for distributed, embodied, design-embedded, just-in-time interfaces that augment the human judgment process. Drawing information from the activity of the user around them, mental prostheses analyze behavioral patterns in a way orthogonal to human cognition. Unlike persuasive interfaces, mental prostheses attempt to align choices with personal goals by cueing the user with just-in-time information. Lastly, these devices provide calm yet understandable feedback to draw the user's attention at the correct time to the information available to them. This dissertation provides several prototypes and design explorations as a means of sampling the various approaches to data collection, synthesis, and feedback. Focusing on self-reflection, these sample designs form a subclass of mental prostheses that we term reflectOns. We show through the studies carried out in the course of this dissertation that these systems are effective in changing behavior to be better aligned with user goals. Lastly, this dissertation provides a set of design guidelines that assist in the creation of new mental prostheses. While we discuss a variety of scenarios in this work, it is only the beginning of the exploration. The design guidelines provide insight into both the critical aspects of the design of such systems, as well as possible input and feedback methodologies. These guidelines, together with the reflectOns themselves, provide a basis for future work in this area.",
    "advisors": ["Patrcia Maes"],
    "text": "ReflectOns : mental prostheses for self-reflection Since the time of the first philosophers, logic and observed human behavior have stood somewhat in contradiction. More recently, scientist have started to delve into decision making to understand why the way we act differs from rational choice, and indeed from our own desires. We believe that it is possible to use just-in-time feedback drawn from machine-observable behavior to help align behavior with personal goals. This dissertation presents mental prosthetics, a model for distributed, embodied, design-embedded, just-in-time interfaces that augment the human judgment process. Drawing information from the activity of the user around them, mental prostheses analyze behavioral patterns in a way orthogonal to human cognition. Unlike persuasive interfaces, mental prostheses attempt to align choices with personal goals by cueing the user with just-in-time information. Lastly, these devices provide calm yet understandable feedback to draw the user's attention at the correct time to the information available to them. This dissertation provides several prototypes and design explorations as a means of sampling the various approaches to data collection, synthesis, and feedback. Focusing on self-reflection, these sample designs form a subclass of mental prostheses that we term reflectOns. We show through the studies carried out in the course of this dissertation that these systems are effective in changing behavior to be better aligned with user goals. Lastly, this dissertation provides a set of design guidelines that assist in the creation of new mental prostheses. While we discuss a variety of scenarios in this work, it is only the beginning of the exploration. The design guidelines provide insight into both the critical aspects of the design of such systems, as well as possible input and feedback methodologies. These guidelines, together with the reflectOns themselves, provide a basis for future work in this area."
}, {
    "id": "oai:dspace.mit.edu:1721.1/29143",
    "title": "Rethinking the book",
    "abstract": "Electronic media have lagged behind their paper progenitors in the clear, usable display of large bodies of information. New visual languages have been created for information display which exploit the computer's unique ability to render dynamic and three-dimensional typography. These languages demonstrate that the use of three dimensional form, expressive movement, visual focus and layering, in harmony with human perceptual abilities, improve navigation and contextual understanding of complex written documents. This thesis shows that graphic displays can be combined with physical interfaces to create interactions with purely typographic information that are rich, tactile and humane.",
    "advisors": ["William J. Mitchell"],
    "text": "Rethinking the book Electronic media have lagged behind their paper progenitors in the clear, usable display of large bodies of information. New visual languages have been created for information display which exploit the computer's unique ability to render dynamic and three-dimensional typography. These languages demonstrate that the use of three dimensional form, expressive movement, visual focus and layering, in harmony with human perceptual abilities, improve navigation and contextual understanding of complex written documents. This thesis shows that graphic displays can be combined with physical interfaces to create interactions with purely typographic information that are rich, tactile and humane."
}, {
    "id": "oai:dspace.mit.edu:1721.1/79301",
    "title": "Data visualization in the first person",
    "abstract": "This dissertation will examine what a first person viewpoint means in the context of data visualization and how it can be used for navigating and presenting large datasets. Recent years have seen rapid growth in Big Data methodologies throughout scientific research, business analytics, and online services. The datasets used in these areas are not only growing exponentially larger, but also more complex, incorporating heterogeneous data from many sources that might include digital sensors, websites, mass media, and others. The scale and complexity of these datasets pose significant challenges in the design of effective tools for navigation and analysis. This work will explore methods of representing large datasets as physical, navigable environments. Much of the related research on first person interfaces and 3D visualization has focused on producing tools for expert users and scientific analysis. Due to the complexities of navigation and perception introduced by 3D interfaces, work in this area has had mixed results. In particular, considerable efforts to develop 3D systems for more abstract data, like file systems and social networks, have had difficulty surpassing the efficiency of 2D approaches. However, 3D may offer advantages that have been less explored in this context. In particular, data visualization can be a valuable tool for disseminating scientific results, sharing insights, and explaining methodology. In these applications, clear communication of concepts and narratives are often more essential than efficient navigation. This dissertation will present novel visualization systems designed for large datasets that include audio-video recordings, social media, and others. Discussion will focus on designing visuals that use the first person perspective to give a physical and intuitive form to abstract data, to combine multiple sources of data within a shared space, to construct narratives, and to engage the viewer at a more visceral and emotional level.",
    "advisors": ["Deb Roy"],
    "text": "Data visualization in the first person This dissertation will examine what a first person viewpoint means in the context of data visualization and how it can be used for navigating and presenting large datasets. Recent years have seen rapid growth in Big Data methodologies throughout scientific research, business analytics, and online services. The datasets used in these areas are not only growing exponentially larger, but also more complex, incorporating heterogeneous data from many sources that might include digital sensors, websites, mass media, and others. The scale and complexity of these datasets pose significant challenges in the design of effective tools for navigation and analysis. This work will explore methods of representing large datasets as physical, navigable environments. Much of the related research on first person interfaces and 3D visualization has focused on producing tools for expert users and scientific analysis. Due to the complexities of navigation and perception introduced by 3D interfaces, work in this area has had mixed results. In particular, considerable efforts to develop 3D systems for more abstract data, like file systems and social networks, have had difficulty surpassing the efficiency of 2D approaches. However, 3D may offer advantages that have been less explored in this context. In particular, data visualization can be a valuable tool for disseminating scientific results, sharing insights, and explaining methodology. In these applications, clear communication of concepts and narratives are often more essential than efficient navigation. This dissertation will present novel visualization systems designed for large datasets that include audio-video recordings, social media, and others. Discussion will focus on designing visuals that use the first person perspective to give a physical and intuitive form to abstract data, to combine multiple sources of data within a shared space, to construct narratives, and to engage the viewer at a more visceral and emotional level."
}, {
    "id": "oai:dspace.mit.edu:1721.1/34183",
    "title": "Distributed-in/ distributed-out sensor networks : a new framework to analyze distributed phenomena",
    "abstract": "With a new way of thinking about organizing sensor networks, we demonstrate that we can more easily deploy and program these networks to solve a variety of different problems. We describe sensor networks that can analyze and actuate distributed phenomena without a central coordinator. Previous implementations of sensor networks have approached the problem from the perspective of centralized reporting of distributed events. By contrast, we create a system that allows users to infer the global state from within the sensor network itself, rather than by accessing an outside, central middleware layer. This is accomplished via dynamic creation of clusters of nodes based on application or intent, rather than proximity. The data collected and returned by these clusters is returned directly to the inquirer at his current location. By creating this Distributed-in/Distributed-out (DiDo) system that bypasses a middleware layer, our networks have the principal advantage of being easily configurable and deployable. We show that a system with this structure can solve path problems in a random graph. These graph problems are directly applicable to real-life applications such as discovering escape routes for people in a building with changing pathways. We show that the system is scalable, as reconfiguration requires only local communication.",
    "advisors": ["Andrew Lippman"],
    "text": "Distributed-in/ distributed-out sensor networks : a new framework to analyze distributed phenomena With a new way of thinking about organizing sensor networks, we demonstrate that we can more easily deploy and program these networks to solve a variety of different problems. We describe sensor networks that can analyze and actuate distributed phenomena without a central coordinator. Previous implementations of sensor networks have approached the problem from the perspective of centralized reporting of distributed events. By contrast, we create a system that allows users to infer the global state from within the sensor network itself, rather than by accessing an outside, central middleware layer. This is accomplished via dynamic creation of clusters of nodes based on application or intent, rather than proximity. The data collected and returned by these clusters is returned directly to the inquirer at his current location. By creating this Distributed-in/Distributed-out (DiDo) system that bypasses a middleware layer, our networks have the principal advantage of being easily configurable and deployable. We show that a system with this structure can solve path problems in a random graph. These graph problems are directly applicable to real-life applications such as discovering escape routes for people in a building with changing pathways. We show that the system is scalable, as reconfiguration requires only local communication."
}, {
    "id": "oai:dspace.mit.edu:1721.1/17614",
    "title": "Sto(ry)chastics : a Bayesian network architecture for combined user modeling, sensor fusion, and computational storytelling for interactive spaces",
    "abstract": "This thesis presents a mathematical framework for real-time sensor-driven stochastic modeling of story and user-story interaction, which I call sto(ry)chastics. Almost all sensor-driven interactive entertainment, art, and architecture installations today rely on one-to-one mappings between content and participant's actions to tell a story. These mappings chain small subsets of scripted content, and do not attempt to understand the public's intention or desires during interaction, and therefore are rigid, ad hoc, prone to error, and lack depth in communication of meaning and expressive power. Sto(ry)chastics uses graphical probabilistic modeling of story fragments and participant input, gathered from sensors, to tell a story to the user, as a function of people's estimated intentions and desires during interaction. Using a Bayesian network approach for combined modeling of users, sensors, and story, sto(ry)chastics, as opposed to traditional systems based on one- to-one mappings, is flexible, reconfigurable, adaptive, context-sensitive, robust, accessible, and able to explain its choices. To illustrate sto(ry)chastics, this thesis describes the museum wearable, which orchestrates an audiovisual narration as a function of the visitor's interests and physical path in the museum. The museum wearable is a lightweight and small computer that people carry inside a shoulder pack. It offers an audiovisual augmentation of the surrounding environment using a small eye-piece display attached to conventional headphones. The wearable prototype described in this document relies on a custom-designed",
    "advisors": ["Kent Larson", "Glorianna Davenport"],
    "text": "Sto(ry)chastics : a Bayesian network architecture for combined user modeling, sensor fusion, and computational storytelling for interactive spaces This thesis presents a mathematical framework for real-time sensor-driven stochastic modeling of story and user-story interaction, which I call sto(ry)chastics. Almost all sensor-driven interactive entertainment, art, and architecture installations today rely on one-to-one mappings between content and participant's actions to tell a story. These mappings chain small subsets of scripted content, and do not attempt to understand the public's intention or desires during interaction, and therefore are rigid, ad hoc, prone to error, and lack depth in communication of meaning and expressive power. Sto(ry)chastics uses graphical probabilistic modeling of story fragments and participant input, gathered from sensors, to tell a story to the user, as a function of people's estimated intentions and desires during interaction. Using a Bayesian network approach for combined modeling of users, sensors, and story, sto(ry)chastics, as opposed to traditional systems based on one- to-one mappings, is flexible, reconfigurable, adaptive, context-sensitive, robust, accessible, and able to explain its choices. To illustrate sto(ry)chastics, this thesis describes the museum wearable, which orchestrates an audiovisual narration as a function of the visitor's interests and physical path in the museum. The museum wearable is a lightweight and small computer that people carry inside a shoulder pack. It offers an audiovisual augmentation of the surrounding environment using a small eye-piece display attached to conventional headphones. The wearable prototype described in this document relies on a custom-designed"
}, {
    "id": "oai:dspace.mit.edu:1721.1/9135",
    "title": "Technological fluency and the art of motorcycle maintenance : emergent design of learning environments",
    "abstract": "The empirical basis of this thesis is a two-year project to bring new learning environments and methodologies to rural Thailand. Five theoretical and practical innovations are emphasized: 1. A new methodology of merging education with the actual use of technology to improve the economy and quality of life of community is demonstrated. 2. A practice of \"applied epistemological anthropology,\" which consists of probing for skills and knowledge resident in a community and using these as bridges to new content, is developed. For example, analysis of learning behaviors led me to identify an \"engine culture\" in rural Thailand as an unrecognized source of \"latent learning potential.\" This theory has already begun to spawn a theoretical inquiry with significant promise for assessment of the learning potential of developing countries. 3. Pilot projects were mounted outside of the education system with the specific purpose of breaking \"educational mindsets\" that have been identified as blocks to educational reform. A salient example is the assumption that the population and teachers of rural areas lack the cognitive foundations for modern technological education. The engine culture is an existence proof for the theory of unrecognized foundational elements. 4. The work required a flexible approach to the design of digital-based educational interventions. Analysis of these design issues has led to a theoretical framework, \"Emergent Design,\" for investigating how choice of design methodology contributes to the success or failure of education reforms. 5. The concept of Emergent Design exposes parallels with developments in the restructuring of noneducational organizations. To help explicate this, I draw from my own experience in reforming a healthcare organization. The work suggests a conclusion with a very broad sweep: The latent learning potential of the world population has been grossly underestimated as a result of prevailing mindsets that limit the design of interventions to improve the evolution of the global learning environment.",
    "advisors": ["Seymour Papert"],
    "text": "Technological fluency and the art of motorcycle maintenance : emergent design of learning environments The empirical basis of this thesis is a two-year project to bring new learning environments and methodologies to rural Thailand. Five theoretical and practical innovations are emphasized: 1. A new methodology of merging education with the actual use of technology to improve the economy and quality of life of community is demonstrated. 2. A practice of \"applied epistemological anthropology,\" which consists of probing for skills and knowledge resident in a community and using these as bridges to new content, is developed. For example, analysis of learning behaviors led me to identify an \"engine culture\" in rural Thailand as an unrecognized source of \"latent learning potential.\" This theory has already begun to spawn a theoretical inquiry with significant promise for assessment of the learning potential of developing countries. 3. Pilot projects were mounted outside of the education system with the specific purpose of breaking \"educational mindsets\" that have been identified as blocks to educational reform. A salient example is the assumption that the population and teachers of rural areas lack the cognitive foundations for modern technological education. The engine culture is an existence proof for the theory of unrecognized foundational elements. 4. The work required a flexible approach to the design of digital-based educational interventions. Analysis of these design issues has led to a theoretical framework, \"Emergent Design,\" for investigating how choice of design methodology contributes to the success or failure of education reforms. 5. The concept of Emergent Design exposes parallels with developments in the restructuring of noneducational organizations. To help explicate this, I draw from my own experience in reforming a healthcare organization. The work suggests a conclusion with a very broad sweep: The latent learning potential of the world population has been grossly underestimated as a result of prevailing mindsets that limit the design of interventions to improve the evolution of the global learning environment."
}, {
    "id": "oai:dspace.mit.edu:1721.1/57695",
    "title": "Creating cohesive video with the narrative-informed use of ubiquitous wearable and imaging sensor networks",
    "abstract": "In today's digital era, elements of anyone's life can be captured, by themselves or others, and be instantly broadcast. With little or no regulation on the proliferation of camera technology and the increasing use of video for social communication, entertainment, and education, we have undoubtedly entered the age of ubiquitous media. A world permeated by connected video devices promises a more democratized approach to mass-media culture, enabling anyone to create and distribute personalized content. While these advancements present a plethora of possibilities, they are not without potential negative effects, particularly with regard to privacy, ownership, and the general decrease in quality associated with minimal barriers to entry. This dissertation presents a first-of-its-kind research platform designed to investigate the world of ubiquitous video devices in order to confront inherent problems and create new media applications. This system takes a novel approach to the creation of user-generated, documentary video by augmenting a network of video cameras integrated into the environment with on-body sensing. The distributed video camera network can record the entire life of anyone within its coverage range and it will be shown that it, almost instantly, records more audio and video than can be viewed without prohibitive human resource cost.",
    "advisors": ["Joseph A. Paradiso"],
    "text": "Creating cohesive video with the narrative-informed use of ubiquitous wearable and imaging sensor networks In today's digital era, elements of anyone's life can be captured, by themselves or others, and be instantly broadcast. With little or no regulation on the proliferation of camera technology and the increasing use of video for social communication, entertainment, and education, we have undoubtedly entered the age of ubiquitous media. A world permeated by connected video devices promises a more democratized approach to mass-media culture, enabling anyone to create and distribute personalized content. While these advancements present a plethora of possibilities, they are not without potential negative effects, particularly with regard to privacy, ownership, and the general decrease in quality associated with minimal barriers to entry. This dissertation presents a first-of-its-kind research platform designed to investigate the world of ubiquitous video devices in order to confront inherent problems and create new media applications. This system takes a novel approach to the creation of user-generated, documentary video by augmenting a network of video cameras integrated into the environment with on-body sensing. The distributed video camera network can record the entire life of anyone within its coverage range and it will be shown that it, almost instantly, records more audio and video than can be viewed without prohibitive human resource cost."
}, {
    "id": "oai:dspace.mit.edu:1721.1/119073",
    "title": "A 3D neuromuscular model of the human ankle-foot complex based on multi-joint biplanar fluoroscopy gait analysis",
    "abstract": "During the gait cycle, the human ankle complex serves as a primary power generator while simultaneously stabilizing the entire limb. These actions are controlled by an intricate interplay of several lower leg muscles that cannot be fully uncovered using experimental methods alone. A combination of experiments and mathematical modeling may be used to estimate aspects of neuromusculoskeletal functions that control human gait. In this research, a three-dimensional neuromuscular model of the human ankle-foot complex based on biplanar fluoroscopy gait analysis is presented. Biplanar fluoroscopy (BiFlo) enables three-dimensional bone kinematics analysis using x-ray videos and bone geometry from segmented CT. Hindered by a small capture volume relative to traditional optical motion capture (MOCAP), BiFlo applications to human movement are generally limited to single-joint motions with constrained range. Here, a hybrid procedure is developed for multi-joint gait analysis using BiFlo and MOCAP in tandem. MOCAP effectively extends BiFlo's field-of-view. Subjects walked at a self-selected pace along a level walkway while BiFlo, MOCAP, and ground reaction forces were collected. A novel methodology was developed to register separate BiFlo measurements of the knee and ankle-foot complex. Kinematic analysis of bones surrounding the knee, ankle, and foot was performed. Kinematics obtained using this technique were compared to those calculated using only MOCAP during stance phase. Results show that this hybrid protocol effectively measures knee and ankle kinematics in all three body planes. Additionally, sagittal plane kinematics for select foot bone segments (proximal phalanges, metatarsals, and midfoot) was realized. The proposed procedure offers a novel approach to human gait analysis that eliminates errors originated by soft tissue artifacts, and is especially useful for ankle joint analysis, whose complexities are often simplified in MOCAP studies. Outcomes of the BiFlo walking experiments helped guide the development of a three-dimensional neuromuscular model of the human ankle-foot complex. Driven by kinematics, kinetics, and electromyography (EMG), the model seeks to solve the redundancy problem, individual muscle-tendon contributions to net joint torque, in ankle and subtalar joint actuation during overground gait. Kinematics and kinetics from BiFlo walking trials enable estimations of muscle-tendon lengths, moment arms, and joint torques. EMG yields estimates of muscle activation. Using each of these as inputs, an optimization approach was employed to calculate sets of morphological parameters that simultaneously maximize the neuromuscular model's metabolic efficiency and fit to experimental joint torques. This approach is based on the hypothesis that the muscle-tendon morphology of the human leg has evolved to maximize metabolic efficiency of walking at self-selected speed. Optimal morphological parameter sets produce estimates of force contributions and states for individual muscles. This research lends insight into the possible roles of individual muscle-tendons in the leg that lead to efficient gait.",
    "advisors": ["Hugh Herr"],
    "text": "A 3D neuromuscular model of the human ankle-foot complex based on multi-joint biplanar fluoroscopy gait analysis During the gait cycle, the human ankle complex serves as a primary power generator while simultaneously stabilizing the entire limb. These actions are controlled by an intricate interplay of several lower leg muscles that cannot be fully uncovered using experimental methods alone. A combination of experiments and mathematical modeling may be used to estimate aspects of neuromusculoskeletal functions that control human gait. In this research, a three-dimensional neuromuscular model of the human ankle-foot complex based on biplanar fluoroscopy gait analysis is presented. Biplanar fluoroscopy (BiFlo) enables three-dimensional bone kinematics analysis using x-ray videos and bone geometry from segmented CT. Hindered by a small capture volume relative to traditional optical motion capture (MOCAP), BiFlo applications to human movement are generally limited to single-joint motions with constrained range. Here, a hybrid procedure is developed for multi-joint gait analysis using BiFlo and MOCAP in tandem. MOCAP effectively extends BiFlo's field-of-view. Subjects walked at a self-selected pace along a level walkway while BiFlo, MOCAP, and ground reaction forces were collected. A novel methodology was developed to register separate BiFlo measurements of the knee and ankle-foot complex. Kinematic analysis of bones surrounding the knee, ankle, and foot was performed. Kinematics obtained using this technique were compared to those calculated using only MOCAP during stance phase. Results show that this hybrid protocol effectively measures knee and ankle kinematics in all three body planes. Additionally, sagittal plane kinematics for select foot bone segments (proximal phalanges, metatarsals, and midfoot) was realized. The proposed procedure offers a novel approach to human gait analysis that eliminates errors originated by soft tissue artifacts, and is especially useful for ankle joint analysis, whose complexities are often simplified in MOCAP studies. Outcomes of the BiFlo walking experiments helped guide the development of a three-dimensional neuromuscular model of the human ankle-foot complex. Driven by kinematics, kinetics, and electromyography (EMG), the model seeks to solve the redundancy problem, individual muscle-tendon contributions to net joint torque, in ankle and subtalar joint actuation during overground gait. Kinematics and kinetics from BiFlo walking trials enable estimations of muscle-tendon lengths, moment arms, and joint torques. EMG yields estimates of muscle activation. Using each of these as inputs, an optimization approach was employed to calculate sets of morphological parameters that simultaneously maximize the neuromuscular model's metabolic efficiency and fit to experimental joint torques. This approach is based on the hypothesis that the muscle-tendon morphology of the human leg has evolved to maximize metabolic efficiency of walking at self-selected speed. Optimal morphological parameter sets produce estimates of force contributions and states for individual muscles. This research lends insight into the possible roles of individual muscle-tendons in the leg that lead to efficient gait."
}, {
    "id": "oai:dspace.mit.edu:1721.1/106066",
    "title": "Measuring college students' sleep, stress, mental health and wellbeing with wearable sensors and mobile phones",
    "abstract": "This thesis carries out a series of studies and develops a methodology and tools to measure and analyze ambulatory physiological, behavioral and social data from wearable sensors and mobile phones with trait data such as personality, for learning about behaviors and traits that impact human health and wellbeing. This thesis also validates the methodology and tools on a selected subset of the questions that can be answered by the data collected. First, I conducted a study to characterize wrist electrodermal activity (EDA) patterns with concurrent polysomnography and conventional palm EDA measurement. I developed a tool to analyze the EDA data quantitatively and found that wrist EDA peaks occur during Non REM2 and 3 sleep. Then, with multi-modal wearable sensor data, I conducted several studies showing how multi-modal wearable sensors can improve characterization of sleep/wake states over motion-sensing alone, and predict sleep-related memory consolidation. We found that wrist-EDA helps discriminate when there is improved sleep-related memory consolidation. Next, with colleagues at MIT and Brigham and Women's hospital, I designed and carried out the first four semesters of the \"SNAPSHOT study\", which measured over 100,000 hours of multi-sensor and smartphone use data from 168 college students, recruited together with their social groups. Each student contributed intensive multi-modal ambulatory data (physiological, behavioral, environmental, and social) for 30 days. Each student also filled out standardized questionnaires on mental health, personality, stress, social interactions, sleep and GPA, and provided a measure of dim light melatonin, enabling circadian phase to be measured. To investigate the value of the data, I examined a subset of the large set of questions that these new data enable us to answer: I examined the associations between sleep regularity and sleep duration on academic performance, physical/mental health, perceived stress and wellbeing-related measures using coarsened exact matching to control covariates. Our data showed that sleep irregularity was statistically significantly more associated with bad health, reported in the morning, and with worse mental health than sleep duration. I also identified features useful for recognition of monthly reported perceived stress (high vs low): daily activities, personality, sleep, physiology, social interactions, phone usage, and mobility.",
    "advisors": ["Rosalind W. Picard"],
    "text": "Measuring college students' sleep, stress, mental health and wellbeing with wearable sensors and mobile phones This thesis carries out a series of studies and develops a methodology and tools to measure and analyze ambulatory physiological, behavioral and social data from wearable sensors and mobile phones with trait data such as personality, for learning about behaviors and traits that impact human health and wellbeing. This thesis also validates the methodology and tools on a selected subset of the questions that can be answered by the data collected. First, I conducted a study to characterize wrist electrodermal activity (EDA) patterns with concurrent polysomnography and conventional palm EDA measurement. I developed a tool to analyze the EDA data quantitatively and found that wrist EDA peaks occur during Non REM2 and 3 sleep. Then, with multi-modal wearable sensor data, I conducted several studies showing how multi-modal wearable sensors can improve characterization of sleep/wake states over motion-sensing alone, and predict sleep-related memory consolidation. We found that wrist-EDA helps discriminate when there is improved sleep-related memory consolidation. Next, with colleagues at MIT and Brigham and Women's hospital, I designed and carried out the first four semesters of the \"SNAPSHOT study\", which measured over 100,000 hours of multi-sensor and smartphone use data from 168 college students, recruited together with their social groups. Each student contributed intensive multi-modal ambulatory data (physiological, behavioral, environmental, and social) for 30 days. Each student also filled out standardized questionnaires on mental health, personality, stress, social interactions, sleep and GPA, and provided a measure of dim light melatonin, enabling circadian phase to be measured. To investigate the value of the data, I examined a subset of the large set of questions that these new data enable us to answer: I examined the associations between sleep regularity and sleep duration on academic performance, physical/mental health, perceived stress and wellbeing-related measures using coarsened exact matching to control covariates. Our data showed that sleep irregularity was statistically significantly more associated with bad health, reported in the morning, and with worse mental health than sleep duration. I also identified features useful for recognition of monthly reported perceived stress (high vs low): daily activities, personality, sleep, physiology, social interactions, phone usage, and mobility."
}, {
    "id": "oai:dspace.mit.edu:1721.1/62382",
    "title": "Printed inorganic transistors",
    "abstract": "Forty years of exponential growth of semiconductor technology have been predicated on the miniaturization of the transistors that comprise integrated circuits. While complexity has greatly increased within a given area of processed silicon, the cost per area has not decreased. Current fabrication methods are further hindered by high facility costs and environmentally unfriendly processing. Moving to a new means of semiconductor fabrication may drastically reduce both financial and environmental costs. One such approach is based on the extension of printing techniques to the fabrication of electronic devices. Such printed electronics are envisioned to enable applications in flexible displays and electronic paper, personal fabrication, wearable computing, and disposable medical diagnostics. This dissertation focuses on the development of printable materials, specifically inorganic semiconductor inks. At the outset of this research, organic semiconductors were the only materials known and pursued as printable semiconductors. The ability to process organic semiconductors in common organic solvents makes them amenable to a wide range of printing technologies, but their electrical performance is fundamentally limited and their utility is confined to applications in which only low speeds are required. The goal of this thesis was to demonstrate the feasibility of printing inorganic materials, the same materials that are used to fabricate high quality semiconductor devices. Cadmium selenide was studied as a model inorganic semiconductor and silicon was studied because of its commercial dominance. The insolubility and high processing temperatures of inorganic semiconductors, both of which can prevent",
    "advisors": ["Joseph Jacobson"],
    "text": "Printed inorganic transistors Forty years of exponential growth of semiconductor technology have been predicated on the miniaturization of the transistors that comprise integrated circuits. While complexity has greatly increased within a given area of processed silicon, the cost per area has not decreased. Current fabrication methods are further hindered by high facility costs and environmentally unfriendly processing. Moving to a new means of semiconductor fabrication may drastically reduce both financial and environmental costs. One such approach is based on the extension of printing techniques to the fabrication of electronic devices. Such printed electronics are envisioned to enable applications in flexible displays and electronic paper, personal fabrication, wearable computing, and disposable medical diagnostics. This dissertation focuses on the development of printable materials, specifically inorganic semiconductor inks. At the outset of this research, organic semiconductors were the only materials known and pursued as printable semiconductors. The ability to process organic semiconductors in common organic solvents makes them amenable to a wide range of printing technologies, but their electrical performance is fundamentally limited and their utility is confined to applications in which only low speeds are required. The goal of this thesis was to demonstrate the feasibility of printing inorganic materials, the same materials that are used to fabricate high quality semiconductor devices. Cadmium selenide was studied as a model inorganic semiconductor and silicon was studied because of its commercial dominance. The insolubility and high processing temperatures of inorganic semiconductors, both of which can prevent"
}, {
    "id": "oai:dspace.mit.edu:1721.1/33879",
    "title": "Impression formation in the information age : a study and design for online dating",
    "abstract": "43% of American adults are single and many are looking for new social and romantic connections. At the same time, the Internet offers services to both research and contact other individuals. As a result, proactive computer savvy singles are logging on to find romantic partners. While the online dating industry advertises its success citing the large number of registered users, other evidence indicates broad dissatisfaction: the analysis of website behavior reveals that most users are inactive and experienced online daters state a preference for dating offline versus on. To account for this dissatisfaction, I locate decision-point failures. To improve the process, I propose and test an alternate model. Part 1 shows that acquiring more information - one of the perceived benefits of meeting online and reading profiles - can have negative effects, such as leading to less liking over time, while failing to make people really believe they know others better. The expectation that getting to know others more will lead to more liking, coupled with the fact that more information leads to less liking, means that online daters are frequently disappointed, causing them to leave dating sites, and to continue to prefer offline dating despite its drawbacks.",
    "advisors": ["Dan Ariely"],
    "text": "Impression formation in the information age : a study and design for online dating 43% of American adults are single and many are looking for new social and romantic connections. At the same time, the Internet offers services to both research and contact other individuals. As a result, proactive computer savvy singles are logging on to find romantic partners. While the online dating industry advertises its success citing the large number of registered users, other evidence indicates broad dissatisfaction: the analysis of website behavior reveals that most users are inactive and experienced online daters state a preference for dating offline versus on. To account for this dissatisfaction, I locate decision-point failures. To improve the process, I propose and test an alternate model. Part 1 shows that acquiring more information - one of the perceived benefits of meeting online and reading profiles - can have negative effects, such as leading to less liking over time, while failing to make people really believe they know others better. The expectation that getting to know others more will lead to more liking, coupled with the fact that more information leads to less liking, means that online daters are frequently disappointed, causing them to leave dating sites, and to continue to prefer offline dating despite its drawbacks."
}, {
    "id": "oai:dspace.mit.edu:1721.1/41709",
    "title": "Mediating disruption in human-computer interaction from implicit metrics of attention",
    "abstract": "Multitasking environments cause people to be interrupted constantly, often disrupting their ongoing activities and impeding reaching their goals. This thesis presents a disruption reducing approach designed to support the user's goals and optimize productivity that is based on a model of the user's receptivity to an interruption. The model uses knowledge of the interruption content, context and priority of the task(s) in progress, user actions and goal-related concepts to mediate interruptions. The disruption management model is distinct from previous work by the addition of implicit sensors that deduce the interruption content and user context to help determine when an interruption will disrupt an ongoing activity. Domain-independent implicit sensors include mouse and keyboard behaviors, and goal-related concepts extracted from the user documents. The model also identifies the contextual relationship between interruptions and user goals as an important factor in how interruptions are controlled. The degree to which interruptions are related to the user goal determines how those interruptions will be received. We tested and evolved the model in various cases and showed significant improvement in both productivity and satisfaction. A disruption manager application controls interruptions on common desktop computing activities, such as web browsing and instant messaging. The disruption manager demonstrates that mediating interruptions by supporting the user goals can improve performance and overall productivity. Our evaluation shows an improvement in success of over 25% across prioritization conditions for real life computing environments.",
    "advisors": ["Ted Selker"],
    "text": "Mediating disruption in human-computer interaction from implicit metrics of attention Multitasking environments cause people to be interrupted constantly, often disrupting their ongoing activities and impeding reaching their goals. This thesis presents a disruption reducing approach designed to support the user's goals and optimize productivity that is based on a model of the user's receptivity to an interruption. The model uses knowledge of the interruption content, context and priority of the task(s) in progress, user actions and goal-related concepts to mediate interruptions. The disruption management model is distinct from previous work by the addition of implicit sensors that deduce the interruption content and user context to help determine when an interruption will disrupt an ongoing activity. Domain-independent implicit sensors include mouse and keyboard behaviors, and goal-related concepts extracted from the user documents. The model also identifies the contextual relationship between interruptions and user goals as an important factor in how interruptions are controlled. The degree to which interruptions are related to the user goal determines how those interruptions will be received. We tested and evolved the model in various cases and showed significant improvement in both productivity and satisfaction. A disruption manager application controls interruptions on common desktop computing activities, such as web browsing and instant messaging. The disruption manager demonstrates that mediating interruptions by supporting the user goals can improve performance and overall productivity. Our evaluation shows an improvement in success of over 25% across prioritization conditions for real life computing environments."
}, {
    "id": "oai:dspace.mit.edu:1721.1/67761",
    "title": "originalMachines : developing tools and methods for object-oriented mechatronics",
    "abstract": "The digital revolution has fundamentally changed our lives by giving us new ways to express ourselves through digital media. For example, accessible multimedia content creation tools allow people to instantiate their ideas and share them easily. However, most of these outcomes only exist on-screen and online. Despite the growing accessibility of digital design and fabrication tools the physical world and everyday objects surrounding us have been largely excluded from a parallel explosion of possibilities to express ourselves. Increasingly, webbased services allow professional and non-professional audiences to access computer-aided manufacturing (CAM) tools like 3D-printing and laser-cutting. Nonetheless, there are few (if any) design tools and methods for creating complex mechanical assemblies that take full advantage of CAM systems. Creating unique mechatronic artifacts or \"originalMachines\" requires more specific and sophisticated design tools than exist today. \"Object-Oriented Mechatronics\" is a parametric design approach that connects knowledge about mechanical assemblies and electronics with the requirements of digital manufacturing processes. Parametric instances like gears, bearing and servos are made available as objects within a CAD environment which can then be implemented into specific projects. The approach addresses the missing link between accessible rapid-manufacturing services and currently available design tools thereby creating new opportunities for self-expression through mechatronic objects and machines. The dissertation matches mechanical components and assemblies with rapid manufacturing methods by exploring transferability of conventional manufacturing techniques to appropriate rapid manufacturing tools. I rebuild various gearing and bearing principles like four-contact point bearings, cross roller bearings, spur and helical gears, planetary gears, cycloidal and harmonic gear reducers using the laser cutter, the CNC-mill and the 3D-printer. These explorations lead to more complex assemblies such as the PlywoodServo, 3DprintedClock and 3-DoF (Degree of Freedom) Head. The lessons from these explorations are summarized in a detailed \"cook book\" of novel mechatronic assemblies enabled by new fabrication tools. Furthermore, I use the results to develop a CAD tool that brings together several existing software packages and plug-ins including Rhino, Grasshopper and the Firefly experiments for Arduino, which will allow animation, fabrication and control of original machines. The tool is an example of an object-oriented design approach to mechatronic assemblies. A user calls a DoF (Degree of Freedom) object (parametric servo) with specific parameters like gearing and bearing types, motor options and control and communication capabilities. The DoF object then creates the corresponding geometry which can be connected and integrated with other actuators and forms. A group of roboticists and designers participated in a workshop to test the tool and make proposals for original machines using the tool. The dissertation has contributions on multiple levels. First, the actuator assembly examples and parametric design tool present a body of novel work that illustrates the benefits of going beyond off-the-shelf actuator assemblies and kit-of-parts for robotic objects. Second, this tool and the accompanying examples enable the design of more original machines with custom actuator assemblies using the latest digital fabrication tools. Finally, these explorations illustrate how new CAD/ CAM tools can facilitate an exchange between more design-oriented users and more engineering-oriented users.",
    "advisors": ["Cynthis Breazeal"],
    "text": "originalMachines : developing tools and methods for object-oriented mechatronics The digital revolution has fundamentally changed our lives by giving us new ways to express ourselves through digital media. For example, accessible multimedia content creation tools allow people to instantiate their ideas and share them easily. However, most of these outcomes only exist on-screen and online. Despite the growing accessibility of digital design and fabrication tools the physical world and everyday objects surrounding us have been largely excluded from a parallel explosion of possibilities to express ourselves. Increasingly, webbased services allow professional and non-professional audiences to access computer-aided manufacturing (CAM) tools like 3D-printing and laser-cutting. Nonetheless, there are few (if any) design tools and methods for creating complex mechanical assemblies that take full advantage of CAM systems. Creating unique mechatronic artifacts or \"originalMachines\" requires more specific and sophisticated design tools than exist today. \"Object-Oriented Mechatronics\" is a parametric design approach that connects knowledge about mechanical assemblies and electronics with the requirements of digital manufacturing processes. Parametric instances like gears, bearing and servos are made available as objects within a CAD environment which can then be implemented into specific projects. The approach addresses the missing link between accessible rapid-manufacturing services and currently available design tools thereby creating new opportunities for self-expression through mechatronic objects and machines. The dissertation matches mechanical components and assemblies with rapid manufacturing methods by exploring transferability of conventional manufacturing techniques to appropriate rapid manufacturing tools. I rebuild various gearing and bearing principles like four-contact point bearings, cross roller bearings, spur and helical gears, planetary gears, cycloidal and harmonic gear reducers using the laser cutter, the CNC-mill and the 3D-printer. These explorations lead to more complex assemblies such as the PlywoodServo, 3DprintedClock and 3-DoF (Degree of Freedom) Head. The lessons from these explorations are summarized in a detailed \"cook book\" of novel mechatronic assemblies enabled by new fabrication tools. Furthermore, I use the results to develop a CAD tool that brings together several existing software packages and plug-ins including Rhino, Grasshopper and the Firefly experiments for Arduino, which will allow animation, fabrication and control of original machines. The tool is an example of an object-oriented design approach to mechatronic assemblies. A user calls a DoF (Degree of Freedom) object (parametric servo) with specific parameters like gearing and bearing types, motor options and control and communication capabilities. The DoF object then creates the corresponding geometry which can be connected and integrated with other actuators and forms. A group of roboticists and designers participated in a workshop to test the tool and make proposals for original machines using the tool. The dissertation has contributions on multiple levels. First, the actuator assembly examples and parametric design tool present a body of novel work that illustrates the benefits of going beyond off-the-shelf actuator assemblies and kit-of-parts for robotic objects. Second, this tool and the accompanying examples enable the design of more original machines with custom actuator assemblies using the latest digital fabrication tools. Finally, these explorations illustrate how new CAD/ CAM tools can facilitate an exchange between more design-oriented users and more engineering-oriented users."
}, {
    "id": "oai:dspace.mit.edu:1721.1/70810",
    "title": "Programmable surfaces",
    "abstract": "Robotic vehicles walk on legs, roll on wheels, are pulled by tracks, pushed by propellers, lifted by wings, and steered by rudders. All of these systems share the common character of momentum transport across their surfaces. These existing approaches rely on bulk response among the fluids and solids. They are often not finely controllable and complex approaches suffer from manufacturing and practical operational challenges. In contrast I present a study of a dynamic, programmable interface between the surface and its surrounding fluids. This research explores a synthetic hydrodynamic regime, using a programmable surface to dynamically alter the flow around an object. Recent advances in distributed computing and communications, actuator integration and batch fabrication, make it feasible to create intelligent active surfaces, with significant implications for improving energy efficiency, recovering energy, introducing novel form factors and control laws, and reducing noise signatures. My approach applies ideas from programmable matter to surfaces rather than volumes. The project is based on covering surfaces with large arrays of small cells that can each compute, communicate, and generate shear or normal forces. The basic element is a cell that can be joined in arrays to tile a surface, each containing a processor, connections for power and communications, and means to control the local wall velocity The cell size is determined by the characteristic length scale of the flow field ranging from millimeters to centimeters to match the desired motion and fluidic system. Because boundary layer effects are significant across fluid states from aerodynamics to hydrodynamics to rheology, the possible implications of active control of the boundary layer are correspondingly far reaching, with applications from transportation to energy generation to building air handling. This thesis presents a feasibility study, evaluating current manufacturing, processing, materials, and technologies capabilities to realize programmable surfaces.",
    "advisors": ["Neil Gershenfeld"],
    "text": "Programmable surfaces Robotic vehicles walk on legs, roll on wheels, are pulled by tracks, pushed by propellers, lifted by wings, and steered by rudders. All of these systems share the common character of momentum transport across their surfaces. These existing approaches rely on bulk response among the fluids and solids. They are often not finely controllable and complex approaches suffer from manufacturing and practical operational challenges. In contrast I present a study of a dynamic, programmable interface between the surface and its surrounding fluids. This research explores a synthetic hydrodynamic regime, using a programmable surface to dynamically alter the flow around an object. Recent advances in distributed computing and communications, actuator integration and batch fabrication, make it feasible to create intelligent active surfaces, with significant implications for improving energy efficiency, recovering energy, introducing novel form factors and control laws, and reducing noise signatures. My approach applies ideas from programmable matter to surfaces rather than volumes. The project is based on covering surfaces with large arrays of small cells that can each compute, communicate, and generate shear or normal forces. The basic element is a cell that can be joined in arrays to tile a surface, each containing a processor, connections for power and communications, and means to control the local wall velocity The cell size is determined by the characteristic length scale of the flow field ranging from millimeters to centimeters to match the desired motion and fluidic system. Because boundary layer effects are significant across fluid states from aerodynamics to hydrodynamics to rheology, the possible implications of active control of the boundary layer are correspondingly far reaching, with applications from transportation to energy generation to building air handling. This thesis presents a feasibility study, evaluating current manufacturing, processing, materials, and technologies capabilities to realize programmable surfaces."
}, {
    "id": "oai:dspace.mit.edu:1721.1/79303",
    "title": "The birth of a word",
    "abstract": "A hallmark of a child's first two years of life is their entry into language, from first productive word use around 12 months of age to the emergence of combinatorial speech in their second year. What is the nature of early language development and how is it shaped by everyday experience? This work builds from the ground up to study early word learning, characterizing vocabulary growth and its relation to the child's environment. Our study is guided by the idea that the natural activities and social structures of daily life provide helpful learning constraints. We study this through analysis of the largest-ever corpus of one child's everyday experience at home. Through the Human Speechome Project, the home of a family with a young child was outfitted with a custom audio-video recording system, capturing more than 200,000 hours of audio and video of daily life from birth to age three. The annotated subset of this data spans the child's 9-24 month age range and contains more than 8 million words of transcribed speech, constituting a detailed record of both the child's input and linguistic development. Such a comprehensive, naturalistic dataset presents new research opportunities but also requires new analysis approaches - questions must be operationalized to leverage the full scale of the data. We begin with the task of speech transcription, then identify \"word births\" - the child's first use of each word in his vocabulary. Vocabulary growth accelerates and then shows a surprising deceleration that coincides with an increase in combinatorial speech. The vocabulary growth timeline provides a means to assess the environmental contributions to word learning, beginning with aspects of caregiver input speech. But language is tied to everyday activity, and we investigate how spatial and activity contexts relate to word learning. Activity contexts, such as \"mealtime\", are identified manually and with probabilistic methods that can scale to large datasets. These new nonlinguistic variables are predictive of when words are learned and are complementary to more traditionally studied linguistic measures. Characterizing word learning and assessing natural input variables can lead to new insights on fundamental learning mechanisms.",
    "advisors": ["Deb Roy"],
    "text": "The birth of a word A hallmark of a child's first two years of life is their entry into language, from first productive word use around 12 months of age to the emergence of combinatorial speech in their second year. What is the nature of early language development and how is it shaped by everyday experience? This work builds from the ground up to study early word learning, characterizing vocabulary growth and its relation to the child's environment. Our study is guided by the idea that the natural activities and social structures of daily life provide helpful learning constraints. We study this through analysis of the largest-ever corpus of one child's everyday experience at home. Through the Human Speechome Project, the home of a family with a young child was outfitted with a custom audio-video recording system, capturing more than 200,000 hours of audio and video of daily life from birth to age three. The annotated subset of this data spans the child's 9-24 month age range and contains more than 8 million words of transcribed speech, constituting a detailed record of both the child's input and linguistic development. Such a comprehensive, naturalistic dataset presents new research opportunities but also requires new analysis approaches - questions must be operationalized to leverage the full scale of the data. We begin with the task of speech transcription, then identify \"word births\" - the child's first use of each word in his vocabulary. Vocabulary growth accelerates and then shows a surprising deceleration that coincides with an increase in combinatorial speech. The vocabulary growth timeline provides a means to assess the environmental contributions to word learning, beginning with aspects of caregiver input speech. But language is tied to everyday activity, and we investigate how spatial and activity contexts relate to word learning. Activity contexts, such as \"mealtime\", are identified manually and with probabilistic methods that can scale to large datasets. These new nonlinguistic variables are predictive of when words are learned and are complementary to more traditionally studied linguistic measures. Characterizing word learning and assessing natural input variables can lead to new insights on fundamental learning mechanisms."
}, {
    "id": "oai:dspace.mit.edu:1721.1/69801",
    "title": "Exertion instruments",
    "abstract": "This dissertation describes the research, development and reasoning behind a family of musical instruments called Exertion Instruments. They use inline electrical generators to run a synthesizer and an amplifier while eliminating the need for batteries. Efficient acoustic design minimizes the power requirements while optimized generator and ergonomic design maximize power generation. As such, they combine the convenience of acoustic instruments with the flexibility of electronic instruments. Also, through new generator designs, nuances of player charging movements become as important to expression as the overall intensity of their playing. The player treats the generator like a typical physical resonator such as a string, using musical instrument playing gestures. Yet they manipulate audio electronically, enabling practices like sampling, synthesis and modulation. During development, a modular approach using amateur-friendly materials was taken to empower future instrument builders to customize and improve the instrument. In addition to technical criteria based on measurements, Exertion Instruments are evaluated through player and builder experiences in a series of technical workshops and realworld performances.",
    "advisors": ["Chris Csikszentmihalyi"],
    "text": "Exertion instruments This dissertation describes the research, development and reasoning behind a family of musical instruments called Exertion Instruments. They use inline electrical generators to run a synthesizer and an amplifier while eliminating the need for batteries. Efficient acoustic design minimizes the power requirements while optimized generator and ergonomic design maximize power generation. As such, they combine the convenience of acoustic instruments with the flexibility of electronic instruments. Also, through new generator designs, nuances of player charging movements become as important to expression as the overall intensity of their playing. The player treats the generator like a typical physical resonator such as a string, using musical instrument playing gestures. Yet they manipulate audio electronically, enabling practices like sampling, synthesis and modulation. During development, a modular approach using amateur-friendly materials was taken to empower future instrument builders to customize and improve the instrument. In addition to technical criteria based on measurements, Exertion Instruments are evaluated through player and builder experiences in a series of technical workshops and realworld performances."
}, {
    "id": "oai:dspace.mit.edu:1721.1/79157",
    "title": "Best of both worlds : issues of structure and agency in computational creation, in and out of school",
    "abstract": "We live in a computational culture - a culture in which we are surrounded by computational systems and interfaces, from social networks to banking infrastructure, to entertainment platforms, to transportation systems. This culture introduces new expectations and new opportunities for learning, creating new demands for what to learn and offering new possibilities for how to learn. In this dissertation, I adopt a predominantly qualitative approach to exploring learning in computational culture, studying how the Scratch programming environment and online community are employed to support learning both in and out of school. To this end, I conducted interviews with 30 kids working with Scratch at home and 30 teachers working with Scratch in K-12 classrooms to develop descriptions of computational creation in these two settings. Using a theoretical framework of agency and structure, I analyze how the at-home and school-classroom contexts enable - or constrain - young people's agency in computational creation. Despite common assumptions that at-home learning is necessarily low-structure/high-agency and that at-school learning is necessarily high-structure/low-agency, I argue that structure and agency need not be in opposition. Designers of learning environments should explore intermediate possibilities, finding ways to employ structure in the service of learner agency.",
    "advisors": ["Mitchel Resnick"],
    "text": "Best of both worlds : issues of structure and agency in computational creation, in and out of school We live in a computational culture - a culture in which we are surrounded by computational systems and interfaces, from social networks to banking infrastructure, to entertainment platforms, to transportation systems. This culture introduces new expectations and new opportunities for learning, creating new demands for what to learn and offering new possibilities for how to learn. In this dissertation, I adopt a predominantly qualitative approach to exploring learning in computational culture, studying how the Scratch programming environment and online community are employed to support learning both in and out of school. To this end, I conducted interviews with 30 kids working with Scratch at home and 30 teachers working with Scratch in K-12 classrooms to develop descriptions of computational creation in these two settings. Using a theoretical framework of agency and structure, I analyze how the at-home and school-classroom contexts enable - or constrain - young people's agency in computational creation. Despite common assumptions that at-home learning is necessarily low-structure/high-agency and that at-school learning is necessarily high-structure/low-agency, I argue that structure and agency need not be in opposition. Designers of learning environments should explore intermediate possibilities, finding ways to employ structure in the service of learner agency."
}, {
    "id": "oai:dspace.mit.edu:1721.1/57898",
    "title": "Modeling the structure of collective intelligence",
    "abstract": "The human problem solution process has attracted an increasing amount of interest among educators, managers, computer scientists and others. However, the discussion of the subject has suffered from the lack of stochastic tools to quantitatively capture both the subtler steps of the problem solution process and the diversity of human thinking. In order to stochastically model the human problem solution, this thesis presents an approach referred to as \"influence modeling,\" that attempts to describe how an individual navigates from one random memory chunk to another related memory chunk, and how a group of people randomly remind one another of memory chunks that could be individually uncommon. As an application of influence modeling, this thesis shows how groups play \"20-questions\" games based on a semantic space, such as ConceptNet (a common-sense database). It also investigates how groups send signals about their behavior, which are collected by embedded devices, how group interaction processes could be automatically monitored with embedded devices, how group performance could be facilitated, and how to map group behavior and performance from the macroscopic level to the microscopic level in experiments in measuring collective intelligence. The influence modeling makes it possible to understand how a group could perform better than an individual. It also allows for the monitoring of the status of the problem solution, and makes it possible to direct group interaction in more fruitful ways.",
    "advisors": ["Alex (Sandy) Pentland"],
    "text": "Modeling the structure of collective intelligence The human problem solution process has attracted an increasing amount of interest among educators, managers, computer scientists and others. However, the discussion of the subject has suffered from the lack of stochastic tools to quantitatively capture both the subtler steps of the problem solution process and the diversity of human thinking. In order to stochastically model the human problem solution, this thesis presents an approach referred to as \"influence modeling,\" that attempts to describe how an individual navigates from one random memory chunk to another related memory chunk, and how a group of people randomly remind one another of memory chunks that could be individually uncommon. As an application of influence modeling, this thesis shows how groups play \"20-questions\" games based on a semantic space, such as ConceptNet (a common-sense database). It also investigates how groups send signals about their behavior, which are collected by embedded devices, how group interaction processes could be automatically monitored with embedded devices, how group performance could be facilitated, and how to map group behavior and performance from the macroscopic level to the microscopic level in experiments in measuring collective intelligence. The influence modeling makes it possible to understand how a group could perform better than an individual. It also allows for the monitoring of the status of the problem solution, and makes it possible to direct group interaction in more fruitful ways."
}, {
    "id": "oai:dspace.mit.edu:1721.1/41553",
    "title": "Giving the head a hand : constructing a microworld to build relationships with ideas in balance control",
    "abstract": "The major promise of computational technology for learning is in making discovery and acquisition of knowledge accessible to a wider range of people. The protean expressive and constructive nature of computational technology facilitates more powerful and effective learning methodologies. Enabling multiple forms of representation through computational approaches to thinking about various phenomena not only potentially opens new domains of knowledge, but also permits a re-structuration of domains by rethinking content and activity. This thesis provides an exemplar of this potential through children learning about Balance Control in Dynamic Systems (BCDS), which adds a particular value given that BCDS is considered too complex for young learners. A Balance Control Microworld was created to help learners think about how to program physical robots to perform balancing acts, such as balancing an inverted pendulum, based on the observations of their own body motions. A Spatial Computing Paradigm (SCP) was developed to allow learners to carry out various control operations using familiar 2D properties of on-screen objects. The physical robots have a dual-mode ability that allowed learners to record and observe motions while controlling the robots manually by hand as well as under program control. The study involved two groups of learners, ages 13 to 15, over twelve months. BCDS concepts that emerged include the role of speed, creating predictions, managing system states, and analyzing system's stability. Moreover, powerful ideas in computational and mathematical thinking helped enable thinking and understanding in BCDS as well as reflection over the whole process. The evolution of the Microworld was guided by a practice of applied epistemological anthropology.",
    "advisors": ["David P. Cavallo"],
    "text": "Giving the head a hand : constructing a microworld to build relationships with ideas in balance control The major promise of computational technology for learning is in making discovery and acquisition of knowledge accessible to a wider range of people. The protean expressive and constructive nature of computational technology facilitates more powerful and effective learning methodologies. Enabling multiple forms of representation through computational approaches to thinking about various phenomena not only potentially opens new domains of knowledge, but also permits a re-structuration of domains by rethinking content and activity. This thesis provides an exemplar of this potential through children learning about Balance Control in Dynamic Systems (BCDS), which adds a particular value given that BCDS is considered too complex for young learners. A Balance Control Microworld was created to help learners think about how to program physical robots to perform balancing acts, such as balancing an inverted pendulum, based on the observations of their own body motions. A Spatial Computing Paradigm (SCP) was developed to allow learners to carry out various control operations using familiar 2D properties of on-screen objects. The physical robots have a dual-mode ability that allowed learners to record and observe motions while controlling the robots manually by hand as well as under program control. The study involved two groups of learners, ages 13 to 15, over twelve months. BCDS concepts that emerged include the role of speed, creating predictions, managing system states, and analyzing system's stability. Moreover, powerful ideas in computational and mathematical thinking helped enable thinking and understanding in BCDS as well as reflection over the whole process. The evolution of the Microworld was guided by a practice of applied epistemological anthropology."
}, {
    "id": "oai:dspace.mit.edu:1721.1/16621",
    "title": "A computational model for the automatic recognition of affect in speech",
    "abstract": "Spoken language, in addition to serving as a primary vehicle for externalizing linguistic structures and meaning, acts as a carrier of various sources of information, including back- ground, age, gender, membership in social structures, as well as physiological, pathological and emotional states. These sources of information are more than just ancillary to the main purpose of linguistic communication: Humans react to the various non-linguistic factors en- coded in the speech signal, shaping and adjusting their interactions to satisfy interpersonal and social protocols. Computer science, artificial intelligence and computational linguistics have devoted much active research to systems that aim to model the production and recovery of linguistic lexico-semantic structures from speech. However, less attention has been devoted to systems that model and understand the paralinguistic and extralinguistic information in the signal. As the breadth and nature of human-computer interaction escalates to levels previously reserved for human-to-human communication, there is a growing need to endow computational systems with human-like abilities which facilitate the interaction and make it more natural. Of paramount importance amongst these is the human ability to make inferences regarding the affective content of our exchanges. This thesis proposes a framework for the recognition of affective qualifiers from prosodic- acoustic parameters extracted from spoken language.",
    "advisors": ["Rosalind W. Picard"],
    "text": "A computational model for the automatic recognition of affect in speech Spoken language, in addition to serving as a primary vehicle for externalizing linguistic structures and meaning, acts as a carrier of various sources of information, including back- ground, age, gender, membership in social structures, as well as physiological, pathological and emotional states. These sources of information are more than just ancillary to the main purpose of linguistic communication: Humans react to the various non-linguistic factors en- coded in the speech signal, shaping and adjusting their interactions to satisfy interpersonal and social protocols. Computer science, artificial intelligence and computational linguistics have devoted much active research to systems that aim to model the production and recovery of linguistic lexico-semantic structures from speech. However, less attention has been devoted to systems that model and understand the paralinguistic and extralinguistic information in the signal. As the breadth and nature of human-computer interaction escalates to levels previously reserved for human-to-human communication, there is a growing need to endow computational systems with human-like abilities which facilitate the interaction and make it more natural. Of paramount importance amongst these is the human ability to make inferences regarding the affective content of our exchanges. This thesis proposes a framework for the recognition of affective qualifiers from prosodic- acoustic parameters extracted from spoken language."
}, {
    "id": "oai:dspace.mit.edu:1721.1/109617",
    "title": "Paper electronics : circuits on paper for learning and self-expression",
    "abstract": "In this dissertation, I explore the theme of wonder in technology, learning and self-expression through the lens of paper electronics, which is circuit building on paper using conductive tapes and circuit components as electronic craft materials. This new medium blends the interactive functionality of electronics with the expressive flexibility of the paper medium. I present an overview of the paper electronics medium as well as its extension in the form of electrified books, books with circuitry integrated with its pages and spine. I then described the design of a paper electronics toolkit called circuit stickers and how this toolkit was deployed through a company called Chibitronics. Finally, through the circuit stickers toolkit, I investigate and evaluate the paper electronics medium as a learning tool and approach, expressive medium and method to engage more diverse communities in technology creation. These investigations show that paper electronics has indeed impacted learners, educators and creators across many backgrounds and disciplines. It has enabled educators to teach a broad range of subjects and skills in new ways. Artists have used paper electronics to explore electricity and interactivity for self-expression, demonstrating the aesthetic flexibility and expressive potency of this medium. Finally, it has engaged creators from diverse communities and backgrounds including educators, Makers, and crafters. It enables not only new approaches to learning and creating technology, it also engages new types of creators in inventing surprising technological artifacts--ones that inspire new experiences, objects and opportunities for wonder.",
    "advisors": ["Joseph A. Paradiso"],
    "text": "Paper electronics : circuits on paper for learning and self-expression In this dissertation, I explore the theme of wonder in technology, learning and self-expression through the lens of paper electronics, which is circuit building on paper using conductive tapes and circuit components as electronic craft materials. This new medium blends the interactive functionality of electronics with the expressive flexibility of the paper medium. I present an overview of the paper electronics medium as well as its extension in the form of electrified books, books with circuitry integrated with its pages and spine. I then described the design of a paper electronics toolkit called circuit stickers and how this toolkit was deployed through a company called Chibitronics. Finally, through the circuit stickers toolkit, I investigate and evaluate the paper electronics medium as a learning tool and approach, expressive medium and method to engage more diverse communities in technology creation. These investigations show that paper electronics has indeed impacted learners, educators and creators across many backgrounds and disciplines. It has enabled educators to teach a broad range of subjects and skills in new ways. Artists have used paper electronics to explore electricity and interactivity for self-expression, demonstrating the aesthetic flexibility and expressive potency of this medium. Finally, it has engaged creators from diverse communities and backgrounds including educators, Makers, and crafters. It enables not only new approaches to learning and creating technology, it also engages new types of creators in inventing surprising technological artifacts--ones that inspire new experiences, objects and opportunities for wonder."
}, {
    "id": "oai:dspace.mit.edu:1721.1/107532",
    "title": "Finding the swing voter : definitions and survey methods for voter classification",
    "abstract": "This thesis proposes a theory mapping emotional reactions to political information onto a theory of vote decisionmaking and then further onto measurable survey response. Using on-line processing based in emotion, voters form affective summaries about candidates, which store previous information as an emotional response. The act of voting is treated as a single realization of a probabilistic event, with the relative probabilities of each vote option being an expression of the affective summary. These summaries are expressed as warmness or feeling towards each candidate, which can be captured using the ANES Feeling Thermometer scales. A metric of the difference between the scores given to the Republican and Democratic candidates is used, based in the work of William Mayer. This metric suffers from significant survey error, but is related to party ID and expressed vote choice, as well as demographic factors and perceived efficacy. Feeling thermometer responses are found to carry meaningful information about a respondent's relationship to the election and candidate preference.",
    "advisors": ["Adam Berinsky"],
    "text": "Finding the swing voter : definitions and survey methods for voter classification This thesis proposes a theory mapping emotional reactions to political information onto a theory of vote decisionmaking and then further onto measurable survey response. Using on-line processing based in emotion, voters form affective summaries about candidates, which store previous information as an emotional response. The act of voting is treated as a single realization of a probabilistic event, with the relative probabilities of each vote option being an expression of the affective summary. These summaries are expressed as warmness or feeling towards each candidate, which can be captured using the ANES Feeling Thermometer scales. A metric of the difference between the scores given to the Republican and Democratic candidates is used, based in the work of William Mayer. This metric suffers from significant survey error, but is related to party ID and expressed vote choice, as well as demographic factors and perceived efficacy. Feeling thermometer responses are found to carry meaningful information about a respondent's relationship to the election and candidate preference."
}, {
    "id": "oai:dspace.mit.edu:1721.1/88381",
    "title": "Enlightened self-interest : how the national economy, ideology, and anti-Americanism influence public opinion on foreign investment",
    "abstract": "Despite the benefits of economic globalization, popular opposition to foreign investment continues to influence policy debates. What explains opposition to foreign investment? Standard political economy theories suggest that support for international trade, immigration, and investment all depend on the impact these policies have on potential earnings in the labor market. According to standard models, those who stand to benefit economically from international exchange are expected to be more supportive than those who will face increased competition and declining wages. An analysis of four cross-national surveys from 57 countries provides empirical evidence that public opinion on foreign investment is not determined by economic self-interest, but rather by evaluations of the national economy, political ideology, and attitudes about the United States. These findings have implications for understanding the debate over globalization policy and domestic support for further liberalization around the world..",
    "advisors": ["Ben Ross Schneider"],
    "text": "Enlightened self-interest : how the national economy, ideology, and anti-Americanism influence public opinion on foreign investment Despite the benefits of economic globalization, popular opposition to foreign investment continues to influence policy debates. What explains opposition to foreign investment? Standard political economy theories suggest that support for international trade, immigration, and investment all depend on the impact these policies have on potential earnings in the labor market. According to standard models, those who stand to benefit economically from international exchange are expected to be more supportive than those who will face increased competition and declining wages. An analysis of four cross-national surveys from 57 countries provides empirical evidence that public opinion on foreign investment is not determined by economic self-interest, but rather by evaluations of the national economy, political ideology, and attitudes about the United States. These findings have implications for understanding the debate over globalization policy and domestic support for further liberalization around the world.."
}, {
    "id": "oai:dspace.mit.edu:1721.1/101807",
    "title": "Coordinating science : White House Office of Science and Technology Policy (OSTP) influence in federal R&D budgets",
    "abstract": "This thesis examines the role of the White House OSTP in the nation's budgeting for science and technology activities. Interviews conducted by the researcher with members of the White House staff as well as federal agency officials are the primary empirical support, with analysis of annual priority memoranda and presidential budget requests reinforcing the findings. The original contribution of this research is to highlight limitations of responsive competence despite presidential attempts to coordinate the R&D bureaucracy. In science policy, presidents obtain responsive competence by hiring entrepreneurial OSTP staff members in the areas that most align with their priorities. The centralized R&D coordination that OSTP does actually perform in budgets is highly constrained by legal authority, bureaucratic resistance, and the epistemic norms of the science policy community itself. The relationship of the President's Science Advisor with the Administration is an important confounder across presidencies",
    "advisors": ["J. Chappell H. Lawson"],
    "text": "Coordinating science : White House Office of Science and Technology Policy (OSTP) influence in federal R&D budgets This thesis examines the role of the White House OSTP in the nation's budgeting for science and technology activities. Interviews conducted by the researcher with members of the White House staff as well as federal agency officials are the primary empirical support, with analysis of annual priority memoranda and presidential budget requests reinforcing the findings. The original contribution of this research is to highlight limitations of responsive competence despite presidential attempts to coordinate the R&D bureaucracy. In science policy, presidents obtain responsive competence by hiring entrepreneurial OSTP staff members in the areas that most align with their priorities. The centralized R&D coordination that OSTP does actually perform in budgets is highly constrained by legal authority, bureaucratic resistance, and the epistemic norms of the science policy community itself. The relationship of the President's Science Advisor with the Administration is an important confounder across presidencies"
}, {
    "id": "oai:dspace.mit.edu:1721.1/53255",
    "title": "The press of a people : the evolution of Spanish-language news and the changing political community",
    "abstract": "Spanish-language news in the United States has grown over the last 20 years into a significant economic and social force. This growth has heightened concerns about the integration of Spanish-speaking groups into American political life and the ability of the media to affect democratic values. Evidence from other countries shows the dangers of fractured mass communication, and a theory based on (a) the treatment of minorities by the state, (b) the special functions demanded by consumers of the ethnic media, and (c) the norms held by both journalists and the community reveals deficiencies in the existing thinking on the mass media. Using content analysis and elite interviews with journalists and editors at a leading Spanish-language newspaper, this thesis examines the potentially polarizing effects of market forces on the Latino media. I find that, after the onset of competition and the transition to a new ownership structure, La Opinidn of Los Angeles changed the information presented to minority audiences, pushing away from its mainstream counterpart and toward more community-based journalism. The most significant findings involve how ethnic groups and their interests are balanced in coverage, as seen through the selection of front-page topics and the representation of said groups within articles. The assignment of causal influence is not, however, as clear-cut as it initially seems; journalistic practices and dynamics with the news organizations shaped how competition would influence coverage.",
    "advisors": ["Chappell Lawson"],
    "text": "The press of a people : the evolution of Spanish-language news and the changing political community Spanish-language news in the United States has grown over the last 20 years into a significant economic and social force. This growth has heightened concerns about the integration of Spanish-speaking groups into American political life and the ability of the media to affect democratic values. Evidence from other countries shows the dangers of fractured mass communication, and a theory based on (a) the treatment of minorities by the state, (b) the special functions demanded by consumers of the ethnic media, and (c) the norms held by both journalists and the community reveals deficiencies in the existing thinking on the mass media. Using content analysis and elite interviews with journalists and editors at a leading Spanish-language newspaper, this thesis examines the potentially polarizing effects of market forces on the Latino media. I find that, after the onset of competition and the transition to a new ownership structure, La Opinidn of Los Angeles changed the information presented to minority audiences, pushing away from its mainstream counterpart and toward more community-based journalism. The most significant findings involve how ethnic groups and their interests are balanced in coverage, as seen through the selection of front-page topics and the representation of said groups within articles. The assignment of causal influence is not, however, as clear-cut as it initially seems; journalistic practices and dynamics with the news organizations shaped how competition would influence coverage."
}, {
    "id": "oai:dspace.mit.edu:1721.1/68957",
    "title": "Justifying power : ruling group dominance and regime justification in multi-ethnic states",
    "abstract": "The current but inconsistent upheaval in the Middle East suggests variations in what will topple regimes, and thus in how regimes have laid the groundwork to remain in power. This thesis examines variation in a social condition, relative dominance of a ruling ethnic group in a multi-ethnic society, as the source for systematic variations in how a mono-ethnic regime will justify its rule to the general population. This thesis argues that the ruling group's relative dominance, defined as its relative percentage to other groups in the population, drives a regime's justifying argument to be either rooted in the presence of universally lauded institutions (democratic-institutional), the regime's demonstrated record of economic and social developmental achievements (economic-social developmental), or the regime's ability to further the interests of an identity common to itself and the population at large (identificational). Relative dominance, it is contended, affects regime behavior by influencing the functioning of two mechanisms: the degree to which a regime can tolerate public accountability and the extent to which it needs to reduce the salience of ethnicity in order to endure. The thesis hypothesizes that the former decreases and the latter increases as dominance decreases. The thesis incorporates quantitative and qualitative analyses to measure and evaluate relationships between relative dominance and justifying arguments. It demonstrates the existence of relationships between dominance and regimes' justifying arguments by means of content analysis of senior leaders' speeches in eight Sunni-dominant, Shi'ite-subordinate countries--Bahrain, Egypt, Iraq (under Saddam Hussein), Jordan, Kuwait, Qatar, the United Arab Emirates, and Yemen. Case studies of one high-dominance country (the UAE), one medium-dominance (Yemen), one low-dominance (Iraq), and one outlier (Bahrain) then illustrate the speculated mechanisms in action.",
    "advisors": ["Roger Petersen"],
    "text": "Justifying power : ruling group dominance and regime justification in multi-ethnic states The current but inconsistent upheaval in the Middle East suggests variations in what will topple regimes, and thus in how regimes have laid the groundwork to remain in power. This thesis examines variation in a social condition, relative dominance of a ruling ethnic group in a multi-ethnic society, as the source for systematic variations in how a mono-ethnic regime will justify its rule to the general population. This thesis argues that the ruling group's relative dominance, defined as its relative percentage to other groups in the population, drives a regime's justifying argument to be either rooted in the presence of universally lauded institutions (democratic-institutional), the regime's demonstrated record of economic and social developmental achievements (economic-social developmental), or the regime's ability to further the interests of an identity common to itself and the population at large (identificational). Relative dominance, it is contended, affects regime behavior by influencing the functioning of two mechanisms: the degree to which a regime can tolerate public accountability and the extent to which it needs to reduce the salience of ethnicity in order to endure. The thesis hypothesizes that the former decreases and the latter increases as dominance decreases. The thesis incorporates quantitative and qualitative analyses to measure and evaluate relationships between relative dominance and justifying arguments. It demonstrates the existence of relationships between dominance and regimes' justifying arguments by means of content analysis of senior leaders' speeches in eight Sunni-dominant, Shi'ite-subordinate countries--Bahrain, Egypt, Iraq (under Saddam Hussein), Jordan, Kuwait, Qatar, the United Arab Emirates, and Yemen. Case studies of one high-dominance country (the UAE), one medium-dominance (Yemen), one low-dominance (Iraq), and one outlier (Bahrain) then illustrate the speculated mechanisms in action."
}, {
    "id": "oai:dspace.mit.edu:1721.1/53085",
    "title": "Blue Helmeted Dragons : explaining China's participation in United Nations peace operations",
    "abstract": "China's personnel contributions to United Nations peace operations has significantly increased in the first decade of the twenty-first century, however little academic or policy attention has been given to examining patterns of Chinese participation. Most current literature examines China's voting behavior on peace operations in the UN Security Council. This thesis employs a research design that combines quantitative and qualitative approaches to assess the drivers behind China's personnel contributions to peace operations. Specifically, the thesis examines the factors that lead China to deploy large contingents of peace keepers to some missions and smaller numbers or none to others. The thesis posits that China's personnel contributions will be higher in peace operations taking place in states that have a high strategic value to China. That is, the peace operation host state is important to China because of the presence of natural resources, Chinese investment, diplomatic interests, or a variety of other factors. The thesis finds that China's participation in peace operations after 2000 is guided by a realist motivation that seeks to maximize access to commercial and diplomatic interests, with higher levels of participation in states with high strategic values. Prior to 2000, fewer Chinese personnel were deployed to peace operations, and the states where they were deployed often had little strategic value to China.",
    "advisors": ["M. Taylor Fravel", "Fotini Christia"],
    "text": "Blue Helmeted Dragons : explaining China's participation in United Nations peace operations China's personnel contributions to United Nations peace operations has significantly increased in the first decade of the twenty-first century, however little academic or policy attention has been given to examining patterns of Chinese participation. Most current literature examines China's voting behavior on peace operations in the UN Security Council. This thesis employs a research design that combines quantitative and qualitative approaches to assess the drivers behind China's personnel contributions to peace operations. Specifically, the thesis examines the factors that lead China to deploy large contingents of peace keepers to some missions and smaller numbers or none to others. The thesis posits that China's personnel contributions will be higher in peace operations taking place in states that have a high strategic value to China. That is, the peace operation host state is important to China because of the presence of natural resources, Chinese investment, diplomatic interests, or a variety of other factors. The thesis finds that China's participation in peace operations after 2000 is guided by a realist motivation that seeks to maximize access to commercial and diplomatic interests, with higher levels of participation in states with high strategic values. Prior to 2000, fewer Chinese personnel were deployed to peace operations, and the states where they were deployed often had little strategic value to China."
}, {
    "id": "oai:dspace.mit.edu:1721.1/118217",
    "title": "Couching intervention : norms, interests, and trends in jurisdictional allocation in Status of Forces Agreements (SOFAs)",
    "abstract": "Much ink has been spilled on the way justifications for and patterns of military intervention have changed, particularly since the end of the Cold War. One aspect of intervention that has not been well explored in this literature, however, is jurisdictional allocation, or which country should try service members who commit crimes while deployed overseas. The sending country normally seeks to retain jurisdiction to protect their service members from criminal systems that may expose them to human rights abuses or lower legal standards than they would enjoy at home. Host countries, on the other hand, often argue against this violation of their sovereignty, which undercuts their legal institutions and ability to regulate internal order. What have been the trends in jurisdictional allocation over time? Has it been consistently allocated to host countries or sending countries, and how has this been justified? What could explain these trends? I explore this question using two cases. The first case focuses on the allocation of jurisdiction in UN peacekeeping Status of Forces Agreements (SOFAs) from 1948-2013, and debates in the 2000s over reforming jurisdictional allocation in the UN's model SOFA and Memorandum of Understanding in light of allegations of rape committed by UN peacekeepers on mission. The second case looks at negotiations surrounding the US-Iraq SOFA in 2008 and 2011. Overall, I argue that patterns of SOFA jurisdictional allocation have consistently favoured the sending country. This is better explained by state interests, or sometimes a mix of interests and norms, rather than norms alone. This paper ultimately points to the need to take a more nuanced look at the dynamics of interventions, which may follow different patterns over time. In other words, not all aspects of intervention may be evolving in the same way.",
    "advisors": ["Roger Petersen"],
    "text": "Couching intervention : norms, interests, and trends in jurisdictional allocation in Status of Forces Agreements (SOFAs) Much ink has been spilled on the way justifications for and patterns of military intervention have changed, particularly since the end of the Cold War. One aspect of intervention that has not been well explored in this literature, however, is jurisdictional allocation, or which country should try service members who commit crimes while deployed overseas. The sending country normally seeks to retain jurisdiction to protect their service members from criminal systems that may expose them to human rights abuses or lower legal standards than they would enjoy at home. Host countries, on the other hand, often argue against this violation of their sovereignty, which undercuts their legal institutions and ability to regulate internal order. What have been the trends in jurisdictional allocation over time? Has it been consistently allocated to host countries or sending countries, and how has this been justified? What could explain these trends? I explore this question using two cases. The first case focuses on the allocation of jurisdiction in UN peacekeeping Status of Forces Agreements (SOFAs) from 1948-2013, and debates in the 2000s over reforming jurisdictional allocation in the UN's model SOFA and Memorandum of Understanding in light of allegations of rape committed by UN peacekeepers on mission. The second case looks at negotiations surrounding the US-Iraq SOFA in 2008 and 2011. Overall, I argue that patterns of SOFA jurisdictional allocation have consistently favoured the sending country. This is better explained by state interests, or sometimes a mix of interests and norms, rather than norms alone. This paper ultimately points to the need to take a more nuanced look at the dynamics of interventions, which may follow different patterns over time. In other words, not all aspects of intervention may be evolving in the same way."
}, {
    "id": "oai:dspace.mit.edu:1721.1/62466",
    "title": "Strategic goods provision in Hezbollah's resistance",
    "abstract": "The provision of goods and services is thought to be a key way that groups are able to gain political power. However, current work has offered a highly fragmentary view of what specific gains can be made with what type of goods provision, and what potential interaction between strategies might exist. This paper integrates key rational actor and electoral models, and tests the resulting predictions against empirical data on Hezbollah's provision of goods and services. Two basic types of models link success in violent contestation to the provision of goods to a restricted community and success in electoral contestation to provision of broadly accessible goods and services. However, across several major types of goods and services Hezbollah consistently provided easily accessible good far before they considered participating in elections, provided more accessible goods relative to restricted goods then can be explained by the importance of electoral and violent contestation, and expanded or contracted the scope of provision at points in time that do not correspond to strategic shifts. As an alternative, I suggest that goods may be geographically rather than temporally strategic, a need to create compliance among the population, and the need to create a sense of agency within the Shia population to increase proactive support for the resistance may be greater drivers of goods provision then has been explored.",
    "advisors": ["Roger D. Petersen"],
    "text": "Strategic goods provision in Hezbollah's resistance The provision of goods and services is thought to be a key way that groups are able to gain political power. However, current work has offered a highly fragmentary view of what specific gains can be made with what type of goods provision, and what potential interaction between strategies might exist. This paper integrates key rational actor and electoral models, and tests the resulting predictions against empirical data on Hezbollah's provision of goods and services. Two basic types of models link success in violent contestation to the provision of goods to a restricted community and success in electoral contestation to provision of broadly accessible goods and services. However, across several major types of goods and services Hezbollah consistently provided easily accessible good far before they considered participating in elections, provided more accessible goods relative to restricted goods then can be explained by the importance of electoral and violent contestation, and expanded or contracted the scope of provision at points in time that do not correspond to strategic shifts. As an alternative, I suggest that goods may be geographically rather than temporally strategic, a need to create compliance among the population, and the need to create a sense of agency within the Shia population to increase proactive support for the resistance may be greater drivers of goods provision then has been explored."
}, {
    "id": "oai:dspace.mit.edu:1721.1/33686",
    "title": "An evaluation of the prescriptive utility of psychological bias theory in international relations",
    "abstract": "I evaluate the practical utility of psychological bias theory by examining two historical cases - the US decision to cross the 38th parallel in 1950 and the British policy of appeasement towards Germany in the 1930s - asking in each of these whether the theory could have helped policymakers to make better decisions. Drawing from the lessons of these two cases, I argue that psychological bias theory can help foreign-policymakers to improve their decisionmaking capabilities and hence increase their chances of achieving favorable outcomes in international politics. However, even if the prescriptions of the theory are adopted, there is no guarantee that positive outcomes will obtain in every case because outcomes are affected by at least two other factors that one largely cannot control: the availability of information and the misperceptions suffered by one's opponent. I also discuss other research methods that could be used to investigate the utility of the theory: examining how useful its prescriptions have been; looking at whether people can actually correct their psychological biases; and considering whether policymakers should attempt to rectify their biases.",
    "advisors": ["Stephen W. Van Evera"],
    "text": "An evaluation of the prescriptive utility of psychological bias theory in international relations I evaluate the practical utility of psychological bias theory by examining two historical cases - the US decision to cross the 38th parallel in 1950 and the British policy of appeasement towards Germany in the 1930s - asking in each of these whether the theory could have helped policymakers to make better decisions. Drawing from the lessons of these two cases, I argue that psychological bias theory can help foreign-policymakers to improve their decisionmaking capabilities and hence increase their chances of achieving favorable outcomes in international politics. However, even if the prescriptions of the theory are adopted, there is no guarantee that positive outcomes will obtain in every case because outcomes are affected by at least two other factors that one largely cannot control: the availability of information and the misperceptions suffered by one's opponent. I also discuss other research methods that could be used to investigate the utility of the theory: examining how useful its prescriptions have been; looking at whether people can actually correct their psychological biases; and considering whether policymakers should attempt to rectify their biases."
}, {
    "id": "oai:dspace.mit.edu:1721.1/44793",
    "title": "Development of a Bayesian Network to monitor the probability of nuclear proliferation",
    "abstract": "Nuclear Proliferation is a complex problem that has plagued national security strategists since the advent of the first nuclear weapons. As the cost to produce nuclear weapons has continued to decline and the availability of nuclear material has become more widespread, the threat of proliferation has increased. The spread of technology and the globalization of the information age has made the threat not only more likely, but also more difficult to detect. Proliferation experts do not agree on the universal factors which cause nations to want to proliferate or the methods to prevent countries from successfully developing nuclear weapons. Historical evidence also indicates that the current nuclear powers pursued their nuclear programs for different reasons and under different conditions. This disparity presents a problem to decision makers who are tasked with preventing further nuclear proliferation. Bayesian Inference is a tool of quantitative analysis that is rapidly gaining interest in numerous fields of scientific study that have previously been limited to purely statistical methods. The Bayesian approach removes the statistical limitations of large-n data sets and strictly numerical types of data. It allows researchers to include sparse and rich data as well as qualitative data based on the opinions of subject matter experts. Bayesian inference allows the inclusion of both the quantitative data and subjective judgments in the determination of predictions about a theory of interest. This means that contrary to classic statistical methods, we can now make accurate predictions with reduced information and apply this probabilistic method to problems in social science. The problem of nuclear proliferation is one that lends itself to a Bayesian analysis. The data set is relatively small and the data is far from consistent from country to country.",
    "advisors": ["Michael W. Golay"],
    "text": "Development of a Bayesian Network to monitor the probability of nuclear proliferation Nuclear Proliferation is a complex problem that has plagued national security strategists since the advent of the first nuclear weapons. As the cost to produce nuclear weapons has continued to decline and the availability of nuclear material has become more widespread, the threat of proliferation has increased. The spread of technology and the globalization of the information age has made the threat not only more likely, but also more difficult to detect. Proliferation experts do not agree on the universal factors which cause nations to want to proliferate or the methods to prevent countries from successfully developing nuclear weapons. Historical evidence also indicates that the current nuclear powers pursued their nuclear programs for different reasons and under different conditions. This disparity presents a problem to decision makers who are tasked with preventing further nuclear proliferation. Bayesian Inference is a tool of quantitative analysis that is rapidly gaining interest in numerous fields of scientific study that have previously been limited to purely statistical methods. The Bayesian approach removes the statistical limitations of large-n data sets and strictly numerical types of data. It allows researchers to include sparse and rich data as well as qualitative data based on the opinions of subject matter experts. Bayesian inference allows the inclusion of both the quantitative data and subjective judgments in the determination of predictions about a theory of interest. This means that contrary to classic statistical methods, we can now make accurate predictions with reduced information and apply this probabilistic method to problems in social science. The problem of nuclear proliferation is one that lends itself to a Bayesian analysis. The data set is relatively small and the data is far from consistent from country to country."
}, {
    "id": "oai:dspace.mit.edu:1721.1/53082",
    "title": "Regime legitimacy and military resilience : lessons from World War II and Yugoslavia",
    "abstract": "This thesis argues that regime legitimacy creates military resilience. A regime is legitimate when its constituents believe-whether because of ideological solidarity, patriotism, nationalism, or good governance-that a government has the right to exercise authority in its regime. Military resilience, which contributes to military effectiveness, refers to the willingness of troops to stay committed in combat. In modern war, dispersion of forces creates the need for a very high degree of troop commitment, making resilience more important than in previous forms of warfare. Resilient units do not disintegrate through desertion, and furthermore commit themselves actively under fire. In arguing that legitimacy matters, this thesis revives a debate between two theories of military resilience. The first school, which comes out of the tradition of the mass army, holds that broad attributes like legitimacy, patriotism, and nationalism are crucial to resilience. In recent political science, a second school has been significantly more influential; these scholars argue that factors like small-unit cohesion and professionalism are the key explanatory variables for military resilience. Settling the debate between these competing methods of generating resilience is critical to effective army building. This thesis strongly supports a revival of the first school of thought, based on the evidence from two cases where legitimacy experienced a sudden shock. The first case examines the military resilience of foreign legions forced to fight for Nazi Germany in World War II.",
    "advisors": ["Barry R. Posen"],
    "text": "Regime legitimacy and military resilience : lessons from World War II and Yugoslavia This thesis argues that regime legitimacy creates military resilience. A regime is legitimate when its constituents believe-whether because of ideological solidarity, patriotism, nationalism, or good governance-that a government has the right to exercise authority in its regime. Military resilience, which contributes to military effectiveness, refers to the willingness of troops to stay committed in combat. In modern war, dispersion of forces creates the need for a very high degree of troop commitment, making resilience more important than in previous forms of warfare. Resilient units do not disintegrate through desertion, and furthermore commit themselves actively under fire. In arguing that legitimacy matters, this thesis revives a debate between two theories of military resilience. The first school, which comes out of the tradition of the mass army, holds that broad attributes like legitimacy, patriotism, and nationalism are crucial to resilience. In recent political science, a second school has been significantly more influential; these scholars argue that factors like small-unit cohesion and professionalism are the key explanatory variables for military resilience. Settling the debate between these competing methods of generating resilience is critical to effective army building. This thesis strongly supports a revival of the first school of thought, based on the evidence from two cases where legitimacy experienced a sudden shock. The first case examines the military resilience of foreign legions forced to fight for Nazi Germany in World War II."
}, {
    "id": "oai:dspace.mit.edu:1721.1/28494",
    "title": "One man's wickedness : malignant narcissism and major blunders in international relations",
    "abstract": "Malignant narcissism is a personality syndrome marked by hubris, paranoia, and reckless indifference to the human consequences of decisions. Malignant-narcissistic leaders tend to commit major blunders in international relations. Examples include Saddam, Hitler, Stalin, Mussolini, Mao, and Nasser. Seventeen hypotheses are presented on malignant narcissism, exploring its relationships to cognition, domestic political systems, and blunders in international relations. The hypotheses are illustrated with historical examples to support their plausibility. Saddam is explained as a malignant narcissist who was especially prone to blunders throughout his political career.",
    "advisors": ["Stephen W. Van Evera"],
    "text": "One man's wickedness : malignant narcissism and major blunders in international relations Malignant narcissism is a personality syndrome marked by hubris, paranoia, and reckless indifference to the human consequences of decisions. Malignant-narcissistic leaders tend to commit major blunders in international relations. Examples include Saddam, Hitler, Stalin, Mussolini, Mao, and Nasser. Seventeen hypotheses are presented on malignant narcissism, exploring its relationships to cognition, domestic political systems, and blunders in international relations. The hypotheses are illustrated with historical examples to support their plausibility. Saddam is explained as a malignant narcissist who was especially prone to blunders throughout his political career."
}, {
    "id": "oai:dspace.mit.edu:1721.1/16640",
    "title": "The Deepwater Program : a case study in organizational transformation inspired by the parallel interaction of internal and external core groups",
    "abstract": "This paper attempts to explain why the United States Coast Guard decided to undertake its most recent major capital asset replacement effort-the Deepwater Program-through the use of a systems approach. Several explanations are considered, but a series of interviews and a review of events during the 1996-2003 timeframe yield an explanation that points to bureaucratic politics and status dynamics as the most likely cause. In particular, the paper finds that the Coast Guard's low status (vis--vis other organizations within the Department of Transportation) combined with the Deepwater community's high status (vis--vis other communities within the Coast Guard) to produce a political environment that made the use of a systems approach almost inevitable. The paper closes by considering the policy ramifications of systems approaches used by relative weak organizations.",
    "advisors": ["Harvey M. Sapolsky"],
    "text": "The Deepwater Program : a case study in organizational transformation inspired by the parallel interaction of internal and external core groups This paper attempts to explain why the United States Coast Guard decided to undertake its most recent major capital asset replacement effort-the Deepwater Program-through the use of a systems approach. Several explanations are considered, but a series of interviews and a review of events during the 1996-2003 timeframe yield an explanation that points to bureaucratic politics and status dynamics as the most likely cause. In particular, the paper finds that the Coast Guard's low status (vis--vis other organizations within the Department of Transportation) combined with the Deepwater community's high status (vis--vis other communities within the Coast Guard) to produce a political environment that made the use of a systems approach almost inevitable. The paper closes by considering the policy ramifications of systems approaches used by relative weak organizations."
}, {
    "id": "oai:dspace.mit.edu:1721.1/53083",
    "title": "Containing the opposition : selective representation in Jordan and Turkey",
    "abstract": "How does elite manipulation of election mechanisms affect the representation of political regime opponents? While the spread of elections has reached all the continents, the number of actual democracies has not increased at a comparable rate. If anything, observers have learned that the presence of elections in a country does not necessarily mean that it is also a democracy. This thesis addresses an underexplored topic in the study of electoral politics: the manipulation of election systems in order to achieve selective representation. I focus on the experience of opposition parties in two cases, Jordan and Turkey, an autocracy and a democracy, to analyze the impact of engineered election mechanisms on their representation. I contend that parties in power exploit the rules of the electoral game to contain their opposition. This is done by different mechanisms, depending on the makeup of the country and the options available to the manipulators. Mechanisms of electoral systems are used to reduce the representation of groups that are considered a threat, and to amplify the representation of those groups that the regime would like to strengthen. Analyzing the effect of malapportioned seats and the use of a single non-transferable voting system in Jordan on the Islamic Action Front Party (IAF), the main political rival to traditional tribal politicians, I expose the power of these targeted electoral mechanisms for control. Examining how the 10% national election threshold in Turkey affects representation of the Islamist political parties in the Grand National Assembly uncovers the distorting effect of this universal mechanism on representation.",
    "advisors": ["Orit Kedar"],
    "text": "Containing the opposition : selective representation in Jordan and Turkey How does elite manipulation of election mechanisms affect the representation of political regime opponents? While the spread of elections has reached all the continents, the number of actual democracies has not increased at a comparable rate. If anything, observers have learned that the presence of elections in a country does not necessarily mean that it is also a democracy. This thesis addresses an underexplored topic in the study of electoral politics: the manipulation of election systems in order to achieve selective representation. I focus on the experience of opposition parties in two cases, Jordan and Turkey, an autocracy and a democracy, to analyze the impact of engineered election mechanisms on their representation. I contend that parties in power exploit the rules of the electoral game to contain their opposition. This is done by different mechanisms, depending on the makeup of the country and the options available to the manipulators. Mechanisms of electoral systems are used to reduce the representation of groups that are considered a threat, and to amplify the representation of those groups that the regime would like to strengthen. Analyzing the effect of malapportioned seats and the use of a single non-transferable voting system in Jordan on the Islamic Action Front Party (IAF), the main political rival to traditional tribal politicians, I expose the power of these targeted electoral mechanisms for control. Examining how the 10% national election threshold in Turkey affects representation of the Islamist political parties in the Grand National Assembly uncovers the distorting effect of this universal mechanism on representation."
}, {
    "id": "oai:dspace.mit.edu:1721.1/53241",
    "title": "Bringing it all to the table : examining variance in strategic approaches within the six-party talk",
    "abstract": "In the approaches seen at the six-party talks on North Korea's nuclear program, why did states faced with the same security problem adopt different strategies? Answering this question will bring understanding to why the process has proceeded in fits and starts, as the countries negotiating with Pyongyang - China, Japan, Russia, South Korea, and the United States - often struggle to coordinate strategy in their quest to resolve a grave issue of international security. This paper approaches the question by taking up three possible drivers behind strategy among the five negotiating countries - realist calculations, domestic political institutions, and national identity - and, tracing each country's strategy within the talks, identifies the most likely of these drivers for each state. This study finds that while the United States, China, and Russia bring primarily realist concerns to the table, they employ separate strategies toward the North Korean nuclear issue, reflecting differing drivers and goals. In addition, South Korea and Japan see their strategies driven by issues related to national identity and domestic politics. In looking at the origins of these drivers, this study finds that China's realist drive stems largely from its particular vision of economic and geopolitical growth; Japan's push for a resolution to the kidnapping issue stems from politicians' aim for domestic political popularity made easier through Japan's lack of a history of relations with North Korea; Russia's realist drive derives from the Putin-led push to regain a semblance of its historical sphere of influence;",
    "advisors": ["Richard J. Samuels"],
    "text": "Bringing it all to the table : examining variance in strategic approaches within the six-party talk In the approaches seen at the six-party talks on North Korea's nuclear program, why did states faced with the same security problem adopt different strategies? Answering this question will bring understanding to why the process has proceeded in fits and starts, as the countries negotiating with Pyongyang - China, Japan, Russia, South Korea, and the United States - often struggle to coordinate strategy in their quest to resolve a grave issue of international security. This paper approaches the question by taking up three possible drivers behind strategy among the five negotiating countries - realist calculations, domestic political institutions, and national identity - and, tracing each country's strategy within the talks, identifies the most likely of these drivers for each state. This study finds that while the United States, China, and Russia bring primarily realist concerns to the table, they employ separate strategies toward the North Korean nuclear issue, reflecting differing drivers and goals. In addition, South Korea and Japan see their strategies driven by issues related to national identity and domestic politics. In looking at the origins of these drivers, this study finds that China's realist drive stems largely from its particular vision of economic and geopolitical growth; Japan's push for a resolution to the kidnapping issue stems from politicians' aim for domestic political popularity made easier through Japan's lack of a history of relations with North Korea; Russia's realist drive derives from the Putin-led push to regain a semblance of its historical sphere of influence;"
}, {
    "id": "oai:dspace.mit.edu:1721.1/68961",
    "title": "\"Who needs MacArthur?\" : analyzing South Korea's counterinvasion capability against North Korea",
    "abstract": "Assuming there is another North Korean invasion; could the South Koreans counterinvade North Korea and prevail even without the United States' assistance? This paper studies the possibility of a South Korean counterinvasion against North Korea by looking at the qualitative combat dynamics and performing a formal campaign analyses based on the Korean peninsula's conventional military balance. This study first analyzes the process of the South Korean defensive against the North Korean invasion, and examines South Korea's likely counterinvasion scenarios and assesses their chances of success. These scenarios vary based on North Korea's likely courses of action once its offensive fails, depending on whether the North Koreans retreat to the military demarcation line or hold their position within the South territory. According to this paper's analysis, South Korea is capable of counterinvading North Korea in all the scenarios suggested. South Korea possesses a qualitatively superior force with better readiness and logistics powered by a stronger economy, while the North Koreans lack the force effectiveness necessary to carry out their theory of victory. First, the South Korean forces are capable of fending off a North Korean invasion while inflicting severe damage to the North Koreans; second, the South Korean forces would inflict considerable casualty to the North Koreans during their retreat; finally, the South Korean offensive would be capable of breaking through the weakened North Korean defense. This study makes several contributions. First, it examines the puzzle of South Korean counterinvasion that has been under-discussed despite its political and strategic significance. In doing so, the study presents an opportunity to explain North Korea's recent behaviors and the United States' redefinition of its role involving the peninsula, hence increasing our understanding of the East Asian security dynamics. Second, by providing an updated survey of the peninsula's conventional balance, this study enhances our knowledge in the two Korea's strategic capabilities which have undergone considerable changes. Third, this study advances our usage of campaign analyses by applying a phased use of the models with changing parameters. This approach enables us to analyze multi-phased campaigns comprised of different dynamics with better accuracy.",
    "advisors": ["Barry R. Posen"],
    "text": "\"Who needs MacArthur?\" : analyzing South Korea's counterinvasion capability against North Korea Assuming there is another North Korean invasion; could the South Koreans counterinvade North Korea and prevail even without the United States' assistance? This paper studies the possibility of a South Korean counterinvasion against North Korea by looking at the qualitative combat dynamics and performing a formal campaign analyses based on the Korean peninsula's conventional military balance. This study first analyzes the process of the South Korean defensive against the North Korean invasion, and examines South Korea's likely counterinvasion scenarios and assesses their chances of success. These scenarios vary based on North Korea's likely courses of action once its offensive fails, depending on whether the North Koreans retreat to the military demarcation line or hold their position within the South territory. According to this paper's analysis, South Korea is capable of counterinvading North Korea in all the scenarios suggested. South Korea possesses a qualitatively superior force with better readiness and logistics powered by a stronger economy, while the North Koreans lack the force effectiveness necessary to carry out their theory of victory. First, the South Korean forces are capable of fending off a North Korean invasion while inflicting severe damage to the North Koreans; second, the South Korean forces would inflict considerable casualty to the North Koreans during their retreat; finally, the South Korean offensive would be capable of breaking through the weakened North Korean defense. This study makes several contributions. First, it examines the puzzle of South Korean counterinvasion that has been under-discussed despite its political and strategic significance. In doing so, the study presents an opportunity to explain North Korea's recent behaviors and the United States' redefinition of its role involving the peninsula, hence increasing our understanding of the East Asian security dynamics. Second, by providing an updated survey of the peninsula's conventional balance, this study enhances our knowledge in the two Korea's strategic capabilities which have undergone considerable changes. Third, this study advances our usage of campaign analyses by applying a phased use of the models with changing parameters. This approach enables us to analyze multi-phased campaigns comprised of different dynamics with better accuracy."
}, {
    "id": "oai:dspace.mit.edu:1721.1/83761",
    "title": "Host nation security force development : a new roadmap",
    "abstract": "A new model concerning the concepts of host nation security force development, or security sector reform (SSR), is proposed. This model is rooted in scholarly literature and seeks to fill current gaps in United States Army doctrine. The model is mobilized as a dependent variable, changing with different conditions of insurgency and ethnicity. This is a novel approach because it considers distinction between military and police forces as the central concept. Additionally, current security sector reform policies and procedures are not well theorized nor codified in doctrine. Empirical studies were carried out in order to validate hypotheses generated by this new proposed model.",
    "advisors": ["Roger Petersen"],
    "text": "Host nation security force development : a new roadmap A new model concerning the concepts of host nation security force development, or security sector reform (SSR), is proposed. This model is rooted in scholarly literature and seeks to fill current gaps in United States Army doctrine. The model is mobilized as a dependent variable, changing with different conditions of insurgency and ethnicity. This is a novel approach because it considers distinction between military and police forces as the central concept. Additionally, current security sector reform policies and procedures are not well theorized nor codified in doctrine. Empirical studies were carried out in order to validate hypotheses generated by this new proposed model."
}, {
    "id": "oai:dspace.mit.edu:1721.1/46280",
    "title": "Law of Peoples and the duty of assistance : Rawls on redistributive justice among people",
    "abstract": "In The Law of Peoples (1999) Rawls offers a model of the world, divided into countries without pressures of nationalism, which he calls \"peoples.\" If some of those peoples were liberal democracies, others consult all citizens, though not equally, and others were badly governed, what obligations would the well-ordered countries have to the badly-ordered ones? There is one class of unjust society called \"burdened\" that is not malicious but lacks the political traditions and institutions needed to be well-ordered, and it may also be unable to care for its citizens. Well-ordered societies owe these burdened ones a \"Duty of Assistance\" to help them become well-ordered. Rawls thinks that what they most need is political assistance to create just institutions, and perhaps some small, temporary economic aid for acute crises, for two reasons. Economics teaches that large-scale crises like famine or mass migration are caused by (bad) governments, and aren't inevitable consequences of drought. If a just people wants to prevent large-scale disasters, donating large amounts of cash won't help. The permanent cure is just government. Also, he denies that there are any real countries that have too few resources to support their population. If so, poverty or hunger is not inevitable anywhere, and what we call problems of poverty are really symptoms of bad government. I agree that political aid is extremely important. However, I disagree with the unimportance of material assistance. First I show that his empirical ground doesn't support an \"institutions only\" approach. Second, I argue that (a) primary goods are heterogeneous, and redistribution means different things for different kinds of goods, (b) needs for some of these may be adequately assured by good government, but not just burdened societies can have long-term need for others. The duty of Assistance requires redistribution of more goods, to more types of society than Rawls asserts. Third, Rawls argues that economic redistribution is an important matter for domestic justice, and not a concern at the international level. I challenge one of his illustrations, showing that it is not completely assured by a just domestic society.",
    "advisors": ["Joshua Cohen"],
    "text": "Law of Peoples and the duty of assistance : Rawls on redistributive justice among people In The Law of Peoples (1999) Rawls offers a model of the world, divided into countries without pressures of nationalism, which he calls \"peoples.\" If some of those peoples were liberal democracies, others consult all citizens, though not equally, and others were badly governed, what obligations would the well-ordered countries have to the badly-ordered ones? There is one class of unjust society called \"burdened\" that is not malicious but lacks the political traditions and institutions needed to be well-ordered, and it may also be unable to care for its citizens. Well-ordered societies owe these burdened ones a \"Duty of Assistance\" to help them become well-ordered. Rawls thinks that what they most need is political assistance to create just institutions, and perhaps some small, temporary economic aid for acute crises, for two reasons. Economics teaches that large-scale crises like famine or mass migration are caused by (bad) governments, and aren't inevitable consequences of drought. If a just people wants to prevent large-scale disasters, donating large amounts of cash won't help. The permanent cure is just government. Also, he denies that there are any real countries that have too few resources to support their population. If so, poverty or hunger is not inevitable anywhere, and what we call problems of poverty are really symptoms of bad government. I agree that political aid is extremely important. However, I disagree with the unimportance of material assistance. First I show that his empirical ground doesn't support an \"institutions only\" approach. Second, I argue that (a) primary goods are heterogeneous, and redistribution means different things for different kinds of goods, (b) needs for some of these may be adequately assured by good government, but not just burdened societies can have long-term need for others. The duty of Assistance requires redistribution of more goods, to more types of society than Rawls asserts. Third, Rawls argues that economic redistribution is an important matter for domestic justice, and not a concern at the international level. I challenge one of his illustrations, showing that it is not completely assured by a just domestic society."
}, {
    "id": "oai:dspace.mit.edu:1721.1/104568",
    "title": "Racial, not rational : economic threat, symbolic racism, and affirmative action",
    "abstract": "For decades, scholars have debated the determinants of whites' attitudes about racialized policies such as welfare, busing, and affirmative action. While some have argued that whites formulate their positions rationally according to perceived economic threat, others have asserted that such policy attitudes are the function of one's level of symbolic racism, with little to no influence from economic considerations. Using data from the 2012 Cooperative Congressional Elections Study and demographic data, I assess the effects of actual economic competition and an individual's other attitudes on white opposition to affirmative action. Furthermore, in order to identify the levels, if any, through which the economic threat mechanism operates, this paper measures economic threat in several different ways: at both the level of the individual and the level of whites as a group, and each of these at both the zip code and county levels. I find strong support for the symbolic racism theory of policy attitude formation, as respondent attitudes are driven mostly by racial affect, ideology, and party identification. No matter the level at which economic threat is measured, objective economic conditions do not seem to influence one's attitudes about affirmative action.",
    "advisors": ["Adam Berinsky"],
    "text": "Racial, not rational : economic threat, symbolic racism, and affirmative action For decades, scholars have debated the determinants of whites' attitudes about racialized policies such as welfare, busing, and affirmative action. While some have argued that whites formulate their positions rationally according to perceived economic threat, others have asserted that such policy attitudes are the function of one's level of symbolic racism, with little to no influence from economic considerations. Using data from the 2012 Cooperative Congressional Elections Study and demographic data, I assess the effects of actual economic competition and an individual's other attitudes on white opposition to affirmative action. Furthermore, in order to identify the levels, if any, through which the economic threat mechanism operates, this paper measures economic threat in several different ways: at both the level of the individual and the level of whites as a group, and each of these at both the zip code and county levels. I find strong support for the symbolic racism theory of policy attitude formation, as respondent attitudes are driven mostly by racial affect, ideology, and party identification. No matter the level at which economic threat is measured, objective economic conditions do not seem to influence one's attitudes about affirmative action."
}, {
    "id": "oai:dspace.mit.edu:1721.1/95520",
    "title": "Safe havens in Syria : missions and requirements for an air campaign",
    "abstract": "What if the United States had led its NATO allies to intervene in Syria's civil war in the midst of calls for humanitarian intervention in mid-2012? Despite the importance of this question for the study and evaluation of U.S. foreign policy, little exists in the way of systematic, open-source analysis of the military missions and material requirements for a possible Syrian intervention. This thesis assesses the scale, scope, and challenges of intervention in Syria at the time its proponents argue it would have been most effective. It does so through open-source analysis of a U.S.-led air campaign designed to mitigate the country's humanitarian crisis. The model of intervention analyzed -- broadly conceived as the establishment of safe havens around major Syrian population areas defended from the air -- offers a template for evaluating the utility of air power in the Syrian context more generally. The analysis suggests an intervention in mid-2012 to establish safe havens in Syria would have been a major military undertaking, likely requiring greater resources and facing greater risks than any of NATO's previous air campaigns in response to humanitarian crises in Bosnia, Kosovo, or Libya. The \"low-risk\" rationale for humanitarian intervention from the air thus appears far less persuasive in the Syrian case. The thesis concludes with implications for the application of air power to future humanitarian crises.",
    "advisors": ["Barry R. Posen"],
    "text": "Safe havens in Syria : missions and requirements for an air campaign What if the United States had led its NATO allies to intervene in Syria's civil war in the midst of calls for humanitarian intervention in mid-2012? Despite the importance of this question for the study and evaluation of U.S. foreign policy, little exists in the way of systematic, open-source analysis of the military missions and material requirements for a possible Syrian intervention. This thesis assesses the scale, scope, and challenges of intervention in Syria at the time its proponents argue it would have been most effective. It does so through open-source analysis of a U.S.-led air campaign designed to mitigate the country's humanitarian crisis. The model of intervention analyzed -- broadly conceived as the establishment of safe havens around major Syrian population areas defended from the air -- offers a template for evaluating the utility of air power in the Syrian context more generally. The analysis suggests an intervention in mid-2012 to establish safe havens in Syria would have been a major military undertaking, likely requiring greater resources and facing greater risks than any of NATO's previous air campaigns in response to humanitarian crises in Bosnia, Kosovo, or Libya. The \"low-risk\" rationale for humanitarian intervention from the air thus appears far less persuasive in the Syrian case. The thesis concludes with implications for the application of air power to future humanitarian crises."
}, {
    "id": "oai:dspace.mit.edu:1721.1/33710",
    "title": "Secrecy, deception and intelligence failure : explaining operational surprise in war",
    "abstract": "Operational surprise attacks are large-scale, theater-level intrawar attacks, which result from a country misestimating the capabilities and intentions of its enemies. This thesis analyzes how these massive surprise attacks occur during war when countries should be especially wary of their enemies and vigilant for any evidence of attack. Three hypotheses may explain the frequency and success of operational surprise attacks including operational secrecy, strategic deception, and intelligence failure. Using the Battle of the Bulge in World War II and the Chinese counteroffensive in the Korean War as case studies, this analysis illustrates these three elements and evaluates their relative causal weight in these attacks. This study concludes that each hypothesis is a contributing element to the surprise attack, but that a failure of intelligence is the critical factor. Moreover, this failure stems from a \"victory disease\" - a belief held by military leaders and their intelligence staff when victory appears near that one's enemy is too weak or has allowed the opportunity to mount a successful counterattack pass.",
    "advisors": ["Barry Posen"],
    "text": "Secrecy, deception and intelligence failure : explaining operational surprise in war Operational surprise attacks are large-scale, theater-level intrawar attacks, which result from a country misestimating the capabilities and intentions of its enemies. This thesis analyzes how these massive surprise attacks occur during war when countries should be especially wary of their enemies and vigilant for any evidence of attack. Three hypotheses may explain the frequency and success of operational surprise attacks including operational secrecy, strategic deception, and intelligence failure. Using the Battle of the Bulge in World War II and the Chinese counteroffensive in the Korean War as case studies, this analysis illustrates these three elements and evaluates their relative causal weight in these attacks. This study concludes that each hypothesis is a contributing element to the surprise attack, but that a failure of intelligence is the critical factor. Moreover, this failure stems from a \"victory disease\" - a belief held by military leaders and their intelligence staff when victory appears near that one's enemy is too weak or has allowed the opportunity to mount a successful counterattack pass."
}, {
    "id": "oai:dspace.mit.edu:1721.1/43195",
    "title": "Playing poker with a committee : assessing the viability of coalitional coercive diplomacy",
    "abstract": "Since the end of the Cold War, 'coercive diplomacy', that is the strategic use of threats aimed at convincing an adversary to stop or undo 'hostile' actions, has become a principle crisis management tool of the Western powers. Yet as a strategy it has a relatively poor track record of success; a record that theorists have struggled to explain. Despite the fact that the majority of such engagements have been undertaken multilaterally, little work to date has focused on the impact that employing a coalitional 'coercing agent' has on the ability to craft a potent package of threats. This study aims to contribute to the debate by testing empirically the impact of coalitional dynamics on the ability of Western powers to employ coercive diplomacy. More specifically, it explores the theoretical tension between the anecdotal assumptions held by certain theorists regarding coalitional action problems (CAP) on the one hand and Jakobsen's four-point ideal policy of coercive diplomacy on the other. I explore this tension through two case studies: the employment of coercive diplomacy by NATO and the Contact Group aimed at halting Serbian aggression in Bosnia between 1992 and 1995 and the Western coalition's attempt to roll back the Iraqi invasion of Kuwait in 1990. The study finds that although CAP do arise as predicted, coalitions have effective mechanisms for overcoming them and thus are able to effectively implement coercive threat packages that approximate to Jakobsen's ideal policy.",
    "advisors": ["Roger Peterson"],
    "text": "Playing poker with a committee : assessing the viability of coalitional coercive diplomacy Since the end of the Cold War, 'coercive diplomacy', that is the strategic use of threats aimed at convincing an adversary to stop or undo 'hostile' actions, has become a principle crisis management tool of the Western powers. Yet as a strategy it has a relatively poor track record of success; a record that theorists have struggled to explain. Despite the fact that the majority of such engagements have been undertaken multilaterally, little work to date has focused on the impact that employing a coalitional 'coercing agent' has on the ability to craft a potent package of threats. This study aims to contribute to the debate by testing empirically the impact of coalitional dynamics on the ability of Western powers to employ coercive diplomacy. More specifically, it explores the theoretical tension between the anecdotal assumptions held by certain theorists regarding coalitional action problems (CAP) on the one hand and Jakobsen's four-point ideal policy of coercive diplomacy on the other. I explore this tension through two case studies: the employment of coercive diplomacy by NATO and the Contact Group aimed at halting Serbian aggression in Bosnia between 1992 and 1995 and the Western coalition's attempt to roll back the Iraqi invasion of Kuwait in 1990. The study finds that although CAP do arise as predicted, coalitions have effective mechanisms for overcoming them and thus are able to effectively implement coercive threat packages that approximate to Jakobsen's ideal policy."
}, {
    "id": "oai:dspace.mit.edu:1721.1/43194",
    "title": "Professionalism, institutionalization and committee services in US. state legislatures",
    "abstract": "This thesis examines the relationship between legislative professionalism and institutionalization in the committee systems of six U.S. states. I examine whether increased professionalization, as defined by increases in levels of member salary, legislative staffing, and time in session, causes legislatures to institutionalize in a manner similar to the U.S. Congress. Specifically, this thesis focuses on the use (or lack thereof) of seniority as an automatic procedure for the assignment to, and transfer between, committees. I find that while it appears that all state legislators value service on committees, legislative professionalization is not an adequate explanatory variable to describe the variation in the institutionalization of committee systems that we see across states in the United States. This finding is especially evident in the analysis of California, the most professionalized state legislature in the U.S.",
    "advisors": ["Charles H. Stewart, III"],
    "text": "Professionalism, institutionalization and committee services in US. state legislatures This thesis examines the relationship between legislative professionalism and institutionalization in the committee systems of six U.S. states. I examine whether increased professionalization, as defined by increases in levels of member salary, legislative staffing, and time in session, causes legislatures to institutionalize in a manner similar to the U.S. Congress. Specifically, this thesis focuses on the use (or lack thereof) of seniority as an automatic procedure for the assignment to, and transfer between, committees. I find that while it appears that all state legislators value service on committees, legislative professionalization is not an adequate explanatory variable to describe the variation in the institutionalization of committee systems that we see across states in the United States. This finding is especially evident in the analysis of California, the most professionalized state legislature in the U.S."
}, {
    "id": "oai:dspace.mit.edu:1721.1/64491",
    "title": "Death before dismount? : mechanization, force employment, and counterinsurgency outcomes in Iraq",
    "abstract": "Recent research suggests that heavily mechanized armies perform worse in counterinsurgency campaigns than those that use fewer vehicles. The U.S. military's 2007 operations in Iraq, however, present an empirical quandary for the mechanization hypothesis: a vehicle-heavy army proved able to suppress an insurgency, allowing Iraqi leaders to work towards a long-term political solution. This paper argues that force employment, not mechanization, drives counterinsurgency outcomes-what matters is not that armies have many vehicles or soldiers, but how they choose to use them. When heavily mechanized forces change their tactics and doctrine to line up with counterinsurgency principles, shifting from an enemy-centric to a population-centric approach, outcomes dramatically improve while military-wide mechanization levels remain constant. Using an original dataset, this paper conducts a large-n regression analysis of the impacts of mechanization at the provincial level in Iraq, and finds little support for the mechanization hypothesis. A subsequent comparative case study, of the heavily mechanized 3rd Armored Cavalry Regiment's operations in Tall Afar and the light infantry 82nd Airborne Division's operations in Fallujah, indicate that force employment rather than mechanization is a key indicator of counterinsurgency outcomes. The finding has important implications for force structure policy, as it indicates that mechanized forces can indeed conduct successful counterinsurgency campaigns.",
    "advisors": ["Roger D. Petersen"],
    "text": "Death before dismount? : mechanization, force employment, and counterinsurgency outcomes in Iraq Recent research suggests that heavily mechanized armies perform worse in counterinsurgency campaigns than those that use fewer vehicles. The U.S. military's 2007 operations in Iraq, however, present an empirical quandary for the mechanization hypothesis: a vehicle-heavy army proved able to suppress an insurgency, allowing Iraqi leaders to work towards a long-term political solution. This paper argues that force employment, not mechanization, drives counterinsurgency outcomes-what matters is not that armies have many vehicles or soldiers, but how they choose to use them. When heavily mechanized forces change their tactics and doctrine to line up with counterinsurgency principles, shifting from an enemy-centric to a population-centric approach, outcomes dramatically improve while military-wide mechanization levels remain constant. Using an original dataset, this paper conducts a large-n regression analysis of the impacts of mechanization at the provincial level in Iraq, and finds little support for the mechanization hypothesis. A subsequent comparative case study, of the heavily mechanized 3rd Armored Cavalry Regiment's operations in Tall Afar and the light infantry 82nd Airborne Division's operations in Fallujah, indicate that force employment rather than mechanization is a key indicator of counterinsurgency outcomes. The finding has important implications for force structure policy, as it indicates that mechanized forces can indeed conduct successful counterinsurgency campaigns."
}, {
    "id": "oai:dspace.mit.edu:1721.1/99561",
    "title": "Electoral backlash against climate policy : a natural experiment on retrospective voting and local resistance to public policy",
    "abstract": "Retrospective voting studies typically examine policies where the public has common interests. By contrast, climate policy has broad public support but concentrated opposition in communities where costs are imposed. This spatial distribution of weak supporters and strong, local opponents mirrors opposition to other policies with diffuse public benefits and concentrated local costs. I use a natural experiment to investigate whether citizens living in proximity to wind energy projects retrospectively punished an incumbent government because of its climate policy. Using both fixed effects and instrumental variable estimators, I identify electoral losses for the incumbent party ranging from 4-10%, with the effect persisting 3 km from wind turbines. Voters also discriminate by correctly punishing the level of government responsible for the policy, providing evidence that voters are informed. I conclude that the spatial distribution of citizens' policy preferences can affect democratic accountability and exacerbate political barriers to addressing climate change.",
    "advisors": ["Christopher Warshaw"],
    "text": "Electoral backlash against climate policy : a natural experiment on retrospective voting and local resistance to public policy Retrospective voting studies typically examine policies where the public has common interests. By contrast, climate policy has broad public support but concentrated opposition in communities where costs are imposed. This spatial distribution of weak supporters and strong, local opponents mirrors opposition to other policies with diffuse public benefits and concentrated local costs. I use a natural experiment to investigate whether citizens living in proximity to wind energy projects retrospectively punished an incumbent government because of its climate policy. Using both fixed effects and instrumental variable estimators, I identify electoral losses for the incumbent party ranging from 4-10%, with the effect persisting 3 km from wind turbines. Voters also discriminate by correctly punishing the level of government responsible for the policy, providing evidence that voters are informed. I conclude that the spatial distribution of citizens' policy preferences can affect democratic accountability and exacerbate political barriers to addressing climate change."
}, {
    "id": "oai:dspace.mit.edu:1721.1/40378",
    "title": "Geologic Storage of carbon dioxide : risk analyses and implications for public acceptance",
    "abstract": "Carbon Capture and Storage (CCS) technology has the potential to enable large reductions in global greenhouse gas emissions, but one of the unanswered questions about CCS is whether it will be accepted by the public. In the past, construction of large facilities such as nuclear power plants has been prevented or delayed by public opposition, and CCS proponents would like to know whether it will provoke similar public opposition. Since the Geologic Storage (GS) component of the CCS architecture has not been widely deployed, this thesis explores the characteristics of GS and how they might affect public perception and acceptance of the larger CCS architecture. To provide insight regarding public acceptance of CCS, this thesis addresses two questions; first asking how GS is likely to be perceived by the public and what can be done to improve that perception, and second asking whether financial compensation can be used to improve public acceptance of energy facilities. To address the first question about the public perception of GS, this thesis begins with a discussion of risk concepts and how it is used differently by experts, who use a realist perspective, and the general public, who use a social constructivist perspective.",
    "advisors": ["Howard Herzog", "Stephen Ansolabehere"],
    "text": "Geologic Storage of carbon dioxide : risk analyses and implications for public acceptance Carbon Capture and Storage (CCS) technology has the potential to enable large reductions in global greenhouse gas emissions, but one of the unanswered questions about CCS is whether it will be accepted by the public. In the past, construction of large facilities such as nuclear power plants has been prevented or delayed by public opposition, and CCS proponents would like to know whether it will provoke similar public opposition. Since the Geologic Storage (GS) component of the CCS architecture has not been widely deployed, this thesis explores the characteristics of GS and how they might affect public perception and acceptance of the larger CCS architecture. To provide insight regarding public acceptance of CCS, this thesis addresses two questions; first asking how GS is likely to be perceived by the public and what can be done to improve that perception, and second asking whether financial compensation can be used to improve public acceptance of energy facilities. To address the first question about the public perception of GS, this thesis begins with a discussion of risk concepts and how it is used differently by experts, who use a realist perspective, and the general public, who use a social constructivist perspective."
}, {
    "id": "oai:dspace.mit.edu:1721.1/101806",
    "title": "Policy legacies and child care politics in Australia and Canada",
    "abstract": "This study explores the puzzle of why Australia and Canada have followed significantly different paths in national-level child care policy despite their otherwise similar welfare state structures. Australia has developed a relatively generous system of public subsidies to support the provision of care for young children, while at the same time relying increasingly on the market to deliver child care. In contrast, Canada has extremely low levels of public spending and service provision, resulting in a less expansive system of regulated child care. I trace these divergent outcomes to the impact of post-WWII child care policy legacies in these countries and the way that these legacies interact with the changing politics of the welfare state to produce variation. In Canada, child care policy was first established within a social assistance framework as a service intended to combat poverty, while in Australia, child care was introduced as an economic policy to facilitate women's workforce participation. The differences in the intended goals of these policies affected the subsequent patterns of child care politics and policy development in these two countries, leading to the divergent outcomes observed today.",
    "advisors": ["Kathleen Thelen"],
    "text": "Policy legacies and child care politics in Australia and Canada This study explores the puzzle of why Australia and Canada have followed significantly different paths in national-level child care policy despite their otherwise similar welfare state structures. Australia has developed a relatively generous system of public subsidies to support the provision of care for young children, while at the same time relying increasingly on the market to deliver child care. In contrast, Canada has extremely low levels of public spending and service provision, resulting in a less expansive system of regulated child care. I trace these divergent outcomes to the impact of post-WWII child care policy legacies in these countries and the way that these legacies interact with the changing politics of the welfare state to produce variation. In Canada, child care policy was first established within a social assistance framework as a service intended to combat poverty, while in Australia, child care was introduced as an economic policy to facilitate women's workforce participation. The differences in the intended goals of these policies affected the subsequent patterns of child care politics and policy development in these two countries, leading to the divergent outcomes observed today."
}, {
    "id": "oai:dspace.mit.edu:1721.1/108970",
    "title": "The effort to cut out-of-pocket medical expenses and the political constraints : examples from the New Cooperative Medical Scheme in China",
    "abstract": "This master thesis examines the impact of the New Cooperative Medical Scheme (NCMS) on rural residents' out-of-pocket medical expenses (OOP) in China. The thesis first uses quantitative methods to identify the statistical relationships between NCMS and rural OOP and finds that enrolling in NCMS is associated with higher OOP. Then, using the 2009 reform, also known as Healthy China 2020, as a starting point, the thesis further explores the factors behind the resistance to the reform efforts to cut down OOP. By utilizing two political science theories - institutional layering and policy feedback - the thesis finds that the 2009 reform has so far failed to achieve noticeable reductions in OOP because, on the one hand, fierce institutional competitions has transformed NCMS into a fragmented program with too many veto players; on the other hand, policy feedback effects, under which previous policies continue to influence the trajectory of future policy-making decisions, strengthen particular interest groups and force the reform to make compromises.",
    "advisors": ["Andrea Louise Campbell"],
    "text": "The effort to cut out-of-pocket medical expenses and the political constraints : examples from the New Cooperative Medical Scheme in China This master thesis examines the impact of the New Cooperative Medical Scheme (NCMS) on rural residents' out-of-pocket medical expenses (OOP) in China. The thesis first uses quantitative methods to identify the statistical relationships between NCMS and rural OOP and finds that enrolling in NCMS is associated with higher OOP. Then, using the 2009 reform, also known as Healthy China 2020, as a starting point, the thesis further explores the factors behind the resistance to the reform efforts to cut down OOP. By utilizing two political science theories - institutional layering and policy feedback - the thesis finds that the 2009 reform has so far failed to achieve noticeable reductions in OOP because, on the one hand, fierce institutional competitions has transformed NCMS into a fragmented program with too many veto players; on the other hand, policy feedback effects, under which previous policies continue to influence the trajectory of future policy-making decisions, strengthen particular interest groups and force the reform to make compromises."
}, {
    "id": "oai:dspace.mit.edu:1721.1/28754",
    "title": "U.S. export controls on encryption technology",
    "abstract": "(cont.) effort that eventually paid off in 1999. Interest group politics also factors into the actions of the national security establishment as they also lobby the Presidency and Congress to maintain restrictive encryption regulations. The study uses organizational culture to explain the motivations and some of the actions of the NSA, particularly with regard to its preference for secrecy, its placement of national security above other values, and its efforts to maintain control over all cryptology, whether government or civilian.",
    "advisors": ["Kenneth A. Oye"],
    "text": "U.S. export controls on encryption technology (cont.) effort that eventually paid off in 1999. Interest group politics also factors into the actions of the national security establishment as they also lobby the Presidency and Congress to maintain restrictive encryption regulations. The study uses organizational culture to explain the motivations and some of the actions of the NSA, particularly with regard to its preference for secrecy, its placement of national security above other values, and its efforts to maintain control over all cryptology, whether government or civilian."
}, {
    "id": "oai:dspace.mit.edu:1721.1/8260",
    "title": "Konfrontasi : rethinking explanations for the Indonesia-Malaysia confrontation, 1963-1966",
    "abstract": "This thesis is a study of the causes of \"Konfrontasi\" or Confrontation, the low-intensity war waged by the Republic of Indonesia, under the leadership of President Sukarno, against the Federation of Malaysia, which became independent in 1963. The Confrontation lasted between 1963 and 1966. The thesis compares three categories of hypotheses or arguments for its causes - threat, ideology, and domestic politics - and evaluates each type of argument in tum. The \"threat\" argument claims that Malaysia posed a security threat to Indonesia, and that the Confrontation was the outcome of a security dilemma between both states. The \"threat\" of Malaysia has some substantive elements necessary to justify Indonesian aggression, but it is shown to be largely an exaggerated claim, and does not provide a sufficient motive for the conflict. The \"ideology\" argument claims that the Indonesia Confrontation was driven by the ideology of the Indonesian Revolution and the central role of Indonesia in leading a struggle of New Emerging Forces against the Old Established Forces of neo-colonialism, colonialism and imperialism. It is shown that this argument has more substance than the \"threat\" argument, since Indonesian ideology traces its existence independently to earlier Indonesian historical experience, and Confrontation could not have been rationalized without recourse to ideological principles. However, this thesis also shows how the \"ideology\" hypothesis for Confrontation is over-determined, as Indonesian ideology did not necessarily make Confrontation an inevitable and necessary outcome. Ideology was necessary, but insufficient for motivating the decision to confront Malaysia. Finally, the \"domestic politics\" argument draws its claims from the idea that the Indonesian Confrontation was a \"diversionary war\" against the Malaysia, where the latter filled the role of \"scapegoat,\" \"bogeyman\" or \"safety valve.\" According to this argument, the Confrontation was started in order to contain serious internal disunities in the Indonesian government, most notably between the army and the ascendant communist party, and to unite these conflicting elements in a nationalist cause. This thesis finds the greatest evidence and theoretical support for this \"domestic politics\" explanation of Confrontation, and finds this account to be the most consistent and satisfactory argument.",
    "advisors": ["Thomas J. Christensen"],
    "text": "Konfrontasi : rethinking explanations for the Indonesia-Malaysia confrontation, 1963-1966 This thesis is a study of the causes of \"Konfrontasi\" or Confrontation, the low-intensity war waged by the Republic of Indonesia, under the leadership of President Sukarno, against the Federation of Malaysia, which became independent in 1963. The Confrontation lasted between 1963 and 1966. The thesis compares three categories of hypotheses or arguments for its causes - threat, ideology, and domestic politics - and evaluates each type of argument in tum. The \"threat\" argument claims that Malaysia posed a security threat to Indonesia, and that the Confrontation was the outcome of a security dilemma between both states. The \"threat\" of Malaysia has some substantive elements necessary to justify Indonesian aggression, but it is shown to be largely an exaggerated claim, and does not provide a sufficient motive for the conflict. The \"ideology\" argument claims that the Indonesia Confrontation was driven by the ideology of the Indonesian Revolution and the central role of Indonesia in leading a struggle of New Emerging Forces against the Old Established Forces of neo-colonialism, colonialism and imperialism. It is shown that this argument has more substance than the \"threat\" argument, since Indonesian ideology traces its existence independently to earlier Indonesian historical experience, and Confrontation could not have been rationalized without recourse to ideological principles. However, this thesis also shows how the \"ideology\" hypothesis for Confrontation is over-determined, as Indonesian ideology did not necessarily make Confrontation an inevitable and necessary outcome. Ideology was necessary, but insufficient for motivating the decision to confront Malaysia. Finally, the \"domestic politics\" argument draws its claims from the idea that the Indonesian Confrontation was a \"diversionary war\" against the Malaysia, where the latter filled the role of \"scapegoat,\" \"bogeyman\" or \"safety valve.\" According to this argument, the Confrontation was started in order to contain serious internal disunities in the Indonesian government, most notably between the army and the ascendant communist party, and to unite these conflicting elements in a nationalist cause. This thesis finds the greatest evidence and theoretical support for this \"domestic politics\" explanation of Confrontation, and finds this account to be the most consistent and satisfactory argument."
}, {
    "id": "oai:dspace.mit.edu:1721.1/37196",
    "title": "Dependence, independence, and interdependence in world politics",
    "abstract": "We implement techniques of graph theory to international trade in order to empirically inspect the international system of trade. Examining macro and submacro levels of the international system of trade from 1962-2003, we find the presence of a Scale-Free Network with a Multiscalar Hierarchy. Such structures are resilient to bottom-up economic collapse, but are susceptible to top-down and horizontal economic failures. Our findings are based upon an especially novel approach for examining submacro systems, applying latent community identification analysis to identify trading communities that are not necessarily formalized or institutionalized as trading blocs. Following this analysis, we examine the role of international institutions in the international trade network, specifically considering macro level institutions for stability solutions and examining the effects of joining a trade bloc. We find evidence that supports the intergovernmentalist framework, whereby certain types of trade blocs seem to succeed while others fail, leading to different results in integration and unification.",
    "advisors": ["Nazli Choucri"],
    "text": "Dependence, independence, and interdependence in world politics We implement techniques of graph theory to international trade in order to empirically inspect the international system of trade. Examining macro and submacro levels of the international system of trade from 1962-2003, we find the presence of a Scale-Free Network with a Multiscalar Hierarchy. Such structures are resilient to bottom-up economic collapse, but are susceptible to top-down and horizontal economic failures. Our findings are based upon an especially novel approach for examining submacro systems, applying latent community identification analysis to identify trading communities that are not necessarily formalized or institutionalized as trading blocs. Following this analysis, we examine the role of international institutions in the international trade network, specifically considering macro level institutions for stability solutions and examining the effects of joining a trade bloc. We find evidence that supports the intergovernmentalist framework, whereby certain types of trade blocs seem to succeed while others fail, leading to different results in integration and unification."
}, {
    "id": "oai:dspace.mit.edu:1721.1/95549",
    "title": "Getting to the table : explaining the incidence of mediation in the insurgencies of Indonesia",
    "abstract": "Indonesia has experienced six insurgencies since it declared independence in 1945. Of these insurgencies, three were resolved through negotiations. There is great variation in the manner the negotiations occurred. The state negotiated with Portugal over East Timor with the United Nations (UN) as mediator while negotiations with the Acehnese were first mediated by the Henry Dunant Centre for Humanitarian Dialogue (HDC) and later the Crisis Management Initiative (CMI). Finally, the state refused any mediators in the case of West Papua, where Indonesia's longest and bloodiest insurgency continues to take place. What explains the variation in the decision to have mediation and the choice of mediators? This is the central question of the thesis. In examining this variation, I hope to contribute to the literature on bargaining in insurgencies as well as examine the effectiveness of mediation, which is disputed. I argue that a state that is not committed or has very low levels of commitment to negotiations will not have a mediator. The more committed the state is to negotiations, the stronger the mediator the state will seek. The level of commitment is a function of the balance of power between the incumbents and insurgents, domestic support, and international pressure for peaceful resolution. Domestic support is the pivotal factor with the military being the most decisive actor. Based on this argument, I develop a scenario-based framework in which states could possibly find themselves in and test it on the three cases of insurgencies in Indonesia. The findings show that the state was more committed to reaching a settlement in East Timor and Aceh than in West Papua and so had mediators to ensure the success of the peace processes, which would not have occurred otherwise. In addition, the findings also suggest that a hurting stalemate is not a necessary precondition for successful mediation, contrary to the literature on mediation. The thesis concludes by drawing some policy implications and directions for further research.",
    "advisors": ["Roger Petersen"],
    "text": "Getting to the table : explaining the incidence of mediation in the insurgencies of Indonesia Indonesia has experienced six insurgencies since it declared independence in 1945. Of these insurgencies, three were resolved through negotiations. There is great variation in the manner the negotiations occurred. The state negotiated with Portugal over East Timor with the United Nations (UN) as mediator while negotiations with the Acehnese were first mediated by the Henry Dunant Centre for Humanitarian Dialogue (HDC) and later the Crisis Management Initiative (CMI). Finally, the state refused any mediators in the case of West Papua, where Indonesia's longest and bloodiest insurgency continues to take place. What explains the variation in the decision to have mediation and the choice of mediators? This is the central question of the thesis. In examining this variation, I hope to contribute to the literature on bargaining in insurgencies as well as examine the effectiveness of mediation, which is disputed. I argue that a state that is not committed or has very low levels of commitment to negotiations will not have a mediator. The more committed the state is to negotiations, the stronger the mediator the state will seek. The level of commitment is a function of the balance of power between the incumbents and insurgents, domestic support, and international pressure for peaceful resolution. Domestic support is the pivotal factor with the military being the most decisive actor. Based on this argument, I develop a scenario-based framework in which states could possibly find themselves in and test it on the three cases of insurgencies in Indonesia. The findings show that the state was more committed to reaching a settlement in East Timor and Aceh than in West Papua and so had mediators to ensure the success of the peace processes, which would not have occurred otherwise. In addition, the findings also suggest that a hurting stalemate is not a necessary precondition for successful mediation, contrary to the literature on mediation. The thesis concludes by drawing some policy implications and directions for further research."
}, {
    "id": "oai:dspace.mit.edu:1721.1/62470",
    "title": "The happy median? : an examination of the role of partisanship on social spending",
    "abstract": "This thesis examines the effect of party-level partisanship on social spending outcomes. Building a model in which party-politician bargaining plays a central role in determining the passage of bills, this paper develops a theory predicting the superior performance of moderate parties in proposing and passing legislation. Testing this theory using historical roll-call and social spending data, this paper finds that the effects of ideology are not so simple, and that the effects of partisanship on party effectiveness varies with the type of policy examined.",
    "advisors": ["James Snyder, Jr"],
    "text": "The happy median? : an examination of the role of partisanship on social spending This thesis examines the effect of party-level partisanship on social spending outcomes. Building a model in which party-politician bargaining plays a central role in determining the passage of bills, this paper develops a theory predicting the superior performance of moderate parties in proposing and passing legislation. Testing this theory using historical roll-call and social spending data, this paper finds that the effects of ideology are not so simple, and that the effects of partisanship on party effectiveness varies with the type of policy examined."
}, {
    "id": "oai:dspace.mit.edu:1721.1/74464",
    "title": "Organizational images : towards a model of organizations",
    "abstract": "This study develops a general theoretical framework for the analysis of organizational behavior by focusing on the notion that organizations develop unique information-processing frameworks, which it labels \"organizational images\" or \"images of operations,\" that strongly determine their behavior. The model is then used to draw inferences about the forms of counterinsurgency strategies practiced by the US military in the second war in Iraq and the war in Afghanistan. The paper argues that militaries tend to view the tasks they undertake in terms of the coercive application of force, and that this tendency tends to determine the forms of counterinsurgency strategies they chose, leading them to eschew strategies that rely on bargaining with enemy forces. The purported dominance of this coercive \"image of operations\" is then investigated in military field reports from the war in Afghanistan.",
    "advisors": ["Roger Petersen"],
    "text": "Organizational images : towards a model of organizations This study develops a general theoretical framework for the analysis of organizational behavior by focusing on the notion that organizations develop unique information-processing frameworks, which it labels \"organizational images\" or \"images of operations,\" that strongly determine their behavior. The model is then used to draw inferences about the forms of counterinsurgency strategies practiced by the US military in the second war in Iraq and the war in Afghanistan. The paper argues that militaries tend to view the tasks they undertake in terms of the coercive application of force, and that this tendency tends to determine the forms of counterinsurgency strategies they chose, leading them to eschew strategies that rely on bargaining with enemy forces. The purported dominance of this coercive \"image of operations\" is then investigated in military field reports from the war in Afghanistan."
}, {
    "id": "oai:dspace.mit.edu:1721.1/37436",
    "title": "A thousand suns : political motivations for nuclear weapons testing",
    "abstract": "Nuclear weapon testing is the final step in the nuclear development process, an announcement of ability and strength. The consequences of a nuclear test are far from easy to bear, however: economic sanctions can be crippling and nuclear capability automatically makes one a nuclear target. Why, then, do states test nuclear weapons? This thesis aims to determine the answer to this question using India as a model. It is well known that India tested nuclear weapons in 1974 and in 1998, but less well known are the near-tests of 1983, 1995, and 1996. This thesis examines the situation in these years and the details of the nuclear decisions based on four hypotheses: technical concerns, security and power, domestic politics, and norms and ideas. This study shows that while all four of these theories play a role, technical concerns (contrary to popular belief) are very minor portion of the overall decision to test a nuclear weapon and are relegated to an excuse for scientists. Domestic politics, especially the political fortunes of those in power, play a large role, especially when combined with real, existential security concerns. Similarly, the prestige and status that leaders believe is imparted by nuclear ability is of major import. Understanding the reasons for nuclear testing will lead to fewer nuclear surprises in the future and may help to address the concerns of the growing number of states with latent nuclear capabilities.",
    "advisors": ["Harvey Sapolsky"],
    "text": "A thousand suns : political motivations for nuclear weapons testing Nuclear weapon testing is the final step in the nuclear development process, an announcement of ability and strength. The consequences of a nuclear test are far from easy to bear, however: economic sanctions can be crippling and nuclear capability automatically makes one a nuclear target. Why, then, do states test nuclear weapons? This thesis aims to determine the answer to this question using India as a model. It is well known that India tested nuclear weapons in 1974 and in 1998, but less well known are the near-tests of 1983, 1995, and 1996. This thesis examines the situation in these years and the details of the nuclear decisions based on four hypotheses: technical concerns, security and power, domestic politics, and norms and ideas. This study shows that while all four of these theories play a role, technical concerns (contrary to popular belief) are very minor portion of the overall decision to test a nuclear weapon and are relegated to an excuse for scientists. Domestic politics, especially the political fortunes of those in power, play a large role, especially when combined with real, existential security concerns. Similarly, the prestige and status that leaders believe is imparted by nuclear ability is of major import. Understanding the reasons for nuclear testing will lead to fewer nuclear surprises in the future and may help to address the concerns of the growing number of states with latent nuclear capabilities."
}, {
    "id": "oai:dspace.mit.edu:1721.1/54610",
    "title": "The Golden Lariat : explaining American aid to Israel",
    "abstract": "An observational study was conducted to determine the most likely explanation of American support for Israel. Several extant hypotheses were considered, most particularly, and at greatest length, that of a pro-Israel domestic lobby in the United States, but also that it had to do with Cold War containment, common values, or precedent. It was ultimately concluded that the domestic lobby hypothesis could not account for American support, since the level of that support correlated negatively with the resources of the lobby, and because sudden, temporary changes in the level of American support did not coincide with any similar changes in the resources of the lobby. Furthermore, statistical analysis indicated that there was on balance no benefit for politicians who supported the lobby's agenda, and no cost for those who opposed it. Likewise, the other explanations also proved unsatisfactory. The containment hypothesis, for instance, could not explain why American support continued after the Cold War ended, while the common values hypothesis could not explain why American support did not begin until 1971, nor why it peaked in 1979 and began to decline in the eighties. Finally, it was concluded that the best explanation of American support was that it gave the United States the leverage to restrain Israeli belligerence, for which the United States was blamed by the Arab states. In this way, the U.S. was able to minimize damage to its relations with the Arabs resulting from the Arab-Israeli conflict.",
    "advisors": ["Stephen Van Evera"],
    "text": "The Golden Lariat : explaining American aid to Israel An observational study was conducted to determine the most likely explanation of American support for Israel. Several extant hypotheses were considered, most particularly, and at greatest length, that of a pro-Israel domestic lobby in the United States, but also that it had to do with Cold War containment, common values, or precedent. It was ultimately concluded that the domestic lobby hypothesis could not account for American support, since the level of that support correlated negatively with the resources of the lobby, and because sudden, temporary changes in the level of American support did not coincide with any similar changes in the resources of the lobby. Furthermore, statistical analysis indicated that there was on balance no benefit for politicians who supported the lobby's agenda, and no cost for those who opposed it. Likewise, the other explanations also proved unsatisfactory. The containment hypothesis, for instance, could not explain why American support continued after the Cold War ended, while the common values hypothesis could not explain why American support did not begin until 1971, nor why it peaked in 1979 and began to decline in the eighties. Finally, it was concluded that the best explanation of American support was that it gave the United States the leverage to restrain Israeli belligerence, for which the United States was blamed by the Arab states. In this way, the U.S. was able to minimize damage to its relations with the Arabs resulting from the Arab-Israeli conflict."
}, {
    "id": "oai:dspace.mit.edu:1721.1/43192",
    "title": "Access to the vote in the 2006 midterm election : evidence from the 2006 Cooperative Congressional Election Study",
    "abstract": "The 2000 and 2004 U.S. national elections were plagued by problems which caused a significant number of citizens to be effectively denied access to the vote. This paper uses data from the 2006 Cooperative Congressional Election Study (CCES) public opinion poll to measure whether certain electoral problems persisted in the 2006 midterm elections. Of particular concern are whether voters were asked to show photo ID in order to vote, whether voters experienced problems with their registrations upon attempting to vote, what demographic groups experienced these problems most frequently, and what remedies were offered to such voters. Additionally, public opinion on whether all voters should be required to show photo IDs in order to vote and on whether polling stations were well operated in this election is also examined. The data shows that while significant percentages of CCES respondents experienced registration problems when voting and/or were asked to show photo ID before voting almost no respondents were prevented from casting ballots. Respondents showed overwhelming support for measures which would require all voters to show photo ID before voting, though this support varied significantly by party ID. Finally, respondents were overwhelmingly pleased with how their polling stations were operated during this election and very few of them were forced to wait in long lines before voting.",
    "advisors": ["Steven Ansolabehere"],
    "text": "Access to the vote in the 2006 midterm election : evidence from the 2006 Cooperative Congressional Election Study The 2000 and 2004 U.S. national elections were plagued by problems which caused a significant number of citizens to be effectively denied access to the vote. This paper uses data from the 2006 Cooperative Congressional Election Study (CCES) public opinion poll to measure whether certain electoral problems persisted in the 2006 midterm elections. Of particular concern are whether voters were asked to show photo ID in order to vote, whether voters experienced problems with their registrations upon attempting to vote, what demographic groups experienced these problems most frequently, and what remedies were offered to such voters. Additionally, public opinion on whether all voters should be required to show photo IDs in order to vote and on whether polling stations were well operated in this election is also examined. The data shows that while significant percentages of CCES respondents experienced registration problems when voting and/or were asked to show photo ID before voting almost no respondents were prevented from casting ballots. Respondents showed overwhelming support for measures which would require all voters to show photo ID before voting, though this support varied significantly by party ID. Finally, respondents were overwhelmingly pleased with how their polling stations were operated during this election and very few of them were forced to wait in long lines before voting."
}, {
    "id": "oai:dspace.mit.edu:1721.1/117307",
    "title": "Evidence of absence : the progressives and strategic non-voting in the house, 1907-1925",
    "abstract": "The Progressive movement presents a puzzle for analysts of Congress: a deeply divided Republican party that appears in roll calls as extremely unified and homogenous. Historical records and theories of Congress suggest that part of the answer lies in missing votes - legislators abstaining in order to reduce cross-pressure between their party and constituents, and the Speaker using quorum calls to exclude disloyal Republicans. Using imputation to \"fill in\" the missing vote data from the 60th House reveals that missing votes had the effect of concealing Republican heterogeneity. This preceded the revolt against Cannon in the 61st House, and was more common for the non-Insurgents who faced the strongest cross-pressure. This pattern continued under Democratic rule in the 62nd House, fading out after the GOP revolt in the Speakership elections of the 68th. This evidence of non-random missingness can help resolve the puzzle of the Progressives, and inform historical study of Congress.",
    "advisors": ["Charles Stewart, III"],
    "text": "Evidence of absence : the progressives and strategic non-voting in the house, 1907-1925 The Progressive movement presents a puzzle for analysts of Congress: a deeply divided Republican party that appears in roll calls as extremely unified and homogenous. Historical records and theories of Congress suggest that part of the answer lies in missing votes - legislators abstaining in order to reduce cross-pressure between their party and constituents, and the Speaker using quorum calls to exclude disloyal Republicans. Using imputation to \"fill in\" the missing vote data from the 60th House reveals that missing votes had the effect of concealing Republican heterogeneity. This preceded the revolt against Cannon in the 61st House, and was more common for the non-Insurgents who faced the strongest cross-pressure. This pattern continued under Democratic rule in the 62nd House, fading out after the GOP revolt in the Speakership elections of the 68th. This evidence of non-random missingness can help resolve the puzzle of the Progressives, and inform historical study of Congress."
}, {
    "id": "oai:dspace.mit.edu:1721.1/68963",
    "title": "Reconstructing respect : the quest for prestige in the international system",
    "abstract": "Prestige is a term that appears in a wide range of international relations literature, but it is rarely ever defined. There is a vague consensus that prestige involves measures of status and respect, but its exact usage is different in every work. This thesis analyzes the various manifestations of prestige to develop a workable definition of the concept and then uses this definition to show how prestige plays a role in the major foreign policy decisions of states. This thesis argues that prestige motivations can overcome security concerns in some instances and cause a state to take an action that seems irrational. This is especially true if the state has recently suffered a severe drop in prestige, such as one incurred after losing a war, becoming isolated from the international community or facing state collapse. When such a dramatic loss occurs a state must take one of four paths to salvage its lost reputation: winning a war, becoming an economic power, taking the lead on an important political negotiation or developing nuclear weapons. This thesis uses two large case studies - Iran and Egypt - along with three smaller case surveys - France, Japan and Pakistan - to illustrate these four paths of status adjustment in action. It also presents a dataset of states that have suffered a severe loss in prestige to show how states can lose prestige and how they can gain it back.",
    "advisors": ["Roger Petersen"],
    "text": "Reconstructing respect : the quest for prestige in the international system Prestige is a term that appears in a wide range of international relations literature, but it is rarely ever defined. There is a vague consensus that prestige involves measures of status and respect, but its exact usage is different in every work. This thesis analyzes the various manifestations of prestige to develop a workable definition of the concept and then uses this definition to show how prestige plays a role in the major foreign policy decisions of states. This thesis argues that prestige motivations can overcome security concerns in some instances and cause a state to take an action that seems irrational. This is especially true if the state has recently suffered a severe drop in prestige, such as one incurred after losing a war, becoming isolated from the international community or facing state collapse. When such a dramatic loss occurs a state must take one of four paths to salvage its lost reputation: winning a war, becoming an economic power, taking the lead on an important political negotiation or developing nuclear weapons. This thesis uses two large case studies - Iran and Egypt - along with three smaller case surveys - France, Japan and Pakistan - to illustrate these four paths of status adjustment in action. It also presents a dataset of states that have suffered a severe loss in prestige to show how states can lose prestige and how they can gain it back."
}, {
    "id": "oai:dspace.mit.edu:1721.1/46631",
    "title": "Politicized armies, militarized politics : civil-military relations in Turkey and Greece",
    "abstract": "Despite their common Ottoman heritage, Greece and Turkey have diverged widely in their modem history of civil-military relations. The armed forces have a long record of intervention in both countries, but there is a crucial difference: the military emerged as a roughly unitary, independent political actor in Turkey, whereas in Greece it remained divided into factions aligned with civilian political parties through patronage relationships. This empirical observation is then used as a basis for an attempt at theory building. Several countries exhibit a pattern of military interventions more similar to Turkey and others to those found in Greece. Societies which developed a strong parliamentary tradition early in the modernization process also acquired organized civilian political groups with clientelist networks extending into the armed forces. On the contrary, in countries with limited or weak parliamentary development and strong security pressures, political activism was often channeled through the military, which emerged as a hotbed of political thinking, predating and pre-empting any civilian party tradition. The former type of civil-military relations was more commonly found in Southern European and Latin American countries while the latter was predominant in non-Western societies that resisted Western colonization.",
    "advisors": ["Roger Peterson"],
    "text": "Politicized armies, militarized politics : civil-military relations in Turkey and Greece Despite their common Ottoman heritage, Greece and Turkey have diverged widely in their modem history of civil-military relations. The armed forces have a long record of intervention in both countries, but there is a crucial difference: the military emerged as a roughly unitary, independent political actor in Turkey, whereas in Greece it remained divided into factions aligned with civilian political parties through patronage relationships. This empirical observation is then used as a basis for an attempt at theory building. Several countries exhibit a pattern of military interventions more similar to Turkey and others to those found in Greece. Societies which developed a strong parliamentary tradition early in the modernization process also acquired organized civilian political groups with clientelist networks extending into the armed forces. On the contrary, in countries with limited or weak parliamentary development and strong security pressures, political activism was often channeled through the military, which emerged as a hotbed of political thinking, predating and pre-empting any civilian party tradition. The former type of civil-military relations was more commonly found in Southern European and Latin American countries while the latter was predominant in non-Western societies that resisted Western colonization."
}, {
    "id": "oai:dspace.mit.edu:1721.1/54611",
    "title": "Congress and the Financial Services Industry, 1989-2008",
    "abstract": "This thesis explores the congressional politics of the financial services industry in the United States between 1989 and 2008. Three approaches are pursued. First, I provide a detailed account of the major legislation concerning the industry during this period, with particular reference to interest group competition between commercial banks, securities firms and insurance companies and to the repeal of the Glass-Steagall Act in 1999. I suggest that intraindustry conflict was instrumental in delaying Glass-Steagall's repeal until 1999, but that these eventually faded away in response to events outside the Congressional sphere and gave way to a period of intra-industry cooperation in the years after 1999 because the repeal of Glass-Steagall effectively aligned the interests of industry sub-sectors. Second, I present statistical evidence that suggest that these changes are reflected in the contribution strategies of PACs aligned with the financial services industry. Before the repeal of Glass-Steagall, competing groups within the industry valued certain individual legislator characteristics (above all, various committee memberships) at quite different levels. However, after 1999, the contribution strategies of the industry sub-sectors converge in patterns consistent with the reduction of interest group competition. Third, I present the results of statistical models that provide further evidence that the repeal of Glass-Steagall represents a turning point with respect to intra-industry competition. I show that after 1999 competing interest groups began to coordinate their contributions to members of committees with jurisdiction over financial services legislation; before the repeal of Glass-Steagall, there is no evidence of this. Taken together, these three approaches suggest that the regulatory environment shapes not only the business practices of corporations, but also the ways they attempt to influence public policy.",
    "advisors": ["Charles Stewart, III"],
    "text": "Congress and the Financial Services Industry, 1989-2008 This thesis explores the congressional politics of the financial services industry in the United States between 1989 and 2008. Three approaches are pursued. First, I provide a detailed account of the major legislation concerning the industry during this period, with particular reference to interest group competition between commercial banks, securities firms and insurance companies and to the repeal of the Glass-Steagall Act in 1999. I suggest that intraindustry conflict was instrumental in delaying Glass-Steagall's repeal until 1999, but that these eventually faded away in response to events outside the Congressional sphere and gave way to a period of intra-industry cooperation in the years after 1999 because the repeal of Glass-Steagall effectively aligned the interests of industry sub-sectors. Second, I present statistical evidence that suggest that these changes are reflected in the contribution strategies of PACs aligned with the financial services industry. Before the repeal of Glass-Steagall, competing groups within the industry valued certain individual legislator characteristics (above all, various committee memberships) at quite different levels. However, after 1999, the contribution strategies of the industry sub-sectors converge in patterns consistent with the reduction of interest group competition. Third, I present the results of statistical models that provide further evidence that the repeal of Glass-Steagall represents a turning point with respect to intra-industry competition. I show that after 1999 competing interest groups began to coordinate their contributions to members of committees with jurisdiction over financial services legislation; before the repeal of Glass-Steagall, there is no evidence of this. Taken together, these three approaches suggest that the regulatory environment shapes not only the business practices of corporations, but also the ways they attempt to influence public policy."
}, {
    "id": "oai:dspace.mit.edu:1721.1/8856",
    "title": "The value of knowledge networks : conceptual framework in application to sustainable production",
    "abstract": "The thesis is motivated by two major trends: the rise of a global information and knowledge economy, and environmental degradation and the search for sustainable solutions. The increasing importance of knowledge has by some been equated with a new industrial revolution, one based on computer technology, digital infrastructure, and highly educated and technically skilled workers. But how do we assess the value of knowledge in this 'new' economy? The question over value is explored through the diffusion and localization of new knowledge via a knowledge network, based on information technology. The central argument is that in the knowledge economy, the value of knowledge lies in the ability to share it over a knowledge network, which allows for diffusion and localization of new knowledge. This central thesis and the value of knowledge networks is further explored by looking at the case of environmentally friendly or sustainable production. The knowledge network targets barriers to environmentally friendly practices by encouraging and enabling diffusion of knowledge related to sustainable products and processes. The knowledge scope for environmental solutions is analyzed, with the objective to develop common categories, and to understand better the increasing complexities and knowledge needs as enterprises engage in sustainable production. In discussing the knowledge economy and knowledge networks, the thesis focuses mostly on the business enterprise. But the development of the knowledge age has much larger implications, such as 'knowledge for whom?' and 'value for whom?'. The information technologies and networks offer new ways for people and groups to interact and influence social issues and can enable the diffusion of wide variety of views and perspectives. Thinking about the information and knowledge age in the larger economic and social context requires us to consider who builds, controls, influences and benefits from the technology and its use. Before we can reasonably approach this analysis, a basic conceptual framework or understanding of knowledge sharing, knowledge networks, and value of knowledge is called for. This thesis is a building block for such a framework, a contribution to future research into the economic and social implications of the knowledge economy.",
    "advisors": ["Nazli Choucri"],
    "text": "The value of knowledge networks : conceptual framework in application to sustainable production The thesis is motivated by two major trends: the rise of a global information and knowledge economy, and environmental degradation and the search for sustainable solutions. The increasing importance of knowledge has by some been equated with a new industrial revolution, one based on computer technology, digital infrastructure, and highly educated and technically skilled workers. But how do we assess the value of knowledge in this 'new' economy? The question over value is explored through the diffusion and localization of new knowledge via a knowledge network, based on information technology. The central argument is that in the knowledge economy, the value of knowledge lies in the ability to share it over a knowledge network, which allows for diffusion and localization of new knowledge. This central thesis and the value of knowledge networks is further explored by looking at the case of environmentally friendly or sustainable production. The knowledge network targets barriers to environmentally friendly practices by encouraging and enabling diffusion of knowledge related to sustainable products and processes. The knowledge scope for environmental solutions is analyzed, with the objective to develop common categories, and to understand better the increasing complexities and knowledge needs as enterprises engage in sustainable production. In discussing the knowledge economy and knowledge networks, the thesis focuses mostly on the business enterprise. But the development of the knowledge age has much larger implications, such as 'knowledge for whom?' and 'value for whom?'. The information technologies and networks offer new ways for people and groups to interact and influence social issues and can enable the diffusion of wide variety of views and perspectives. Thinking about the information and knowledge age in the larger economic and social context requires us to consider who builds, controls, influences and benefits from the technology and its use. Before we can reasonably approach this analysis, a basic conceptual framework or understanding of knowledge sharing, knowledge networks, and value of knowledge is called for. This thesis is a building block for such a framework, a contribution to future research into the economic and social implications of the knowledge economy."
}, {
    "id": "oai:dspace.mit.edu:1721.1/84713",
    "title": "Japan's delayed antinuclear power mobilization after 3.11",
    "abstract": "The meltdown of Fukushima Daiichi's nuclear plant was one of three disasters that rocked Japan on 11 March 2011, a day often referred to as \"3.11.\" This nuclear accident led to increased attention to and disapproval of nuclear power among the Japanese public. However, despite widespread antinuclear sentiment, the public did not mobilize into sustained mass protests until June 2012. Using historical and contemporary comparisons, this thesis shows that Japan's 15- month delay in antinuclear mobilization was unusual. Both the 1979 Three Mile Island and the 1986 Chernobyl accidents had been quickly followed by mobilized protests. Moreover, the 3.11 Fukushima meltdown prompted mass protests in Germany almost immediately. Given these patterns, one would expect to have seen the Japanese mobilize earlier. The question that drives this thesis is: What led to Japan's 15-month delay in antinuclear power mobilization? Using social movement theory, I test to see whether low levels of grievance, limited availability of resources, or the lack of effective mobilizing structure and strategy help to explain this delay. Of the three explanations, I find the mobilization structure and strategy explanation to be best supported. Due to a history of overlooking antinuclear power issues in Japanese civil society, the early post-3.11 movement lacked longstanding true believers and activists, the two types of participants most effective at mobilizing. Furthermore, the use of social media platforms to organize the early protests may have contributed to why sustained protests were delayed. The empirical findings from this thesis allow us to examine more closely the devastation resulting from 3.11's nuclear meltdown and assess the strengths and weaknesses in Japanese civil society after the disaster. On a theoretical level, these findings may encourage us to question the relevance of grievance to mobilization, refine how resource availability is measured, and ask if the growing use of social media and other online tools should change the way we study social movement mobilization.",
    "advisors": ["Richard J. Samuels"],
    "text": "Japan's delayed antinuclear power mobilization after 3.11 The meltdown of Fukushima Daiichi's nuclear plant was one of three disasters that rocked Japan on 11 March 2011, a day often referred to as \"3.11.\" This nuclear accident led to increased attention to and disapproval of nuclear power among the Japanese public. However, despite widespread antinuclear sentiment, the public did not mobilize into sustained mass protests until June 2012. Using historical and contemporary comparisons, this thesis shows that Japan's 15- month delay in antinuclear mobilization was unusual. Both the 1979 Three Mile Island and the 1986 Chernobyl accidents had been quickly followed by mobilized protests. Moreover, the 3.11 Fukushima meltdown prompted mass protests in Germany almost immediately. Given these patterns, one would expect to have seen the Japanese mobilize earlier. The question that drives this thesis is: What led to Japan's 15-month delay in antinuclear power mobilization? Using social movement theory, I test to see whether low levels of grievance, limited availability of resources, or the lack of effective mobilizing structure and strategy help to explain this delay. Of the three explanations, I find the mobilization structure and strategy explanation to be best supported. Due to a history of overlooking antinuclear power issues in Japanese civil society, the early post-3.11 movement lacked longstanding true believers and activists, the two types of participants most effective at mobilizing. Furthermore, the use of social media platforms to organize the early protests may have contributed to why sustained protests were delayed. The empirical findings from this thesis allow us to examine more closely the devastation resulting from 3.11's nuclear meltdown and assess the strengths and weaknesses in Japanese civil society after the disaster. On a theoretical level, these findings may encourage us to question the relevance of grievance to mobilization, refine how resource availability is measured, and ask if the growing use of social media and other online tools should change the way we study social movement mobilization."
}, {
    "id": "oai:dspace.mit.edu:1721.1/9314",
    "title": "Sustaining agricultural growth in China : a case for land privatization?",
    "abstract": "With the decollectivization of agriculture based on a system of private incentives, China's agricultural output increased significantly. After the successful implementation of the Household Responsibility System (HRS) in agrarian China after the 1978 economic reforms, agricultural productivity finally surpassed the levels in the early 1950s. This led to significant increases in Chinese peasants' real incomes but agricultural productivity began to decline after 1984 for a myriad of reasons. This thesis seeks to do two things. Firstly, it intends to account for the increases in agricultural output during the post-78 period and to explain the causes of agricultural stagnation after 1984. Secondly, having investigated the causes of agricultural growth and decline after the introduction of the HRS, I will deliberate the case for and against land privatization as a policy prescription to sustain agricultural growth in China.",
    "advisors": ["Zinyuan Cui"],
    "text": "Sustaining agricultural growth in China : a case for land privatization? With the decollectivization of agriculture based on a system of private incentives, China's agricultural output increased significantly. After the successful implementation of the Household Responsibility System (HRS) in agrarian China after the 1978 economic reforms, agricultural productivity finally surpassed the levels in the early 1950s. This led to significant increases in Chinese peasants' real incomes but agricultural productivity began to decline after 1984 for a myriad of reasons. This thesis seeks to do two things. Firstly, it intends to account for the increases in agricultural output during the post-78 period and to explain the causes of agricultural stagnation after 1984. Secondly, having investigated the causes of agricultural growth and decline after the introduction of the HRS, I will deliberate the case for and against land privatization as a policy prescription to sustain agricultural growth in China."
}, {
    "id": "oai:dspace.mit.edu:1721.1/35545",
    "title": "Ethnic fractionalization and Sub-Saharan violence, 1970-1996.",
    "abstract": "This study examines the statistical correlations between metrics of ethnic fractionalization and categories of violence in Sub-Saharan Africa from 1970 to 1995. By examining these correlations both prior to and after controlling for income, the study is able to determine whether or not various types of conflict are linked to patterns of ethnic grouping. The study uses newer, more refined measures to evaluate the correlations between specific categories of violence and specific measures of ethnic fractionalization. Using simple and multivariate linear regressions, the study examines each of the correlations between a total of twenty-two sub-metrics of four categories of violence, per capita income and metrics of ethnic fractionalization on three tiers. This allows the study to gauge the impacts (both separately and in interaction) of dichotomous top-tier cleavages in deeply divided societies, general ethnic fractionalization and nested ethnic sub-grouping. The study finds that the majority of the categories of violence used are not correlated with ethnic fractionalization, neither prior to nor after controlling for income.",
    "advisors": ["Roger D. Peterson"],
    "text": "Ethnic fractionalization and Sub-Saharan violence, 1970-1996. This study examines the statistical correlations between metrics of ethnic fractionalization and categories of violence in Sub-Saharan Africa from 1970 to 1995. By examining these correlations both prior to and after controlling for income, the study is able to determine whether or not various types of conflict are linked to patterns of ethnic grouping. The study uses newer, more refined measures to evaluate the correlations between specific categories of violence and specific measures of ethnic fractionalization. Using simple and multivariate linear regressions, the study examines each of the correlations between a total of twenty-two sub-metrics of four categories of violence, per capita income and metrics of ethnic fractionalization on three tiers. This allows the study to gauge the impacts (both separately and in interaction) of dichotomous top-tier cleavages in deeply divided societies, general ethnic fractionalization and nested ethnic sub-grouping. The study finds that the majority of the categories of violence used are not correlated with ethnic fractionalization, neither prior to nor after controlling for income."
}, {
    "id": "oai:dspace.mit.edu:1721.1/84848",
    "title": "Broadcast news and abortion : the effects of conservative narratives on the reproductive health debate",
    "abstract": "How have changes in the elite discussion of reproductive health narratives affected the debate on abortion and influenced state legislation and popular opinion? Using analysis of broadcast transcripts from CNN and FOX News, I examine the arguments articulated by politicians, activists, and members of the media on issues concerning reproductive health. I argue that, beginning in 1996, conservatives used the venue provided by broadcast media to seize on changes to the political climate and frame debate to their advantage. Continually, conservatives forced liberals into reactionary positions through discussion of \"partial-birth abortion,\" expansion of narratives, and-most recently-misinformation. By dictating the terms of the discussion, conservatives lessened the impact of liberal narratives and saw gains in state legislation and public opinion as a result.",
    "advisors": ["Melissa Nobles"],
    "text": "Broadcast news and abortion : the effects of conservative narratives on the reproductive health debate How have changes in the elite discussion of reproductive health narratives affected the debate on abortion and influenced state legislation and popular opinion? Using analysis of broadcast transcripts from CNN and FOX News, I examine the arguments articulated by politicians, activists, and members of the media on issues concerning reproductive health. I argue that, beginning in 1996, conservatives used the venue provided by broadcast media to seize on changes to the political climate and frame debate to their advantage. Continually, conservatives forced liberals into reactionary positions through discussion of \"partial-birth abortion,\" expansion of narratives, and-most recently-misinformation. By dictating the terms of the discussion, conservatives lessened the impact of liberal narratives and saw gains in state legislation and public opinion as a result."
}, {
    "id": "oai:dspace.mit.edu:1721.1/88384",
    "title": "The regulation of irregular work in Japan : from collusion to conflict",
    "abstract": "Japan's labor markets are clearly segmented between regular and irregular workers. Regular workers enjoy employment stability, good wages and promotion, and access to good pensions and health plans. Irregular workers-contract, dispatch and part time workers-can be fired easily, are paid less, and don't have access to fringe benefits. In Japan irregular work contracts have been progressively liberalized since the 1980s, and the share of irregular employment over the same time period has more than doubled to over one third of all workers. However, there are important cases of re-regulation. How can we account for Japan's specific policy path in regard to irregular work contracts? A good explanation ought to shed light on the politics of similar labor market phenomena across the affluent democracies. In this project I argue the policy process by which labor policies are decided substantially impacts whether or not irregular work contracts are liberalized or re-regulated. When labor unions and employer associations bargain over policy in consensus-based deliberative councils housed in the labor ministry the resultant policies are very unlikely to be favorable to irregular workers, though they are likely to be favorable to regular workers. This is the way most policies were decided until the 1990s. In contrast, when labor policies are processed through parliamentary politics the content of policy is shaped by electoral competition between the parties of the right and left. Irregular work contracts receive favorable policies only when there is an electorally credible party of the left. When there is not a credible leftist party both regular and irregular work contracts are liberalized. Political competition rather than formal inclusion of labor representatives most often results in favorable policies for irregular workers.",
    "advisors": ["Kathleen Thelen"],
    "text": "The regulation of irregular work in Japan : from collusion to conflict Japan's labor markets are clearly segmented between regular and irregular workers. Regular workers enjoy employment stability, good wages and promotion, and access to good pensions and health plans. Irregular workers-contract, dispatch and part time workers-can be fired easily, are paid less, and don't have access to fringe benefits. In Japan irregular work contracts have been progressively liberalized since the 1980s, and the share of irregular employment over the same time period has more than doubled to over one third of all workers. However, there are important cases of re-regulation. How can we account for Japan's specific policy path in regard to irregular work contracts? A good explanation ought to shed light on the politics of similar labor market phenomena across the affluent democracies. In this project I argue the policy process by which labor policies are decided substantially impacts whether or not irregular work contracts are liberalized or re-regulated. When labor unions and employer associations bargain over policy in consensus-based deliberative councils housed in the labor ministry the resultant policies are very unlikely to be favorable to irregular workers, though they are likely to be favorable to regular workers. This is the way most policies were decided until the 1990s. In contrast, when labor policies are processed through parliamentary politics the content of policy is shaped by electoral competition between the parties of the right and left. Irregular work contracts receive favorable policies only when there is an electorally credible party of the left. When there is not a credible leftist party both regular and irregular work contracts are liberalized. Political competition rather than formal inclusion of labor representatives most often results in favorable policies for irregular workers."
}, {
    "id": "oai:dspace.mit.edu:1721.1/28756",
    "title": "A game-theoretic analysis of electronic warfare tactics with applications to the World War II era",
    "abstract": "This thesis considers electronic countermeasures as well thought-out signals sent by the \"attacker\" to a recipient, the \"defender\" in order to create uncertainty, and argues that tactics that incorporate the judicious use of bluffing further such uncertainty. I discuss two forms of bluffing, bluffing to create uncertainty as to the location of an attack (bluffing in space), and bluffing to create uncertainty as to the time of attack (bluffing in time). Two electronic warfare tactics used by the Allied air forces during World War II, representing an example of each, are modeled as dynamic zero-sum games with incomplete information. I show that in most instances, Perfect Bayesian Nash Equilibria dictate that the defender delay cuing his interceptors longer than he would so otherwise, and that in those situations where he should cue his interceptors, he must do so at random. Furthermore, except where the cost to bluff is prohibitive, the attacker always benefits from the use of tactics that incorporate bluffing, though bluffing in space is generally more effective than bluffing in time for a given set of detection probabilities.",
    "advisors": ["James M Snyder"],
    "text": "A game-theoretic analysis of electronic warfare tactics with applications to the World War II era This thesis considers electronic countermeasures as well thought-out signals sent by the \"attacker\" to a recipient, the \"defender\" in order to create uncertainty, and argues that tactics that incorporate the judicious use of bluffing further such uncertainty. I discuss two forms of bluffing, bluffing to create uncertainty as to the location of an attack (bluffing in space), and bluffing to create uncertainty as to the time of attack (bluffing in time). Two electronic warfare tactics used by the Allied air forces during World War II, representing an example of each, are modeled as dynamic zero-sum games with incomplete information. I show that in most instances, Perfect Bayesian Nash Equilibria dictate that the defender delay cuing his interceptors longer than he would so otherwise, and that in those situations where he should cue his interceptors, he must do so at random. Furthermore, except where the cost to bluff is prohibitive, the attacker always benefits from the use of tactics that incorporate bluffing, though bluffing in space is generally more effective than bluffing in time for a given set of detection probabilities."
}, {
    "id": "oai:dspace.mit.edu:1721.1/64619",
    "title": "Why some airport-rail links get built and others do not : the role of institutions, equity and financing",
    "abstract": "The thesis seeks to provide an understanding of reasons for different outcomes of airport ground access projects. Five in-depth case studies (Hongkong, Tokyo-Narita, London- Heathrow, Chicago- O'Hare and Paris-Charles de Gaulle) and eight smaller case studies (Kuala Lumpur, Seoul, Shanghai-Pudong, Bangkok, Beijing, Rome- Fiumicino, Istanbul-Atatirk and Munich- Franz Josef Strauss) are conducted. The thesis builds on existing literature that compares airport-rail links by explicitly considering the influence of the institutional environment of an airport on its ground access situation and by paying special attention to recently opened dedicated airport expresses in Asia. It is found that sustained government support and a sense of urgency for better airport access are the main motivating forces that need to be present if a dedicated airport express is to be constructed. For these reasons a number of dedicated airport express systems were constructed in Asia (Hong Kong, Tokyo, Kuala Lumpur, Seoul, Shanghai, Bangkok), where they were conceived simultaneously with the airports they serve. In cases with less focused objectives (Chicago, Paris, Chicago) lengthy planning periods have not yet led to the construction of an airport-rail link. London was the first airport-rail link in the Western world and exhibited strong government support for rail investments during a period of generally favorable conditions, which jointly led to the construction of the Heathrow Express. Five of eight dedicated systems that are studied exhibit underestimation of ridership and underestimation of delivery time. The finding replicates for Asian examples (Hong Kong, Seoul, Bangkok, Shanghai) Flyvbjerg's (2009) observations on UK and US examples that transportation projects tend to systematically overestimate project benefits. The enduring and systematic overestimates of ridership hint at deliberate strategic misrepresentation rather than psychological optimism bias or technical error as reason for the erroneous estimates. Planners are advised to be aware of incentives for strategic misrepresentation among public and private agencies that prepare technical studies as basis for decision making. In a number of systems that have dedicated rail service to an airport and are generally considered successful, fierce competition from buses has emerged recently (Hong Kong, Tokyo, London). It is recommended to planners of airport-rail links today to consider realization through bus rapid transit on dedicated rights-of-way in addition to airport-rail links because of their lower cost, wider scope in dropoff and pick-up destinations and easier scalability of capacity in times of low demand.",
    "advisors": ["Kenneth Oye"],
    "text": "Why some airport-rail links get built and others do not : the role of institutions, equity and financing The thesis seeks to provide an understanding of reasons for different outcomes of airport ground access projects. Five in-depth case studies (Hongkong, Tokyo-Narita, London- Heathrow, Chicago- O'Hare and Paris-Charles de Gaulle) and eight smaller case studies (Kuala Lumpur, Seoul, Shanghai-Pudong, Bangkok, Beijing, Rome- Fiumicino, Istanbul-Atatirk and Munich- Franz Josef Strauss) are conducted. The thesis builds on existing literature that compares airport-rail links by explicitly considering the influence of the institutional environment of an airport on its ground access situation and by paying special attention to recently opened dedicated airport expresses in Asia. It is found that sustained government support and a sense of urgency for better airport access are the main motivating forces that need to be present if a dedicated airport express is to be constructed. For these reasons a number of dedicated airport express systems were constructed in Asia (Hong Kong, Tokyo, Kuala Lumpur, Seoul, Shanghai, Bangkok), where they were conceived simultaneously with the airports they serve. In cases with less focused objectives (Chicago, Paris, Chicago) lengthy planning periods have not yet led to the construction of an airport-rail link. London was the first airport-rail link in the Western world and exhibited strong government support for rail investments during a period of generally favorable conditions, which jointly led to the construction of the Heathrow Express. Five of eight dedicated systems that are studied exhibit underestimation of ridership and underestimation of delivery time. The finding replicates for Asian examples (Hong Kong, Seoul, Bangkok, Shanghai) Flyvbjerg's (2009) observations on UK and US examples that transportation projects tend to systematically overestimate project benefits. The enduring and systematic overestimates of ridership hint at deliberate strategic misrepresentation rather than psychological optimism bias or technical error as reason for the erroneous estimates. Planners are advised to be aware of incentives for strategic misrepresentation among public and private agencies that prepare technical studies as basis for decision making. In a number of systems that have dedicated rail service to an airport and are generally considered successful, fierce competition from buses has emerged recently (Hong Kong, Tokyo, London). It is recommended to planners of airport-rail links today to consider realization through bus rapid transit on dedicated rights-of-way in addition to airport-rail links because of their lower cost, wider scope in dropoff and pick-up destinations and easier scalability of capacity in times of low demand."
}, {
    "id": "oai:dspace.mit.edu:1721.1/8259",
    "title": "Legions or legends : assessing U.S. Army and Marine effectiveness in the Korean War, 1950-1951",
    "abstract": "This study compares the military effectiveness of the United States Army and United States Marine Corps during the first 10 months of the Korean War. Representative battles selected from the Pusan Perimeter, the Liberation of Seoul, and the Retreat from the Yalu are analyzed using a process-tracing methodology to identify variations in performance between the two services and to determine the source of these differences when they exist. Predictions drawn from functional and cultural theories are employed to determine which theory provides the best explanation for variations in battlefield performance. Based on this historical analysis, there is little evidence to support general claims of superior Marine Corps effectiveness. When operating under similar conditions, the military effectiveness of both organizations was roughly the same. Those variations in battlefield performance that did exist were largely the result of idiosyncratic geographic conditions combined with physical advantages gained through superior weaponry and organic close air support. Differences in organizational culture had marginal impact. Popular perceptions of Marine Corps achievements based on combat during this period resulted from an organizational strategy that emphasized battlefield exploits as part of a conscious effort to maintain a positive public image.",
    "advisors": ["Barry R. Posen"],
    "text": "Legions or legends : assessing U.S. Army and Marine effectiveness in the Korean War, 1950-1951 This study compares the military effectiveness of the United States Army and United States Marine Corps during the first 10 months of the Korean War. Representative battles selected from the Pusan Perimeter, the Liberation of Seoul, and the Retreat from the Yalu are analyzed using a process-tracing methodology to identify variations in performance between the two services and to determine the source of these differences when they exist. Predictions drawn from functional and cultural theories are employed to determine which theory provides the best explanation for variations in battlefield performance. Based on this historical analysis, there is little evidence to support general claims of superior Marine Corps effectiveness. When operating under similar conditions, the military effectiveness of both organizations was roughly the same. Those variations in battlefield performance that did exist were largely the result of idiosyncratic geographic conditions combined with physical advantages gained through superior weaponry and organic close air support. Differences in organizational culture had marginal impact. Popular perceptions of Marine Corps achievements based on combat during this period resulted from an organizational strategy that emphasized battlefield exploits as part of a conscious effort to maintain a positive public image."
}, {
    "id": "oai:dspace.mit.edu:1721.1/62471",
    "title": "Estimating the effects of foreign bribery legislation in the international economy",
    "abstract": "Foreign bribery - the payment of bribes across borders - poses a classic collective action problem in theory. A firm may extract benefits through the payment of bribes to foreign public officials without its own country bearing the associated costs of governmental corruption, and hence while eliminating foreign bribery may be in the best interests of all who are engaged with the global economy, there are few obvious incentives for any one national government to be the first to take action. Over the last two decades, however, an unprecedented degree of multilateral cooperation on the issue of foreign bribery has been achieved. In particular, the Organization for Economic Cooperation and Development (OECD) has been a key institutional locus of activity, serving as the coordinating body for the monitoring and enforcement of a comprehensive anti-bribery convention that was adopted in 1997. This convention appears to have been largely successful at least in terms of spurring legislative change: all OECD member countries as well as several nonmember nations have since adopted laws that explicitly criminalize the act of bribing foreign public officials, and the capacity of the state to monitor, detect, and prosecute the offense of foreign bribery has ostensibly been enhanced. Given the potential for collective action problems to develop, it is thus important to ask whether the legislative action that has been taken thus far is meaningful in any measurable sense. I answer this question by constructing an original measure of the strictness of foreign bribery legislation, which I then employ as the main independent variable in an empirical study of export data, utilizing both difference-in-difference estimators and regression analysis. The results of my analysis provide support for the hypothesis that the enactment of stricter foreign bribery legislation amongst the countries party to the OECD convention has reduced exports to more corrupt countries more so than it has exports to less corrupt countries. These findings are robust to a variety of sensitivity tests, and I thus conclude that the OECD's multilateral anti-bribery initiatives have indeed had a meaningful impact on business decisions in the international economy.",
    "advisors": ["Gabriel Lenz"],
    "text": "Estimating the effects of foreign bribery legislation in the international economy Foreign bribery - the payment of bribes across borders - poses a classic collective action problem in theory. A firm may extract benefits through the payment of bribes to foreign public officials without its own country bearing the associated costs of governmental corruption, and hence while eliminating foreign bribery may be in the best interests of all who are engaged with the global economy, there are few obvious incentives for any one national government to be the first to take action. Over the last two decades, however, an unprecedented degree of multilateral cooperation on the issue of foreign bribery has been achieved. In particular, the Organization for Economic Cooperation and Development (OECD) has been a key institutional locus of activity, serving as the coordinating body for the monitoring and enforcement of a comprehensive anti-bribery convention that was adopted in 1997. This convention appears to have been largely successful at least in terms of spurring legislative change: all OECD member countries as well as several nonmember nations have since adopted laws that explicitly criminalize the act of bribing foreign public officials, and the capacity of the state to monitor, detect, and prosecute the offense of foreign bribery has ostensibly been enhanced. Given the potential for collective action problems to develop, it is thus important to ask whether the legislative action that has been taken thus far is meaningful in any measurable sense. I answer this question by constructing an original measure of the strictness of foreign bribery legislation, which I then employ as the main independent variable in an empirical study of export data, utilizing both difference-in-difference estimators and regression analysis. The results of my analysis provide support for the hypothesis that the enactment of stricter foreign bribery legislation amongst the countries party to the OECD convention has reduced exports to more corrupt countries more so than it has exports to less corrupt countries. These findings are robust to a variety of sensitivity tests, and I thus conclude that the OECD's multilateral anti-bribery initiatives have indeed had a meaningful impact on business decisions in the international economy."
}, {
    "id": "oai:dspace.mit.edu:1721.1/43190",
    "title": "Clear interests and clouded future : force structure and strategy options for the People's Liberation Army Navy (PLAN)",
    "abstract": "As China's participation in the global economy continues to expand, its increasing reliance on imported resources and overseas trade has exerted pressure on China to safeguard its growing maritime economic and political interests. Although Chinese national interests are well understood, there is no clear consensus regarding the long-term orientation or intended goals of China's modernizing military. By examining how sea power theory, maritime interests, economic and political constraints, and military/naval doctrine may influence China's naval force structure and maritime strategy, the author seeks to answer whether it is possible to deduce the most probable future roles of the People's Liberation Army Navy (PLAN). This paper suggest the answer is yes and identifies three force structure and maritime strategy models that the PLAN may utilize to support China's expanding global and maritime interests. The author posits that the PLAN will continue to make quantitative and qualitative improvements, but due primarily to fiscal and technological constraints, China will not directly challenge the United States by matching its extensive multiple mission naval force structure. The PLAN is still in a nascent stage of development and already has many of the pieces in place to proceed toward each model presented, but China must make choices that require it to develop the PLAN in conjunction with specific and elaborated maritime strategies or risk being a \"jack-of-all-trades and a master of none.\" The PLAN will therefore develop either as 1) a \"Unification\" Navy, maximizing coercive pressure on Taiwan by focusing on regional anti-access strategies; 2) an \"Influence Projection\" Navy, capable of a wide range of operations, but not with a capability nearing a U.S. carrier strike group; or 3) a \"Global/Hemispheric Sea Denial\"",
    "advisors": ["M. Taylor Fravel"],
    "text": "Clear interests and clouded future : force structure and strategy options for the People's Liberation Army Navy (PLAN) As China's participation in the global economy continues to expand, its increasing reliance on imported resources and overseas trade has exerted pressure on China to safeguard its growing maritime economic and political interests. Although Chinese national interests are well understood, there is no clear consensus regarding the long-term orientation or intended goals of China's modernizing military. By examining how sea power theory, maritime interests, economic and political constraints, and military/naval doctrine may influence China's naval force structure and maritime strategy, the author seeks to answer whether it is possible to deduce the most probable future roles of the People's Liberation Army Navy (PLAN). This paper suggest the answer is yes and identifies three force structure and maritime strategy models that the PLAN may utilize to support China's expanding global and maritime interests. The author posits that the PLAN will continue to make quantitative and qualitative improvements, but due primarily to fiscal and technological constraints, China will not directly challenge the United States by matching its extensive multiple mission naval force structure. The PLAN is still in a nascent stage of development and already has many of the pieces in place to proceed toward each model presented, but China must make choices that require it to develop the PLAN in conjunction with specific and elaborated maritime strategies or risk being a \"jack-of-all-trades and a master of none.\" The PLAN will therefore develop either as 1) a \"Unification\" Navy, maximizing coercive pressure on Taiwan by focusing on regional anti-access strategies; 2) an \"Influence Projection\" Navy, capable of a wide range of operations, but not with a capability nearing a U.S. carrier strike group; or 3) a \"Global/Hemispheric Sea Denial\""
}, {
    "id": "oai:dspace.mit.edu:1721.1/54604",
    "title": "Political violence in the American South: 1882-1890",
    "abstract": "The racial status quo in the American South persisted through an unspoken detente between the federal government and the Southern state governments during the second half of the 19 th century. The political disenfranchisement of blacks took place in distinct stages following Reconstruction. In the 1880s, Jim Crowe had not yet been enacted but Reconstruction was over. Blacks were technically allowed to vote, but turnout was around five percent at any given election. The prevailing historical theory is that the threat of violence was a form of de facto disenfranchisement that prevented blacks from turning out to vote. Both historians and political scientists assume lynching to be the method through which the white population of the American South prevented political and social equality. Lynching is a form of ethnic violence, but there has not yet been a rigorous methodological examination of it as a potential form of political violence. In the following thesis I will examine the claims regarding the use of political violence within lynching in the southern United States. Under what circumstances would political violence be used or not be used in equilibrium? I begin with the assumption that lynching increases due to an impending election. Violence would be a function of the temporal proximity of a certain election. I will examine this claim using the dates of lynching and elections from 1880 to 1890. The second analysis of the paper examines whether or not political violence is due to factional politics. Violence would then be a function of the margin of the Republican or Democrat victory. The temptation to engage in political violence to manipulate election outcomes increases as the election draws closer. In this analysis, we examine the violence leading up to the election date with controls, including fixed effects (by state and county), census data and clustered standard error.",
    "advisors": ["James Snyder"],
    "text": "Political violence in the American South: 1882-1890 The racial status quo in the American South persisted through an unspoken detente between the federal government and the Southern state governments during the second half of the 19 th century. The political disenfranchisement of blacks took place in distinct stages following Reconstruction. In the 1880s, Jim Crowe had not yet been enacted but Reconstruction was over. Blacks were technically allowed to vote, but turnout was around five percent at any given election. The prevailing historical theory is that the threat of violence was a form of de facto disenfranchisement that prevented blacks from turning out to vote. Both historians and political scientists assume lynching to be the method through which the white population of the American South prevented political and social equality. Lynching is a form of ethnic violence, but there has not yet been a rigorous methodological examination of it as a potential form of political violence. In the following thesis I will examine the claims regarding the use of political violence within lynching in the southern United States. Under what circumstances would political violence be used or not be used in equilibrium? I begin with the assumption that lynching increases due to an impending election. Violence would be a function of the temporal proximity of a certain election. I will examine this claim using the dates of lynching and elections from 1880 to 1890. The second analysis of the paper examines whether or not political violence is due to factional politics. Violence would then be a function of the margin of the Republican or Democrat victory. The temptation to engage in political violence to manipulate election outcomes increases as the election draws closer. In this analysis, we examine the violence leading up to the election date with controls, including fixed effects (by state and county), census data and clustered standard error."
}, {
    "id": "oai:dspace.mit.edu:1721.1/95548",
    "title": "What do abortion policies accomplish? : understanding how abortion laws and court cases affect public opinion",
    "abstract": "Abortion is a loaded, controversial, and divisive sociocultural and political term, concept, and debate. Yet little empirical research has been conducted to examine what effects abortion rights legislation and court cases have had on the public and our society. After analyzing a broad overview of the history of the abortion rights debate in the US, I conduct bivariate and multivariate regression analyses from 1972-2004 using NES and personally-collected data to see how these laws and court opinions in various states at the individual level influence public opinion of abortion rights and of the government. In the end, I conclude that, of the possible iterated relationships therefrom, anti-choice policies have statistically significant impacts on both how people view abortion rights and their own state governments. In doing so, I challenge extant models that describe the interaction between public opinion and policy. I also further develop the idea of Policy Overreach, where policymakers go \"too far\"-at least, in the eyes of the public-in setting anti-choice policies, causing the public to retaliate in various ways. Not only does this thesis answer some important questions, but also introduces new measures, concepts, questions, and data for future research into this important area of study.",
    "advisors": ["Charles Stewart, III"],
    "text": "What do abortion policies accomplish? : understanding how abortion laws and court cases affect public opinion Abortion is a loaded, controversial, and divisive sociocultural and political term, concept, and debate. Yet little empirical research has been conducted to examine what effects abortion rights legislation and court cases have had on the public and our society. After analyzing a broad overview of the history of the abortion rights debate in the US, I conduct bivariate and multivariate regression analyses from 1972-2004 using NES and personally-collected data to see how these laws and court opinions in various states at the individual level influence public opinion of abortion rights and of the government. In the end, I conclude that, of the possible iterated relationships therefrom, anti-choice policies have statistically significant impacts on both how people view abortion rights and their own state governments. In doing so, I challenge extant models that describe the interaction between public opinion and policy. I also further develop the idea of Policy Overreach, where policymakers go \"too far\"-at least, in the eyes of the public-in setting anti-choice policies, causing the public to retaliate in various ways. Not only does this thesis answer some important questions, but also introduces new measures, concepts, questions, and data for future research into this important area of study."
}, {
    "id": "oai:dspace.mit.edu:1721.1/30267",
    "title": "Using institutions to moderate separatist tendencies : a focus on Iraqi Kurdistan",
    "abstract": "This thesis offers an alternate theory to the problem of secessionism by integrating two separate fields of research: nationalism and constitutional engineering. In particular, I apply two prominent theories of nationalism, those of Ernest Gellner and Benedict Anderson, to the problem of constitutional engineering. The theories developed by Gellner and Anderson have furthered our understanding of the historical and evolutionary processes of nationhood and nationalism. I argue that the insight offered by these theories can therefore better guide policy makers, scholars, and constitutional engineers in the design of political institutions for deeply-divided societies. The engineering of institutions has the capacity to contain separatist conflict by striking at what I argue are the two necessary cause of secessionism - desire and ability. In my thesis I focus on one case study in particular, that of Iraqi Kurdistan. I argue that a secessionist movement by Iraqi Kurds can best be thwarted by applying a two-pronged strategy: First, devolve enough power to the Kurds so that their rights as a minority group are protected and their desire for self-rule is fulfilled. Likewise, ensure the representation of Kurds in all levels of government. Second, encourage diversity within the Kurdish political arena. Both components of this strategy can be achieved by adopting a proportional representation electoral formula, selecting a territorially-based federalism, and choosing a parliamentary system.",
    "advisors": ["Roger D. Petersen"],
    "text": "Using institutions to moderate separatist tendencies : a focus on Iraqi Kurdistan This thesis offers an alternate theory to the problem of secessionism by integrating two separate fields of research: nationalism and constitutional engineering. In particular, I apply two prominent theories of nationalism, those of Ernest Gellner and Benedict Anderson, to the problem of constitutional engineering. The theories developed by Gellner and Anderson have furthered our understanding of the historical and evolutionary processes of nationhood and nationalism. I argue that the insight offered by these theories can therefore better guide policy makers, scholars, and constitutional engineers in the design of political institutions for deeply-divided societies. The engineering of institutions has the capacity to contain separatist conflict by striking at what I argue are the two necessary cause of secessionism - desire and ability. In my thesis I focus on one case study in particular, that of Iraqi Kurdistan. I argue that a secessionist movement by Iraqi Kurds can best be thwarted by applying a two-pronged strategy: First, devolve enough power to the Kurds so that their rights as a minority group are protected and their desire for self-rule is fulfilled. Likewise, ensure the representation of Kurds in all levels of government. Second, encourage diversity within the Kurdish political arena. Both components of this strategy can be achieved by adopting a proportional representation electoral formula, selecting a territorially-based federalism, and choosing a parliamentary system."
}, {
    "id": "oai:dspace.mit.edu:1721.1/68943",
    "title": "Assessing guerrilla doctrine: battlefield lessons on network structure and multi-front insurgency",
    "abstract": "This thesis will assess influential guerrilla doctrine advising optimal insurgent (1) operational environment, (2) organizational structure, (3) logistical capacity, and (4) dependent tactics - items of consistent, yet incomplete analysis in counter-insurgency (COIN) analytics and civil war scholarship. Following a review of five of the most infamously disseminated guerrilla doctrine and training manuals, this thesis derives a model of advised guerrilla war production. The derived model illustrating the strategic and operational principles of reviewed doctrine presents an organizational hybrid between guerrilla hierarchies and networks, positing optimal organizational structure as a function of operational environment. Logistical capacity, tactics, target set, and military classification are in turn best determined by organizational structure. An otherwise unobserved operational dynamic relating each of the above four enumerated strategic considerations is thereby illustrated, with implications for COIN theory and praxis. To present a more systematic evaluation of reviewed guerrilla doctrine, the derived model is evaluated against a qualitative case-study of Peru's Shining Path insurgency. Case-study empirics are drawn from the Uppsala/PRIO Armed Conflicts Dataset (ACD) on dyadic conflict (Cederman, Min, Wimmer 2010), Arc Geographic Information Systems (GIS) spatial analysis software, declassified Peruvian intelligence reports, and captured Shining Path documents and directives from the Peruvian insurrection between 1969 - 1989. This study finds that evaluated guerrilla doctrine accurately reflects the material conditions of insurgency. There is one caveat: The global trends of increasing urbanization, dissolution of state power, and decline in politically motivated insurgency, may contribute to an observed shift in non-state actor war production from rural to increasingly urban and civilian-centric economies of violence. Lessons on strategically important considerations like sanctuary, terrain, state capacity, network evolution, and weapons proliferation, attend.",
    "advisors": ["Fotini Christia"],
    "text": "Assessing guerrilla doctrine: battlefield lessons on network structure and multi-front insurgency This thesis will assess influential guerrilla doctrine advising optimal insurgent (1) operational environment, (2) organizational structure, (3) logistical capacity, and (4) dependent tactics - items of consistent, yet incomplete analysis in counter-insurgency (COIN) analytics and civil war scholarship. Following a review of five of the most infamously disseminated guerrilla doctrine and training manuals, this thesis derives a model of advised guerrilla war production. The derived model illustrating the strategic and operational principles of reviewed doctrine presents an organizational hybrid between guerrilla hierarchies and networks, positing optimal organizational structure as a function of operational environment. Logistical capacity, tactics, target set, and military classification are in turn best determined by organizational structure. An otherwise unobserved operational dynamic relating each of the above four enumerated strategic considerations is thereby illustrated, with implications for COIN theory and praxis. To present a more systematic evaluation of reviewed guerrilla doctrine, the derived model is evaluated against a qualitative case-study of Peru's Shining Path insurgency. Case-study empirics are drawn from the Uppsala/PRIO Armed Conflicts Dataset (ACD) on dyadic conflict (Cederman, Min, Wimmer 2010), Arc Geographic Information Systems (GIS) spatial analysis software, declassified Peruvian intelligence reports, and captured Shining Path documents and directives from the Peruvian insurrection between 1969 - 1989. This study finds that evaluated guerrilla doctrine accurately reflects the material conditions of insurgency. There is one caveat: The global trends of increasing urbanization, dissolution of state power, and decline in politically motivated insurgency, may contribute to an observed shift in non-state actor war production from rural to increasingly urban and civilian-centric economies of violence. Lessons on strategically important considerations like sanctuary, terrain, state capacity, network evolution, and weapons proliferation, attend."
}, {
    "id": "oai:dspace.mit.edu:1721.1/41811",
    "title": "Gulbuddin Hekmatyar : Afghanistan's persistent insurgent",
    "abstract": "Afghanistan has proved to be one of the most dangerous and unstable political environments in South Asia, if not the world. Against this backdrop, Islamic fundamentalist Gulbuddin Hekmatyar has waged a continuous guerilla war to gain control of the state. Towards this end, he has fought ambitious rivals, every Afghan government, the Soviet Union, and now US forces sent to drive out al Qaeda and destroy the Taliban. This thesis explores Hekmatyar's persistence in such a fluid and violent political landscape, tracing his islamist Hezb-e Islami party's genesis and evolution in the last 30 years. With a rigid Leninist-styled party, Hekmatyar's ability to draw from religious, ethno-regional, and socioeconomic support, as well as foreign aid, has provided him the means to survive and flourish during Afghanistan's enduring devastation",
    "advisors": ["Harvey M. Sapolsky"],
    "text": "Gulbuddin Hekmatyar : Afghanistan's persistent insurgent Afghanistan has proved to be one of the most dangerous and unstable political environments in South Asia, if not the world. Against this backdrop, Islamic fundamentalist Gulbuddin Hekmatyar has waged a continuous guerilla war to gain control of the state. Towards this end, he has fought ambitious rivals, every Afghan government, the Soviet Union, and now US forces sent to drive out al Qaeda and destroy the Taliban. This thesis explores Hekmatyar's persistence in such a fluid and violent political landscape, tracing his islamist Hezb-e Islami party's genesis and evolution in the last 30 years. With a rigid Leninist-styled party, Hekmatyar's ability to draw from religious, ethno-regional, and socioeconomic support, as well as foreign aid, has provided him the means to survive and flourish during Afghanistan's enduring devastation"
}, {
    "id": "oai:dspace.mit.edu:1721.1/42939",
    "title": "Fighting for frames or prospects for peace? : building a prospect theory model of ethnic civil war termination",
    "abstract": "Ethnic civil wars are the most abundant form of large-scale, deadly conflict in the world today, yet the dedicated study of ethnic civil war is relatively new within political science. One empirical observation repeated in the literature is that civil wars are less likely than interstate wars to end in negotiated settlements, and more likely to end in military victory for one side. Recently, scholars have employed expected utility theory and the security dilemma to construct models of how ethnic combatants choose between settling their differences at the bargaining table or on the battlefield. Rooted in the rational choice paradigm, these models draw upon utility calculations and security concerns to describe ethnic combatants' decision processes and explain the low rate of ethnic war settlement. Two problems with these rational choice models, however, are that they cannot account for cases of ethnic war in which combatants elect to continue fighting losing battles when a normatively \"rational\" settlement is available, and that they do not seem to accurately describe the decision-making behavior of ethnic combatants. In this thesis, I draw upon the principles of prospect theory, a descriptive theory of choice under conditions of risk and uncertainty, to construct a psychological model of ethnic war termination. I argue that ethnic combatants do not behave as rational choice theories suggest; rather, they choose to settle or fight by framing the possible outcomes as gains or losses relative to a subjective reference point. I analyze my theory by comparing it to three prominent rational choice models: Mason and Fett's expected utility theory, Walter's credible commitment theory, and Kaufmann's demographic separation theory. I then perform a first-brush test of my theory by applying all four models to two case studies, the Ethiopian-Eritrean War and the Bosnian Civil War, to determine which model best explains the behavior of the combatants.",
    "advisors": ["Barry Posen"],
    "text": "Fighting for frames or prospects for peace? : building a prospect theory model of ethnic civil war termination Ethnic civil wars are the most abundant form of large-scale, deadly conflict in the world today, yet the dedicated study of ethnic civil war is relatively new within political science. One empirical observation repeated in the literature is that civil wars are less likely than interstate wars to end in negotiated settlements, and more likely to end in military victory for one side. Recently, scholars have employed expected utility theory and the security dilemma to construct models of how ethnic combatants choose between settling their differences at the bargaining table or on the battlefield. Rooted in the rational choice paradigm, these models draw upon utility calculations and security concerns to describe ethnic combatants' decision processes and explain the low rate of ethnic war settlement. Two problems with these rational choice models, however, are that they cannot account for cases of ethnic war in which combatants elect to continue fighting losing battles when a normatively \"rational\" settlement is available, and that they do not seem to accurately describe the decision-making behavior of ethnic combatants. In this thesis, I draw upon the principles of prospect theory, a descriptive theory of choice under conditions of risk and uncertainty, to construct a psychological model of ethnic war termination. I argue that ethnic combatants do not behave as rational choice theories suggest; rather, they choose to settle or fight by framing the possible outcomes as gains or losses relative to a subjective reference point. I analyze my theory by comparing it to three prominent rational choice models: Mason and Fett's expected utility theory, Walter's credible commitment theory, and Kaufmann's demographic separation theory. I then perform a first-brush test of my theory by applying all four models to two case studies, the Ethiopian-Eritrean War and the Bosnian Civil War, to determine which model best explains the behavior of the combatants."
}, {
    "id": "oai:dspace.mit.edu:1721.1/84846",
    "title": "Population strategies to decrease sodium intake : a global cost-effectiveness analysis",
    "abstract": "Excessive sodium consumption is both prevalent and very costly in many countries around the world. Recent research has found that more than 90% of the world's adult population live in countries with mean intakes exceeding the World Health Organization's recommendation, and that more than a million deaths every year may be attributable to excess sodium. This study uses a simulation model to estimate, for the first time, the cost-effectiveness of government interventions to reduce population sodium consumption in every country in the world. It reveals substantial heterogeneity in cost-effectiveness by country that has never before been identified, and illustrates, also for the first time, the sensitivity of intervention efficacy to the theoretical-minimum-risk exposure distribution of sodium intake. The study makes a number of additional contributions. It offers a comprehensive appraisal of the methodological strengths and limitations of the surveys, imputation models, randomized controlled trials, prospective cohort studies, meta-analyses, and simulation models that together constitute the evidence base for public health recommendations on sodium intake, as well as for this study's own analysis. These methodological issues, some raised for the first time, are evaluated systematically to allow the relative quality of each input to be assessed and to inform prioritization of further research. The study also uses economic theory to ground a discussion of the proper nature and scope of government policies targeting population sodium consumption, and presents an up-to-date survey of sodium reduction initiatives around the world.",
    "advisors": ["David Andrew Singer"],
    "text": "Population strategies to decrease sodium intake : a global cost-effectiveness analysis Excessive sodium consumption is both prevalent and very costly in many countries around the world. Recent research has found that more than 90% of the world's adult population live in countries with mean intakes exceeding the World Health Organization's recommendation, and that more than a million deaths every year may be attributable to excess sodium. This study uses a simulation model to estimate, for the first time, the cost-effectiveness of government interventions to reduce population sodium consumption in every country in the world. It reveals substantial heterogeneity in cost-effectiveness by country that has never before been identified, and illustrates, also for the first time, the sensitivity of intervention efficacy to the theoretical-minimum-risk exposure distribution of sodium intake. The study makes a number of additional contributions. It offers a comprehensive appraisal of the methodological strengths and limitations of the surveys, imputation models, randomized controlled trials, prospective cohort studies, meta-analyses, and simulation models that together constitute the evidence base for public health recommendations on sodium intake, as well as for this study's own analysis. These methodological issues, some raised for the first time, are evaluated systematically to allow the relative quality of each input to be assessed and to inform prioritization of further research. The study also uses economic theory to ground a discussion of the proper nature and scope of government policies targeting population sodium consumption, and presents an up-to-date survey of sodium reduction initiatives around the world."
}, {
    "id": "oai:dspace.mit.edu:1721.1/77830",
    "title": "Asymmetry of will : the effect of religious radicalism on state military doctrine",
    "abstract": "How is a state's military doctrine affected by the presence of radical religious ideology in its military? Using analysis of satellite imagery, recent military exercises, and a series of source interviews, I examine the evolution of the Iranian Revolutionary Guard Corps since the 1979 Islamic Revolution. In particular I explore the effect of religious radicalism on Iran's acceptable casualty rates for its naval operations. A successful ideologically based strategy appears to have three necessary components: terrorism as a tool for pursuing political objectives, religious ideology as a generator of potential violence, and a regime which exercises tight control over the military. Combined, these factors allow a military to mobilize a large cadre of troops that are willing to sacrifice themselves in suicide operations. Ideology overcomes conventional acceptable casualty rates for sustained military sorties. Finally, I compare the Iranian case to similar militaries in the Sudan and Yugoslavia to determine how the presence and absence of each factor affects the military's development.",
    "advisors": ["Roger Petersen"],
    "text": "Asymmetry of will : the effect of religious radicalism on state military doctrine How is a state's military doctrine affected by the presence of radical religious ideology in its military? Using analysis of satellite imagery, recent military exercises, and a series of source interviews, I examine the evolution of the Iranian Revolutionary Guard Corps since the 1979 Islamic Revolution. In particular I explore the effect of religious radicalism on Iran's acceptable casualty rates for its naval operations. A successful ideologically based strategy appears to have three necessary components: terrorism as a tool for pursuing political objectives, religious ideology as a generator of potential violence, and a regime which exercises tight control over the military. Combined, these factors allow a military to mobilize a large cadre of troops that are willing to sacrifice themselves in suicide operations. Ideology overcomes conventional acceptable casualty rates for sustained military sorties. Finally, I compare the Iranian case to similar militaries in the Sudan and Yugoslavia to determine how the presence and absence of each factor affects the military's development."
}, {
    "id": "oai:dspace.mit.edu:1721.1/37437",
    "title": "An analysis of the impact of the Bipartisan Campaign Reform Act of 2002 on the congressional committee assignment process",
    "abstract": "With the passage of the 2002 Bipartisan Campaign Reform Act (BCRA), a flurry of research has been conducted on the impact on political parties. However, there exists a gap in the research regarding the impact of the legislation on the role of Members as fundraisers for their parties. What impact did BCRA have on the size and significance of contributions from Members of Congress to party committees and candidates? Furthermore, are Member contributions significant in determining a Member's likelihood of transferring to a committee and is this effect amplified post-BCRA? This thesis provides historical data on the importance of Member contributions from 1996 to 2004 and then turns to determining what, if any, impact financial prowess has on a Member's likelihood to advance upwards in the committee hierarchy. The principal findings of this research are twofold. First, money matters because BCRA cut soft money, therefore political parties have turned to their Members to serve as vital sources of campaign funds. Second, Member contributions do not significantly impact or influence a Member's probability of transferring to a more prestigious committee.",
    "advisors": ["James M. Snyder, Jr"],
    "text": "An analysis of the impact of the Bipartisan Campaign Reform Act of 2002 on the congressional committee assignment process With the passage of the 2002 Bipartisan Campaign Reform Act (BCRA), a flurry of research has been conducted on the impact on political parties. However, there exists a gap in the research regarding the impact of the legislation on the role of Members as fundraisers for their parties. What impact did BCRA have on the size and significance of contributions from Members of Congress to party committees and candidates? Furthermore, are Member contributions significant in determining a Member's likelihood of transferring to a committee and is this effect amplified post-BCRA? This thesis provides historical data on the importance of Member contributions from 1996 to 2004 and then turns to determining what, if any, impact financial prowess has on a Member's likelihood to advance upwards in the committee hierarchy. The principal findings of this research are twofold. First, money matters because BCRA cut soft money, therefore political parties have turned to their Members to serve as vital sources of campaign funds. Second, Member contributions do not significantly impact or influence a Member's probability of transferring to a more prestigious committee."
}, {
    "id": "oai:dspace.mit.edu:1721.1/33687",
    "title": "The weight of an assassin's mace : vulnerabilities in the US military's satellite communications and China's information warfare threat",
    "abstract": "Believing that an information Revolution of Military Affairs has occurred, the US military is currently transforming to achieve dominance over the full spectrum of deployment scenarios with a lighter, more mobile, and more capable force. Establishing a far-reaching, robust, ubiquitous ISR and telecommunications network, and a network-centered fighting doctrine are keys to this endeavor. Of the many systems needed, satellite communications are especially significant because they are the prime method of transmitting high quantities of information to remote and mobile units. The People's Republic of China too has become aware of the information Revolution of Military Affairs, as well as the vulnerabilities associated with it. Though the People's Republic is still in the process of modernizing its society and military, the doctrines and advantages of Information Warfare have not been lost to it. It seeks to equip itself with the IT and skill sets that are becoming increasingly more available to asymmetrically affect the information usage of a technologically superior adversary. As it stands, the military's use of satellite communications is vulnerable.",
    "advisors": ["Taylor Fravel"],
    "text": "The weight of an assassin's mace : vulnerabilities in the US military's satellite communications and China's information warfare threat Believing that an information Revolution of Military Affairs has occurred, the US military is currently transforming to achieve dominance over the full spectrum of deployment scenarios with a lighter, more mobile, and more capable force. Establishing a far-reaching, robust, ubiquitous ISR and telecommunications network, and a network-centered fighting doctrine are keys to this endeavor. Of the many systems needed, satellite communications are especially significant because they are the prime method of transmitting high quantities of information to remote and mobile units. The People's Republic of China too has become aware of the information Revolution of Military Affairs, as well as the vulnerabilities associated with it. Though the People's Republic is still in the process of modernizing its society and military, the doctrines and advantages of Information Warfare have not been lost to it. It seeks to equip itself with the IT and skill sets that are becoming increasingly more available to asymmetrically affect the information usage of a technologically superior adversary. As it stands, the military's use of satellite communications is vulnerable."
}, {
    "id": "oai:dspace.mit.edu:1721.1/62467",
    "title": "Microfinance regulation in China and India",
    "abstract": "The regulatory responses of Governments in different countries to emerging microfinance sectors have varied dramatically and as a result so have the outcomes for these sectors. As two of the fastest growing developing countries in the world over the last two decades, both with vast poor rural populations lacking access to credit, the potential demand for microfinance in India and China is enormous. Yet where the microfinance sector in India has been one of the fastest growing in the world with a diverse range of successful for-profit and non-profit microfinance institutions, the microfinance sector in China has failed to find its feet with microfinance institutions unable to attract commercial funding to expand or to achieve financial self-sufficiency. In this thesis I provide a comparative analysis of the regulatory frameworks for microfinance in China and India in order to demonstrate how the more restrictive and uncertain regulatory environment in China has hindered the development of the sector. In the next section of the thesis I bring the discussion of the regulatory frameworks into the broader political and economic contexts of the countries to answer the question: why have the Governments in India and China regulated the emerging microfinance sectors so differently? I argue that rising inequality and poverty alleviation plans conditioned the goals of the Governments for the microfinance sector and that the broader level of financial sector liberalization conditioned the feasible set of microfinance regulations for the Governments.",
    "advisors": ["David Andrew Singer"],
    "text": "Microfinance regulation in China and India The regulatory responses of Governments in different countries to emerging microfinance sectors have varied dramatically and as a result so have the outcomes for these sectors. As two of the fastest growing developing countries in the world over the last two decades, both with vast poor rural populations lacking access to credit, the potential demand for microfinance in India and China is enormous. Yet where the microfinance sector in India has been one of the fastest growing in the world with a diverse range of successful for-profit and non-profit microfinance institutions, the microfinance sector in China has failed to find its feet with microfinance institutions unable to attract commercial funding to expand or to achieve financial self-sufficiency. In this thesis I provide a comparative analysis of the regulatory frameworks for microfinance in China and India in order to demonstrate how the more restrictive and uncertain regulatory environment in China has hindered the development of the sector. In the next section of the thesis I bring the discussion of the regulatory frameworks into the broader political and economic contexts of the countries to answer the question: why have the Governments in India and China regulated the emerging microfinance sectors so differently? I argue that rising inequality and poverty alleviation plans conditioned the goals of the Governments for the microfinance sector and that the broader level of financial sector liberalization conditioned the feasible set of microfinance regulations for the Governments."
}, {
    "id": "oai:dspace.mit.edu:1721.1/64618",
    "title": "Economic globalization and expressions of nationalism : a quantitative analysis of the present-day Chinese identity",
    "abstract": "Economic globalization and growing interdependence have greatly facilitated the flows of trade, investment, and ideas among formerly hostile countries. However, political antagonism driven by nationalistic sentiments continues to break out in countries with increased inflows of foreign investments, such as China. The media identifies that a majority of participants in China's recent anti-foreign antagonism are urban young Chinese and nicknames them China's \"angry youths\". This thesis focuses on the micro-foundations underlying expressions of nationalism in the forms of economic protectionism and xenophobic political hostility, and musters both public and original survey data to examine the dynamic effects of interpersonal contact and patriotic predisposition as explanations for individual attitudes on international policy issues. The empirical analyses of three main hypotheses suggest that the Chinese generation of \"angry youth\" is not as radical as the media has portrayed. Yet, their significantly higher levels of comfort with western values and rules of the game in the era of globalization do not necessarily mean that they are any more eager to embrace the global and transnational socioeconomic and sociopolitical identities than their seniors. This thesis finds that China's integration into the global economy over the past three decades has not yet promoted more popular cosmopolitan identities and perspectives among the general public as various theoretical accounts project, even among citizens who have high levels of exposure to foreign business and economic influence. The persistence of nationalistic worldviews and identities among the Chinese public has serious implications for foreign businesses and governments that wish to proactively engage a rising China in international and regional affairs.",
    "advisors": ["Roger D. Petersen"],
    "text": "Economic globalization and expressions of nationalism : a quantitative analysis of the present-day Chinese identity Economic globalization and growing interdependence have greatly facilitated the flows of trade, investment, and ideas among formerly hostile countries. However, political antagonism driven by nationalistic sentiments continues to break out in countries with increased inflows of foreign investments, such as China. The media identifies that a majority of participants in China's recent anti-foreign antagonism are urban young Chinese and nicknames them China's \"angry youths\". This thesis focuses on the micro-foundations underlying expressions of nationalism in the forms of economic protectionism and xenophobic political hostility, and musters both public and original survey data to examine the dynamic effects of interpersonal contact and patriotic predisposition as explanations for individual attitudes on international policy issues. The empirical analyses of three main hypotheses suggest that the Chinese generation of \"angry youth\" is not as radical as the media has portrayed. Yet, their significantly higher levels of comfort with western values and rules of the game in the era of globalization do not necessarily mean that they are any more eager to embrace the global and transnational socioeconomic and sociopolitical identities than their seniors. This thesis finds that China's integration into the global economy over the past three decades has not yet promoted more popular cosmopolitan identities and perspectives among the general public as various theoretical accounts project, even among citizens who have high levels of exposure to foreign business and economic influence. The persistence of nationalistic worldviews and identities among the Chinese public has serious implications for foreign businesses and governments that wish to proactively engage a rising China in international and regional affairs."
}, {
    "id": "oai:dspace.mit.edu:1721.1/62468",
    "title": "Explaining civil-military relations in Southeast Asia",
    "abstract": "Civil-military relations describe the interactions and balance of power between the civilians and the military in a nation state. Due to the organizational apparatus and capacity for forcible coercion that the military possesses, it can be an important determinant on whether a civilian government survives or falls, as well as what policies are formulated and implemented. This thesis analyses Southeast Asian civil-military relations in a comparative perspective. By looking at seven states in the region - Thailand, Indonesia, Singapore, Malaysia, the Philippines, Vietnam and Myanmar - it finds a rich diversity of such relations, ranging from situations of civilian control to civil-military partnerships to military control. The thesis therefore aims to answer the question: why has there been this variance in civil-military relations in the region? The thesis first examines briefly the history of civil-military relations theory as well as the history of the seven states mentioned above, building an analytical framework and proposing three alternative explanations for variance. Firstly, it asserts that pre-independence legacies created path dependencies that structure the shape of civil-military relations in the region. Secondly, the thesis argues that the structure of the political party environment mattered and assesses the case studies through indicators of concordance and discordance. Finally, the thesis looks at the presence of military entrepreneurship, asserting that variance depends on military capacity to engage in external business activities and civilian willingness to allow such activities. The thesis concludes by assessing the explanatory power of the three factors above and concluding that a combination of pre-independence legacies and party structure best explains civil-military relations in the region.",
    "advisors": ["Richard J. Samuels"],
    "text": "Explaining civil-military relations in Southeast Asia Civil-military relations describe the interactions and balance of power between the civilians and the military in a nation state. Due to the organizational apparatus and capacity for forcible coercion that the military possesses, it can be an important determinant on whether a civilian government survives or falls, as well as what policies are formulated and implemented. This thesis analyses Southeast Asian civil-military relations in a comparative perspective. By looking at seven states in the region - Thailand, Indonesia, Singapore, Malaysia, the Philippines, Vietnam and Myanmar - it finds a rich diversity of such relations, ranging from situations of civilian control to civil-military partnerships to military control. The thesis therefore aims to answer the question: why has there been this variance in civil-military relations in the region? The thesis first examines briefly the history of civil-military relations theory as well as the history of the seven states mentioned above, building an analytical framework and proposing three alternative explanations for variance. Firstly, it asserts that pre-independence legacies created path dependencies that structure the shape of civil-military relations in the region. Secondly, the thesis argues that the structure of the political party environment mattered and assesses the case studies through indicators of concordance and discordance. Finally, the thesis looks at the presence of military entrepreneurship, asserting that variance depends on military capacity to engage in external business activities and civilian willingness to allow such activities. The thesis concludes by assessing the explanatory power of the three factors above and concluding that a combination of pre-independence legacies and party structure best explains civil-military relations in the region."
}, {
    "id": "oai:dspace.mit.edu:1721.1/43191",
    "title": "Political representation of the poor in the U.S. political system : a discussion of theory and practice",
    "abstract": "This thesis examines the nature of political representation of the American poor from both a theoretical and an empirical perspective. A normative framework, based on the major theories of representation, is used to examine the empirical mechanisms through which the poor can obtain representation. \"Formal\" mechanisms include voting, formal participatory activities and membership in political institutions. The primary \"informal\" mechanism examined is the public opinion survey. The normative framework is grounded in the notion that being poor in an affluent nation is an attached, personal interest--a substantive understanding of the interests of the poor cannot be determined through intellectual deliberation alone. Social and economic closeness (or similarity in relevant descriptive characteristics such as income and race) informs poverty advocates about the nature of poverty and the impact of anti-poverty policies. A significant amount of evidence suggests that the poor are underrepresented (relative to other groups with more economic and political resources) in formal participation mechanisms. There is less evidence about how well and to what extent the poor are represented in the primary \"informal mechanism\"--public opinion. Preliminary evidence suggests that the political voice of the poor and their advocates may be muted in opinion polls. This research motivates the original empirical analysis in this thesis that examines who is advocating for the poor in public opinion surveys and what those advocates are saying. The original research produces two key findings. First, over the longer term period (1980s-2002) descriptive similarity of poverty advocates declined. .",
    "advisors": ["Adam J. Berinsky"],
    "text": "Political representation of the poor in the U.S. political system : a discussion of theory and practice This thesis examines the nature of political representation of the American poor from both a theoretical and an empirical perspective. A normative framework, based on the major theories of representation, is used to examine the empirical mechanisms through which the poor can obtain representation. \"Formal\" mechanisms include voting, formal participatory activities and membership in political institutions. The primary \"informal\" mechanism examined is the public opinion survey. The normative framework is grounded in the notion that being poor in an affluent nation is an attached, personal interest--a substantive understanding of the interests of the poor cannot be determined through intellectual deliberation alone. Social and economic closeness (or similarity in relevant descriptive characteristics such as income and race) informs poverty advocates about the nature of poverty and the impact of anti-poverty policies. A significant amount of evidence suggests that the poor are underrepresented (relative to other groups with more economic and political resources) in formal participation mechanisms. There is less evidence about how well and to what extent the poor are represented in the primary \"informal mechanism\"--public opinion. Preliminary evidence suggests that the political voice of the poor and their advocates may be muted in opinion polls. This research motivates the original empirical analysis in this thesis that examines who is advocating for the poor in public opinion surveys and what those advocates are saying. The original research produces two key findings. First, over the longer term period (1980s-2002) descriptive similarity of poverty advocates declined. ."
}, {
    "id": "oai:dspace.mit.edu:1721.1/49806",
    "title": "Strategies of public diplomacy : an assessment of the current U.S. public diplomacy strategy in light of a directional, elite-oriented model and two historical cases",
    "abstract": "While undoubtedly the most powerful nation in the world, the U.S. is constrained in its ability to further its national interests by the attitudes and actions of foreign governments and, more importantly, foreign publics. The growing ability of individuals and small groups to threaten the security and vital interests of the United States necessitates improved cooperation and empathy on the part of foreign publics as well as their governments. Moreover, the concurrent decline in the utility of military statecraft due to both higher costs and decreasing international legitimacy means that policymakers are forced to consider alternative means of influencing the behavior of others. Public diplomacy is a critically important example of such an alternative means. Unfortunately, understanding public diplomacy and its effects is hampered by a lack of conceptual tools used to develop and evaluate various strategies of public diplomacy. Consequently, the following paper seeks to combine existing theoretical arguments regarding strategic communication and mass persuasion to develop an analytical model that can be used to critique the United States' current public diplomacy strategy. According to the model developed and tested herein, strategic directionality and the level of elite orientation are the two primary causal factors that determine the outcome of a particular public diplomacy campaign. Target audience predisposition and the degree to which the campaign itself is concealed from the audience serve as two intervening variables that may retard the progress of a campaign. Thus, the diagrammatical structure of the model is as follows: Directional Message + Elite-Oriented Message --> Successful PD Campaign Intervening Variables: Predisposition Concealment of Campaign. When tested against Britain's efforts to recruit the U.S. as an ally prior to the end of 1941 and Kuwait's efforts to retain the support of the U.S. after Iraq's invasion in August 1990, the model is found to be sound. The model predicts that the current U.S. public diplomacy campaign in the Middle East will fail due to a lack of strategic directionality, limited elite orientation, and poor concealment of the campaign itself. By way of conclusion, several policy prescriptions are offered to improve the levels of each of these variables.",
    "advisors": ["Roger Petersen"],
    "text": "Strategies of public diplomacy : an assessment of the current U.S. public diplomacy strategy in light of a directional, elite-oriented model and two historical cases While undoubtedly the most powerful nation in the world, the U.S. is constrained in its ability to further its national interests by the attitudes and actions of foreign governments and, more importantly, foreign publics. The growing ability of individuals and small groups to threaten the security and vital interests of the United States necessitates improved cooperation and empathy on the part of foreign publics as well as their governments. Moreover, the concurrent decline in the utility of military statecraft due to both higher costs and decreasing international legitimacy means that policymakers are forced to consider alternative means of influencing the behavior of others. Public diplomacy is a critically important example of such an alternative means. Unfortunately, understanding public diplomacy and its effects is hampered by a lack of conceptual tools used to develop and evaluate various strategies of public diplomacy. Consequently, the following paper seeks to combine existing theoretical arguments regarding strategic communication and mass persuasion to develop an analytical model that can be used to critique the United States' current public diplomacy strategy. According to the model developed and tested herein, strategic directionality and the level of elite orientation are the two primary causal factors that determine the outcome of a particular public diplomacy campaign. Target audience predisposition and the degree to which the campaign itself is concealed from the audience serve as two intervening variables that may retard the progress of a campaign. Thus, the diagrammatical structure of the model is as follows: Directional Message + Elite-Oriented Message --> Successful PD Campaign Intervening Variables: Predisposition Concealment of Campaign. When tested against Britain's efforts to recruit the U.S. as an ally prior to the end of 1941 and Kuwait's efforts to retain the support of the U.S. after Iraq's invasion in August 1990, the model is found to be sound. The model predicts that the current U.S. public diplomacy campaign in the Middle East will fail due to a lack of strategic directionality, limited elite orientation, and poor concealment of the campaign itself. By way of conclusion, several policy prescriptions are offered to improve the levels of each of these variables."
}, {
    "id": "oai:dspace.mit.edu:1721.1/33688",
    "title": "Innovation, wargaming, and the development of armored warfare",
    "abstract": "This thesis examines the role of simulation in the development of armored warfare doctrine during the interwar period. All the Great Powers faced the challenge of how to integrate new technologies, particularly the tank, radio, and aircraft, into a coherent combined arms doctrinal framework. I compare the French and German experiences in order to assess the role that wargames played in driving doctrinal development. The case studies show that wargames, on the map and in the field, gave the German army a significant edge as it sought to develop new doctrine for armored warfare. This finding is an important addition to existing theory on military innovation, which tends to view doctrine as the product of geopolitical and organizational forces. Wargames provided a means of testing doctrinal ideas in a simulated wartime environment, and the lessons learned during these simulations fed into ongoing debates on doctrinal development. Wargaming well is a technically challenging business, and requires particular technical skills and capabilities. The Germans developed these capabilities earlier than their French counterparts, in part because the German army traditionally favored a rationalist, corporate approach to the management of military affairs.",
    "advisors": ["Barry R. Posen"],
    "text": "Innovation, wargaming, and the development of armored warfare This thesis examines the role of simulation in the development of armored warfare doctrine during the interwar period. All the Great Powers faced the challenge of how to integrate new technologies, particularly the tank, radio, and aircraft, into a coherent combined arms doctrinal framework. I compare the French and German experiences in order to assess the role that wargames played in driving doctrinal development. The case studies show that wargames, on the map and in the field, gave the German army a significant edge as it sought to develop new doctrine for armored warfare. This finding is an important addition to existing theory on military innovation, which tends to view doctrine as the product of geopolitical and organizational forces. Wargames provided a means of testing doctrinal ideas in a simulated wartime environment, and the lessons learned during these simulations fed into ongoing debates on doctrinal development. Wargaming well is a technically challenging business, and requires particular technical skills and capabilities. The Germans developed these capabilities earlier than their French counterparts, in part because the German army traditionally favored a rationalist, corporate approach to the management of military affairs."
}, {
    "id": "oai:dspace.mit.edu:1721.1/43193",
    "title": "Pumping up : Russian energy and national power",
    "abstract": "Russia has organized its energy industry similarly to a vertically integrated energy corporation. Not only does Russia possess vast oil and gas reserves, it also has capabilities at every step in the production chain. The execution of Russian energy strategy is incredibly intricate and flows from all points including the state, firms, suppliers, degrees of ownership and transit locales. This work reviews five important aspects of Russia's vertical integration strategy. Firstly, Russia is brining the domestic industry under state control. Secondly, it has pushed out particular types of foreign investment in order to gain control of domestic reserves, their monetization and the development of important projects. Thirdly, Russia seeks to keep resource rich states in its near abroad in its sphere of influence in order to keep their supply within its grasp. Fourthly, it manages relations with neighboring states in possession of transit infrastructure to keep supply routes open to markets. Lastly, it invests abroad in order to increase market presence, cut out middlemen, and further build production chains. Russia expects to strengthen its international position, both economically and geopolitically by undertaking this strategy of vertical integration.",
    "advisors": ["Kenneth A. Oye"],
    "text": "Pumping up : Russian energy and national power Russia has organized its energy industry similarly to a vertically integrated energy corporation. Not only does Russia possess vast oil and gas reserves, it also has capabilities at every step in the production chain. The execution of Russian energy strategy is incredibly intricate and flows from all points including the state, firms, suppliers, degrees of ownership and transit locales. This work reviews five important aspects of Russia's vertical integration strategy. Firstly, Russia is brining the domestic industry under state control. Secondly, it has pushed out particular types of foreign investment in order to gain control of domestic reserves, their monetization and the development of important projects. Thirdly, Russia seeks to keep resource rich states in its near abroad in its sphere of influence in order to keep their supply within its grasp. Fourthly, it manages relations with neighboring states in possession of transit infrastructure to keep supply routes open to markets. Lastly, it invests abroad in order to increase market presence, cut out middlemen, and further build production chains. Russia expects to strengthen its international position, both economically and geopolitically by undertaking this strategy of vertical integration."
}, {
    "id": "oai:dspace.mit.edu:1721.1/62469",
    "title": "Social status and governmental trust : a study of civil society organizations in Guangzhou, China",
    "abstract": "Previously, two extreme points describe civil society organizers' intention to engage with the government. Western modernization theories suggest that all civil society organizations oppose the state; and State-dominant theories claim that all the civil society groups depend on the state and support the state. Based on my two months' fieldwork in Guangzhou City between May and July 2010, however, I find that some civil society organizations support the state while other groups oppose the state. Specifically, elite civil society organizations, which are established by people with mid or high social status, have more contacts with the government and show low trust in the government. On the other hand, non-elite civil society organizations, which are established by people with low social status, have fewer contacts with the government and show their willingness to engage with the government. This conclusion is valid in both the public good provision organizations and the advocacy groups. Four case studies of civil society groups in Guangzhou are used to illustrate the points above.",
    "advisors": ["Lily L. Tsai"],
    "text": "Social status and governmental trust : a study of civil society organizations in Guangzhou, China Previously, two extreme points describe civil society organizers' intention to engage with the government. Western modernization theories suggest that all civil society organizations oppose the state; and State-dominant theories claim that all the civil society groups depend on the state and support the state. Based on my two months' fieldwork in Guangzhou City between May and July 2010, however, I find that some civil society organizations support the state while other groups oppose the state. Specifically, elite civil society organizations, which are established by people with mid or high social status, have more contacts with the government and show low trust in the government. On the other hand, non-elite civil society organizations, which are established by people with low social status, have fewer contacts with the government and show their willingness to engage with the government. This conclusion is valid in both the public good provision organizations and the advocacy groups. Four case studies of civil society groups in Guangzhou are used to illustrate the points above."
}, {
    "id": "oai:dspace.mit.edu:1721.1/59796",
    "title": "The urbanization of insurgency : shifts in the geography of conflict",
    "abstract": "The 20th century witnessed the steady decline of the ability of states, particularly great powers, to defeat insurgencies. During the same period, the world has become both more populous and more urban. As people have taken to the cities, so too have insurgents increasingly made battlefields out of urban areas. This study has sought to determine the impact of urbanization on insurgency outcomes using a post-war dataset of insurgencies. It has predicted that urbanized insurgencies favor the insurgent by facilitating concealment and cover, nullifying the relatively power differential enjoyed by states, and providing them with an abundance of soft targets useful for undermining the counterinsurgent's legitimacy. Although constrained by a number of data limitations, the results demonstrated that more urbanized insurgencies were a significant challenge to counter insurgents. By partitioning the dataset by insurgency type, the study was able to determine unique predictors of conflict outcome for each type. Urbanized insurgencies are particularly hard to defeat when the counterinsurgent is a foreign occupier, more democratic, and the insurgency has external support. Rural insurgencies become more difficult to defeat the more linguistically diverse the population. Furthermore, by increasing the number of conflict casualties, rural insurgents can particularly benefit from rough terrain.",
    "advisors": ["Roger Petersen"],
    "text": "The urbanization of insurgency : shifts in the geography of conflict The 20th century witnessed the steady decline of the ability of states, particularly great powers, to defeat insurgencies. During the same period, the world has become both more populous and more urban. As people have taken to the cities, so too have insurgents increasingly made battlefields out of urban areas. This study has sought to determine the impact of urbanization on insurgency outcomes using a post-war dataset of insurgencies. It has predicted that urbanized insurgencies favor the insurgent by facilitating concealment and cover, nullifying the relatively power differential enjoyed by states, and providing them with an abundance of soft targets useful for undermining the counterinsurgent's legitimacy. Although constrained by a number of data limitations, the results demonstrated that more urbanized insurgencies were a significant challenge to counter insurgents. By partitioning the dataset by insurgency type, the study was able to determine unique predictors of conflict outcome for each type. Urbanized insurgencies are particularly hard to defeat when the counterinsurgent is a foreign occupier, more democratic, and the insurgency has external support. Rural insurgencies become more difficult to defeat the more linguistically diverse the population. Furthermore, by increasing the number of conflict casualties, rural insurgents can particularly benefit from rough terrain."
}, {
    "id": "oai:dspace.mit.edu:1721.1/28755",
    "title": "On economic bicameralism",
    "abstract": "(cont.) for both economic profitability and democratic justice, is explored after the roots of the idea of economic bicameralism in socio-economic history and existing socio-economic institutions (such as Works Councils) are reviewed. Economic bicameralism is thus an original form of governance of the firm with regards to both its philosophy and its institutions. It is founded on the recognition that two quests take place within the context of the firm, each of which is pursued primarily by one of the firm's two major agents, capital and labor. In the structure of economic bicameralism, two chambers, one representing each group of agents, govern the firm jointly. The Chamber of Capital assembles the investors in capital, who value an instrumental rationality while seeking profit as their foremost objective; and the Chamber of Labor assembles the investors in person, who display a political rationality and are best understood as seeking democratic justice as their primary goal. While investors in capital remain the sole legal shareholders of the bicameral firm, the governance of the firm is managed by these two chambers, which occupy an equal footing and are consequently bound to cooperate in order to allow the firm to function efficiently. The collaboration hence induced between investors in capital and investors in labor enables the firm to effectively respect the aspiration towards democratic justice that infuses the work experience with the objective of economic profitability that motivates first the investors in capital.",
    "advisors": ["Joshua Cohen"],
    "text": "On economic bicameralism (cont.) for both economic profitability and democratic justice, is explored after the roots of the idea of economic bicameralism in socio-economic history and existing socio-economic institutions (such as Works Councils) are reviewed. Economic bicameralism is thus an original form of governance of the firm with regards to both its philosophy and its institutions. It is founded on the recognition that two quests take place within the context of the firm, each of which is pursued primarily by one of the firm's two major agents, capital and labor. In the structure of economic bicameralism, two chambers, one representing each group of agents, govern the firm jointly. The Chamber of Capital assembles the investors in capital, who value an instrumental rationality while seeking profit as their foremost objective; and the Chamber of Labor assembles the investors in person, who display a political rationality and are best understood as seeking democratic justice as their primary goal. While investors in capital remain the sole legal shareholders of the bicameral firm, the governance of the firm is managed by these two chambers, which occupy an equal footing and are consequently bound to cooperate in order to allow the firm to function efficiently. The collaboration hence induced between investors in capital and investors in labor enables the firm to effectively respect the aspiration towards democratic justice that infuses the work experience with the objective of economic profitability that motivates first the investors in capital."
}, {
    "id": "oai:dspace.mit.edu:1721.1/35521",
    "title": "Examining the influence of civilian casualties on insurgent attacks in Iraq",
    "abstract": "Although there have been several attempts to tabulate civilian casualties in the Iraq War, the effect of these casualties on the Iraqi population and insurgent organizations has not been thoroughly examined. From the literature of the motives and mechanisms behind the formation and expansion of insurgencies, as well as the culture and values of Iraqi society, it is expected that increased civilian casualties will create grievance among the population, causing support for insurgents and increased attacks on coalition forces. This paper statistically analyzes data on Iraqi civilian and coalition force casualties to determine if there is a causal relationship between the two variables. It also recognizes the limitations and potential biases of the available data. Control variables are included in the statistical analysis to compare the influence of civilian casualties to competing theories of insurgency formation. Analysis demonstrates that while civilian casualties and coalition casualties have a positive relationship, significant causality between the two variables cannot be established. Alternative hypotheses examine unique factors in Anbar and Baghdad provinces and the role of focus events in insurgent activity. The paper concludes with recommendations for further study.",
    "advisors": ["Roger Petersen"],
    "text": "Examining the influence of civilian casualties on insurgent attacks in Iraq Although there have been several attempts to tabulate civilian casualties in the Iraq War, the effect of these casualties on the Iraqi population and insurgent organizations has not been thoroughly examined. From the literature of the motives and mechanisms behind the formation and expansion of insurgencies, as well as the culture and values of Iraqi society, it is expected that increased civilian casualties will create grievance among the population, causing support for insurgents and increased attacks on coalition forces. This paper statistically analyzes data on Iraqi civilian and coalition force casualties to determine if there is a causal relationship between the two variables. It also recognizes the limitations and potential biases of the available data. Control variables are included in the statistical analysis to compare the influence of civilian casualties to competing theories of insurgency formation. Analysis demonstrates that while civilian casualties and coalition casualties have a positive relationship, significant causality between the two variables cannot be established. Alternative hypotheses examine unique factors in Anbar and Baghdad provinces and the role of focus events in insurgent activity. The paper concludes with recommendations for further study."
}, {
    "id": "oai:dspace.mit.edu:1721.1/107533",
    "title": "Pension benefits and social cohesion",
    "abstract": "How does the expansion of social protection programs to the poor in developing democracies affect social cohesion? I address this question by examining Bolivia's central government run, non-contributory pension program, Renta Dignidad. Using a regression discontinuity design as well as a novel difference-in-discontinuities design, I find that recipients of pension benefits are overall more likely to display increased support for the central government and that in provinces where both territorial tensions and class tensions are most acute, support for the central government is significantly greater. This is consistent with a theoretical argument that expansion of social protection to the poor can mitigate opposition on active dimensions of social conflict that intersect the socio-economic dimension.",
    "advisors": ["Teppei Yamamoto"],
    "text": "Pension benefits and social cohesion How does the expansion of social protection programs to the poor in developing democracies affect social cohesion? I address this question by examining Bolivia's central government run, non-contributory pension program, Renta Dignidad. Using a regression discontinuity design as well as a novel difference-in-discontinuities design, I find that recipients of pension benefits are overall more likely to display increased support for the central government and that in provinces where both territorial tensions and class tensions are most acute, support for the central government is significantly greater. This is consistent with a theoretical argument that expansion of social protection to the poor can mitigate opposition on active dimensions of social conflict that intersect the socio-economic dimension."
}, {
    "id": "oai:dspace.mit.edu:1721.1/46632",
    "title": "Ganging up on Jolly Roger in Asia : International cooperation and maritime piracy",
    "abstract": "As non-traditional security threats such as terrorism and organized transnational crime gain greater prominence around the globe, the need for international cooperation against these non-state actors has consequently acquired greater urgency. Due to the cross-boundary nature of these activities, international cooperation is particularly critical for eradicating these threats. This thesis analyzes a particular instance of a non-state threat, maritime piracy, and uses it as a probe for understanding the nature of international cooperation vis-a-vis non-state actors. I observe a somewhat surprising trend while collating a database of all instances of international cooperation against maritime piracy throughout the world - Asia, and in particular Southeast Asia, has been the source of a disproportionately high level of international cooperation that is focused on eradicating the piracy problem. Furthermore, this trend has occurred even though Asia is often regarded as lacking the conditions necessary for international cooperation in the traditional security domain - binding multilateral institutions that can facilitate the institutionalization of cooperation agreements, as well as a hegemonic power with the ability to enforce cooperation. What has enabled international cooperation against maritime piracy to flourish in Asia, and what does this imply about non-traditional forms of security cooperation? I propose that non-traditional security cooperation has thrived in Asia for at least two unconventional reasons - the ability of non-binding institutions such as ASEAN to facilitate and promote non-traditional security cooperation, as well as the effective use of national coast guard agencies to avoid the political sensitivities that often result from security cooperation in the traditional domain.",
    "advisors": ["Richard J. Samuels", "M. Taylor Fravel"],
    "text": "Ganging up on Jolly Roger in Asia : International cooperation and maritime piracy As non-traditional security threats such as terrorism and organized transnational crime gain greater prominence around the globe, the need for international cooperation against these non-state actors has consequently acquired greater urgency. Due to the cross-boundary nature of these activities, international cooperation is particularly critical for eradicating these threats. This thesis analyzes a particular instance of a non-state threat, maritime piracy, and uses it as a probe for understanding the nature of international cooperation vis-a-vis non-state actors. I observe a somewhat surprising trend while collating a database of all instances of international cooperation against maritime piracy throughout the world - Asia, and in particular Southeast Asia, has been the source of a disproportionately high level of international cooperation that is focused on eradicating the piracy problem. Furthermore, this trend has occurred even though Asia is often regarded as lacking the conditions necessary for international cooperation in the traditional security domain - binding multilateral institutions that can facilitate the institutionalization of cooperation agreements, as well as a hegemonic power with the ability to enforce cooperation. What has enabled international cooperation against maritime piracy to flourish in Asia, and what does this imply about non-traditional forms of security cooperation? I propose that non-traditional security cooperation has thrived in Asia for at least two unconventional reasons - the ability of non-binding institutions such as ASEAN to facilitate and promote non-traditional security cooperation, as well as the effective use of national coast guard agencies to avoid the political sensitivities that often result from security cooperation in the traditional domain."
}, {
    "id": "oai:dspace.mit.edu:1721.1/28669",
    "title": "Crossed swords : divided militaries and politics in East Asia",
    "abstract": "(cont.) Domestic politics, then, frequently have a decisive impact on strategic planning and produces policies that the consideration of external threats alone would not suggest.",
    "advisors": ["Barry Posen"],
    "text": "Crossed swords : divided militaries and politics in East Asia (cont.) Domestic politics, then, frequently have a decisive impact on strategic planning and produces policies that the consideration of external threats alone would not suggest."
}, {
    "id": "oai:dspace.mit.edu:1721.1/113487",
    "title": "\"Courage First\" : dissent, debate, and the origins of US responsiveness to mass killing",
    "abstract": "The United States has developed a reputation for consistently failing to respond to mass killing and genocide throughout history. The conventional wisdom is that the United States has the resources and intelligence to act, but fails to do so because of a lack of political will. However, a closer examination of history reveals that although the modal response of the United States is indeed to refrain from devoting significant resources to these crises, at times the United States reverses course to pursue policies aimed at assisting victims of atrocity. Previous analyses have not fully explained the sources this policy variation. Drawing on extensive archival research, this dissertation proposes a theory explaining when these shifts in US policy occur. I suggest that three factors-the level at which dissent occurs within the government, the degree of congressional pressure, and the direction of a variable that I term political liability-are responsible for shifting US policy toward a more robust response. I illustrate the theory with case studies covering US responsiveness to the following cases: the Holocaust (193 8-1945) under presidents Franklin D. Roosevelt and Harry S. Truman; mass killing in Bosnia (1992-1995) under presidents George H.W. Bush and William J. Clinton; and mass killing in Rwanda (1994) under Clinton. A comparative analysis of US responsiveness to the",
    "advisors": ["Stephen Van Evera"],
    "text": "\"Courage First\" : dissent, debate, and the origins of US responsiveness to mass killing The United States has developed a reputation for consistently failing to respond to mass killing and genocide throughout history. The conventional wisdom is that the United States has the resources and intelligence to act, but fails to do so because of a lack of political will. However, a closer examination of history reveals that although the modal response of the United States is indeed to refrain from devoting significant resources to these crises, at times the United States reverses course to pursue policies aimed at assisting victims of atrocity. Previous analyses have not fully explained the sources this policy variation. Drawing on extensive archival research, this dissertation proposes a theory explaining when these shifts in US policy occur. I suggest that three factors-the level at which dissent occurs within the government, the degree of congressional pressure, and the direction of a variable that I term political liability-are responsible for shifting US policy toward a more robust response. I illustrate the theory with case studies covering US responsiveness to the following cases: the Holocaust (193 8-1945) under presidents Franklin D. Roosevelt and Harry S. Truman; mass killing in Bosnia (1992-1995) under presidents George H.W. Bush and William J. Clinton; and mass killing in Rwanda (1994) under Clinton. A comparative analysis of US responsiveness to the"
}, {
    "id": "oai:dspace.mit.edu:1721.1/8174",
    "title": "Women in politics : a cross-national demand and supply analysis",
    "abstract": "It is striking that the sharp increase in the number of countries moving towards self-governance and democracy has not been accompanied by more equal political representation of women. What is equally puzzling is the contrast in the share of women in positions of political authority observed between countries, with many developed nations having fewer women legislators than a number of lesser-developed countries. Why are there so few women in most parliaments and why is there such variation across countries? To understand gender-based inequality in political authority, we look at the various stages of candidacy and identify potential bottlenecks to women participation and election into public office. There are three stages which one must pass through successfully to become a legislator. The first is becoming eligible and a part of the pool from which politicians are drawn, then being selected as a candidate and finally being elected to office. Potential barriers to entry for women in the legislative process may exist at any or all of these three stages. Each of these candidacy stages is discussed through a cross-national analysis and a case study of India. The cross-national data is for 175 countries at three points in time: 1975, 1985 and 1995. The Indian case study looks at women in parliament from the first general elections in 1951-1952 and focuses most on the 1996 parliamentary data. We argue that the key factor limiting the recruitment of women into politics is women's sparse representation in the pool from which politicians are recruited. Just as in thecase of men, women are drawn from an elite pool based on their occupational achievements.",
    "advisors": ["Stephen Daniel Ansolabehere"],
    "text": "Women in politics : a cross-national demand and supply analysis It is striking that the sharp increase in the number of countries moving towards self-governance and democracy has not been accompanied by more equal political representation of women. What is equally puzzling is the contrast in the share of women in positions of political authority observed between countries, with many developed nations having fewer women legislators than a number of lesser-developed countries. Why are there so few women in most parliaments and why is there such variation across countries? To understand gender-based inequality in political authority, we look at the various stages of candidacy and identify potential bottlenecks to women participation and election into public office. There are three stages which one must pass through successfully to become a legislator. The first is becoming eligible and a part of the pool from which politicians are drawn, then being selected as a candidate and finally being elected to office. Potential barriers to entry for women in the legislative process may exist at any or all of these three stages. Each of these candidacy stages is discussed through a cross-national analysis and a case study of India. The cross-national data is for 175 countries at three points in time: 1975, 1985 and 1995. The Indian case study looks at women in parliament from the first general elections in 1951-1952 and focuses most on the 1996 parliamentary data. We argue that the key factor limiting the recruitment of women into politics is women's sparse representation in the pool from which politicians are recruited. Just as in thecase of men, women are drawn from an elite pool based on their occupational achievements."
}, {
    "id": "oai:dspace.mit.edu:1721.1/8238",
    "title": "Organizing coercion in authoritarian Chile",
    "abstract": "Coercion is at the center of politics, yet how it is organized has remained poorly understood. This dissertation analyzes how the Chilean military regime (1973-90) organized coercion, focusing especially on two major shifts during the period of most institutional flux, from 1973-78. Available explanations for the shifts fail to account for the magnitude of organizational changes. As an alternative, this dissertation provides a typology of coercion, based on measurements of how well principals monitor agents' operations and performance. Principals can monitor from within their own organization (internal monitoring), or from information sources outside their direct control (external monitoring). Measuring levels of internal and external monitoring, using various criteria for the breadth and depth of information, yields a matrix with types that are mutually exclusive and jointly exhaustive. The four basic types are blind, bureaucratic, transparent, and hide and seek coercion. There are tradeoffs to each type of coercion, which can prompt principals to shift from one to another. In Chile, measurements of internal and external monitoring before and after each of the two major shifts, alongside counterfactual analysis and tests of the competing available explanations, reveal that the regime in each case grappled with organizing coercion as a discrete problem of governance. In 1974 the regime created a powerful secret police to better coordinate coercion through higher internal monitoring. The police resolved many organizational problems but failed to increase internal monitoring substantially.",
    "advisors": ["Joshua Cohen"],
    "text": "Organizing coercion in authoritarian Chile Coercion is at the center of politics, yet how it is organized has remained poorly understood. This dissertation analyzes how the Chilean military regime (1973-90) organized coercion, focusing especially on two major shifts during the period of most institutional flux, from 1973-78. Available explanations for the shifts fail to account for the magnitude of organizational changes. As an alternative, this dissertation provides a typology of coercion, based on measurements of how well principals monitor agents' operations and performance. Principals can monitor from within their own organization (internal monitoring), or from information sources outside their direct control (external monitoring). Measuring levels of internal and external monitoring, using various criteria for the breadth and depth of information, yields a matrix with types that are mutually exclusive and jointly exhaustive. The four basic types are blind, bureaucratic, transparent, and hide and seek coercion. There are tradeoffs to each type of coercion, which can prompt principals to shift from one to another. In Chile, measurements of internal and external monitoring before and after each of the two major shifts, alongside counterfactual analysis and tests of the competing available explanations, reveal that the regime in each case grappled with organizing coercion as a discrete problem of governance. In 1974 the regime created a powerful secret police to better coordinate coercion through higher internal monitoring. The police resolved many organizational problems but failed to increase internal monitoring substantially."
}, {
    "id": "oai:dspace.mit.edu:1721.1/113488",
    "title": "Essays on networks and social interaction",
    "abstract": "This dissertation presents new models and experimental designs for understanding network behavior and social interaction. The first paper develops a model for a new kind of data, \"path data,\" that represents the sequential decisions made by actors navigating social, geographic, and other kinds of networks. The model is validated in a benchmark test, then used to measure sectarian influences in the ways that Sunni and Shia individuals navigate the streets of Baghdad in a smartphone-based field activity. The second paper uses a novel experimental design to examine the social network search patterns employed by both sects. Using smartphone and self-reported data, the paper shows that differing search strategies result in differential access to public services in Baghdad. The third paper presents a new model for measuring rhetorical style and other modes of speech in political deliberation. The model is validated in a benchmark test of conflictual speech in political debates.",
    "advisors": ["Teppei Yamamoto"],
    "text": "Essays on networks and social interaction This dissertation presents new models and experimental designs for understanding network behavior and social interaction. The first paper develops a model for a new kind of data, \"path data,\" that represents the sequential decisions made by actors navigating social, geographic, and other kinds of networks. The model is validated in a benchmark test, then used to measure sectarian influences in the ways that Sunni and Shia individuals navigate the streets of Baghdad in a smartphone-based field activity. The second paper uses a novel experimental design to examine the social network search patterns employed by both sects. Using smartphone and self-reported data, the paper shows that differing search strategies result in differential access to public services in Baghdad. The third paper presents a new model for measuring rhetorical style and other modes of speech in political deliberation. The model is validated in a benchmark test of conflictual speech in political debates."
}, {
    "id": "oai:dspace.mit.edu:1721.1/64617",
    "title": "Essays on the political economy of welfare and redistribution",
    "abstract": "This dissertation explores two main puzzles. First, why do some countries have more generous welfare policies than others? Second, why do some people support welfare policies more than others? This collection of essays aims to answer these two questions, focusing on the political and economic determinants of welfare policy and attitudes. Chapter 2 deals with methodological issues that will be addressed in the later substantive chapters. While this chapter discusses measurement error in general, it focuses on the problem that some respondents are likely to choose around the middle for reasons other than their true moderate attitudes in many survey items. The chapter formally analyzes the effects of this \"concentrated measurement error\" on the bias in regression coefficient estimates. It then proposes two estimation strategies for the handling of this problem. Turning to substantive research questions, Chapter 3 addresses the determinants of government welfare spending around the world. With the use of a unique dataset that has been constructed from six different cross-country social surveys and government finance statistics, this chapter demonstrates that public ideological preferences influence government decisions regarding the size of welfare expenditure. The chapter further presents a meaningful difference between fully and less democratic countries in welfare policy responsiveness; among less democratic countries, welfare spending policies have been little affected by public preferences. The empirical findings presented in this chapter serve as better evidence to support the mechanisms that traditional representation theories offer. In Chapter 4, I turn my attention to individual-level determinants. Recognizing the unique situation of the US, where the immigrant population is large and the natives have a distinctively individualistic taste for redistribution, this chapter assesses the role of socialization and assimilation by examining the political preferences of first-, second-, and third-generation immigrants with regard to welfare spending. It provides empirical evidence that first-generation immigrants show greater support for welfare than US-born natives; however, it also shows that the political views of immigrants more closely resemble those of US-born natives the longer that the immigrants stay in the US, thereby suggesting their assimilation into US society. Furthermore, this chapter documents that the more liberal views of first-generation immigrants do not persist into the next generation due to the effects of assimilation and socialization.",
    "advisors": ["James M. Snyder, Jr"],
    "text": "Essays on the political economy of welfare and redistribution This dissertation explores two main puzzles. First, why do some countries have more generous welfare policies than others? Second, why do some people support welfare policies more than others? This collection of essays aims to answer these two questions, focusing on the political and economic determinants of welfare policy and attitudes. Chapter 2 deals with methodological issues that will be addressed in the later substantive chapters. While this chapter discusses measurement error in general, it focuses on the problem that some respondents are likely to choose around the middle for reasons other than their true moderate attitudes in many survey items. The chapter formally analyzes the effects of this \"concentrated measurement error\" on the bias in regression coefficient estimates. It then proposes two estimation strategies for the handling of this problem. Turning to substantive research questions, Chapter 3 addresses the determinants of government welfare spending around the world. With the use of a unique dataset that has been constructed from six different cross-country social surveys and government finance statistics, this chapter demonstrates that public ideological preferences influence government decisions regarding the size of welfare expenditure. The chapter further presents a meaningful difference between fully and less democratic countries in welfare policy responsiveness; among less democratic countries, welfare spending policies have been little affected by public preferences. The empirical findings presented in this chapter serve as better evidence to support the mechanisms that traditional representation theories offer. In Chapter 4, I turn my attention to individual-level determinants. Recognizing the unique situation of the US, where the immigrant population is large and the natives have a distinctively individualistic taste for redistribution, this chapter assesses the role of socialization and assimilation by examining the political preferences of first-, second-, and third-generation immigrants with regard to welfare spending. It provides empirical evidence that first-generation immigrants show greater support for welfare than US-born natives; however, it also shows that the political views of immigrants more closely resemble those of US-born natives the longer that the immigrants stay in the US, thereby suggesting their assimilation into US society. Furthermore, this chapter documents that the more liberal views of first-generation immigrants do not persist into the next generation due to the effects of assimilation and socialization."
}, {
    "id": "oai:dspace.mit.edu:1721.1/118219",
    "title": "Community carrots and social sticks : why the poor vote in a dominant-party system",
    "abstract": "In dominant-party states, why do individuals vote in elections with foregone conclusions when they are neither bought nor coerced? It is especially curious in these cases why the rural poor decide to cast their ballots. I posit that communities that collectively rely on the government for public services foster social norms of voting to influence turnout. Motivated by the perception that regimes reward high turnout areas with public goods, communities use esteem \"carrots\" and social \"sticks\" to overcome free-rider incentives and increase the likelihood of receiving services. The norm is strongest in less politically-competitive areas, precisely where the puzzle of participation is most obvious. At the individual level, those who rely on their local community for non-material goods, such as information and kinship, are more likely to comply with the norm in order to secure their access to these social benefits. Findings from a lab-in-the-field voting experiment in rural Tanzania indicate a strong influence of the social norm of voting. In the experiment, when turnout is public to their neighbors, respondents are 11 percentage points more likely to vote, compared to when they are in private. The theory, which applies broadly to many patronage-based regimes, explains how communities sustain social norms of voting even when elections lack legitimacy, elucidating the paradox of high turnout in dominant-party systems.",
    "advisors": ["Lily L. Tsai"],
    "text": "Community carrots and social sticks : why the poor vote in a dominant-party system In dominant-party states, why do individuals vote in elections with foregone conclusions when they are neither bought nor coerced? It is especially curious in these cases why the rural poor decide to cast their ballots. I posit that communities that collectively rely on the government for public services foster social norms of voting to influence turnout. Motivated by the perception that regimes reward high turnout areas with public goods, communities use esteem \"carrots\" and social \"sticks\" to overcome free-rider incentives and increase the likelihood of receiving services. The norm is strongest in less politically-competitive areas, precisely where the puzzle of participation is most obvious. At the individual level, those who rely on their local community for non-material goods, such as information and kinship, are more likely to comply with the norm in order to secure their access to these social benefits. Findings from a lab-in-the-field voting experiment in rural Tanzania indicate a strong influence of the social norm of voting. In the experiment, when turnout is public to their neighbors, respondents are 11 percentage points more likely to vote, compared to when they are in private. The theory, which applies broadly to many patronage-based regimes, explains how communities sustain social norms of voting even when elections lack legitimacy, elucidating the paradox of high turnout in dominant-party systems."
}, {
    "id": "oai:dspace.mit.edu:1721.1/8025",
    "title": "Nodes without roads : pockets of success, networks of failure in Chinese industrial technology development",
    "abstract": "This thesis examines technological innovation capabilities in the Chinese machine tool, automobile, telecommunications equipment, and information technology sectors in the era of reform ( 1979-2002). Contrary to other studies, the thesis reveals that the foundations for Chinese industrial learning and innovation are weak. I evaluate several models for explaining why this is true, including market-, developmental state-, and regional-based explanations. Finding these unsatisfactory, I propose an alternative model: particularist political structure and industrial strategic culture undermine the ability of Chinese firms to associate, learn, and innovate. Particularism and strategic culture interact to undermine reciprocity and trust, weakening the ability of Chinese industrial firms to form effective horizontal association. As observed in economies from the developed (e.g. United States) to the developing (e.g. Taiwan), horizontal association between manufacturers as well as customers, suppliers, financers, research institutes, and universities is critical to industrial learning and technology development. The weakness of associational means for reducing risk and promoting information and resource flows inhibits the ability of Chinese firms to effectively indigenize, diffuse, innovate, and leverage industrial technology. Political particularism - manipulation of state policy and resources by agents of the state in pursuit of parochial goals - intensified during the reform period. Market reforms rendered existing industrial governance regimes ineffective, but the state suppressed formation of new mutual monitoring and consensus-building regimes appropriate to liberalized markets and society.",
    "advisors": ["Richard J. Samuels"],
    "text": "Nodes without roads : pockets of success, networks of failure in Chinese industrial technology development This thesis examines technological innovation capabilities in the Chinese machine tool, automobile, telecommunications equipment, and information technology sectors in the era of reform ( 1979-2002). Contrary to other studies, the thesis reveals that the foundations for Chinese industrial learning and innovation are weak. I evaluate several models for explaining why this is true, including market-, developmental state-, and regional-based explanations. Finding these unsatisfactory, I propose an alternative model: particularist political structure and industrial strategic culture undermine the ability of Chinese firms to associate, learn, and innovate. Particularism and strategic culture interact to undermine reciprocity and trust, weakening the ability of Chinese industrial firms to form effective horizontal association. As observed in economies from the developed (e.g. United States) to the developing (e.g. Taiwan), horizontal association between manufacturers as well as customers, suppliers, financers, research institutes, and universities is critical to industrial learning and technology development. The weakness of associational means for reducing risk and promoting information and resource flows inhibits the ability of Chinese firms to effectively indigenize, diffuse, innovate, and leverage industrial technology. Political particularism - manipulation of state policy and resources by agents of the state in pursuit of parochial goals - intensified during the reform period. Market reforms rendered existing industrial governance regimes ineffective, but the state suppressed formation of new mutual monitoring and consensus-building regimes appropriate to liberalized markets and society."
}, {
    "id": "oai:dspace.mit.edu:1721.1/8757",
    "title": "What causes credibility? : reputation, power, and assessments of credibility during crises",
    "abstract": "Year after year, Americans are told that their country's reputation is on the line. If we do not carry out our commitments, our foreign policy leaders warn, no one will believe our threats and promises in the future. This claim rests on the hypothesis, which I call the \"reputation hypothesis,\" that tomorrow's enemies will assess America's credibility on the basis of U.S. actions today. Is the reputation hypothesis true? Will our adversaries predict our future actions by looking at today's decisions? More generally, do decision makers predict their adversaries' actions in a crisis by looking at the adversaries' previous actions? I test the reputation hypothesis against a hypothesis which I call the \"power/interests\" hypothesis. This hypothesis posits that decision makers assess the credibility of an adversary's threats by assessing the current balance of power and interests; commitments are credible when they support important interests and are backed up by the power to carry them out. I test these theories by studying American and British decision making during three crises. From 1958-1962, the Soviet Union and the NATO allies faced each other in a series of crises over Berlin and Cuba. I use evidence from American and British archives to discover how decision makers assessed Soviet credibility during these crises. I look for evidence that they based their assessments of Soviet credibility on past Soviet actions, and for evidence that they assessed Soviet credibility by evaluating the current balance of power and interests. The results are striking: during this period the Soviets repeatedly made threats and then backed down. But years of unfulfilled threats did not damage Soviet credibility. In fact, Soviet credibility grew from 1958-62, as the power/interests hypothesis predicts. American and British decision makers worried constantly about their own reputation, but they did not use Soviet past behavior to assess Soviet credibility. This research suggests that countries should not fight to build a reputation for credibility - threats will be credible if and only if they promote substantial interests and are backed up by sufficient power.",
    "advisors": ["Barry R. Posen"],
    "text": "What causes credibility? : reputation, power, and assessments of credibility during crises Year after year, Americans are told that their country's reputation is on the line. If we do not carry out our commitments, our foreign policy leaders warn, no one will believe our threats and promises in the future. This claim rests on the hypothesis, which I call the \"reputation hypothesis,\" that tomorrow's enemies will assess America's credibility on the basis of U.S. actions today. Is the reputation hypothesis true? Will our adversaries predict our future actions by looking at today's decisions? More generally, do decision makers predict their adversaries' actions in a crisis by looking at the adversaries' previous actions? I test the reputation hypothesis against a hypothesis which I call the \"power/interests\" hypothesis. This hypothesis posits that decision makers assess the credibility of an adversary's threats by assessing the current balance of power and interests; commitments are credible when they support important interests and are backed up by the power to carry them out. I test these theories by studying American and British decision making during three crises. From 1958-1962, the Soviet Union and the NATO allies faced each other in a series of crises over Berlin and Cuba. I use evidence from American and British archives to discover how decision makers assessed Soviet credibility during these crises. I look for evidence that they based their assessments of Soviet credibility on past Soviet actions, and for evidence that they assessed Soviet credibility by evaluating the current balance of power and interests. The results are striking: during this period the Soviets repeatedly made threats and then backed down. But years of unfulfilled threats did not damage Soviet credibility. In fact, Soviet credibility grew from 1958-62, as the power/interests hypothesis predicts. American and British decision makers worried constantly about their own reputation, but they did not use Soviet past behavior to assess Soviet credibility. This research suggests that countries should not fight to build a reputation for credibility - threats will be credible if and only if they promote substantial interests and are backed up by sufficient power."
}, {
    "id": "oai:dspace.mit.edu:1721.1/99826",
    "title": "Fear and frustration : rising state perceptions of threats and opportunities",
    "abstract": "Do a dominant state's policies have a greater effect on a rising state's threat perceptions or its assessment of the dominant state's resolve? Existing theory, rooted in Jervis's spiral and deterrence models, contends that the answer depends on whether the state has status quo or revisionist intentions. Rising states are typically seen as revisionist, a type of state that is said to be easily emboldened by conciliation but not easily threatened by competition. This project, on the other hand, argues that rising states - even those with revisionist aims - are more easily threatened than emboldened. Anarchy and uncertainty surrounding the dominant state's intentions give all rising states incentive to be cautious in their assessments. Underestimating threats could leave a rising state more vulnerable to coercion or unprepared for war with a materially stronger dominant state. Rising states, therefore, increase their threat assessments in response to almost any kind of competition by the dominant state. The risks of underestimating the dominant state's resolve are also significant: a resolute dominant state might respond to a challenge with overwhelming force. Therefore, rising states only downgrade their assessments of the dominant state's resolve in the face of very strong signals, such as large, militarily useful concessions. This dissertation tests these competing arguments during periods when Britain was a dominant state facing a rising, revisionist power - the United States (1837-1846) and Wilhelmine Germany (1894-1898). Detailed, historical analysis identified each British policy change and assessed the impact on U.S. and German perceptions. The results suggest that existing theory overstates the risks of conciliating rising states and understates the impact that competition has on a rising state's threat perceptions. Rising states may be ambitious, but they do not lose sight of their material weakness, the threats they face, or the limits to what they might gain.",
    "advisors": ["Barry R. Posen"],
    "text": "Fear and frustration : rising state perceptions of threats and opportunities Do a dominant state's policies have a greater effect on a rising state's threat perceptions or its assessment of the dominant state's resolve? Existing theory, rooted in Jervis's spiral and deterrence models, contends that the answer depends on whether the state has status quo or revisionist intentions. Rising states are typically seen as revisionist, a type of state that is said to be easily emboldened by conciliation but not easily threatened by competition. This project, on the other hand, argues that rising states - even those with revisionist aims - are more easily threatened than emboldened. Anarchy and uncertainty surrounding the dominant state's intentions give all rising states incentive to be cautious in their assessments. Underestimating threats could leave a rising state more vulnerable to coercion or unprepared for war with a materially stronger dominant state. Rising states, therefore, increase their threat assessments in response to almost any kind of competition by the dominant state. The risks of underestimating the dominant state's resolve are also significant: a resolute dominant state might respond to a challenge with overwhelming force. Therefore, rising states only downgrade their assessments of the dominant state's resolve in the face of very strong signals, such as large, militarily useful concessions. This dissertation tests these competing arguments during periods when Britain was a dominant state facing a rising, revisionist power - the United States (1837-1846) and Wilhelmine Germany (1894-1898). Detailed, historical analysis identified each British policy change and assessed the impact on U.S. and German perceptions. The results suggest that existing theory overstates the risks of conciliating rising states and understates the impact that competition has on a rising state's threat perceptions. Rising states may be ambitious, but they do not lose sight of their material weakness, the threats they face, or the limits to what they might gain."
}, {
    "id": "oai:dspace.mit.edu:1721.1/68930",
    "title": "The political effectiveness of non-state violence : paradox, polarity, and the pursuit of power",
    "abstract": "When is non-state violence politically effective? Existing scholarship suggests that insurgency and terrorism are generally effective or ineffective based on the analysis of unitary non-state coercers operating solely at the strategic level. Although this approach provides useful insight, a failure to account for the internal dynamics of social movements within which armed groups are embedded obscures many of the most important causes and effects of non-state violence. The structuralist theory of non-state violence holds that the structure of power within social movements explains the greatest variation in both the use of violence by armed groups and its political effectiveness. Armed groups pursue common strategic goals that are characterized by collective action challenges against external enemies, such as the founding of a new state, while they simultaneously engage in zero-sum competition for organizational dominance with internal rivals. The central hypothesis of the structuralist theory is that violence is more likely to be strategically effective when employed by a unipolar social movement with one dominant armed group than by a multipolar social movement with two or more significant armed groups. The secondary hypothesis is that the strongest armed group in a social movement is the most likely to pursue strategic goals, whereas weaker groups in the hierarchy are more likely to pursue organizational goals exclusively, to the detriment of the movement. This theory is tested with a longitudinal analysis of 29 groups in 33 campaigns marked by a mix of violent and nonviolent action within the Palestinian, Irish, Zionist, and Algerian national movements. Analysis of primary sources and extensive interviews with key participants and observers help to demonstrate that the hierarchical position of groups within each of the four movements drove their relative focus towards strategic or organizational objectives as well as their associated use or non-use of violence. Furthermore, violence was more politically effective during periods of unipolarity than during periods of multipolarity within each movement. The structuralist theory of non-state violence thus reveals and explains greater variation in the political effectiveness of non-state violence than previous scholarship.",
    "advisors": ["Stephen Van Evera"],
    "text": "The political effectiveness of non-state violence : paradox, polarity, and the pursuit of power When is non-state violence politically effective? Existing scholarship suggests that insurgency and terrorism are generally effective or ineffective based on the analysis of unitary non-state coercers operating solely at the strategic level. Although this approach provides useful insight, a failure to account for the internal dynamics of social movements within which armed groups are embedded obscures many of the most important causes and effects of non-state violence. The structuralist theory of non-state violence holds that the structure of power within social movements explains the greatest variation in both the use of violence by armed groups and its political effectiveness. Armed groups pursue common strategic goals that are characterized by collective action challenges against external enemies, such as the founding of a new state, while they simultaneously engage in zero-sum competition for organizational dominance with internal rivals. The central hypothesis of the structuralist theory is that violence is more likely to be strategically effective when employed by a unipolar social movement with one dominant armed group than by a multipolar social movement with two or more significant armed groups. The secondary hypothesis is that the strongest armed group in a social movement is the most likely to pursue strategic goals, whereas weaker groups in the hierarchy are more likely to pursue organizational goals exclusively, to the detriment of the movement. This theory is tested with a longitudinal analysis of 29 groups in 33 campaigns marked by a mix of violent and nonviolent action within the Palestinian, Irish, Zionist, and Algerian national movements. Analysis of primary sources and extensive interviews with key participants and observers help to demonstrate that the hierarchical position of groups within each of the four movements drove their relative focus towards strategic or organizational objectives as well as their associated use or non-use of violence. Furthermore, violence was more politically effective during periods of unipolarity than during periods of multipolarity within each movement. The structuralist theory of non-state violence thus reveals and explains greater variation in the political effectiveness of non-state violence than previous scholarship."
}, {
    "id": "oai:dspace.mit.edu:1721.1/113491",
    "title": "Local accountability : the role of attribution, institutions, and communication",
    "abstract": "How do people hold local governments accountable? My dissertation shows how cognitive and perceptual biases, as well as electoral institutions and strategic communication, can hinder voters' ability to hold government accountable. I gather data on local politics -- a level of government that people interact with on a daily basis, and one that encompasses the vast majority of elected officials and elections in the United States. My evidence comes from large-scale elections and communications data, surveys, and partnerships with governments and service providers. My findings indicate that widespread confusion around government responsibilities and a cognitive bias favoring recent information shape how voters evaluate government for performance, that election timing can prevent voters from effectively holding their incumbent politicians accountable, and that strategic communication by municipal governments can further bias the balance of information that citizens rely on to judge government. Together, these papers demonstrate how three facets of politics can frustrate accountability in cities. This work contributes to theoretical knowledge on political behavior and political institutions, as well as the urban politics literature, and does so using three independent sources of data that provide fertile ground for future extensions of this work.",
    "advisors": ["Adam J. Berinsky"],
    "text": "Local accountability : the role of attribution, institutions, and communication How do people hold local governments accountable? My dissertation shows how cognitive and perceptual biases, as well as electoral institutions and strategic communication, can hinder voters' ability to hold government accountable. I gather data on local politics -- a level of government that people interact with on a daily basis, and one that encompasses the vast majority of elected officials and elections in the United States. My evidence comes from large-scale elections and communications data, surveys, and partnerships with governments and service providers. My findings indicate that widespread confusion around government responsibilities and a cognitive bias favoring recent information shape how voters evaluate government for performance, that election timing can prevent voters from effectively holding their incumbent politicians accountable, and that strategic communication by municipal governments can further bias the balance of information that citizens rely on to judge government. Together, these papers demonstrate how three facets of politics can frustrate accountability in cities. This work contributes to theoretical knowledge on political behavior and political institutions, as well as the urban politics literature, and does so using three independent sources of data that provide fertile ground for future extensions of this work."
}, {
    "id": "oai:dspace.mit.edu:1721.1/28207",
    "title": "Banking on the environment : multilateral development banks and environmental policymaking in Central and Eastern Europe",
    "abstract": "The dissertation is a comparative study of three multilateral development banks (MDBs)-the World Bank, European Bank for Reconstruction and Development (EBRD), and European Investment Bank (EIB)-and their struggles to operationalize and implement relatively new environmental mandates. MDBs are increasingly being relied upon to address environmental issues in their activities, while at the same time facing criticism for allegedly promoting serious environmental degradation in borrowing countries. The dissertation focuses on the activities of these banks in Central and Eastern Europe, where the fall of the Iron Curtain revealed the most polluted countries in Europe, and where these MDBs are among the top donors. There is significant variation in the degree to which these MDBs have incorporated environmental goals into their work. The World Bank has played an important role in providing policy support for environmental reform in the region, while financing the largest scope of \"green\" projects of the three banks. The EIB has responded to its environmental goals in minimal ways, and the EBRD has an intermediate position between the other two. I argue that external pressure from major shareholder countries, usually supported or pushed by NGOs, is a key factor determining the depth of an MDB's commitment to new mandates, such as the environment. However, shareholder commitment is a necessary but not sufficient condition in explaining the banks' environmental behavior. Governance structures for all three banks are diffuse, and, as a result, institutional design and incentive systems play critical roles in how environmental objectives are translated into activities. In all three cases, the banks' internal incentive systems are poorly aligned with their environmental goals, and even where institutional variables are structured to promote greater awareness of environmental issues within the banks, they do not always work as envisioned. Theoretically, the dissertation argues that different causal variables matter at different stages of the policy process. Neorealist approaches have the most explanatory power in accounting for how environmental ideas are brought to the MDBs, but are insufficient in explaining outcomes. Approaches drawn from institutionalist and organizational theories, in turn, provide guidance in analyzing the mechanisms by which environmental objectives are translated into practice. The argument calls for a better integration of international relations theories emphasizing the importance of shareholder politics with theories that focus on how institutional arrangements shape behavior.",
    "advisors": ["Kenneth Oye"],
    "text": "Banking on the environment : multilateral development banks and environmental policymaking in Central and Eastern Europe The dissertation is a comparative study of three multilateral development banks (MDBs)-the World Bank, European Bank for Reconstruction and Development (EBRD), and European Investment Bank (EIB)-and their struggles to operationalize and implement relatively new environmental mandates. MDBs are increasingly being relied upon to address environmental issues in their activities, while at the same time facing criticism for allegedly promoting serious environmental degradation in borrowing countries. The dissertation focuses on the activities of these banks in Central and Eastern Europe, where the fall of the Iron Curtain revealed the most polluted countries in Europe, and where these MDBs are among the top donors. There is significant variation in the degree to which these MDBs have incorporated environmental goals into their work. The World Bank has played an important role in providing policy support for environmental reform in the region, while financing the largest scope of \"green\" projects of the three banks. The EIB has responded to its environmental goals in minimal ways, and the EBRD has an intermediate position between the other two. I argue that external pressure from major shareholder countries, usually supported or pushed by NGOs, is a key factor determining the depth of an MDB's commitment to new mandates, such as the environment. However, shareholder commitment is a necessary but not sufficient condition in explaining the banks' environmental behavior. Governance structures for all three banks are diffuse, and, as a result, institutional design and incentive systems play critical roles in how environmental objectives are translated into activities. In all three cases, the banks' internal incentive systems are poorly aligned with their environmental goals, and even where institutional variables are structured to promote greater awareness of environmental issues within the banks, they do not always work as envisioned. Theoretically, the dissertation argues that different causal variables matter at different stages of the policy process. Neorealist approaches have the most explanatory power in accounting for how environmental ideas are brought to the MDBs, but are insufficient in explaining outcomes. Approaches drawn from institutionalist and organizational theories, in turn, provide guidance in analyzing the mechanisms by which environmental objectives are translated into practice. The argument calls for a better integration of international relations theories emphasizing the importance of shareholder politics with theories that focus on how institutional arrangements shape behavior."
}, {
    "id": "oai:dspace.mit.edu:1721.1/118197",
    "title": "Essays on the behavioral political economy of housing",
    "abstract": "This dissertation examines the ways in which housing markets shape and are shaped by the political decisions that citizens make, as well as the political beliefs that they hold. It contributes to theoretical knowledge on the political economy of urban development and housing by revisiting existing debates through a behavioralist lens. The first paper develops the theory that a noticeable change in the built environment serves as a reminder to vote when housing issues are salient. I analyze turnout in the 2015 San Francisco municipal election, and show that voters who lived in the neighborhood of infill development projects that began construction just before the election were 3 to 4 percentage points more likely to vote than those who lived near projects that began construction after the election. The second paper explores how localism, the belief that the interests of established members of the local community trump those of newcomers and outsiders, and liberalism, a preference for egalitarian norms, jointly shape attitudes toward housing growth. I use a novel survey instrument and rich observational data on land use ballot measures in San Francisco to measure these two dimensions of political ideology, and document that localism is negatively associated with support for development projects, whereas the correlation between liberalism and support for development is moderated by features of the development. The third paper proposes the status quo bias hypothesis, which predicts that housing wealth increases preference for status quo arrangements with respect to Social Security. The hypothesis is tested using a survey experiment that induces different home price expectations among respondents, as well as data from the 2000-2004 American national Election Studies panel.",
    "advisors": ["Kathleen Thelen"],
    "text": "Essays on the behavioral political economy of housing This dissertation examines the ways in which housing markets shape and are shaped by the political decisions that citizens make, as well as the political beliefs that they hold. It contributes to theoretical knowledge on the political economy of urban development and housing by revisiting existing debates through a behavioralist lens. The first paper develops the theory that a noticeable change in the built environment serves as a reminder to vote when housing issues are salient. I analyze turnout in the 2015 San Francisco municipal election, and show that voters who lived in the neighborhood of infill development projects that began construction just before the election were 3 to 4 percentage points more likely to vote than those who lived near projects that began construction after the election. The second paper explores how localism, the belief that the interests of established members of the local community trump those of newcomers and outsiders, and liberalism, a preference for egalitarian norms, jointly shape attitudes toward housing growth. I use a novel survey instrument and rich observational data on land use ballot measures in San Francisco to measure these two dimensions of political ideology, and document that localism is negatively associated with support for development projects, whereas the correlation between liberalism and support for development is moderated by features of the development. The third paper proposes the status quo bias hypothesis, which predicts that housing wealth increases preference for status quo arrangements with respect to Social Security. The hypothesis is tested using a survey experiment that induces different home price expectations among respondents, as well as data from the 2000-2004 American national Election Studies panel."
}, {
    "id": "oai:dspace.mit.edu:1721.1/28498",
    "title": "Pathogens as weapons : the international security implications of biological warfare",
    "abstract": "This dissertation assesses the international security implications of biological weapons and the strategic consequences of their proliferation. It examines the impact of biological weapons on four key areas of concern for international security: proliferation, deterrence, civil-military relations, and threat assessment. The dissertation draws upon a range of theories from the field of security studies and a wealth of newly available information regarding the biological weapons programs of Iraq, the former Soviet Union, the United States, United Kingdom, and South Africa. My analysis yields four major findings. First, it is extremely difficult to prevent the spread of biological warfare capabilities to actors that want them and these actors tend to be motivated by a desire to challenge the status quo. Contrary to conventional wisdom, biological weapons have utility across the spectrum of conflict and are well suited to supporting asymmetric strategies against stronger opponents. Second, biological weapons do not confer the deterrent benefits associated with nuclear weapons and will undermine reliance on deterrence as a security strategy. Biological weapons are not suitable as strategic deterrents due to the uncertainty regarding their effects, the availability of defenses and the reliance of these weapons on secrecy and surprise for their effectiveness. The accessibility of these weapons to a diverse range of actors, including terrorists, and the ease of clandestine attacks undermines the effectiveness of deterrence as a security strategy. Third, civilian oversight of biological warfare programs is hindered by the intense secrecy that shrouds these programs. This lack of supervision leads to abuse and corruption by",
    "advisors": ["Stephen Van Evera"],
    "text": "Pathogens as weapons : the international security implications of biological warfare This dissertation assesses the international security implications of biological weapons and the strategic consequences of their proliferation. It examines the impact of biological weapons on four key areas of concern for international security: proliferation, deterrence, civil-military relations, and threat assessment. The dissertation draws upon a range of theories from the field of security studies and a wealth of newly available information regarding the biological weapons programs of Iraq, the former Soviet Union, the United States, United Kingdom, and South Africa. My analysis yields four major findings. First, it is extremely difficult to prevent the spread of biological warfare capabilities to actors that want them and these actors tend to be motivated by a desire to challenge the status quo. Contrary to conventional wisdom, biological weapons have utility across the spectrum of conflict and are well suited to supporting asymmetric strategies against stronger opponents. Second, biological weapons do not confer the deterrent benefits associated with nuclear weapons and will undermine reliance on deterrence as a security strategy. Biological weapons are not suitable as strategic deterrents due to the uncertainty regarding their effects, the availability of defenses and the reliance of these weapons on secrecy and surprise for their effectiveness. The accessibility of these weapons to a diverse range of actors, including terrorists, and the ease of clandestine attacks undermines the effectiveness of deterrence as a security strategy. Third, civilian oversight of biological warfare programs is hindered by the intense secrecy that shrouds these programs. This lack of supervision leads to abuse and corruption by"
}, {
    "id": "oai:dspace.mit.edu:1721.1/42391",
    "title": "Ethnic leftists, populist ethnics : the new politics of identity",
    "abstract": "Group identifications - in particular, those based on ethnicity and class - are central to political mobilization during elections. This dissertation asks: when and why does the salience of ethnic and class categories vary across elections in emerging democracies? It argues that which categories are politicized has less to do with which categories are most salient to voters and more to do with which are most useful to politicians. The strategies of politicians, however, are contrained in a particular ways, by opportunity, which is provided by party system crises, and by the political space, which is given by the structure of existing social identity categories, particularly their sizes and degrees of overlap with traditionally-politicized categories. Given the institutional rules, size and overlap affect which identity groups have the numbers to win and which describe similar constituencies that could be switched between for political expediency. The project nests the theory within an explanatory framework describing four key factors that drive variation in identification: voter preferences, political institutions, party institutions, and elite manipulation. The dissertation presents data from three sources: a fieldwork-based study of Bolivian party politics, focusing on the democratic period from 1982 to 2005; data from the \"Constructivist Dataset on Ethnicity and Institutions (CDEI)\" on political parties and elections in Latin America in the early 1990s; and four shadow cases from the Andean region (Colombia, Ecuador, Peru, and Venezuela). These data are used to map variation in identification across countries and over time; to illustrate the plausibility of the argument and to test it against predictions drawn from alternative hypotheses; and to explore the generalizability of the argument.",
    "advisors": ["Roger D. Petersen"],
    "text": "Ethnic leftists, populist ethnics : the new politics of identity Group identifications - in particular, those based on ethnicity and class - are central to political mobilization during elections. This dissertation asks: when and why does the salience of ethnic and class categories vary across elections in emerging democracies? It argues that which categories are politicized has less to do with which categories are most salient to voters and more to do with which are most useful to politicians. The strategies of politicians, however, are contrained in a particular ways, by opportunity, which is provided by party system crises, and by the political space, which is given by the structure of existing social identity categories, particularly their sizes and degrees of overlap with traditionally-politicized categories. Given the institutional rules, size and overlap affect which identity groups have the numbers to win and which describe similar constituencies that could be switched between for political expediency. The project nests the theory within an explanatory framework describing four key factors that drive variation in identification: voter preferences, political institutions, party institutions, and elite manipulation. The dissertation presents data from three sources: a fieldwork-based study of Bolivian party politics, focusing on the democratic period from 1982 to 2005; data from the \"Constructivist Dataset on Ethnicity and Institutions (CDEI)\" on political parties and elections in Latin America in the early 1990s; and four shadow cases from the Andean region (Colombia, Ecuador, Peru, and Venezuela). These data are used to map variation in identification across countries and over time; to illustrate the plausibility of the argument and to test it against predictions drawn from alternative hypotheses; and to explore the generalizability of the argument."
}, {
    "id": "oai:dspace.mit.edu:1721.1/59795",
    "title": "Ballot box and tinder box : can electoral engineering save multiethnic democracy?",
    "abstract": "The objective of this dissertation is to systematize the existing hypotheses in the electoral engineering literature and to test them in a set of selected case studies in order to answer a central question: does the electoral system affect the structure of political parties in ethnically divided societies and if so how? The academic debate on electoral design for divided societies has focused on the impact of institutional choices on ultimate conflict outcomes. The findings of previous studies have been generally inconclusive, while the lack of sub-national data on ethnic composition and voting patterns has made it difficult to examine mechanisms regarding the role of demographics. To approach the problem from a different angle, I propose a research design focusing on the intermediate link from electoral institutions to the ethnic structure of the party system. For the empirical portion of my work, I chose to conduct a structured historical comparison of four societies which implemented major electoral reforms: Turkey, Northern Ireland, Guyana, and Sri Lanka. Based on the study of these cases, I am arguing that politicians and voters have not responded to electoral incentives in the ways predicted by existent theories, and that no clear relationship can be observed between the electoral system's proportionality, the heterogeneity of electoral constituencies, and the number of parties or the types of ethnic appeals they make to voters. These findings indicate that the hopes placed in electoral system design for divided societies are unwarranted and that attention among political scientists and policymakers should shift to other peace-building approaches.",
    "advisors": ["Roger Petersen"],
    "text": "Ballot box and tinder box : can electoral engineering save multiethnic democracy? The objective of this dissertation is to systematize the existing hypotheses in the electoral engineering literature and to test them in a set of selected case studies in order to answer a central question: does the electoral system affect the structure of political parties in ethnically divided societies and if so how? The academic debate on electoral design for divided societies has focused on the impact of institutional choices on ultimate conflict outcomes. The findings of previous studies have been generally inconclusive, while the lack of sub-national data on ethnic composition and voting patterns has made it difficult to examine mechanisms regarding the role of demographics. To approach the problem from a different angle, I propose a research design focusing on the intermediate link from electoral institutions to the ethnic structure of the party system. For the empirical portion of my work, I chose to conduct a structured historical comparison of four societies which implemented major electoral reforms: Turkey, Northern Ireland, Guyana, and Sri Lanka. Based on the study of these cases, I am arguing that politicians and voters have not responded to electoral incentives in the ways predicted by existent theories, and that no clear relationship can be observed between the electoral system's proportionality, the heterogeneity of electoral constituencies, and the number of parties or the types of ethnic appeals they make to voters. These findings indicate that the hopes placed in electoral system design for divided societies are unwarranted and that attention among political scientists and policymakers should shift to other peace-building approaches."
}, {
    "id": "oai:dspace.mit.edu:1721.1/16800",
    "title": "Between global flows & territorial control : the state, tourism development, and the politics of reterritorialization in the Middle East",
    "abstract": "This dissertation presents a new type of theory about the impact of increased transnational capital flows on state building processes. Most of the political science literature about globalization has been focused on debating the question of deterritorialization where the powers of territorial nation-states are viewed as being eroded by the increased transnational mobility of capital. This dissertation, in contrast, demonstrates how international tourism development-like many other aspects of globalization--can also produce \"reterritorialization\" characterized by the increased relevance of \"place\" for global economic activity. With increased globalization firms often seek to locate themselves in specific territories in order to capture what I call locational rents and external economies. In contrast to the dislocating effects of deterritorialization, reterritorialization can increase the political influence of state, societal, and transnational agents able to create localities that generate locational rents and external economies. The dissertation explores how states in the Middle East have promoted reterritorialization through tourism development in an attempt to enhance their control over capital and cultural flows as they promote economic liberalization and the incorporation of their economies into global markets. My fieldwork explored two national case studies with different configurations of territorial control. In Tunisia, I found that centralized state control over the territorially defined resources and institutions driving the reterritorialization process allowed the state to extend its control over transnational capital flows, the local private sector,",
    "advisors": ["Nazli Choucri"],
    "text": "Between global flows & territorial control : the state, tourism development, and the politics of reterritorialization in the Middle East This dissertation presents a new type of theory about the impact of increased transnational capital flows on state building processes. Most of the political science literature about globalization has been focused on debating the question of deterritorialization where the powers of territorial nation-states are viewed as being eroded by the increased transnational mobility of capital. This dissertation, in contrast, demonstrates how international tourism development-like many other aspects of globalization--can also produce \"reterritorialization\" characterized by the increased relevance of \"place\" for global economic activity. With increased globalization firms often seek to locate themselves in specific territories in order to capture what I call locational rents and external economies. In contrast to the dislocating effects of deterritorialization, reterritorialization can increase the political influence of state, societal, and transnational agents able to create localities that generate locational rents and external economies. The dissertation explores how states in the Middle East have promoted reterritorialization through tourism development in an attempt to enhance their control over capital and cultural flows as they promote economic liberalization and the incorporation of their economies into global markets. My fieldwork explored two national case studies with different configurations of territorial control. In Tunisia, I found that centralized state control over the territorially defined resources and institutions driving the reterritorialization process allowed the state to extend its control over transnational capital flows, the local private sector,"
}, {
    "id": "oai:dspace.mit.edu:1721.1/8244",
    "title": "Mobilizing and sustaining grassroots activism in the National Organization for Women, 1966-2000 : by Maryann Barakso.",
    "abstract": "This dissertation examines how the National Organization for Women (NOW) survived the vagaries of both the political environment and its intraorganizational problems and controversies over its thirty-five year history. It considers the role that patrons, the state, mobilizing structures, leaders, organizational structure, strategic flexibility, and collective identity played in NOW's creation and maintenance. Little support is found for the role of patrons or of the state in NOW's origination and sustenance. Mobilizing structures, in the form of social networks, however, proved crucial factors supporting NOW between 1966 and 1971, its founding period. NOW's organizational structure, particularly its (limited) professionalization and its federalization both assisted NOW in overcoming potentially crippling information gaps between members and leaders. However, federalization has not had an entirely benign effect. In addition to allowing a great deal of autonomy, NOW's federal structure also permitted the development of strong intraorganizational factions. Leaders positively influenced organizational stability by enhancing NOW's collective identity and by stewarding the group towards new strategies. However, NOW's strong identity acts as a constraint upon leaders' ability to change the organization's goals or tactical approach. NOW's longevity and institutionalization over time suggests a second set of issues which are examined in this study. How has NOW's aging affected the organization's attention to its founding principles?",
    "advisors": ["Stephen Ansolabehere"],
    "text": "Mobilizing and sustaining grassroots activism in the National Organization for Women, 1966-2000 : by Maryann Barakso. This dissertation examines how the National Organization for Women (NOW) survived the vagaries of both the political environment and its intraorganizational problems and controversies over its thirty-five year history. It considers the role that patrons, the state, mobilizing structures, leaders, organizational structure, strategic flexibility, and collective identity played in NOW's creation and maintenance. Little support is found for the role of patrons or of the state in NOW's origination and sustenance. Mobilizing structures, in the form of social networks, however, proved crucial factors supporting NOW between 1966 and 1971, its founding period. NOW's organizational structure, particularly its (limited) professionalization and its federalization both assisted NOW in overcoming potentially crippling information gaps between members and leaders. However, federalization has not had an entirely benign effect. In addition to allowing a great deal of autonomy, NOW's federal structure also permitted the development of strong intraorganizational factions. Leaders positively influenced organizational stability by enhancing NOW's collective identity and by stewarding the group towards new strategies. However, NOW's strong identity acts as a constraint upon leaders' ability to change the organization's goals or tactical approach. NOW's longevity and institutionalization over time suggests a second set of issues which are examined in this study. How has NOW's aging affected the organization's attention to its founding principles?"
}, {
    "id": "oai:dspace.mit.edu:1721.1/8241",
    "title": "Democracy and city life",
    "abstract": "The deliberative turn in recent political theory ties legitimacy to public deliberation about reasons, offered for and against exercises of authority by informed and sincere citizens, who share a commitment to finding mutually acceptable terms of social cooperation. But even such reasonable citizens may disagree on important matters, and some citizens will only rarely, if ever, see their sincere reasoned judgements reflected in democratic outcomes. I argue that, to be widely perceived as legitimate in plural settings, fair deliberative procedures must not only be inclusive and self-evidently grounded in a commitment to reasonableness and mutual respect; they must further ensure that dissenting parties have a reasonable expectation of eventually transforming features of the public sphere to better accommodate their distinctive values and interests. The result is a fair deliberative pluralism that reflects the cacophonous and variegated character of the public sphere in modern democracies. But I caution that this ideal requires conditions that can sustain spatial patterns of wealth and control over land uses that undermine the interests of certain spatially fixed groups. I draw on the experiences of U.S. cities to illustrate this tension. These cities feature profound and enduring inequalities of wealth and political influence, and urbanization generates patterns of industry and habitat that reinforce these inequalities. Municipal and state politics rarely alter the prevailing incentives for home and industry location that perpetuate these patterns,",
    "advisors": ["Joshua Cohen"],
    "text": "Democracy and city life The deliberative turn in recent political theory ties legitimacy to public deliberation about reasons, offered for and against exercises of authority by informed and sincere citizens, who share a commitment to finding mutually acceptable terms of social cooperation. But even such reasonable citizens may disagree on important matters, and some citizens will only rarely, if ever, see their sincere reasoned judgements reflected in democratic outcomes. I argue that, to be widely perceived as legitimate in plural settings, fair deliberative procedures must not only be inclusive and self-evidently grounded in a commitment to reasonableness and mutual respect; they must further ensure that dissenting parties have a reasonable expectation of eventually transforming features of the public sphere to better accommodate their distinctive values and interests. The result is a fair deliberative pluralism that reflects the cacophonous and variegated character of the public sphere in modern democracies. But I caution that this ideal requires conditions that can sustain spatial patterns of wealth and control over land uses that undermine the interests of certain spatially fixed groups. I draw on the experiences of U.S. cities to illustrate this tension. These cities feature profound and enduring inequalities of wealth and political influence, and urbanization generates patterns of industry and habitat that reinforce these inequalities. Municipal and state politics rarely alter the prevailing incentives for home and industry location that perpetuate these patterns,"
}, {
    "id": "oai:dspace.mit.edu:1721.1/113490",
    "title": "Electoral participation in a new democracy : essays on voting in the new South Africa",
    "abstract": "Electoral participation is at the heart of democratic politics. Who comes to the polls, who does not, and why? In three essays that leverage natural experimental research designs, I advance new explanations for individual turnout, focused on citizens' access to, and experiences with the core electoral institutions of democracy. In paper 1 I show that policies focused on increasing access to elections can increase the size of the electorate, but also that they may carry compositional costs. Using new administrative data from South Africa and a difference-in-differences design, I show that the Independent Electoral Commission's large scale expansion of access to voting stations has increased national turnout by between 2.3 and 4.7 percentage points over the period 1999 to 2014. I then demonstrate, in the context of an administrative quasi-experiment, that those of high socio-economic status and those who are older are much more sensitive to electoral access than others. In paper 2 I explore whether differential representation in government shapes political behavior. I leverage the fact that the size of local government councils in South Africa follows a population-based formula with cutoffs that alter the degree of local representation. The results from a regression kink design suggest that at the margin the degree of representation is not particularly important for informing voters' decision to vote. Finally, paper 3 considers the effect of voting in South Africa's first democratic election in 1994 on future voting, and present evidence of the lasting behavioral effects of past participation using a regression discontinuity design. Eligibility to participate in 1994 affects future voting by 3 percentage points, with an average treatment effect of actually voting between 3.5 and 8.5 percentage points. I argue that persistence (or habituation) in voting behavior is at least partly driven by the creation of associations between first time voting and positive emotional states. If those who have positive electoral experiences are more likely to be particular types of voters, this can influence the trajectory of democracy.",
    "advisors": ["Adam J. Berinsky"],
    "text": "Electoral participation in a new democracy : essays on voting in the new South Africa Electoral participation is at the heart of democratic politics. Who comes to the polls, who does not, and why? In three essays that leverage natural experimental research designs, I advance new explanations for individual turnout, focused on citizens' access to, and experiences with the core electoral institutions of democracy. In paper 1 I show that policies focused on increasing access to elections can increase the size of the electorate, but also that they may carry compositional costs. Using new administrative data from South Africa and a difference-in-differences design, I show that the Independent Electoral Commission's large scale expansion of access to voting stations has increased national turnout by between 2.3 and 4.7 percentage points over the period 1999 to 2014. I then demonstrate, in the context of an administrative quasi-experiment, that those of high socio-economic status and those who are older are much more sensitive to electoral access than others. In paper 2 I explore whether differential representation in government shapes political behavior. I leverage the fact that the size of local government councils in South Africa follows a population-based formula with cutoffs that alter the degree of local representation. The results from a regression kink design suggest that at the margin the degree of representation is not particularly important for informing voters' decision to vote. Finally, paper 3 considers the effect of voting in South Africa's first democratic election in 1994 on future voting, and present evidence of the lasting behavioral effects of past participation using a regression discontinuity design. Eligibility to participate in 1994 affects future voting by 3 percentage points, with an average treatment effect of actually voting between 3.5 and 8.5 percentage points. I argue that persistence (or habituation) in voting behavior is at least partly driven by the creation of associations between first time voting and positive emotional states. If those who have positive electoral experiences are more likely to be particular types of voters, this can influence the trajectory of democracy."
}, {
    "id": "oai:dspace.mit.edu:1721.1/117449",
    "title": "Recruitment, rhetoric and reform : new labour's politicians and the transformation of British welfare provision",
    "abstract": "In the 1960s, the UK had some of the most generous welfare provision in the world, assisting people 'from the cradle to the grave', in the words of its designer William Beveridge. Widely supported by voters and politicians of all stripes, it remained largely intact into the early 1980s. Yet since then, the benefits system has been radically transformed into one of the developed world's least generous, with major implications for poverty and social cohesion. Public opinion has also turned against it to a degree that is unmatched anywhere else. Until recently, both major parties largely embraced the new settlement, using increasingly harsh rhetoric to describe welfare - and its users. Looking at welfare programs that provide relief from unemployment, poverty, and disability, I ask why this transformation occurred. I offer an explicitly political and top-down explanation, focusing on the role of party competition and a large change in the composition of the UK's Labour party, which originally set up the welfare state. As it increasingly recruited legislators from outside of the working-class, both its stance and rhetoric on welfare reform shifted dramatically. I show that this rhetoric ultimately turned the British public into welfare skeptics who are willing to endorse far-reaching retrenchment. Hence this case study offers a cautionary tale of how the political coalitions underpinning social policy can quickly unravel. Political and popular support for welfare provision is by no means guaranteed, even in an era of rising insecurity and inequality, particularly as social democratic parties become increasingly unrecognizable compared to their working-class roots, and welfare is subjected to means-testing, drawing lines between recipients and taxpayers. This thesis includes six chapters, and uses a database I have assembled of every speech made about welfare issues in the British Parliament from 1987-2015, together with a wealth of public opinion data. It combines historical accounts, computational and qualitative text analysis, and quantitative observational and experimental evidence to explain how British welfare provision, rhetoric and public opinion were all transformed in the space of a single generation.",
    "advisors": ["Kathleen Thelen"],
    "text": "Recruitment, rhetoric and reform : new labour's politicians and the transformation of British welfare provision In the 1960s, the UK had some of the most generous welfare provision in the world, assisting people 'from the cradle to the grave', in the words of its designer William Beveridge. Widely supported by voters and politicians of all stripes, it remained largely intact into the early 1980s. Yet since then, the benefits system has been radically transformed into one of the developed world's least generous, with major implications for poverty and social cohesion. Public opinion has also turned against it to a degree that is unmatched anywhere else. Until recently, both major parties largely embraced the new settlement, using increasingly harsh rhetoric to describe welfare - and its users. Looking at welfare programs that provide relief from unemployment, poverty, and disability, I ask why this transformation occurred. I offer an explicitly political and top-down explanation, focusing on the role of party competition and a large change in the composition of the UK's Labour party, which originally set up the welfare state. As it increasingly recruited legislators from outside of the working-class, both its stance and rhetoric on welfare reform shifted dramatically. I show that this rhetoric ultimately turned the British public into welfare skeptics who are willing to endorse far-reaching retrenchment. Hence this case study offers a cautionary tale of how the political coalitions underpinning social policy can quickly unravel. Political and popular support for welfare provision is by no means guaranteed, even in an era of rising insecurity and inequality, particularly as social democratic parties become increasingly unrecognizable compared to their working-class roots, and welfare is subjected to means-testing, drawing lines between recipients and taxpayers. This thesis includes six chapters, and uses a database I have assembled of every speech made about welfare issues in the British Parliament from 1987-2015, together with a wealth of public opinion data. It combines historical accounts, computational and qualitative text analysis, and quantitative observational and experimental evidence to explain how British welfare provision, rhetoric and public opinion were all transformed in the space of a single generation."
}, {
    "id": "oai:dspace.mit.edu:1721.1/72850",
    "title": "The grand strategies of rising powers: reassurance, coercion, and balancing responses",
    "abstract": "This dissertation asks: what explains variation in how other great powers respond to rising powers? It tries to explain why the emergence of a rising power sometimes leads to tension, rivalry, and war, and other times leads to less competitive responses. This project analyzes the effect of the rising power's grand strategy-whether it is reassurance or coercion--on the severity of the balancing response by the other major powers. I develop a theory of successful reassurance that shows how a rising power can prevent or minimize the severity of the balancing response by other great powers. Reassurance can limit the balancing response through two causal mechanisms: 1) reduced estimates that rising power is a threat; and 2) reaping the benefits from a rising power. I also develop a theory of coercion backfire that shows how a rising power that implements a grand strategy of coercion is more likely to make others feel especially threatened, and therefore more likely to provoke an early and especially firm response, exacerbating the severity of the balancing response. I apply this theory to explain the balancing responses to the rise of Germany from 1871 to 1907 and the rise of China in the post-Cold War world. The empirical tests and process tracing evidence demonstrate that rising powers, contrary to the expectations of most realist balance of power and rationalist accounts, have considerable agency to affect the balancing response. In the cases of the rising powers of contemporary China and Bismarckian Germany, grand strategies of reassurance convinced states to minimize the severity of their balancing responses, even as the rising power's material power continued to grow. In contrast, Wilhelmine Germany's grand strategy of coercion antagonized the other powers and pushed them to respond by balancing very severely. For the contemporary case of the rise of China, I use a variety of sources such as Chinese-language materials and extensive interviews from over two years of field work in China and Asia to examine China's grand strategy of reassurance and its effect on the responses by the United States, Japan, Russia, and India.",
    "advisors": ["Stephen Van Evera"],
    "text": "The grand strategies of rising powers: reassurance, coercion, and balancing responses This dissertation asks: what explains variation in how other great powers respond to rising powers? It tries to explain why the emergence of a rising power sometimes leads to tension, rivalry, and war, and other times leads to less competitive responses. This project analyzes the effect of the rising power's grand strategy-whether it is reassurance or coercion--on the severity of the balancing response by the other major powers. I develop a theory of successful reassurance that shows how a rising power can prevent or minimize the severity of the balancing response by other great powers. Reassurance can limit the balancing response through two causal mechanisms: 1) reduced estimates that rising power is a threat; and 2) reaping the benefits from a rising power. I also develop a theory of coercion backfire that shows how a rising power that implements a grand strategy of coercion is more likely to make others feel especially threatened, and therefore more likely to provoke an early and especially firm response, exacerbating the severity of the balancing response. I apply this theory to explain the balancing responses to the rise of Germany from 1871 to 1907 and the rise of China in the post-Cold War world. The empirical tests and process tracing evidence demonstrate that rising powers, contrary to the expectations of most realist balance of power and rationalist accounts, have considerable agency to affect the balancing response. In the cases of the rising powers of contemporary China and Bismarckian Germany, grand strategies of reassurance convinced states to minimize the severity of their balancing responses, even as the rising power's material power continued to grow. In contrast, Wilhelmine Germany's grand strategy of coercion antagonized the other powers and pushed them to respond by balancing very severely. For the contemporary case of the rise of China, I use a variety of sources such as Chinese-language materials and extensive interviews from over two years of field work in China and Asia to examine China's grand strategy of reassurance and its effect on the responses by the United States, Japan, Russia, and India."
}, {
    "id": "oai:dspace.mit.edu:1721.1/54600",
    "title": "And the truth shall make you free : the international norm of truth-seeking",
    "abstract": "The theoretical question this dissertation addresses is how do international norms emerge and spread. The theory I propose focuses on changes in the ideational content of norms and on the international agents who facilitate these changes. In the norm emergence stage, the theory's first explanatory variable is a successful precedent, which provides an important point of reference for the ideas associated with the practice. The second explanatory variable is an active epistemic community, which is a committed network of professionals who are strongly attached to the practice and to its rationales, and who actively advance the practice. The third variable is change in the content of the norm, which reflects the assessment of why a specific practice is positive or good and what is it good for or for whom. In the norm cascading stage, the theory proposes two necessary processes: the international institutionalization of the practice; and the emergence of new international expectations and incentives that motivate state leaders to act in accordance with the norm. In the dissertation I utilize my theory to explain the worldwide proliferation of truth and reconciliation commissions. I argue that in the last decade truth and reconciliation commissions and the truth-seeking principle they endorse have emerged and become institutionalized as an international norm. My research traces the emergence of this norm to the Transitional Justice epistemic community, which was consolidated during and after the South African TRC.",
    "advisors": ["Stephen Van Evera"],
    "text": "And the truth shall make you free : the international norm of truth-seeking The theoretical question this dissertation addresses is how do international norms emerge and spread. The theory I propose focuses on changes in the ideational content of norms and on the international agents who facilitate these changes. In the norm emergence stage, the theory's first explanatory variable is a successful precedent, which provides an important point of reference for the ideas associated with the practice. The second explanatory variable is an active epistemic community, which is a committed network of professionals who are strongly attached to the practice and to its rationales, and who actively advance the practice. The third variable is change in the content of the norm, which reflects the assessment of why a specific practice is positive or good and what is it good for or for whom. In the norm cascading stage, the theory proposes two necessary processes: the international institutionalization of the practice; and the emergence of new international expectations and incentives that motivate state leaders to act in accordance with the norm. In the dissertation I utilize my theory to explain the worldwide proliferation of truth and reconciliation commissions. I argue that in the last decade truth and reconciliation commissions and the truth-seeking principle they endorse have emerged and become institutionalized as an international norm. My research traces the emergence of this norm to the Transitional Justice epistemic community, which was consolidated during and after the South African TRC."
}, {
    "id": "oai:dspace.mit.edu:1721.1/28757",
    "title": "Explaining ethnic conflict in the South Caucasus : Mountainous Karabagh, Abkhazia, and South Ossetia",
    "abstract": "(cont.) the USSR and finds that a focus on opportunity provides the best explanation for the presence or absence of mass mobilization. Finally, the dissertation argues that conventional state security concerns best explain the start of escalation. Union republic opponents, Azerbaijanis and Georgians, perceived regional mobilization to be manifestations of broader \"interstate\" conflicts pitting Azerbaijan and Georgia against, respectively, Armenia and Russia. They did not consider the actions of regional groups to be a product of group insecurities. The dissertation concludes by applying the above findings to the practice of conflict resolution.",
    "advisors": ["Stephen Van Evera"],
    "text": "Explaining ethnic conflict in the South Caucasus : Mountainous Karabagh, Abkhazia, and South Ossetia (cont.) the USSR and finds that a focus on opportunity provides the best explanation for the presence or absence of mass mobilization. Finally, the dissertation argues that conventional state security concerns best explain the start of escalation. Union republic opponents, Azerbaijanis and Georgians, perceived regional mobilization to be manifestations of broader \"interstate\" conflicts pitting Azerbaijan and Georgia against, respectively, Armenia and Russia. They did not consider the actions of regional groups to be a product of group insecurities. The dissertation concludes by applying the above findings to the practice of conflict resolution."
}, {
    "id": "oai:dspace.mit.edu:1721.1/95550",
    "title": "An emerging architecture of local experimentalist governance in China : a study of local innovations in Baoding, 1992-2012",
    "abstract": "What are the conditions under which local actors are more likely to carry out policy innovations that produce environmental or social benefits in local China? Previous studies on \"Chinese experimentalism\" suggest that local innovations in contemporary China stem from its ability to couple reflexive central coordination and decentralized local policy experiments. The strength of this \"closed\" architecture of experimentalist governance lies on the underlying principal-agent distributive bargaining principles between the center and the local, i.e., career prospects, preferential policies, and so on. All of the major mechanisms, processes, and procedures are confined to the center-local dynamics in the political hierarchy. Building on more than two years of site-intensive fieldwork, this dissertation reveals a distinct architecture of experimentalist governance emerging in local China that does not exclusively depend on the requirements of central coordination and formal incentives in the hierarchy. In its basic form, certain local government actors are increasingly opening themselves up and constructing and immersing themselves in communities of practice with extra-local and non-state actors across different levels and sectors. But the uniqueness of this governance architecture lies not only on its form or ostensible structure, but also on its modalities and substance. In those communities, the protagonists enact a recursive process of joint goal-setting, interpretation of policy problems, search and discovery, and mutual learning. In the course of this process, they collectively set achievable provisional \"frameworks\" (kuangfia) or \"platforms\" (pingtai), which are continually recalibrated going forward. That is, the protagonists in this governance architecture continuously reflect on and revise their expectations and practices in light of what they learn about local conditions and respective capabilities. This emerging open architecture of experimentalist governance, which encompasses actors and processes beyond the immediate confines of the locality and the political hierarchy, is primarily responsible for particular policy innovations in local China, such as low-carbon cities and clean energy cities. In short, it is suggested that local innovation is more about creating a social space for joint deliberation, as opposed to \"getting the incentives right.\"",
    "advisors": ["Edward S. Steinfeld"],
    "text": "An emerging architecture of local experimentalist governance in China : a study of local innovations in Baoding, 1992-2012 What are the conditions under which local actors are more likely to carry out policy innovations that produce environmental or social benefits in local China? Previous studies on \"Chinese experimentalism\" suggest that local innovations in contemporary China stem from its ability to couple reflexive central coordination and decentralized local policy experiments. The strength of this \"closed\" architecture of experimentalist governance lies on the underlying principal-agent distributive bargaining principles between the center and the local, i.e., career prospects, preferential policies, and so on. All of the major mechanisms, processes, and procedures are confined to the center-local dynamics in the political hierarchy. Building on more than two years of site-intensive fieldwork, this dissertation reveals a distinct architecture of experimentalist governance emerging in local China that does not exclusively depend on the requirements of central coordination and formal incentives in the hierarchy. In its basic form, certain local government actors are increasingly opening themselves up and constructing and immersing themselves in communities of practice with extra-local and non-state actors across different levels and sectors. But the uniqueness of this governance architecture lies not only on its form or ostensible structure, but also on its modalities and substance. In those communities, the protagonists enact a recursive process of joint goal-setting, interpretation of policy problems, search and discovery, and mutual learning. In the course of this process, they collectively set achievable provisional \"frameworks\" (kuangfia) or \"platforms\" (pingtai), which are continually recalibrated going forward. That is, the protagonists in this governance architecture continuously reflect on and revise their expectations and practices in light of what they learn about local conditions and respective capabilities. This emerging open architecture of experimentalist governance, which encompasses actors and processes beyond the immediate confines of the locality and the political hierarchy, is primarily responsible for particular policy innovations in local China, such as low-carbon cities and clean energy cities. In short, it is suggested that local innovation is more about creating a social space for joint deliberation, as opposed to \"getting the incentives right.\""
}, {
    "id": "oai:dspace.mit.edu:1721.1/38599",
    "title": "The private sector's capacity to manage climate risks and finance carbon neutral energy infrastructure",
    "abstract": "This dissertation examines the financial aspects of climate change relating to the private sector's capacity to manage climate risks and finance carbon neutral energy infrastructure. The dissertation examines (a) potential risks posed by climate change to private sector investment in critical infrastructure, (b) the potential effectiveness of standard private contractual methods for mitigating risks posed by climate change, (c) the capacity of private capital markets to finance carbon neutral energy infrastructure, and (d) the potential for market failure in developing carbon neutral energy infrastructure. The dissertation first identifies climate risks to infrastructure by examining scientific evidence concerning climate change from studies and atmospheric models. Based on this data, it modifies a framework widely used by practitioners in the finance field for purposes of evaluating financial risks in infrastructure projects. Using the modified risk assessment framework, the dissertation identifies financial risks posed by climate change to financing and developing infrastructure. The dissertation then assesses whether these climate risks can be mitigated and managed by employing private contractual methods typically used in infrastructure finance, such as insurance, derivatives, and carbon offsets.",
    "advisors": ["Nazli Choucri"],
    "text": "The private sector's capacity to manage climate risks and finance carbon neutral energy infrastructure This dissertation examines the financial aspects of climate change relating to the private sector's capacity to manage climate risks and finance carbon neutral energy infrastructure. The dissertation examines (a) potential risks posed by climate change to private sector investment in critical infrastructure, (b) the potential effectiveness of standard private contractual methods for mitigating risks posed by climate change, (c) the capacity of private capital markets to finance carbon neutral energy infrastructure, and (d) the potential for market failure in developing carbon neutral energy infrastructure. The dissertation first identifies climate risks to infrastructure by examining scientific evidence concerning climate change from studies and atmospheric models. Based on this data, it modifies a framework widely used by practitioners in the finance field for purposes of evaluating financial risks in infrastructure projects. Using the modified risk assessment framework, the dissertation identifies financial risks posed by climate change to financing and developing infrastructure. The dissertation then assesses whether these climate risks can be mitigated and managed by employing private contractual methods typically used in infrastructure finance, such as insurance, derivatives, and carbon offsets."
}, {
    "id": "oai:dspace.mit.edu:1721.1/28670",
    "title": "Overcoming shadows of the past : post-conflict interstate reconciliation in East Asia and Europe",
    "abstract": "This dissertation explores the origins of interstate reconciliation after traumatic conflicts, mainly through the comparative study of postwar Sino-Japanese and (West) German-Polish relations. While Germany and Poland have basically achieved deep reconciliation, the Sino-Japanese relationship is still dominated by mistrust and simmering animosity. I test and compare two competing theories to explain the different reconciliation outcomes. Realist theory argues that common security interests solely drive post-conflict reconciliation. I develop the second theory, historical mythmaking theory, which suggests that elite mythmaking of the conflict history for instrumental purposes will obstruct long-term reconciliation. Because national myths glorify and whitewash the action of their own nations and belittle others, they can cause the memories of former adversary states to clash. Such mutually divergent narratives will provoke negative emotions and perception of each other's hostile intention, both mechanisms contributing to bilateral conflict. The case studies show the relative strength of historical mythmaking theory. The Cold War structural pressure initially blocked reconciliation in both dyads. At that time Chinese and Japanese war memories actually converged on a common myth that blames only a small handful of Japanese militarists for the war. It is because China tried to win the hearts and minds of the Japanese people in order to obtain Japanese official recognition of the Communist regime. Since the Sino-U.S. rapprochement and East-West detente in the 1970s, however, structural conditions turned favorable to reconciliation. But China and Japan only brushed aside historical legacy to make way for diplomatic normalization. A",
    "advisors": ["Stephen Van Evera"],
    "text": "Overcoming shadows of the past : post-conflict interstate reconciliation in East Asia and Europe This dissertation explores the origins of interstate reconciliation after traumatic conflicts, mainly through the comparative study of postwar Sino-Japanese and (West) German-Polish relations. While Germany and Poland have basically achieved deep reconciliation, the Sino-Japanese relationship is still dominated by mistrust and simmering animosity. I test and compare two competing theories to explain the different reconciliation outcomes. Realist theory argues that common security interests solely drive post-conflict reconciliation. I develop the second theory, historical mythmaking theory, which suggests that elite mythmaking of the conflict history for instrumental purposes will obstruct long-term reconciliation. Because national myths glorify and whitewash the action of their own nations and belittle others, they can cause the memories of former adversary states to clash. Such mutually divergent narratives will provoke negative emotions and perception of each other's hostile intention, both mechanisms contributing to bilateral conflict. The case studies show the relative strength of historical mythmaking theory. The Cold War structural pressure initially blocked reconciliation in both dyads. At that time Chinese and Japanese war memories actually converged on a common myth that blames only a small handful of Japanese militarists for the war. It is because China tried to win the hearts and minds of the Japanese people in order to obtain Japanese official recognition of the Communist regime. Since the Sino-U.S. rapprochement and East-West detente in the 1970s, however, structural conditions turned favorable to reconciliation. But China and Japan only brushed aside historical legacy to make way for diplomatic normalization. A"
}, {
    "id": "oai:dspace.mit.edu:1721.1/53077",
    "title": "Defeat in victory : organizational learning dysfunction in counterinsurgency",
    "abstract": "Two puzzles dominate the study of organizational learning and counterinsurgency. First, militaries often struggle to develop effective strategies to address the problem of counterinsurgency. Second, their strategic performance seldom improves over successive counterinsurgency campaigns. This study offers a theoretical explanation for these dominant patterns of learning dysfunction. It argues that a set of closely held, professional beliefs - the military operational code - and bureaucratic preferences distort the organizations' initial response, subsequent adaptation and interwar retention. The military operational code leads militaries to misunderstand counterinsurgency in a systematic and debilitating fashion; bureaucratic interests lead them to reject the most effective strategies once they have been uncovered. When militaries manage to break with this dysfunctional pattern, it because their professional judgment is constrained; high civilian participation and/or resource scarcity force often force militaries to adopt political strategies that are less congenial but more effective in restoring state authority. This study tests the theory against six empirical cases: Indochina, the Indochina-Algeria interlude, Algeria, British Palestine, Malaya, and Thailand. These cases strongly suggest that the dysfunctional learning patterns are the product of broadly shared, professional beliefs and bureaucratic interests rather than the common, alternative explanations based on experience, culture or normative and material constraints.",
    "advisors": ["Roger D. Peterson"],
    "text": "Defeat in victory : organizational learning dysfunction in counterinsurgency Two puzzles dominate the study of organizational learning and counterinsurgency. First, militaries often struggle to develop effective strategies to address the problem of counterinsurgency. Second, their strategic performance seldom improves over successive counterinsurgency campaigns. This study offers a theoretical explanation for these dominant patterns of learning dysfunction. It argues that a set of closely held, professional beliefs - the military operational code - and bureaucratic preferences distort the organizations' initial response, subsequent adaptation and interwar retention. The military operational code leads militaries to misunderstand counterinsurgency in a systematic and debilitating fashion; bureaucratic interests lead them to reject the most effective strategies once they have been uncovered. When militaries manage to break with this dysfunctional pattern, it because their professional judgment is constrained; high civilian participation and/or resource scarcity force often force militaries to adopt political strategies that are less congenial but more effective in restoring state authority. This study tests the theory against six empirical cases: Indochina, the Indochina-Algeria interlude, Algeria, British Palestine, Malaya, and Thailand. These cases strongly suggest that the dysfunctional learning patterns are the product of broadly shared, professional beliefs and bureaucratic interests rather than the common, alternative explanations based on experience, culture or normative and material constraints."
}, {
    "id": "oai:dspace.mit.edu:1721.1/8759",
    "title": "Final solutions : the causes of mass killing and genocide",
    "abstract": "This dissertation seeks to identify the causes of genocide and mass killing. Many of the most widely accepted explanations of genocide and mass killing seek the causes of these events in the social structure, system of government or the collective psychology of the societies in which they take place. Although the factors highlighted by these explanations play an important role in many cases of mass killing, I find that society at large plays a smaller role in this kind of violence than is commonly assumed. Mass killing is rarely a popular enterprise in which neighbor turns against neighbor. I argue that the causes of mass killing are best understood when the phenomenon is studied from a \"strategic\" perspective. The strategic approach suggests that the impetus for mass killing usually originates from a relatively small group of powerful political or military leaders and is often carried out without the active support of broader society. Mass killing is most accurately viewed as a goal-oriented policy -- a brutal strategy designed to accomplish leaders' most important objectives, counter their most dangerous threats, and solve their most difficult problems. In order to understand and predict mass killing, therefore, this dissertation seeks to identify the specific factors and conditions that contribute to leaders' decisions to launch mass killing. Cases of mass killing in the Soviet Union, China, Cambodia, Armenia, Nazi Germany, Rwanda, Guatemala and Afghanistan are examined.",
    "advisors": ["Stephen Van Evera"],
    "text": "Final solutions : the causes of mass killing and genocide This dissertation seeks to identify the causes of genocide and mass killing. Many of the most widely accepted explanations of genocide and mass killing seek the causes of these events in the social structure, system of government or the collective psychology of the societies in which they take place. Although the factors highlighted by these explanations play an important role in many cases of mass killing, I find that society at large plays a smaller role in this kind of violence than is commonly assumed. Mass killing is rarely a popular enterprise in which neighbor turns against neighbor. I argue that the causes of mass killing are best understood when the phenomenon is studied from a \"strategic\" perspective. The strategic approach suggests that the impetus for mass killing usually originates from a relatively small group of powerful political or military leaders and is often carried out without the active support of broader society. Mass killing is most accurately viewed as a goal-oriented policy -- a brutal strategy designed to accomplish leaders' most important objectives, counter their most dangerous threats, and solve their most difficult problems. In order to understand and predict mass killing, therefore, this dissertation seeks to identify the specific factors and conditions that contribute to leaders' decisions to launch mass killing. Cases of mass killing in the Soviet Union, China, Cambodia, Armenia, Nazi Germany, Rwanda, Guatemala and Afghanistan are examined."
}, {
    "id": "oai:dspace.mit.edu:1721.1/38601",
    "title": "It takes more than a village : mobilization, networks, and the state in Central Asia",
    "abstract": "This dissertation develops and demonstrates a theory to account for the outbreak of mass mobilization in authoritarian settings. Two conditions make the expansion of protest across community boundaries more likely: (1) low levels of public goods, coupled with (2) economic opportunities that allow elites autonomous from the state to earn revenue. Under regimes where the rule of law is weak, non-state elites have an incentive to protect their assets from state predation by developing a social support base. They do this by making symbolic gestures and providing surrogate public goods to communities. If the regime threatens to harm this relationship, by restricting elites' freedoms or denying them access to resources, top-down mobilization is one of the few means available to advance or defend their position. Elites base their appeal on shared local identity and the material benefit that people derive from elite charity. The ultimate scale of mobilization is determined by the number and geographic dispersion of elites who mobilize locally and then unite their protests.",
    "advisors": ["Roger Peterson"],
    "text": "It takes more than a village : mobilization, networks, and the state in Central Asia This dissertation develops and demonstrates a theory to account for the outbreak of mass mobilization in authoritarian settings. Two conditions make the expansion of protest across community boundaries more likely: (1) low levels of public goods, coupled with (2) economic opportunities that allow elites autonomous from the state to earn revenue. Under regimes where the rule of law is weak, non-state elites have an incentive to protect their assets from state predation by developing a social support base. They do this by making symbolic gestures and providing surrogate public goods to communities. If the regime threatens to harm this relationship, by restricting elites' freedoms or denying them access to resources, top-down mobilization is one of the few means available to advance or defend their position. Elites base their appeal on shared local identity and the material benefit that people derive from elite charity. The ultimate scale of mobilization is determined by the number and geographic dispersion of elites who mobilize locally and then unite their protests."
}, {
    "id": "oai:dspace.mit.edu:1721.1/107540",
    "title": "Nuclear weapons and foreign policy",
    "abstract": "How do states change their foreign policies when they acquire nuclear weapons? This question is central to both academic and policy debates about the consequences of nuclear proliferation, and the lengths that the United States and other states should go to to prevent proliferation. Despite this importance to scholars and practitioners, existing literature has largely avoided answering this question. This dissertation aims to fill this gap. In answering this question, I first offer a typology of conceptually distinct and empirically distinguishable foreign policy behaviors that nuclear weapons may facilitate. Specifically, I distinguish between aggression, expansion, independence, bolstering, steadfastness, and compromise. The typology allows scholars and practitioners to move beyond catch-all terms such as \"emboldenment\" when thinking about how states may change their foreign policies after nuclear acquisition. Second, I offer a theory for why different states use nuclear weapons to facilitate different combinations of these behaviors. I argue that states in different geopolitical circumstances have different political priorities. Different states therefore find different combinations of foreign policy behaviors attractive, and thus use nuclear weapons to facilitate different foreign policy behaviors. The theory uses a sequence of three variables-the existence of severe territorial threats or an ongoing war, the presence of senior allies, and the state's power trajectory-to predict the combinations of foreign policy behaviors states will use nuclear weapons to facilitate. Third, I test the theory using case studies of the United Kingdom, South Africa, and the United States, each drawing on interviews and multi-archival research. In each case, I look for discontinuities in the state's foreign policy behaviors that occur at the point of nuclear acquisition and use process tracing to assess whether nuclear weapons caused the changes observed. The dissertation makes several contributions. It provides an answer to a foundational question about the nuclear revolution: how do states use nuclear weapons to facilitate their goals in international politics? It offers a new dependent variable and theory with potentially broader applicability to other questions about comparative foreign policy. Finally, it offers policy-relevant insights into how new nuclear states might behave in the future.",
    "advisors": ["Barry R. Posen"],
    "text": "Nuclear weapons and foreign policy How do states change their foreign policies when they acquire nuclear weapons? This question is central to both academic and policy debates about the consequences of nuclear proliferation, and the lengths that the United States and other states should go to to prevent proliferation. Despite this importance to scholars and practitioners, existing literature has largely avoided answering this question. This dissertation aims to fill this gap. In answering this question, I first offer a typology of conceptually distinct and empirically distinguishable foreign policy behaviors that nuclear weapons may facilitate. Specifically, I distinguish between aggression, expansion, independence, bolstering, steadfastness, and compromise. The typology allows scholars and practitioners to move beyond catch-all terms such as \"emboldenment\" when thinking about how states may change their foreign policies after nuclear acquisition. Second, I offer a theory for why different states use nuclear weapons to facilitate different combinations of these behaviors. I argue that states in different geopolitical circumstances have different political priorities. Different states therefore find different combinations of foreign policy behaviors attractive, and thus use nuclear weapons to facilitate different foreign policy behaviors. The theory uses a sequence of three variables-the existence of severe territorial threats or an ongoing war, the presence of senior allies, and the state's power trajectory-to predict the combinations of foreign policy behaviors states will use nuclear weapons to facilitate. Third, I test the theory using case studies of the United Kingdom, South Africa, and the United States, each drawing on interviews and multi-archival research. In each case, I look for discontinuities in the state's foreign policy behaviors that occur at the point of nuclear acquisition and use process tracing to assess whether nuclear weapons caused the changes observed. The dissertation makes several contributions. It provides an answer to a foundational question about the nuclear revolution: how do states use nuclear weapons to facilitate their goals in international politics? It offers a new dependent variable and theory with potentially broader applicability to other questions about comparative foreign policy. Finally, it offers policy-relevant insights into how new nuclear states might behave in the future."
}, {
    "id": "oai:dspace.mit.edu:1721.1/101808",
    "title": "Participatory autocracy : private entrepreneurs, legislatures, and property protection in China",
    "abstract": "This dissertation addresses the puzzle of why individuals in authoritarian systems seek office in formal institutions, which are often dismissed as weak and ineffective. I argue that individuals seek office mainly to protect their property from government expropriation in China. In contrast to prior work, I argue that instead of being passive takers of existing institutional arrangements, private entrepreneurs in China actively seek opportunities within formal institutions to advance their interests. By holding seats in local legislatures, entrepreneurs signal to local bureaucrats that they have access to higher-level government officials to report illicit predatory behavior. This signal, in turn, deters local officials from demanding bribes, ad hoc taxes, and other types of informal payments. I deploy both qualitative and quantitative methods to support the argument. First, to understand state-business relations in China, I conducted 106 in-depth interviews with private entrepreneurs, government officials, and local scholars in five provinces during 16 months of fieldwork. I show that even while government expropriation is an endemic problem, private entrepreneurs who are also legislative officeholders are less likely to experience severe expropriation. Second, using a nationally representative survey of private entrepreneurs, I quantitatively show that entrepreneurs who have seats in the local legislatures on average spend 25 percent less on informal payments to local officials compared to entrepreneurs without such a political status. To investigate the causal link between formal office and protection of property, I conducted field experiments on Chinese bureaucrats to understand how local bureaucracies respond to constituents with connections to formal institutions. These experiments involved directly contacting officials to examine how they respond to realistic messages from citizens. Using an experimental manipulation, I demonstrate that Chinese bureaucrats are 35 percent more likely to respond to a constituent with connections to formal institutions. These findings challenge prominent theories of authoritarian politics, which see authoritarian institutions as instruments to arrange power sharing, rent distribution, or information collection. Adopting an \"institution as resource\" perspective, I show that within authoritarian institutions, entrepreneurial actors can seek opportunities to advance their interests and improve their well-being through formal means, even when these formal institutions are relatively weak..",
    "advisors": ["Lily L. Tsai"],
    "text": "Participatory autocracy : private entrepreneurs, legislatures, and property protection in China This dissertation addresses the puzzle of why individuals in authoritarian systems seek office in formal institutions, which are often dismissed as weak and ineffective. I argue that individuals seek office mainly to protect their property from government expropriation in China. In contrast to prior work, I argue that instead of being passive takers of existing institutional arrangements, private entrepreneurs in China actively seek opportunities within formal institutions to advance their interests. By holding seats in local legislatures, entrepreneurs signal to local bureaucrats that they have access to higher-level government officials to report illicit predatory behavior. This signal, in turn, deters local officials from demanding bribes, ad hoc taxes, and other types of informal payments. I deploy both qualitative and quantitative methods to support the argument. First, to understand state-business relations in China, I conducted 106 in-depth interviews with private entrepreneurs, government officials, and local scholars in five provinces during 16 months of fieldwork. I show that even while government expropriation is an endemic problem, private entrepreneurs who are also legislative officeholders are less likely to experience severe expropriation. Second, using a nationally representative survey of private entrepreneurs, I quantitatively show that entrepreneurs who have seats in the local legislatures on average spend 25 percent less on informal payments to local officials compared to entrepreneurs without such a political status. To investigate the causal link between formal office and protection of property, I conducted field experiments on Chinese bureaucrats to understand how local bureaucracies respond to constituents with connections to formal institutions. These experiments involved directly contacting officials to examine how they respond to realistic messages from citizens. Using an experimental manipulation, I demonstrate that Chinese bureaucrats are 35 percent more likely to respond to a constituent with connections to formal institutions. These findings challenge prominent theories of authoritarian politics, which see authoritarian institutions as instruments to arrange power sharing, rent distribution, or information collection. Adopting an \"institution as resource\" perspective, I show that within authoritarian institutions, entrepreneurial actors can seek opportunities to advance their interests and improve their well-being through formal means, even when these formal institutions are relatively weak.."
}, {
    "id": "oai:dspace.mit.edu:1721.1/8756",
    "title": "Associative activism : organizing support for foreign workers in contemporary Japan",
    "abstract": "Japan is a country known for its suspicion of foreigners, but Japanese citizens have established non-government organizations to support illegal foreign migrants. The problems and conditions of illegal foreign workers are rooted in Japanese government policies. The 1990 Immigration Control Law created a category of illegal foreign workers. Later, the Ministry of Health and Welfare excluded illegal foreigners from Japan's insurance system. Illegal foreign workers face challenges in dealing with employers, state officials, medical institutions, and family life. These problems range from unpaid wages and enormous medical cost to marriage/divorce registration and the forced break up of the families due to deportation. To solve these problems, Japanese engage in associative activism and institutional experimentation, which has transformed local politics in Japan. Illegal Asian workers in Japan rarely seek assistance from existing government organizations or ethnic associations. Government organizations provide mainly information and interpretation services and government officials lack the know-how to help illegal foreign workers with serious labor and immigration problems. Ethnic associations in Japan do not support their illegal compatriots. Illegal foreigners turn instead to Japanese NGOs, which have extensive experience in helping the underprivileged in Japanese society. Japanese activists, who found these NGOs, came from other social movement organizations. Christians, community workers unions organizers, women activists, labor lawyers, health workers, and civil rights activists have created separate support groups to help solve problems for illegal foreigners. Japanese activists created these groups in order to: a) acquire and accumulate knowledge on how best to help illegal foreigners; b) strengthen their bargaining power with employers and state officials; and, c) gain financial support for their activities. By working on behalf of illegal foreigners, these Japanese activists build a new community of action. Local governments have invited these activists to share their expertise and are increasingly relying on these support groups to provide public services to illegal foreigners. In some localities, local government officials have joined these groups and experimented with new institutions of governance. As a result, local governments are now breaking with national policies regarding illegal foreign workers. Japan has done quite a bit over the last two decades to open its borders and accommodate immigration. Associative activism by Japanese citizens impresses upon other industrialized societies that Japan's efforts to accommodate immigration are surprisingly humanitarian for a historically xenophobic culture.",
    "advisors": ["Richard J. Samuels"],
    "text": "Associative activism : organizing support for foreign workers in contemporary Japan Japan is a country known for its suspicion of foreigners, but Japanese citizens have established non-government organizations to support illegal foreign migrants. The problems and conditions of illegal foreign workers are rooted in Japanese government policies. The 1990 Immigration Control Law created a category of illegal foreign workers. Later, the Ministry of Health and Welfare excluded illegal foreigners from Japan's insurance system. Illegal foreign workers face challenges in dealing with employers, state officials, medical institutions, and family life. These problems range from unpaid wages and enormous medical cost to marriage/divorce registration and the forced break up of the families due to deportation. To solve these problems, Japanese engage in associative activism and institutional experimentation, which has transformed local politics in Japan. Illegal Asian workers in Japan rarely seek assistance from existing government organizations or ethnic associations. Government organizations provide mainly information and interpretation services and government officials lack the know-how to help illegal foreign workers with serious labor and immigration problems. Ethnic associations in Japan do not support their illegal compatriots. Illegal foreigners turn instead to Japanese NGOs, which have extensive experience in helping the underprivileged in Japanese society. Japanese activists, who found these NGOs, came from other social movement organizations. Christians, community workers unions organizers, women activists, labor lawyers, health workers, and civil rights activists have created separate support groups to help solve problems for illegal foreigners. Japanese activists created these groups in order to: a) acquire and accumulate knowledge on how best to help illegal foreigners; b) strengthen their bargaining power with employers and state officials; and, c) gain financial support for their activities. By working on behalf of illegal foreigners, these Japanese activists build a new community of action. Local governments have invited these activists to share their expertise and are increasingly relying on these support groups to provide public services to illegal foreigners. In some localities, local government officials have joined these groups and experimented with new institutions of governance. As a result, local governments are now breaking with national policies regarding illegal foreign workers. Japan has done quite a bit over the last two decades to open its borders and accommodate immigration. Associative activism by Japanese citizens impresses upon other industrialized societies that Japan's efforts to accommodate immigration are surprisingly humanitarian for a historically xenophobic culture."
}, {
    "id": "oai:dspace.mit.edu:1721.1/62654",
    "title": "Explaining cohesion, fragmentation, and control in insurgent groups",
    "abstract": "The internal unity and discipline of insurgent groups helps us understand the military effectiveness of armed groups, patterns of violence against civilians, and the ability of insurgent organizations to negotiate and demobilize, but the causes of insurgent cohesion and fragmentation have not been systematically or comparatively examined. This study offers a theory to explain why some armed groups are more cohesive and controlled than others. It argues that the trajectories of insurgent organizations can be substantially explained by focusing on two variables: the structure of the social networks and institutions upon which the organization is built, and the organization's access to material resources from outside the war zone. First, the structure of the core networks upon which an organization is constructed determines the internal social environment of the group: its social base shapes its organizational form. The denser the core networks, and more tightly they pull together local communities, the more robust will be the organization that emerges. Social embeddedness can therefore be more important than mass political popularity, public goods provision, or ideology in providing the basis for enduring organizational cohesion. Organizations built around coalitions of localized pockets of collective action or leaders operating among populations with whom they lack social ties will face severe problems of internal control - regardless of organizational blueprints or ethnic and class appeals. Second, external material support from states and diasporas tends to centralize internal control and to enhance insurgent military power. Rather than encouraging looting and thuggishness, resource-wealth can fuel highly cohesive and disciplined armed organizations. The interaction of social bases and external support generates empirically distinct trajectories of organizational cohesion. Mechanisms explaining change over time are derived from the structural underpinnings of this argument. This theory is tested with a study of 26 armed groups in nine civil wars. The primary research design is a set of within-conflict comparisons of insurgent organizations in civil wars in Kashmir, Northern Ireland, and Sri Lanka. Within each war there is dramatic variation across groups within a shared structural context. Fieldwork, primary sources, and secondary sources are used to trace out the different trajectories of militancy and their origins. An external validity check is provided by a study of Southeast Asia, relying on a cross-national comparison of communist insurgents in Malaya, Vietnam, and the Philippines, a sub-national, cross-conflict comparison of armed groups in Aceh and East Timor, and a within-conflict comparison of separatists in the southern Philippines. These comparisons reveal strong support for the theory relative to its competitors while also uncovering new mechanisms of change and evolution.",
    "advisors": ["Roger Petersen"],
    "text": "Explaining cohesion, fragmentation, and control in insurgent groups The internal unity and discipline of insurgent groups helps us understand the military effectiveness of armed groups, patterns of violence against civilians, and the ability of insurgent organizations to negotiate and demobilize, but the causes of insurgent cohesion and fragmentation have not been systematically or comparatively examined. This study offers a theory to explain why some armed groups are more cohesive and controlled than others. It argues that the trajectories of insurgent organizations can be substantially explained by focusing on two variables: the structure of the social networks and institutions upon which the organization is built, and the organization's access to material resources from outside the war zone. First, the structure of the core networks upon which an organization is constructed determines the internal social environment of the group: its social base shapes its organizational form. The denser the core networks, and more tightly they pull together local communities, the more robust will be the organization that emerges. Social embeddedness can therefore be more important than mass political popularity, public goods provision, or ideology in providing the basis for enduring organizational cohesion. Organizations built around coalitions of localized pockets of collective action or leaders operating among populations with whom they lack social ties will face severe problems of internal control - regardless of organizational blueprints or ethnic and class appeals. Second, external material support from states and diasporas tends to centralize internal control and to enhance insurgent military power. Rather than encouraging looting and thuggishness, resource-wealth can fuel highly cohesive and disciplined armed organizations. The interaction of social bases and external support generates empirically distinct trajectories of organizational cohesion. Mechanisms explaining change over time are derived from the structural underpinnings of this argument. This theory is tested with a study of 26 armed groups in nine civil wars. The primary research design is a set of within-conflict comparisons of insurgent organizations in civil wars in Kashmir, Northern Ireland, and Sri Lanka. Within each war there is dramatic variation across groups within a shared structural context. Fieldwork, primary sources, and secondary sources are used to trace out the different trajectories of militancy and their origins. An external validity check is provided by a study of Southeast Asia, relying on a cross-national comparison of communist insurgents in Malaya, Vietnam, and the Philippines, a sub-national, cross-conflict comparison of armed groups in Aceh and East Timor, and a within-conflict comparison of separatists in the southern Philippines. These comparisons reveal strong support for the theory relative to its competitors while also uncovering new mechanisms of change and evolution."
}, {
    "id": "oai:dspace.mit.edu:1721.1/118220",
    "title": "Indebted societies : modern labor markets, social policy, and everyday borrowing",
    "abstract": "Debt has become an essential part of families' daily lives in many countries. This dissertation examines under what circumstances credit markets replace the role of welfare states to address social risks and promote social mobility in advanced democracies. It sheds light on the socio-economic and political consequences of growing debt levels. I offer a theory that explains variation in household debt across and within countries by demonstrating that credit fills gaps between households' financial needs and demand for social services on the one hand and welfare states' supply of social services on the other-a gap I refer to as social policy shortfall. The transformation of stable Fordist economies into flexible knowledge economies led to increasingly fragmented employment patterns and life-course trajectories. Welfare states, however, have often not kept up with these disruptions and leave households with larger financial burdens. Households increasingly go into debt to address the financial consequences of social risk such as unemployment or sickness as well as to seize social opportunity by investing in childcare and family, education, and housing. Cross-nationally, two factors explain the variation in household debt: the size and type of social policy shortfall determine individuals' financial needs. But whether credit emerges as a private alternative to welfare states is contingent upon the structure of a country's credit regime, which shapes how easily individuals can borrow money. Drawing on full-population administrative records from Denmark and micro-level panel data from the U.S. and Germany, I show that the permissive credit regimes of the U.S. and Denmark grant households easy access to credit, but the distribution of debt across households differs because welfare states in both countries protect and support households differently. In Germany, the restrictive credit regime results in less borrowing even in light of social policy reforms. The findings have implications for how scholars and policymakers think about the role of financial markets and household debt in a world of changing labor markets and welfare states. It shows how credit markets and welfare states appear to fulfill similar functions but follow different underlying logics, each with its own socio-economic and distributional consequences that shape and amplify insecurity and inequality.",
    "advisors": ["Kathleen Thelen"],
    "text": "Indebted societies : modern labor markets, social policy, and everyday borrowing Debt has become an essential part of families' daily lives in many countries. This dissertation examines under what circumstances credit markets replace the role of welfare states to address social risks and promote social mobility in advanced democracies. It sheds light on the socio-economic and political consequences of growing debt levels. I offer a theory that explains variation in household debt across and within countries by demonstrating that credit fills gaps between households' financial needs and demand for social services on the one hand and welfare states' supply of social services on the other-a gap I refer to as social policy shortfall. The transformation of stable Fordist economies into flexible knowledge economies led to increasingly fragmented employment patterns and life-course trajectories. Welfare states, however, have often not kept up with these disruptions and leave households with larger financial burdens. Households increasingly go into debt to address the financial consequences of social risk such as unemployment or sickness as well as to seize social opportunity by investing in childcare and family, education, and housing. Cross-nationally, two factors explain the variation in household debt: the size and type of social policy shortfall determine individuals' financial needs. But whether credit emerges as a private alternative to welfare states is contingent upon the structure of a country's credit regime, which shapes how easily individuals can borrow money. Drawing on full-population administrative records from Denmark and micro-level panel data from the U.S. and Germany, I show that the permissive credit regimes of the U.S. and Denmark grant households easy access to credit, but the distribution of debt across households differs because welfare states in both countries protect and support households differently. In Germany, the restrictive credit regime results in less borrowing even in light of social policy reforms. The findings have implications for how scholars and policymakers think about the role of financial markets and household debt in a world of changing labor markets and welfare states. It shows how credit markets and welfare states appear to fulfill similar functions but follow different underlying logics, each with its own socio-economic and distributional consequences that shape and amplify insecurity and inequality."
}, {
    "id": "oai:dspace.mit.edu:1721.1/8312",
    "title": "National security panics : overestimating threats to national security",
    "abstract": "Three times in this century the US public has panicked with fear because of exaggerations of external threats to the nation. These panics peaked in 1950,1960, and 1980. Why did the U.S. markedly exaggerate the Soviet threat at these times? These periods of widespread public fear were \"defining moments\" when the US created confrontational and militarized containment policies. These panics ratcheted up the arms race tremendously between the US and the Soviet Union, and arguably these panics led to unnecessary confrontations and crises. In this study I test leading explanations of these cases--eight hypotheses drawn from three different perspectives. The Rational Perspective argues insufficient information and uncertainty about present and future capabilities and intentions causes overestimations. The Psychological Perspective argues cognitive errors could cause these overestimations (attribution theory and schema theory/analogical reasoning, tested here). The Domestic Politics Perspective argues oversell, logrolling, electoral politics and/or militarism causes public overestimations. Domestic Politics best explains the national misperceptions examined. In each case, the sources of the specific misperceptions examined were clearly rooted in domestic politics (1950: oversell and militarism; 1960 and 1980: electoral politics and militarism.) Uncertainty about the threat was found to be a significant contributing factor in 1950 (but not the source/elites did not unintentionally overestimate when the misperceptions first formed).",
    "advisors": ["Stephen Van Evera"],
    "text": "National security panics : overestimating threats to national security Three times in this century the US public has panicked with fear because of exaggerations of external threats to the nation. These panics peaked in 1950,1960, and 1980. Why did the U.S. markedly exaggerate the Soviet threat at these times? These periods of widespread public fear were \"defining moments\" when the US created confrontational and militarized containment policies. These panics ratcheted up the arms race tremendously between the US and the Soviet Union, and arguably these panics led to unnecessary confrontations and crises. In this study I test leading explanations of these cases--eight hypotheses drawn from three different perspectives. The Rational Perspective argues insufficient information and uncertainty about present and future capabilities and intentions causes overestimations. The Psychological Perspective argues cognitive errors could cause these overestimations (attribution theory and schema theory/analogical reasoning, tested here). The Domestic Politics Perspective argues oversell, logrolling, electoral politics and/or militarism causes public overestimations. Domestic Politics best explains the national misperceptions examined. In each case, the sources of the specific misperceptions examined were clearly rooted in domestic politics (1950: oversell and militarism; 1960 and 1980: electoral politics and militarism.) Uncertainty about the threat was found to be a significant contributing factor in 1950 (but not the source/elites did not unintentionally overestimate when the misperceptions first formed)."
}, {
    "id": "oai:dspace.mit.edu:1721.1/107541",
    "title": "Competitive intervention and its consequences for civil wars",
    "abstract": "This dissertation explores two interrelated puzzles about external intervention and internal war. The first asks why rebels, governments, and third party interveners often continue to invest in costly and protracted conflicts rather than sue for peace and a negotiated settlement. The second considers the consequences of these behaviors for temporal variation in the average duration and global prevalence of civil wars. A central finding that emerges concerns the critical role of competitive intervention-two sided, simultaneous military assistance from different third party states to both government and rebel combatants-in the dynamics and intractability of civil wars across time and around the globe. Developing a generalizable theory of competitive intervention, the dissertation explains the distortionary effects this form of external meddling has on domestic bargaining processes, describes the unique strategic dilemmas it entails for third party interveners, and links its varying prevalence to international systemic change. In doing so, it moves beyond popular anecdotes about \"proxy wars\" by deriving theoretically-grounded propositions about the strategic logics motivating competitive intervention in civil wars. It also uncovers a heretofore overlooked feature of this form of intervention-namely, that \"not losing\" is often more important than \"winning\" from the perspective of third party interveners under the shadow of inadvertent escalation. The theory is tested with a mixed-method design that combines statistical analyses of all civil wars fought between 1975 and 2009 with detailed case studies of competitive intervention in Angola (1975-1991) and Afghanistan (1979-1992). The dissertation's theoretical and empirical results shed new light on the international dimensions of civil war, address ongoing debates concerning the utility of intervention as a conflict management tool, and inform policy prescriptions aimed at resolving some of today's most violent internal conflicts.",
    "advisors": ["Roger D. Petersen"],
    "text": "Competitive intervention and its consequences for civil wars This dissertation explores two interrelated puzzles about external intervention and internal war. The first asks why rebels, governments, and third party interveners often continue to invest in costly and protracted conflicts rather than sue for peace and a negotiated settlement. The second considers the consequences of these behaviors for temporal variation in the average duration and global prevalence of civil wars. A central finding that emerges concerns the critical role of competitive intervention-two sided, simultaneous military assistance from different third party states to both government and rebel combatants-in the dynamics and intractability of civil wars across time and around the globe. Developing a generalizable theory of competitive intervention, the dissertation explains the distortionary effects this form of external meddling has on domestic bargaining processes, describes the unique strategic dilemmas it entails for third party interveners, and links its varying prevalence to international systemic change. In doing so, it moves beyond popular anecdotes about \"proxy wars\" by deriving theoretically-grounded propositions about the strategic logics motivating competitive intervention in civil wars. It also uncovers a heretofore overlooked feature of this form of intervention-namely, that \"not losing\" is often more important than \"winning\" from the perspective of third party interveners under the shadow of inadvertent escalation. The theory is tested with a mixed-method design that combines statistical analyses of all civil wars fought between 1975 and 2009 with detailed case studies of competitive intervention in Angola (1975-1991) and Afghanistan (1979-1992). The dissertation's theoretical and empirical results shed new light on the international dimensions of civil war, address ongoing debates concerning the utility of intervention as a conflict management tool, and inform policy prescriptions aimed at resolving some of today's most violent internal conflicts."
}, {
    "id": "oai:dspace.mit.edu:1721.1/68931",
    "title": "Diplomacy derailed : the consequences of U.S. diplomatic disengagement",
    "abstract": "Advocates of diplomatic engagement with states of concern argue that talking to both allies and adversaries is essential for advancing U.S. foreign policy interests. Critics of this approach argue that engagement with these regimes is tantamount to appeasement and signals acceptance of behavior that ought to be condemned. In their view, little can be gained by talking to these states. Thus, diplomatic sanctions are seen as a low-cost means of isolating and delegitimizing regimes. This perspective, however, fails to recognize that maintaining diplomatic sanctions may actually entail a number of substantial costs to the United States and may even undermine economic sanctions' effectiveness. Although the U.S. has employed policies of diplomatic disengagement in approximately 30% of its economic sanctions episodes, studies have focused solely on economic sanctions. Seeking to weigh in on this debate, my doctoral dissertation focuses on two central questions: (1) What are the effects of diplomatic sanctions as a foreign policy tool? and (2) Do diplomatic sanctions increase or decrease the likelihood of target state compliance with U.S. demands? I develop and test a new theory of sanctions effectiveness focusing on the role of information, communication, and diplomatic ties. I argue that diplomatic sanctions and disengagement result in unintended consequences, including a loss of valuable intelligence, increased difficulty of communication, and reduced capabilities for public diplomacy in the target state. I also argue that when United States is more diplomatically engaged with the target state, economic sanctions are more likely to be effective in getting the target state to comply with U.S. demands. To reach these conclusions, I use both quantitative and qualitative analysis. I use economic sanctions data from 1945-2000 from the Hufbauer, Schott and Elliott database, along with original data on diplomatic sanctions. I conduct ordered logit multivariate regressions to test the diplomatic sanctions hypotheses and assess whether or not diplomatic sanctions impact the effectiveness of economic sanctions. I also conduct comprehensive longitudinal case studies of Sudan and Libya, along with a series of shorter mini-case studies focusing on Afghanistan, South Africa and Burma.",
    "advisors": ["Kenneth Oye"],
    "text": "Diplomacy derailed : the consequences of U.S. diplomatic disengagement Advocates of diplomatic engagement with states of concern argue that talking to both allies and adversaries is essential for advancing U.S. foreign policy interests. Critics of this approach argue that engagement with these regimes is tantamount to appeasement and signals acceptance of behavior that ought to be condemned. In their view, little can be gained by talking to these states. Thus, diplomatic sanctions are seen as a low-cost means of isolating and delegitimizing regimes. This perspective, however, fails to recognize that maintaining diplomatic sanctions may actually entail a number of substantial costs to the United States and may even undermine economic sanctions' effectiveness. Although the U.S. has employed policies of diplomatic disengagement in approximately 30% of its economic sanctions episodes, studies have focused solely on economic sanctions. Seeking to weigh in on this debate, my doctoral dissertation focuses on two central questions: (1) What are the effects of diplomatic sanctions as a foreign policy tool? and (2) Do diplomatic sanctions increase or decrease the likelihood of target state compliance with U.S. demands? I develop and test a new theory of sanctions effectiveness focusing on the role of information, communication, and diplomatic ties. I argue that diplomatic sanctions and disengagement result in unintended consequences, including a loss of valuable intelligence, increased difficulty of communication, and reduced capabilities for public diplomacy in the target state. I also argue that when United States is more diplomatically engaged with the target state, economic sanctions are more likely to be effective in getting the target state to comply with U.S. demands. To reach these conclusions, I use both quantitative and qualitative analysis. I use economic sanctions data from 1945-2000 from the Hufbauer, Schott and Elliott database, along with original data on diplomatic sanctions. I conduct ordered logit multivariate regressions to test the diplomatic sanctions hypotheses and assess whether or not diplomatic sanctions impact the effectiveness of economic sanctions. I also conduct comprehensive longitudinal case studies of Sudan and Libya, along with a series of shorter mini-case studies focusing on Afghanistan, South Africa and Burma."
}, {
    "id": "oai:dspace.mit.edu:1721.1/104570",
    "title": "Causal inference with time-series cross-sectional data : with applications to positive political economy",
    "abstract": "Time-series cross-sectional (TSCS) data are widely used in today's social sciences. Researchers often rely on two-way fixed effect models to estimate causal quantities of interest with TSCS data. However, they face the challenge that such models are not applicable when the so called \"parallel trends\" assumption fails, that is, the average treated counterfactual and average control outcome do not follow parallel paths. The first chapter of this dissertation introduces the generalized synthetic control method that addresses this challenge. It imputes counterfactuals for each treated unit using control group information based on a linear interactive fixed effect model that incorporates unit-specific intercepts interacted with time-varying coefficients. It not only relaxes the often-violated \"parallel trends\" assumption, but also unifies the synthetic control method with linear fixed effect models under a simple framework. The second chapter examines the effect of Election Day Registration (EDR) laws on voter turnout in the United States. Conventional difference-in-differences approach suggests that EDR laws had almost no impact on voter turnout. Using the generalized synthetic control method, I show that EDR laws increased turnout in early adopting states but not in states that introduced them more recently. The third chapter investigates the role of informal institutions on the quality of governance in the context of rural China. Using TSCS analysis and a regression discontinuity design, I show that village leaders from large lineage groups are associated with considerably more local public investment. This association is stronger when the groups appeared to be more cohesive.",
    "advisors": ["Teppei Yamamoto"],
    "text": "Causal inference with time-series cross-sectional data : with applications to positive political economy Time-series cross-sectional (TSCS) data are widely used in today's social sciences. Researchers often rely on two-way fixed effect models to estimate causal quantities of interest with TSCS data. However, they face the challenge that such models are not applicable when the so called \"parallel trends\" assumption fails, that is, the average treated counterfactual and average control outcome do not follow parallel paths. The first chapter of this dissertation introduces the generalized synthetic control method that addresses this challenge. It imputes counterfactuals for each treated unit using control group information based on a linear interactive fixed effect model that incorporates unit-specific intercepts interacted with time-varying coefficients. It not only relaxes the often-violated \"parallel trends\" assumption, but also unifies the synthetic control method with linear fixed effect models under a simple framework. The second chapter examines the effect of Election Day Registration (EDR) laws on voter turnout in the United States. Conventional difference-in-differences approach suggests that EDR laws had almost no impact on voter turnout. Using the generalized synthetic control method, I show that EDR laws increased turnout in early adopting states but not in states that introduced them more recently. The third chapter investigates the role of informal institutions on the quality of governance in the context of rural China. Using TSCS analysis and a regression discontinuity design, I show that village leaders from large lineage groups are associated with considerably more local public investment. This association is stronger when the groups appeared to be more cohesive."
}, {
    "id": "oai:dspace.mit.edu:1721.1/77829",
    "title": "Democratization and the development of Japan's uneven welfare state",
    "abstract": "Comparative data reveal that Japan consistently has had one of the highest poverty rates among advanced industrialized nations, yet its government taxes the poor more heavily and gives them less in public cash transfers than its peers. Why does a country, endowed with democratic institutions, deep pockets, and a sizable social welfare system provide so little public assistance to the poor? I identify two features of Japan's political and economic development that gave rise to a distinctively threadbare safety net. First, the country's late-developer status paired with state-led industrial development incentivized the primary interest groups-namely, the agrarian landlords, industrialists, and organized labor-to oppose redistribution. Second, the manner in which democratic institutions were introduced in the late nineteenth century and the subsequent expansion of suffrage enabled these groups to gain political influence and block expansion of poor relief in the Diet. Beyond formulating redistributive policies, they locked in the minimalist pattern of redistribution by denying the poor the right to vote (pre-1945) and adopting an electoral system that muted their political voice after suffrage was obtained (post-1945). Consequently, Japan's welfare state developed unevenly, featuring a heavy layer of social insurance programs that benefit well-organized interest groups and an exceptionally minimalist public assistance program for the poor. Thus, contrary to extant theories that associate democracy, economic modernization, and a robust labor movement with higher social spending for the poor, I show that these factors stifled redistribution in the case of Japan. My findings strongly suggest that how a country built its democracy and wealth influences whether a welfare state reinforces or ameliorates existing inequality.",
    "advisors": ["Richard J. Samuels"],
    "text": "Democratization and the development of Japan's uneven welfare state Comparative data reveal that Japan consistently has had one of the highest poverty rates among advanced industrialized nations, yet its government taxes the poor more heavily and gives them less in public cash transfers than its peers. Why does a country, endowed with democratic institutions, deep pockets, and a sizable social welfare system provide so little public assistance to the poor? I identify two features of Japan's political and economic development that gave rise to a distinctively threadbare safety net. First, the country's late-developer status paired with state-led industrial development incentivized the primary interest groups-namely, the agrarian landlords, industrialists, and organized labor-to oppose redistribution. Second, the manner in which democratic institutions were introduced in the late nineteenth century and the subsequent expansion of suffrage enabled these groups to gain political influence and block expansion of poor relief in the Diet. Beyond formulating redistributive policies, they locked in the minimalist pattern of redistribution by denying the poor the right to vote (pre-1945) and adopting an electoral system that muted their political voice after suffrage was obtained (post-1945). Consequently, Japan's welfare state developed unevenly, featuring a heavy layer of social insurance programs that benefit well-organized interest groups and an exceptionally minimalist public assistance program for the poor. Thus, contrary to extant theories that associate democracy, economic modernization, and a robust labor movement with higher social spending for the poor, I show that these factors stifled redistribution in the case of Japan. My findings strongly suggest that how a country built its democracy and wealth influences whether a welfare state reinforces or ameliorates existing inequality."
}, {
    "id": "oai:dspace.mit.edu:1721.1/84853",
    "title": "Publicity-driven accountability in China",
    "abstract": "What, if anything, renders unelected bureaucrats accountable to the public? This thesis draws upon field research on contemporary China's news media, officials, and activists to theorize the role of publicity in non-electoral accountability. \"Publicity-driven accountability\" argues that even in highly undemocratic settings officials respond to critical media coverage for two reasons: revealing agency slippage and producing common knowledge about government failings. This mechanism empowers the news media and individual citizens even when formal political rights are severely curtailed, producing a degree of public accountability within authoritarian institutions. The study begins with original evidence that China's Internet news outlets created forms of journalistic autonomy within the constraints of state censorship. Next it documents the sensitivity of Chinese officials to negative media coverage with an original survey experiment on local bureaucrats. The third empirical chapter provides case studies of contemporary activists in China wielding publicity to change the behavior of unelected officials. Publicity-driven accountability has consequences for theories of political development and the roles of both authority and information in aligning nondemocratic governance with the public interest.",
    "advisors": ["Edward Steinfeld"],
    "text": "Publicity-driven accountability in China What, if anything, renders unelected bureaucrats accountable to the public? This thesis draws upon field research on contemporary China's news media, officials, and activists to theorize the role of publicity in non-electoral accountability. \"Publicity-driven accountability\" argues that even in highly undemocratic settings officials respond to critical media coverage for two reasons: revealing agency slippage and producing common knowledge about government failings. This mechanism empowers the news media and individual citizens even when formal political rights are severely curtailed, producing a degree of public accountability within authoritarian institutions. The study begins with original evidence that China's Internet news outlets created forms of journalistic autonomy within the constraints of state censorship. Next it documents the sensitivity of Chinese officials to negative media coverage with an original survey experiment on local bureaucrats. The third empirical chapter provides case studies of contemporary activists in China wielding publicity to change the behavior of unelected officials. Publicity-driven accountability has consequences for theories of political development and the roles of both authority and information in aligning nondemocratic governance with the public interest."
}, {
    "id": "oai:dspace.mit.edu:1721.1/8777",
    "title": "The helicopter innovation in Army aviation",
    "abstract": "This study was performed to test competing theories of innovation for their explanatory power in describing the series of innovations in United States Army aviation centered on the helicopter. The theories of strategic threat, civilian intervention, and inter service rivalry were applied to the innovations of air mobility, the anti-tank helicopter, and recent developments in Army doctrinal thought. This study found that while strategic threat and inter-service rivalry theories provided adequate explanation for the developmental phases of the innovation, only civilian intervention could fully explain the implementation of these innovations into Army force structure.",
    "advisors": ["Barry R. Posen"],
    "text": "The helicopter innovation in Army aviation This study was performed to test competing theories of innovation for their explanatory power in describing the series of innovations in United States Army aviation centered on the helicopter. The theories of strategic threat, civilian intervention, and inter service rivalry were applied to the innovations of air mobility, the anti-tank helicopter, and recent developments in Army doctrinal thought. This study found that while strategic threat and inter-service rivalry theories provided adequate explanation for the developmental phases of the innovation, only civilian intervention could fully explain the implementation of these innovations into Army force structure."
}, {
    "id": "oai:dspace.mit.edu:1721.1/29980",
    "title": "The subnational politics of structured adjustment in Argentina : the case of San Luis",
    "abstract": "The launching of liberal economic reforms in emerging democratic societies during the 1980s and 1990s arouse widespread scholarly interest. The Argentine policy shift to structural adjustment under President Carlos Menem (1989-1995 and 1995-1999) was largely regarded by the literature as a successful example of radical market-oriented reforms. Despite major policy achievements, crisis and socio-political conflict in the provincial arena pervaded the Argentine road to structural adjustment. After the mid-1990s, it was clear that the political and fiscal situation of the provinces was one of the weakest spots the Argentine unconstrained model of liberal economic reforms. In a federal country characterized by dramatic regional asymmetries, the reversal of past equalizing polices had significant political and economic effects. Although subnational responses to structural adjustment varied, successful provincial performers were the exception rather than the rule. San Luis, an intermediate development province traditionally dependent on national revenue transfers appeared to be one of the most successful cases of policy adaptation to market-oriented reforms. Not only did San Luis perform quite successfully in the hostile environment of structural adjustment but the province represents an almost unique case of enduring late state-led industrialization and state building in a situation characterized by the decreasing capacity of the provincial administrations to efficiently manage the implementation of belt-tightening economic reforms. This thesis is about an unexpected subnational outcome:",
    "advisors": ["Joshua Cohen"],
    "text": "The subnational politics of structured adjustment in Argentina : the case of San Luis The launching of liberal economic reforms in emerging democratic societies during the 1980s and 1990s arouse widespread scholarly interest. The Argentine policy shift to structural adjustment under President Carlos Menem (1989-1995 and 1995-1999) was largely regarded by the literature as a successful example of radical market-oriented reforms. Despite major policy achievements, crisis and socio-political conflict in the provincial arena pervaded the Argentine road to structural adjustment. After the mid-1990s, it was clear that the political and fiscal situation of the provinces was one of the weakest spots the Argentine unconstrained model of liberal economic reforms. In a federal country characterized by dramatic regional asymmetries, the reversal of past equalizing polices had significant political and economic effects. Although subnational responses to structural adjustment varied, successful provincial performers were the exception rather than the rule. San Luis, an intermediate development province traditionally dependent on national revenue transfers appeared to be one of the most successful cases of policy adaptation to market-oriented reforms. Not only did San Luis perform quite successfully in the hostile environment of structural adjustment but the province represents an almost unique case of enduring late state-led industrialization and state building in a situation characterized by the decreasing capacity of the provincial administrations to efficiently manage the implementation of belt-tightening economic reforms. This thesis is about an unexpected subnational outcome:"
}, {
    "id": "oai:dspace.mit.edu:1721.1/99775",
    "title": "Red lines and faits accomplis in interstate coercion and crisis",
    "abstract": "The International Relations literature has an established view of interstate crises that explains how states pursue victory in terms of signaling resolve. States make gains with credible coercive threats (compellence). In contrast, this dissertation conceives of each crisis as a strategic competition between a challenger seeking to make gains unilaterally by fait accompli and its adversary's countervailing efforts to set red lines to deter these faits accomplis. After clarifying the neglected concepts of \"red line\" and \"fait accompli,\" the dissertation takes up two questions the literature has left unexplored: When are faits accomplis likely to occur? When are they likely to lead to war? The result is a theory of coercive conflict explaining why deterrent red lines that contain any of four weaknesses -- types of gray areas, in essence -- are especially vulnerable to faits accomplis. This theory is tested with two case studies -- the 1948-1949 Berlin Blockade Crisis and the 1962 Cuban Missile Crisis -- and an analysis of gray areas and land grabs in territorial crises since 1918. Making extensive use of declassified documents, the case studies show that the \"game\" of crises need not be a matter of convincing the adversary of one's willingness to fight. Instead, states pursue victory by finding gray areas and other weaknesses in deterrent red lines that they can exploit to unilaterally take as much as possible -- often by fait accompli -- without crossing the line and overtly firing on the other side. Crises, from this standpoint, are a game of finding ways to advance without attacking. The analysis of territorial crises makes use of original data on all land grab faits accomplis since 1918. It shows first that states far more often make territorial gains by fait accompli than by coercing a territorial cession. It then focuses on the impact of geographical gray areas, which take two forms: islands located awkwardly between two core territories and border ambiguities. It finds that two-thirds of all land grabs since 1918 targeted a gray area. These gray areas render faits accomplis more effective at making a gain without provoking war and, consequently, more likely to occur.",
    "advisors": ["Barry Posen"],
    "text": "Red lines and faits accomplis in interstate coercion and crisis The International Relations literature has an established view of interstate crises that explains how states pursue victory in terms of signaling resolve. States make gains with credible coercive threats (compellence). In contrast, this dissertation conceives of each crisis as a strategic competition between a challenger seeking to make gains unilaterally by fait accompli and its adversary's countervailing efforts to set red lines to deter these faits accomplis. After clarifying the neglected concepts of \"red line\" and \"fait accompli,\" the dissertation takes up two questions the literature has left unexplored: When are faits accomplis likely to occur? When are they likely to lead to war? The result is a theory of coercive conflict explaining why deterrent red lines that contain any of four weaknesses -- types of gray areas, in essence -- are especially vulnerable to faits accomplis. This theory is tested with two case studies -- the 1948-1949 Berlin Blockade Crisis and the 1962 Cuban Missile Crisis -- and an analysis of gray areas and land grabs in territorial crises since 1918. Making extensive use of declassified documents, the case studies show that the \"game\" of crises need not be a matter of convincing the adversary of one's willingness to fight. Instead, states pursue victory by finding gray areas and other weaknesses in deterrent red lines that they can exploit to unilaterally take as much as possible -- often by fait accompli -- without crossing the line and overtly firing on the other side. Crises, from this standpoint, are a game of finding ways to advance without attacking. The analysis of territorial crises makes use of original data on all land grab faits accomplis since 1918. It shows first that states far more often make territorial gains by fait accompli than by coercing a territorial cession. It then focuses on the impact of geographical gray areas, which take two forms: islands located awkwardly between two core territories and border ambiguities. It finds that two-thirds of all land grabs since 1918 targeted a gray area. These gray areas render faits accomplis more effective at making a gain without provoking war and, consequently, more likely to occur."
}, {
    "id": "oai:dspace.mit.edu:1721.1/34404",
    "title": "Do parties still matter? : the politics of gubernatorial nominations",
    "abstract": "Who controls the nomination in gubernatorial elections? This dissertation seeks to answer this simple question. Parties have classically been the organizations held responsible for throwing their collective effort behind a candidate and controlling the nominations. Yet, in recent years, scholars have noted a steady weakening of American political parties through a succession of major alterations in the political landscape: the loss of patronage-based organizations traditionally used to uphold party organizations; competition from interest groups; and the ascendancy of media-based campaigns and political consultants which buoy candidates' personal organizations. Not only that, recent work suggests that national party organizations have displaced their state-level counterparts. The combined result of these strains on the party system, scholars conclude, is the rise of a candidate-centered politics and of an electoral politics that can no longer count parties as critical factors in the political system. My dissertation tests whether parties have been dealt out of the nominations process in gubernatorial primary elections in six states: Colorado, Illinois, Ohio, Massachusetts, New Mexico, and Texas. My principal evidence is elite public endorsements of candidates. I find that the tempo, quantity, and quality of endorsement activity varies from election to election according to many factors. My research finds that endorsement activity fluctuates within four principal domains - across election type (general or primary), across the level of competition in a given election, across party, and across states. Contrary to many recent studies, I do not find evidence of an \"extended party\" - of a broad set of actors (interest groups and highly-partisan influential elites)",
    "advisors": ["Stephen Ansolabehere"],
    "text": "Do parties still matter? : the politics of gubernatorial nominations Who controls the nomination in gubernatorial elections? This dissertation seeks to answer this simple question. Parties have classically been the organizations held responsible for throwing their collective effort behind a candidate and controlling the nominations. Yet, in recent years, scholars have noted a steady weakening of American political parties through a succession of major alterations in the political landscape: the loss of patronage-based organizations traditionally used to uphold party organizations; competition from interest groups; and the ascendancy of media-based campaigns and political consultants which buoy candidates' personal organizations. Not only that, recent work suggests that national party organizations have displaced their state-level counterparts. The combined result of these strains on the party system, scholars conclude, is the rise of a candidate-centered politics and of an electoral politics that can no longer count parties as critical factors in the political system. My dissertation tests whether parties have been dealt out of the nominations process in gubernatorial primary elections in six states: Colorado, Illinois, Ohio, Massachusetts, New Mexico, and Texas. My principal evidence is elite public endorsements of candidates. I find that the tempo, quantity, and quality of endorsement activity varies from election to election according to many factors. My research finds that endorsement activity fluctuates within four principal domains - across election type (general or primary), across the level of competition in a given election, across party, and across states. Contrary to many recent studies, I do not find evidence of an \"extended party\" - of a broad set of actors (interest groups and highly-partisan influential elites)"
}, {
    "id": "oai:dspace.mit.edu:1721.1/42390",
    "title": "Effects of anti-poverty programs on electoral behavior : evidence from the Mexican Education, Health, and Nutrition Program",
    "abstract": "Ever since Latin American economies collapsed in the 1980s and early 1990s, traditional redistributive programs began to coexist with new anti-poverty programs that usually took the form of conditional cash transfers (CCT). I examine the effects of the Mexican Education, Health, and Nutrition program (Progresa), the first and largest CCT implemented in the region, on electoral behavior. I argue that Progresa not only was substantially different from traditional clientelism, but that it challenged local monopolies on political power by increasing voter's income and giving recipients implicit and explicit information about its non-political nature. This weakening of monopolies, in turn, gave political parties incentives to compete for the votes of Progresa recipients. As a consequence, recipients increased their electoral participation, at least in the short term, and clientelism was irrevocably eroded. Despite the increased competition, however, recipients rewarded parties that proposed and retained Progresa. My understanding of Progresa's electoral effects is based on theory, field research on four villages, interviews with Progresa's designers and personnel, and analysis of media sources from 1996 until 2003. To test this argument, I use the Mexico 2000 Panel Study; aggregate data at the municipality level from 1997-2003; and to explicitly deal with the historic correlation between poverty, rural residence, and support for the seventy-year incumbent party, Institutional Revolutionary Party, I take advantage of the fact that early assignment of program benefits included a randomized component originally designed to evaluate the program effects on schooling and health.",
    "advisors": ["Chappell Lawson"],
    "text": "Effects of anti-poverty programs on electoral behavior : evidence from the Mexican Education, Health, and Nutrition Program Ever since Latin American economies collapsed in the 1980s and early 1990s, traditional redistributive programs began to coexist with new anti-poverty programs that usually took the form of conditional cash transfers (CCT). I examine the effects of the Mexican Education, Health, and Nutrition program (Progresa), the first and largest CCT implemented in the region, on electoral behavior. I argue that Progresa not only was substantially different from traditional clientelism, but that it challenged local monopolies on political power by increasing voter's income and giving recipients implicit and explicit information about its non-political nature. This weakening of monopolies, in turn, gave political parties incentives to compete for the votes of Progresa recipients. As a consequence, recipients increased their electoral participation, at least in the short term, and clientelism was irrevocably eroded. Despite the increased competition, however, recipients rewarded parties that proposed and retained Progresa. My understanding of Progresa's electoral effects is based on theory, field research on four villages, interviews with Progresa's designers and personnel, and analysis of media sources from 1996 until 2003. To test this argument, I use the Mexico 2000 Panel Study; aggregate data at the municipality level from 1997-2003; and to explicitly deal with the historic correlation between poverty, rural residence, and support for the seventy-year incumbent party, Institutional Revolutionary Party, I take advantage of the fact that early assignment of program benefits included a randomized component originally designed to evaluate the program effects on schooling and health."
}, {
    "id": "oai:dspace.mit.edu:1721.1/49677",
    "title": "Freedom and order : how democratic governments abridge civil liberties after terrorist attacks -- and why sometimes they don't",
    "abstract": "This dissertation is driven by the following question: \"What explains the variation in governments' civil liberty-abridging responses to terrorist attacks?\" In the United States, it was not until a year after the 1995 Oklahoma City bombing-and three years after the 1993 World Trade Center bombing-that Bill Clinton signed major civil liberty-limiting, counter-terror legislation in the form of the 1996 Antiterrorism and Effective Death Penalty Act. By contrast, George W. Bush passed the much more comprehensive and repressive Patriot Act through a divided Congress in a month-and-a-half after the September 11, 2001 attacks. In Great Britain, Tony Blair's own party blocked clauses in his anti-terrorism legislation that would have created national ID cards and extended the duration terror suspects could be held without charge to 90 days after the July 7, 2005 London bombings. Yet liberty-reducing counter-terror laws were easily passed time and again after IRA terror attacks in the 1970s, 1980s and 1990s. In Israel, Yitzchak Rabin's government largely forewent abridging liberties during the Oslo peace process, but Ariel Sharon passed numerous liberty-abridging laws such as one prohibiting the granting of citizenship to Palestinians that marry Israelis during the second intifada.",
    "advisors": ["Roger Peterson"],
    "text": "Freedom and order : how democratic governments abridge civil liberties after terrorist attacks -- and why sometimes they don't This dissertation is driven by the following question: \"What explains the variation in governments' civil liberty-abridging responses to terrorist attacks?\" In the United States, it was not until a year after the 1995 Oklahoma City bombing-and three years after the 1993 World Trade Center bombing-that Bill Clinton signed major civil liberty-limiting, counter-terror legislation in the form of the 1996 Antiterrorism and Effective Death Penalty Act. By contrast, George W. Bush passed the much more comprehensive and repressive Patriot Act through a divided Congress in a month-and-a-half after the September 11, 2001 attacks. In Great Britain, Tony Blair's own party blocked clauses in his anti-terrorism legislation that would have created national ID cards and extended the duration terror suspects could be held without charge to 90 days after the July 7, 2005 London bombings. Yet liberty-reducing counter-terror laws were easily passed time and again after IRA terror attacks in the 1970s, 1980s and 1990s. In Israel, Yitzchak Rabin's government largely forewent abridging liberties during the Oslo peace process, but Ariel Sharon passed numerous liberty-abridging laws such as one prohibiting the granting of citizenship to Palestinians that marry Israelis during the second intifada."
}, {
    "id": "oai:dspace.mit.edu:1721.1/8237",
    "title": "Bombs unbuilt : power, ideas and institutions in international politics",
    "abstract": "Nuclear weapons are the most powerful weapons in human history, but contrary to virtually every prediction by scholars, relatively few states have acquired them. Why are there so few nuclear weapons states? What factors lead governments to reject and even renounce the ultimate weapon? What do the disconfirmed predictions of widespread proliferation tell us about contemporary theories of international relations? To answer these questions, this study tests 15 hypotheses based on core categories in international politics: power, resources, ideas, and institutions. The hypotheses on power suggest that a state's nuclear decisions are a function of its external threats and its place in the international system. They claim that the slow pace of proliferation can be explained by several factors: a lack of threat, bipolarity, security guarantees, and superpower pressure. The resource hypotheses emphasize material capability, i.e., whether a state has the money, scientific talent, or access to foreign technology required to develop nuclear weapons. Hypotheses on the role of ideas often focus on the beliefs held by decision makers. This study tests the influence of anti-nuclear norms on proliferation decision making. Institutional explanations highlight either domestic institutional arrangements (whether a state is democratic, whether it is liberalizing economically, its organizational politics) or international institutions like the nonproliferation regime. Many of the tests employ a data set consisting of 132 nuclear decisions and outcomes.",
    "advisors": ["George W. Rathjens"],
    "text": "Bombs unbuilt : power, ideas and institutions in international politics Nuclear weapons are the most powerful weapons in human history, but contrary to virtually every prediction by scholars, relatively few states have acquired them. Why are there so few nuclear weapons states? What factors lead governments to reject and even renounce the ultimate weapon? What do the disconfirmed predictions of widespread proliferation tell us about contemporary theories of international relations? To answer these questions, this study tests 15 hypotheses based on core categories in international politics: power, resources, ideas, and institutions. The hypotheses on power suggest that a state's nuclear decisions are a function of its external threats and its place in the international system. They claim that the slow pace of proliferation can be explained by several factors: a lack of threat, bipolarity, security guarantees, and superpower pressure. The resource hypotheses emphasize material capability, i.e., whether a state has the money, scientific talent, or access to foreign technology required to develop nuclear weapons. Hypotheses on the role of ideas often focus on the beliefs held by decision makers. This study tests the influence of anti-nuclear norms on proliferation decision making. Institutional explanations highlight either domestic institutional arrangements (whether a state is democratic, whether it is liberalizing economically, its organizational politics) or international institutions like the nonproliferation regime. Many of the tests employ a data set consisting of 132 nuclear decisions and outcomes."
}, {
    "id": "oai:dspace.mit.edu:1721.1/68923",
    "title": "Enforcement without autonomy : the politics of labor and environmental regulation in Argentina",
    "abstract": "How can states with weak and politicized bureaucracies enforce labor and environmental regulations? Through a study of subnational variation in Argentina, this dissertation develops a framework to explain why bureaucrats are able to enforce regulations in some cases and not others. The framework focuses on two factors: the strength of linkages between bureaucrats and civil society organizations, and the level of administrative capacity in the bureaucracy. Strong linkages can facilitate routinized resource sharing and the construction of pro-enforcement coalitions, and administrative capacity determines whether bureaucrats passively or strategically use societal resources. By explaining variation in patterns of enforcement that are obscured by existing approaches, this research opens up new possibilities for crafting strategies to strengthen regulatory institutions. The dissertation draws on data collected during sixteen months of field research, including over 250 semi-structured interviews and an original survey of labor inspectors.",
    "advisors": ["Richard M. Locke"],
    "text": "Enforcement without autonomy : the politics of labor and environmental regulation in Argentina How can states with weak and politicized bureaucracies enforce labor and environmental regulations? Through a study of subnational variation in Argentina, this dissertation develops a framework to explain why bureaucrats are able to enforce regulations in some cases and not others. The framework focuses on two factors: the strength of linkages between bureaucrats and civil society organizations, and the level of administrative capacity in the bureaucracy. Strong linkages can facilitate routinized resource sharing and the construction of pro-enforcement coalitions, and administrative capacity determines whether bureaucrats passively or strategically use societal resources. By explaining variation in patterns of enforcement that are obscured by existing approaches, this research opens up new possibilities for crafting strategies to strengthen regulatory institutions. The dissertation draws on data collected during sixteen months of field research, including over 250 semi-structured interviews and an original survey of labor inspectors."
}, {
    "id": "oai:dspace.mit.edu:1721.1/30266",
    "title": "The privatization age? : which services are privatized and why",
    "abstract": "This dissertation examines the determinants of the extent of privatization of service delivery. Despite the fact that right-wing governments extolling neo-liberal policies have placed privatization at the top of the policy agenda in recent years, there are some services that have experienced very little privatization, if at all. The aim of this dissertation is to explain the considerable variation in privatization experiences across service programs in different policy domains, political systems and over time. The research is based on a qualitative analysis of privatization of service delivery, while funding remains public, in three policy domains - education K-12, mental health care, and incarceration - across three different political systems - Massachusetts, Texas and England. The comparison of the nine case studies revealed significant differences in the extent of privatization across the three policy domains. Privatization rates in education were considerably lower than in the two other domains, while privatization rates in mental healthcare were higher. Despite considerable differences in institutional structure and the balance of right-left power, the differences in privatization rates across political systems did not follow any clear pattern. The explanation for the difference across policy domains primarily focus on institutional factors. Programs in policy domains with many stakeholders are less likely to be privatized than programs that are smaller by comparison. Also, programs that are institutionally stable are more difficult to privatize than programs that are undergoing radical institutional restructuring for reasons unrelated to privatization.",
    "advisors": ["Joshua Cohen"],
    "text": "The privatization age? : which services are privatized and why This dissertation examines the determinants of the extent of privatization of service delivery. Despite the fact that right-wing governments extolling neo-liberal policies have placed privatization at the top of the policy agenda in recent years, there are some services that have experienced very little privatization, if at all. The aim of this dissertation is to explain the considerable variation in privatization experiences across service programs in different policy domains, political systems and over time. The research is based on a qualitative analysis of privatization of service delivery, while funding remains public, in three policy domains - education K-12, mental health care, and incarceration - across three different political systems - Massachusetts, Texas and England. The comparison of the nine case studies revealed significant differences in the extent of privatization across the three policy domains. Privatization rates in education were considerably lower than in the two other domains, while privatization rates in mental healthcare were higher. Despite considerable differences in institutional structure and the balance of right-left power, the differences in privatization rates across political systems did not follow any clear pattern. The explanation for the difference across policy domains primarily focus on institutional factors. Programs in policy domains with many stakeholders are less likely to be privatized than programs that are smaller by comparison. Also, programs that are institutionally stable are more difficult to privatize than programs that are undergoing radical institutional restructuring for reasons unrelated to privatization."
}, {
    "id": "oai:dspace.mit.edu:1721.1/107539",
    "title": "The politics of new industrial policy : sectoral governance reform in Vietnam's agro-export industries",
    "abstract": "Research on new industrial policy suggests that developing economies' ability to enter and upgrade in new export industries in the context of globalization depends significantly on the existence of supporting institutions and services, developed through public-private collaboration. Yet despite the consensus that \"good\" sectoral governance matters, we have little understanding of how it emerges, particularly in countries that lack the prerequisites for successful industrial policy. What drives sectoral governance reforms - defined as shifts in sector-specific institutional arrangements or regulations that lower barriers to entry and/or provide collective resources to support firm-level upgrading - in export industries in developing economies? Through a comparative and longitudinal study of variation in governance outcomes within and across the seafood and rice export sectors in Vietnam, this dissertation develops a political framework to explain why some export sectors, at some moments in time, develop nimble, market-responsive governance and others do not. The argument revolves around three factors: industry stakeholder pressure and buy-in, bureaucratic space, and sectoral policy entrepreneurs. By examining variation in governance outcomes, this research moves beyond describing new industrial policymaking to explaining its political origins. It seeks to update literatures on business-government relations and the politics of industrialization to account for a broader set of cases, and in so doing to identify new opportunities for developing economies to take advantage of trade liberalization and globalization, particularly in the growing global food trade. The dissertation draws on data collected during eight months of fieldwork in Vietnam involving 160 interviews with firms, government officials, industry associations and global buyers..",
    "advisors": ["Ben Ross Schneider"],
    "text": "The politics of new industrial policy : sectoral governance reform in Vietnam's agro-export industries Research on new industrial policy suggests that developing economies' ability to enter and upgrade in new export industries in the context of globalization depends significantly on the existence of supporting institutions and services, developed through public-private collaboration. Yet despite the consensus that \"good\" sectoral governance matters, we have little understanding of how it emerges, particularly in countries that lack the prerequisites for successful industrial policy. What drives sectoral governance reforms - defined as shifts in sector-specific institutional arrangements or regulations that lower barriers to entry and/or provide collective resources to support firm-level upgrading - in export industries in developing economies? Through a comparative and longitudinal study of variation in governance outcomes within and across the seafood and rice export sectors in Vietnam, this dissertation develops a political framework to explain why some export sectors, at some moments in time, develop nimble, market-responsive governance and others do not. The argument revolves around three factors: industry stakeholder pressure and buy-in, bureaucratic space, and sectoral policy entrepreneurs. By examining variation in governance outcomes, this research moves beyond describing new industrial policymaking to explaining its political origins. It seeks to update literatures on business-government relations and the politics of industrialization to account for a broader set of cases, and in so doing to identify new opportunities for developing economies to take advantage of trade liberalization and globalization, particularly in the growing global food trade. The dissertation draws on data collected during eight months of fieldwork in Vietnam involving 160 interviews with firms, government officials, industry associations and global buyers.."
}, {
    "id": "oai:dspace.mit.edu:1721.1/117308",
    "title": "One fighting machine : joint learning and tactical airpower operations in World War II",
    "abstract": "militaries learn in war by developing and testing a theory of wartime military learning in the joint operational context. In doing so, the dissertation makes three related arguments. First, it examines not just whether militaries learn in war, but how militaries learn in war. Specifically, it defines learning as a process that includes two distinct but related phases: first, identifying a problem and, second, implementing a solution. From this conceptual standpoint, the dissertation then proposes a theory of wartime military learning that can explain and predict both whether and how a military is likely to learn the lessons of war, which I call Military Filtration Theory (MFT). MFT argues that wartime military learning is best explained by examining the interaction of two key variables: first, the state's national military strategy and, second, the military's resource endowments. These two variables act as a filter on the information that is identified and absorbed by military organizations throughout the learning process. Finally, the dissertation tests MFT against several alternative explanations in the novel and challenging empirical setting of joint operations. Specifically, I examine the different experiences of the British, American, and German militaries in successfully learning to execute tactical airpower operations during World War II. In addition to demonstrating variation in the learning process, these three cases allow me to focus on a subset of joint operations that sets a high bar for existing theories of military learning in ways that the extant literature does not. The findings of this study provide new theoretical and empirical insights for students of military learning, as well as several practical lessons for policymakers and warfighters.",
    "advisors": ["Barry R. Posen"],
    "text": "One fighting machine : joint learning and tactical airpower operations in World War II militaries learn in war by developing and testing a theory of wartime military learning in the joint operational context. In doing so, the dissertation makes three related arguments. First, it examines not just whether militaries learn in war, but how militaries learn in war. Specifically, it defines learning as a process that includes two distinct but related phases: first, identifying a problem and, second, implementing a solution. From this conceptual standpoint, the dissertation then proposes a theory of wartime military learning that can explain and predict both whether and how a military is likely to learn the lessons of war, which I call Military Filtration Theory (MFT). MFT argues that wartime military learning is best explained by examining the interaction of two key variables: first, the state's national military strategy and, second, the military's resource endowments. These two variables act as a filter on the information that is identified and absorbed by military organizations throughout the learning process. Finally, the dissertation tests MFT against several alternative explanations in the novel and challenging empirical setting of joint operations. Specifically, I examine the different experiences of the British, American, and German militaries in successfully learning to execute tactical airpower operations during World War II. In addition to demonstrating variation in the learning process, these three cases allow me to focus on a subset of joint operations that sets a high bar for existing theories of military learning in ways that the extant literature does not. The findings of this study provide new theoretical and empirical insights for students of military learning, as well as several practical lessons for policymakers and warfighters."
}, {
    "id": "oai:dspace.mit.edu:1721.1/83759",
    "title": "Fueled by crisis : U.S. alternative fuel policy, 1975-2007",
    "abstract": "This dissertation investigates the policy-making process that led to three \"crash programs\" for alternative fuels after energy shocks in the 1970s and early 2000s: (1) the proposed Energy Independence Authority in 1975-1976, (2) the Synthetic Fuels Corporation in 1979-1980, and (3) the revised Renewable Fuel Standard in 2007. These were massively ambitious programs, with enormous budgets and unachievable technological goals. What makes them truly puzzling, though, is that they were major policies that emerged without major advocates. Although various interest groups and constituencies supported the development of alternative fuels, neither the powerful industry lobbies (oil, coal, corn, ethanol) nor the public interest groups (environment) had previously advocated for interventions of this scope and scale. This presents a fundamental empirical puzzle for public policy scholars, as it contradicts our understanding of the drivers of policy change. Typically, the policy process literature portrays radical policy change as resulting from the strategic efforts of interest or advocacy groups during a window of opportunity. Here, however, radical policy change occurred in the absence of lobbying or advocacy efforts. What explains this phenomenon? How do we account for the creation of these programs? What conditions and sequence of decision-making led to these policy outcomes? This dissertation develops an alternative model of \"politician-driven policymaking.\" Public alarm over a deepening national crisis is the catalyst for this process. It gives rise to two coupled mechanisms: \"bidding up,\" in which the President and Congress compete for leadership during the crisis, and \"signing on,\" in which interest groups and minority Congressional groups bargain and often bandwagon with the legislative proposals.",
    "advisors": ["Kenneth A. Oye"],
    "text": "Fueled by crisis : U.S. alternative fuel policy, 1975-2007 This dissertation investigates the policy-making process that led to three \"crash programs\" for alternative fuels after energy shocks in the 1970s and early 2000s: (1) the proposed Energy Independence Authority in 1975-1976, (2) the Synthetic Fuels Corporation in 1979-1980, and (3) the revised Renewable Fuel Standard in 2007. These were massively ambitious programs, with enormous budgets and unachievable technological goals. What makes them truly puzzling, though, is that they were major policies that emerged without major advocates. Although various interest groups and constituencies supported the development of alternative fuels, neither the powerful industry lobbies (oil, coal, corn, ethanol) nor the public interest groups (environment) had previously advocated for interventions of this scope and scale. This presents a fundamental empirical puzzle for public policy scholars, as it contradicts our understanding of the drivers of policy change. Typically, the policy process literature portrays radical policy change as resulting from the strategic efforts of interest or advocacy groups during a window of opportunity. Here, however, radical policy change occurred in the absence of lobbying or advocacy efforts. What explains this phenomenon? How do we account for the creation of these programs? What conditions and sequence of decision-making led to these policy outcomes? This dissertation develops an alternative model of \"politician-driven policymaking.\" Public alarm over a deepening national crisis is the catalyst for this process. It gives rise to two coupled mechanisms: \"bidding up,\" in which the President and Congress compete for leadership during the crisis, and \"signing on,\" in which interest groups and minority Congressional groups bargain and often bandwagon with the legislative proposals."
}, {
    "id": "oai:dspace.mit.edu:1721.1/33685",
    "title": "Creative cleavages : distributive politics and electoral alignment",
    "abstract": "Distributive politics plays an important role for political elites for their electoral goal. Since the resources that politicians can distribute are limited, they have to decide how to distribute them in order to maximize their utility. And region becomes one of the most important yardsticks. Politicians first distribute public goods where they are supported. Economically rational voters react to what they receive and cast votes for those who have brought most benefits to them. This establishes the relationship between politicians and voters in a region, and solidifies regional voting behavior further. Therefore, regional voting is the product of politician's strategic distributive politics and rational voters' behavior. Regionalism does not only arise from economic or social cleavages that we are able to observe, but they are also intentionally cultivated. There are numerous possible reasons why regionalism started in the first place. It could have been from ethnic cleavages, unequal economic development or historical and political events that occurred in the past. However, whether or not to breed regionalism is determined by how regionalism works for the political purpose.",
    "advisors": ["Stephen D. Ansolabehere"],
    "text": "Creative cleavages : distributive politics and electoral alignment Distributive politics plays an important role for political elites for their electoral goal. Since the resources that politicians can distribute are limited, they have to decide how to distribute them in order to maximize their utility. And region becomes one of the most important yardsticks. Politicians first distribute public goods where they are supported. Economically rational voters react to what they receive and cast votes for those who have brought most benefits to them. This establishes the relationship between politicians and voters in a region, and solidifies regional voting behavior further. Therefore, regional voting is the product of politician's strategic distributive politics and rational voters' behavior. Regionalism does not only arise from economic or social cleavages that we are able to observe, but they are also intentionally cultivated. There are numerous possible reasons why regionalism started in the first place. It could have been from ethnic cleavages, unequal economic development or historical and political events that occurred in the past. However, whether or not to breed regionalism is determined by how regionalism works for the political purpose."
}, {
    "id": "oai:dspace.mit.edu:1721.1/92054",
    "title": "Made for export : labor migration, state power, and higher education in a developing Philippine economy",
    "abstract": "Development scholars, heavily influenced by the cases of the four Asian Tigers (Hong Kong, Singapore, South Korea, and Taiwan), have attributed success in economic development to education. Although the Philippines seemed even more promising before the Asian Tigers began developing, the educational advances in the Philippines have led to an enormous exodus of labor. Failing to integrate its highly educated labor force in the domestic economy, the Philippine state focused its attention on exporting college-educated/highly-educated workers by creating a set of elaborate institutions to facilitate overseas employment. As a result, currently over 10 percent of its citizens live abroad in over 160 countries and about 4,600 Filipinos leave the country every day for overseas work. Why did the Philippine government develop institutions for exporting labor and why has it continued for the past four decades? This dissertation explains how the management of post-secondary educational institutions influenced the initiation and continuation of the Philippine labor export program. From its start, two interrelated problems motivated the creation of the Philippine labor exporting state: (1) over-development of the educational system through an unregulated, laissez-faire approach to private higher education and (2) underdevelopment of the economy to absorb high-skilled labor in the domestic labor market. President Ferdinand Marcos and his technocrats developed the 1974 labor export program to relieve the country of these twin problems by providing overseas employment for the educated unemployed and generating foreign currency revenues from the remittances received from Filipinos working abroad. Over time, political pressures from overseas Filipinos and migrant households, coupled with growing remittance revenue and a large private recruitment industry, led to further development of the labor exporting state with the creation of new state emigrant institutions for managing, protecting, and representing Overseas Filipino Workers (OFWs). These new state institutions, overseas demand for Filipino workers, domestic demand for remittances, and a highly flexible and unregulated private higher educational system continues to drive the exporting of Filipino labor to this day. Empirically, this dissertation is based on twelve months of fieldwork in the Philippines and relies on multiple research methods: archival research, statistical methods empirically testing the relationship between post-secondary education and out-migration, over one hundred interviews of key actors in the labor export and higher education industries, quantitative data analysis using survey and census data from the 1950s through 2011, the creation and analysis of an original dataset of family ownership of all private higher educational institutions in the Philippines, and a review of government documents and legislation.",
    "advisors": ["Michael J. Piore"],
    "text": "Made for export : labor migration, state power, and higher education in a developing Philippine economy Development scholars, heavily influenced by the cases of the four Asian Tigers (Hong Kong, Singapore, South Korea, and Taiwan), have attributed success in economic development to education. Although the Philippines seemed even more promising before the Asian Tigers began developing, the educational advances in the Philippines have led to an enormous exodus of labor. Failing to integrate its highly educated labor force in the domestic economy, the Philippine state focused its attention on exporting college-educated/highly-educated workers by creating a set of elaborate institutions to facilitate overseas employment. As a result, currently over 10 percent of its citizens live abroad in over 160 countries and about 4,600 Filipinos leave the country every day for overseas work. Why did the Philippine government develop institutions for exporting labor and why has it continued for the past four decades? This dissertation explains how the management of post-secondary educational institutions influenced the initiation and continuation of the Philippine labor export program. From its start, two interrelated problems motivated the creation of the Philippine labor exporting state: (1) over-development of the educational system through an unregulated, laissez-faire approach to private higher education and (2) underdevelopment of the economy to absorb high-skilled labor in the domestic labor market. President Ferdinand Marcos and his technocrats developed the 1974 labor export program to relieve the country of these twin problems by providing overseas employment for the educated unemployed and generating foreign currency revenues from the remittances received from Filipinos working abroad. Over time, political pressures from overseas Filipinos and migrant households, coupled with growing remittance revenue and a large private recruitment industry, led to further development of the labor exporting state with the creation of new state emigrant institutions for managing, protecting, and representing Overseas Filipino Workers (OFWs). These new state institutions, overseas demand for Filipino workers, domestic demand for remittances, and a highly flexible and unregulated private higher educational system continues to drive the exporting of Filipino labor to this day. Empirically, this dissertation is based on twelve months of fieldwork in the Philippines and relies on multiple research methods: archival research, statistical methods empirically testing the relationship between post-secondary education and out-migration, over one hundred interviews of key actors in the labor export and higher education industries, quantitative data analysis using survey and census data from the 1950s through 2011, the creation and analysis of an original dataset of family ownership of all private higher educational institutions in the Philippines, and a review of government documents and legislation."
}, {
    "id": "oai:dspace.mit.edu:1721.1/8239",
    "title": "Political liberalism, social pluralism and group conflict",
    "abstract": "This dissertation develops a political liberal approach to multiculturalism as an alternative to its dismissal by some egalitarian liberals and its celebration by some multicultural liberals. Some egalitarian liberals overstate the liberal tension with group-specific claims, disregard the role of culture in a person's life, and exaggerate the propensity of group-specific claims to exacerbate conflict. Confusing religion with culture, they assign to religion the status of an all purpose good that liberals traditionally assign to income and wealth. While political liberals require that the state grant exemptions to religious practices that violate uniform rules, these egalitarian liberals do not. Some multicultural liberals overstate the liberal failure to accommodate group-specific political claims, exaggerate the role of culture in a person's life, and ignore the invented nature of culture. Confusing culture with religion, they assign to culture the moral weight liberals traditionally assign to religion. Political liberals, however, assign to culture the same social weight they assign to a person's family, firm, neighborhood and other associations. Political liberals also distinguish encompassing groups, such as language-nations or factory-towns, whose members primarily live, work and socialize with their own group, from other groups, whose members do not. The former have greater social weight, though not moral weight, than the latter. This leads political liberals to require state support for encompassing groups to adjust to new social and economic circumstances, irrespective of whether they are cultural.",
    "advisors": ["Joshua Cohen"],
    "text": "Political liberalism, social pluralism and group conflict This dissertation develops a political liberal approach to multiculturalism as an alternative to its dismissal by some egalitarian liberals and its celebration by some multicultural liberals. Some egalitarian liberals overstate the liberal tension with group-specific claims, disregard the role of culture in a person's life, and exaggerate the propensity of group-specific claims to exacerbate conflict. Confusing religion with culture, they assign to religion the status of an all purpose good that liberals traditionally assign to income and wealth. While political liberals require that the state grant exemptions to religious practices that violate uniform rules, these egalitarian liberals do not. Some multicultural liberals overstate the liberal failure to accommodate group-specific political claims, exaggerate the role of culture in a person's life, and ignore the invented nature of culture. Confusing culture with religion, they assign to culture the moral weight liberals traditionally assign to religion. Political liberals, however, assign to culture the same social weight they assign to a person's family, firm, neighborhood and other associations. Political liberals also distinguish encompassing groups, such as language-nations or factory-towns, whose members primarily live, work and socialize with their own group, from other groups, whose members do not. The former have greater social weight, though not moral weight, than the latter. This leads political liberals to require state support for encompassing groups to adjust to new social and economic circumstances, irrespective of whether they are cultural."
}, {
    "id": "oai:dspace.mit.edu:1721.1/92080",
    "title": "Inference in tough places : essays on modeling and matching with applications to civil conflict",
    "abstract": "This dissertation focuses on the challenges of making inferences from observational data in the social sciences, with particular application to situations of violent conflict. The first essay utilizes quasi-experimental conditions to examine the effects of violence against civilians in Darfur, Sudan on attitudes towards peace and reconciliation. The second and third essays both address a common but overlooked challenge to making inferences from observational data: even when unobserved confounding can be ruled out, correctly \"conditioning on\" or \"adjusting for\" covariates remains a challenge. In all but the simplest cases, existing methods ensure unbiased estimation only when the investigator can correctly specify the functional relationship between covariates and the outcome. The second essay (with Jens Hainmueller) introduces Kernel Regularized Least Sqaures (KRLS), a flexible modelling approach that provides investigators with a powerful tool to estimate marginal effects, without linearity or additivity assumptions, and at low risk of misspecification bias. The third essay introduces Kernel Balancing (KBAL), a weighting method that mitigates the risk of misspecification bias by establishing high-order balance between treated and control samples without balance testing or a specification search.",
    "advisors": ["Jens Hainmueller"],
    "text": "Inference in tough places : essays on modeling and matching with applications to civil conflict This dissertation focuses on the challenges of making inferences from observational data in the social sciences, with particular application to situations of violent conflict. The first essay utilizes quasi-experimental conditions to examine the effects of violence against civilians in Darfur, Sudan on attitudes towards peace and reconciliation. The second and third essays both address a common but overlooked challenge to making inferences from observational data: even when unobserved confounding can be ruled out, correctly \"conditioning on\" or \"adjusting for\" covariates remains a challenge. In all but the simplest cases, existing methods ensure unbiased estimation only when the investigator can correctly specify the functional relationship between covariates and the outcome. The second essay (with Jens Hainmueller) introduces Kernel Regularized Least Sqaures (KRLS), a flexible modelling approach that provides investigators with a powerful tool to estimate marginal effects, without linearity or additivity assumptions, and at low risk of misspecification bias. The third essay introduces Kernel Balancing (KBAL), a weighting method that mitigates the risk of misspecification bias by establishing high-order balance between treated and control samples without balance testing or a specification search."
}, {
    "id": "oai:dspace.mit.edu:1721.1/28668",
    "title": "The pobladores and local democracy in Chile : the case of El Bosque and Pealoln",
    "abstract": "(cont.) models of local governance, questioning blanket statements about the virtues of political decentralization. The managerial elitist\" model favors individual participation and technical/centralized decision-making that precludes public deliberation. It hardly engages the pobladores' organizations in the local polity and policymaking, fostering organizational fragmentation, selective deactivation and cientism. The \"participatory-deliberative\" style combines innovative adaptation of public policies to \"fit\" the local demand, extensive use of networks and public forums. It generates pre-political spaces that pave the way for the pobladores' organizations to scale-up decisionmaking in the local government or along policy networks at higher levels.",
    "advisors": ["Richard M. Locke"],
    "text": "The pobladores and local democracy in Chile : the case of El Bosque and Pealoln (cont.) models of local governance, questioning blanket statements about the virtues of political decentralization. The managerial elitist\" model favors individual participation and technical/centralized decision-making that precludes public deliberation. It hardly engages the pobladores' organizations in the local polity and policymaking, fostering organizational fragmentation, selective deactivation and cientism. The \"participatory-deliberative\" style combines innovative adaptation of public policies to \"fit\" the local demand, extensive use of networks and public forums. It generates pre-political spaces that pave the way for the pobladores' organizations to scale-up decisionmaking in the local government or along policy networks at higher levels."
}, {
    "id": "oai:dspace.mit.edu:1721.1/35544",
    "title": "Beyond the letter of the law : transforming labor institutions and regulations in Argentina",
    "abstract": "This dissertation analyzes the factors that lead to the transformation of labor regulations and institutions after the opening of previously closed economies, using the case of Argentina as a \"crucial case\". In the 1990s, almost every government in the Latin American region attempted to reform its labor code and systems of labor relations. However, despite these attempts at reform, the labor codes and their systems of labor relations appeared resilient to change. The bulk of the literature on the political economy of reforms had concluded that labor unions had managed to stall or derailed these attempts, although unions had been unsuccessful at stopping all other market-oriented reforms. I conceptualized the labor codes and the system of labor relations considering the way they work in practice, including informal arrangements, and I use the notion of \"labor regimes\". This conceptualization differs from the dominant approach on this issue, which focuses almost exclusively on changes in labor codes approved by Congress. Using this approach, I argue that a system of rigid labor laws and centralized bargaining institutions in a more competitive, pro-business environment tends to get relaxed and more decentralized.",
    "advisors": ["Richard M. Locke"],
    "text": "Beyond the letter of the law : transforming labor institutions and regulations in Argentina This dissertation analyzes the factors that lead to the transformation of labor regulations and institutions after the opening of previously closed economies, using the case of Argentina as a \"crucial case\". In the 1990s, almost every government in the Latin American region attempted to reform its labor code and systems of labor relations. However, despite these attempts at reform, the labor codes and their systems of labor relations appeared resilient to change. The bulk of the literature on the political economy of reforms had concluded that labor unions had managed to stall or derailed these attempts, although unions had been unsuccessful at stopping all other market-oriented reforms. I conceptualized the labor codes and the system of labor relations considering the way they work in practice, including informal arrangements, and I use the notion of \"labor regimes\". This conceptualization differs from the dominant approach on this issue, which focuses almost exclusively on changes in labor codes approved by Congress. Using this approach, I argue that a system of rigid labor laws and centralized bargaining institutions in a more competitive, pro-business environment tends to get relaxed and more decentralized."
}, {
    "id": "oai:dspace.mit.edu:1721.1/64616",
    "title": "Bankruptcy, guns or campaigns : explaining armed organizations' post-war trajectories",
    "abstract": "This project seeks to explain what happens to armed organizations after they sign peace accords. Why do they dissolve, return to war, or form non-violent socio-political entities (political parties or civic associations)? To explain variation in post-war outcomes, my argument centers on the human geography of armed groups. Recruitment, deployment, and post-war migration patterns generate distinct configurations of a) collective capacity, b) relations with civilians, and c) inter-armed group dynamics. I propose that, if a rebel or paramilitary unit recruits in a geographically concentrated area and deploys its fighters in their home communities, the organization will persist and transform into a socio-political entity after disarming. If instead the organization recruits in a dispersed manner, deploys its soldiers away from their towns of origin, and the soldiers either return home or displace to a third locale, the group will disintegrate; it will lose its capacity for collective action. By bankrupting some organizations and preserving others, demobilization has differential effects on armed group capacity. Where it weakens a group, it destabilizes the territorial bargains between the ex-armed group and state and between the group and its contiguous, non-state armed actors. As a result, resumed war becomes likely. If instead, the distribution of power within the system is maintained, the groups will, over time, fully demilitarize and be brought into the state's legal framework. This dissertation is based on rich data collected during fourteen months of fieldwork in Colombia from 2006 to the present during which time I went inside each demobilizing organization to reconstruct and map its postwar trajectory. Exploiting Colombia's unparalleled comparative laboratory for this research, I test the effect of recruitment, deployment, and post-war migration patterns on organizational outcomes using two strategies. First, I conduct a detailed, controlled comparison of armed groups in three regions of Colombia based on interviews of over 200 ex-combatants, civilians, and victims. The second strategy combines these qualitative sources with quantitative ones to evaluate the proposed hypotheses on the entire universe of municipality-armed group dyads in Colombia (n=1040). For this analysis, I rely on municipal-level violent event data, interviews of nearly 100 Colombian experts on the armed conflict, a database of seven years of news articles, and statistical evidence from a series of surveys of former paramilitaries (n=31,472). The empirics provide strong support for the proposed model. The project has significant implications for debates on reintegration, state-building, consolidating peace, reconciliation, decriminalization, and transitions to democracy.",
    "advisors": ["Roger Petersen"],
    "text": "Bankruptcy, guns or campaigns : explaining armed organizations' post-war trajectories This project seeks to explain what happens to armed organizations after they sign peace accords. Why do they dissolve, return to war, or form non-violent socio-political entities (political parties or civic associations)? To explain variation in post-war outcomes, my argument centers on the human geography of armed groups. Recruitment, deployment, and post-war migration patterns generate distinct configurations of a) collective capacity, b) relations with civilians, and c) inter-armed group dynamics. I propose that, if a rebel or paramilitary unit recruits in a geographically concentrated area and deploys its fighters in their home communities, the organization will persist and transform into a socio-political entity after disarming. If instead the organization recruits in a dispersed manner, deploys its soldiers away from their towns of origin, and the soldiers either return home or displace to a third locale, the group will disintegrate; it will lose its capacity for collective action. By bankrupting some organizations and preserving others, demobilization has differential effects on armed group capacity. Where it weakens a group, it destabilizes the territorial bargains between the ex-armed group and state and between the group and its contiguous, non-state armed actors. As a result, resumed war becomes likely. If instead, the distribution of power within the system is maintained, the groups will, over time, fully demilitarize and be brought into the state's legal framework. This dissertation is based on rich data collected during fourteen months of fieldwork in Colombia from 2006 to the present during which time I went inside each demobilizing organization to reconstruct and map its postwar trajectory. Exploiting Colombia's unparalleled comparative laboratory for this research, I test the effect of recruitment, deployment, and post-war migration patterns on organizational outcomes using two strategies. First, I conduct a detailed, controlled comparison of armed groups in three regions of Colombia based on interviews of over 200 ex-combatants, civilians, and victims. The second strategy combines these qualitative sources with quantitative ones to evaluate the proposed hypotheses on the entire universe of municipality-armed group dyads in Colombia (n=1040). For this analysis, I rely on municipal-level violent event data, interviews of nearly 100 Colombian experts on the armed conflict, a database of seven years of news articles, and statistical evidence from a series of surveys of former paramilitaries (n=31,472). The empirics provide strong support for the proposed model. The project has significant implications for debates on reintegration, state-building, consolidating peace, reconciliation, decriminalization, and transitions to democracy."
}, {
    "id": "oai:dspace.mit.edu:1721.1/59149",
    "title": "The U.S. global AIDS response : norms, interests and the duty to treat",
    "abstract": "The dissertation seeks to explain the transformation in the response by the United States to the challenge of global AIDS. Between 1998 and 2008, U.S. spending on global AIDS increased 50-fold to over $6 billion. Most conventional explanations of international politics and foreign assistance give a dominant role to various conceptions of interest, including key economic interests and the strategic interest of powerful states. This dissertation tests these dominant theories against a hypothesis that suggests a more significant role for norms and norm entrepreneurs in shaping political decisions. Neither the influence of important economic interests nor the national security interest of the United States can adequately explain the transformation in U.S. global AIDS policy. Instead, an emerging norm around the duty to provide AIDS treatment and the norm entrepreneurs who championed this idea were the driving force in shaping the U.S. response to global AIDS. Emerging norms require effective champions to capture the attention of a wider public and the support of political leaders. Norm entrepreneurs will be most successful when they adopt the strategies of symbolic politics, leverage politics and accountability politics to influence political leaders.",
    "advisors": ["Michael Piore"],
    "text": "The U.S. global AIDS response : norms, interests and the duty to treat The dissertation seeks to explain the transformation in the response by the United States to the challenge of global AIDS. Between 1998 and 2008, U.S. spending on global AIDS increased 50-fold to over $6 billion. Most conventional explanations of international politics and foreign assistance give a dominant role to various conceptions of interest, including key economic interests and the strategic interest of powerful states. This dissertation tests these dominant theories against a hypothesis that suggests a more significant role for norms and norm entrepreneurs in shaping political decisions. Neither the influence of important economic interests nor the national security interest of the United States can adequately explain the transformation in U.S. global AIDS policy. Instead, an emerging norm around the duty to provide AIDS treatment and the norm entrepreneurs who championed this idea were the driving force in shaping the U.S. response to global AIDS. Emerging norms require effective champions to capture the attention of a wider public and the support of political leaders. Norm entrepreneurs will be most successful when they adopt the strategies of symbolic politics, leverage politics and accountability politics to influence political leaders."
}, {
    "id": "oai:dspace.mit.edu:1721.1/29582",
    "title": "Human cloning : science, ethics, policy, society",
    "abstract": "The interplay of science, ethics, policy and society contribute to our understanding of and relation with human cloning. Genetic science and technology at the end of the twentieth century has permitted successful cloning of mammals and other animals. Such advancement has raised key ethical issues regarding the prospect of cloning human beings. Evaluation of these issues has led to policies aimed at regulating this novel technology. In tum, these policies strive to prepare our society for the scientific possibilities and ethical implications of human cloning.",
    "advisors": ["Hugh Gusterson"],
    "text": "Human cloning : science, ethics, policy, society The interplay of science, ethics, policy and society contribute to our understanding of and relation with human cloning. Genetic science and technology at the end of the twentieth century has permitted successful cloning of mammals and other animals. Such advancement has raised key ethical issues regarding the prospect of cloning human beings. Evaluation of these issues has led to policies aimed at regulating this novel technology. In tum, these policies strive to prepare our society for the scientific possibilities and ethical implications of human cloning."
}, {
    "id": "oai:dspace.mit.edu:1721.1/16818",
    "title": "Digital technology and copyright law",
    "abstract": "Intellectual Property is an ideology of the late Twentieth Century which reserves property-like rights in information, so that creators may extract its economic value. Current American copyright law draws mainly from this concept; it has been constructed through history by negotiation between various established economic interests. Information Freedom is a competing ideology which has been successful in the software community. It emphasizes the dangers of over-propertization and the benefits of freely accessible resources, especially non-depletable information resources. Compromise must be reached in a practical (non-ideological) fashion in order to achieve the social goals of: production of creative content (encouraged by fair but not excessive compensation for creators); promotion of scientific, political, technical, artistic, cultural, and economic progress by removing obstacles to accessing content and taking advantage of innovations which change the status quo; protection of creative freedom; and ensuring quality and diversity in the content which is created. Civil disobedience as a means to achieve these goals may be counterproductive if it results in tighter technological restrictions on content availability or stricter legal mechanisms; legal reforms proposed by Lawrence Lessig and Jessica Litman are unlikely to be enacted. Internet-based technologies have strong potential to increase exposure to diversity, decrease costs, and improve the subjective experience for music consumers. Cheaper film-making equipment may have similar positive effects for motion pictures to a lesser degree. Internet bandwidth and other practical limitations suggest that immediate changes in video distribution and consumption patterns are more likely to be driven by the availability of Digital Video Recorders, or perhaps competing Video On Demand services. Different economic models which fund content creation may be appropriate for different applications, and may in some cases further social goals better than strong propertization. Alternative models include voluntary contributions (either from creators or consumers); indirect benefit by establishing reputation, selling related services, cross-promotion, or selling advertising; and public funding. The history of telecommunication, including the telegraph, radio, television, and the Internet, provides evidence that important uses for new technology may not be initially obvious, that the maturation of digital information technology and related economic models is just beginning.",
    "advisors": ["David I. Kaiser"],
    "text": "Digital technology and copyright law Intellectual Property is an ideology of the late Twentieth Century which reserves property-like rights in information, so that creators may extract its economic value. Current American copyright law draws mainly from this concept; it has been constructed through history by negotiation between various established economic interests. Information Freedom is a competing ideology which has been successful in the software community. It emphasizes the dangers of over-propertization and the benefits of freely accessible resources, especially non-depletable information resources. Compromise must be reached in a practical (non-ideological) fashion in order to achieve the social goals of: production of creative content (encouraged by fair but not excessive compensation for creators); promotion of scientific, political, technical, artistic, cultural, and economic progress by removing obstacles to accessing content and taking advantage of innovations which change the status quo; protection of creative freedom; and ensuring quality and diversity in the content which is created. Civil disobedience as a means to achieve these goals may be counterproductive if it results in tighter technological restrictions on content availability or stricter legal mechanisms; legal reforms proposed by Lawrence Lessig and Jessica Litman are unlikely to be enacted. Internet-based technologies have strong potential to increase exposure to diversity, decrease costs, and improve the subjective experience for music consumers. Cheaper film-making equipment may have similar positive effects for motion pictures to a lesser degree. Internet bandwidth and other practical limitations suggest that immediate changes in video distribution and consumption patterns are more likely to be driven by the availability of Digital Video Recorders, or perhaps competing Video On Demand services. Different economic models which fund content creation may be appropriate for different applications, and may in some cases further social goals better than strong propertization. Alternative models include voluntary contributions (either from creators or consumers); indirect benefit by establishing reputation, selling related services, cross-promotion, or selling advertising; and public funding. The history of telecommunication, including the telegraph, radio, television, and the Internet, provides evidence that important uses for new technology may not be initially obvious, that the maturation of digital information technology and related economic models is just beginning."
}, {
    "id": "oai:dspace.mit.edu:1721.1/112017",
    "title": "Network control in a globalized world : how Visa and Swift's founding structures serve their stakeholders on the International stage",
    "abstract": "The Visa credit card network and the Society for Worldwide Interbank Financial Telecommunications (Swift) network both provide a backbone for financial interchange across the world. Visa's network connects consumers, merchants, banks, and processors to ease the purchases of millions of consumer-facing products worldwide. Swift's interbank network connects banks, corporates, and other financial institutions to ease the flow of high-value, highly-secure international financial transactions. Both networks grew to become industry incumbents in the second half of the 20th century, connecting nearly every country on earth. However, the globalized networks differ in their organizational structures: Visa utilizes a centralized, U.S. focused, hub-and-spoke model; Swift uses a decentralized, transaction-volume neutral, point-to-point network. Although Visa's centralized network fosters innovation, standardization, and security, its U.S.-centered hub pulls the organization from global neutrality and aligns it with the United States on global issues. Meanwhile, although Swift's decentralized network nurtures technological localization-at the expense of technological standardizationits transaction-based global governing structure promotes a relative international neutrality among global organizations. This contrast between Visa and Swift-both networks that balance local and global, centralized and decentralized, and technical and non-technical tensions across the world-reveals the structural effects of worldwide networks, and how network system design impacts global stakeholders in the societies that they touch.",
    "advisors": ["William Deringer"],
    "text": "Network control in a globalized world : how Visa and Swift's founding structures serve their stakeholders on the International stage The Visa credit card network and the Society for Worldwide Interbank Financial Telecommunications (Swift) network both provide a backbone for financial interchange across the world. Visa's network connects consumers, merchants, banks, and processors to ease the purchases of millions of consumer-facing products worldwide. Swift's interbank network connects banks, corporates, and other financial institutions to ease the flow of high-value, highly-secure international financial transactions. Both networks grew to become industry incumbents in the second half of the 20th century, connecting nearly every country on earth. However, the globalized networks differ in their organizational structures: Visa utilizes a centralized, U.S. focused, hub-and-spoke model; Swift uses a decentralized, transaction-volume neutral, point-to-point network. Although Visa's centralized network fosters innovation, standardization, and security, its U.S.-centered hub pulls the organization from global neutrality and aligns it with the United States on global issues. Meanwhile, although Swift's decentralized network nurtures technological localization-at the expense of technological standardizationits transaction-based global governing structure promotes a relative international neutrality among global organizations. This contrast between Visa and Swift-both networks that balance local and global, centralized and decentralized, and technical and non-technical tensions across the world-reveals the structural effects of worldwide networks, and how network system design impacts global stakeholders in the societies that they touch."
}, {
    "id": "oai:dspace.mit.edu:1721.1/8065",
    "title": "The role of military-industrial relations in the history of vaccine innovation",
    "abstract": "This thesis examines the historical conditions that have contributed to high rates of vaccine innovation in the U.S. during the twentieth century. Empirical analysis of vaccine license data demonstrates that the highest rates of innovation were achieved during the 1940's. Historical analysis of this data indicates that a large percentage of these innovations were the product of World War II vaccine development programs. Participation in these programs fostered a uniquely productive culture of collaboration between military and industrial vaccine developers that persisted through the postwar era, maintaining innovation rates through the 1960's and early 1970's. By the mid-1970's, however, the historical circumstances and cultural factors that engendered and sustained military-industrial collaboration began to change, causing rates of vaccine innovation to fall and vaccine stocks to dwindle. Poor economic incentives for vaccine development are often cited as the reason for falling rates of innovation. This explanation is correct but incomplete, because, for example, economic incentives for vaccine development were poor during the 1940's and 1950's, when innovation rates were high. I demonstrate that vaccine innovation is tied to levels of military-industrial collaboration and that declining rates of innovation in recent decades are associated with the disruption of this military-industrial culture of collaboration. Finally, drawing on lessons from this history of military-industrial relations, I examine the opportunities and challenges that the new \"war on terrorism\" presents for efforts to improve vaccine innovation and supplies.",
    "advisors": ["David Mindell"],
    "text": "The role of military-industrial relations in the history of vaccine innovation This thesis examines the historical conditions that have contributed to high rates of vaccine innovation in the U.S. during the twentieth century. Empirical analysis of vaccine license data demonstrates that the highest rates of innovation were achieved during the 1940's. Historical analysis of this data indicates that a large percentage of these innovations were the product of World War II vaccine development programs. Participation in these programs fostered a uniquely productive culture of collaboration between military and industrial vaccine developers that persisted through the postwar era, maintaining innovation rates through the 1960's and early 1970's. By the mid-1970's, however, the historical circumstances and cultural factors that engendered and sustained military-industrial collaboration began to change, causing rates of vaccine innovation to fall and vaccine stocks to dwindle. Poor economic incentives for vaccine development are often cited as the reason for falling rates of innovation. This explanation is correct but incomplete, because, for example, economic incentives for vaccine development were poor during the 1940's and 1950's, when innovation rates were high. I demonstrate that vaccine innovation is tied to levels of military-industrial collaboration and that declining rates of innovation in recent decades are associated with the disruption of this military-industrial culture of collaboration. Finally, drawing on lessons from this history of military-industrial relations, I examine the opportunities and challenges that the new \"war on terrorism\" presents for efforts to improve vaccine innovation and supplies."
}, {
    "id": "oai:dspace.mit.edu:1721.1/34340",
    "title": "Genetic manipulation : the paradox of control in a flexible corporation",
    "abstract": "This dissertation is a two-theme ethnography focusing on the early history of one company within the context of the turbulent business environment of the 1990's. One theme is the control exercised by a corporation to mold its people to achieve certain productive ends, focusing on three areas: culture, physical environment and technology. The second theme is the ability of a corporation to be flexible. Taken together, the two themes form the self-contradictory notion of trying to control a group to increase its ability to be flexible. Many writers who focus on organizations have found the biological metaphor of evolution a useful way to conceptualize some aspects of a successful firm. In contrast I find the biological metaphor of genetic manipulation best illustrates the kind of control exercised by the leadership of this particular firm. From its inception, the leadership team wanted to create a flexible firm, one that could thrive in a turbulent environment. Rather than rely on a multiplicity of heterogeneous experiments, they actively manipulated specific aspects of the firm. The early results, the formation of a successful company, suggested that those controls and the decision to actively mold the firm using such controls were the right choices. When faced with a radical change in the marketplace, the arrival of the Internet economy, the leaders of this firm responded with the same technique and once again were able to mold a successful firm. To the extent that the Internet economy requires companies to change at Internet speed, this firm's ability to manipulate its own \"DNA\" may well be a model for success for other firms in this environment.",
    "advisors": ["Hugh Gusterson"],
    "text": "Genetic manipulation : the paradox of control in a flexible corporation This dissertation is a two-theme ethnography focusing on the early history of one company within the context of the turbulent business environment of the 1990's. One theme is the control exercised by a corporation to mold its people to achieve certain productive ends, focusing on three areas: culture, physical environment and technology. The second theme is the ability of a corporation to be flexible. Taken together, the two themes form the self-contradictory notion of trying to control a group to increase its ability to be flexible. Many writers who focus on organizations have found the biological metaphor of evolution a useful way to conceptualize some aspects of a successful firm. In contrast I find the biological metaphor of genetic manipulation best illustrates the kind of control exercised by the leadership of this particular firm. From its inception, the leadership team wanted to create a flexible firm, one that could thrive in a turbulent environment. Rather than rely on a multiplicity of heterogeneous experiments, they actively manipulated specific aspects of the firm. The early results, the formation of a successful company, suggested that those controls and the decision to actively mold the firm using such controls were the right choices. When faced with a radical change in the marketplace, the arrival of the Internet economy, the leaders of this firm responded with the same technique and once again were able to mold a successful firm. To the extent that the Internet economy requires companies to change at Internet speed, this firm's ability to manipulate its own \"DNA\" may well be a model for success for other firms in this environment."
}, {
    "id": "oai:dspace.mit.edu:1721.1/107312",
    "title": "Platformizing higher education : computer science and the making of MOOC infrastructures",
    "abstract": "This dissertation investigates the role of software in institutional transformation using the example of Massive Open Online Courses or MOOCs. It ethnographically tracks the development of the software infrastructure being built for MOOCs, focusing on three communities-programmers, instructors, and researchers-who centrally participate in the MOOC start-ups' stated mission of reinventing higher education. It argues that MOOC infrastructures are best viewed as an example of a heterogeneous software assemblage that I call the \"software-as-platform,\" that is today being widely deployed and used in a number of industries and institutions. The software-as-platform consists primarily of software that holds together a variety of normative logics: open-endedness; fast, iterative, production processes; data-driven decision-making; governance for emergent effects; scalability; and personalization. Of these, the most important is that its creators give to it an open-endedness as to its ultimate purpose: thus, the assemblage is often framed using the language of \"tools\" or \"platform.\" I then argue that the software-as-platform is a vehicle through which the norms and practices of Silicon Valley are making their way into other institutions, a process I call \"platformization.\" Finally, I suggest that the software-as-platform enables the emergence of a new form of expertise: tool-making. Tool-makers see themselves as building software tools, whose ultimate purpose comes from their users. The tools themselves draw on many other kinds of expert knowledge chosen at the discretion of the tool-builders. The dissertation consists of four chapters bookended by an Introduction and a Conclusion. Chapter 2 is an analysis of the public discourse around MOOCs. Chapter 3 describes MOOC infrastructures, showing how a cluster of institutions, software, and people are organized to produce the plethora of courses as well knowledge about education. Chapter 4 tells the story about how edX, a MOOC start-up, turned itself from an educational organization into a software organization by deploying the software-as-platform, thereby transforming and displacing particular institutional roles. In Chapter 5, I analyze the practices of a rising class of tool-makers, computer scientists, and describe how they are able to draw on other kinds of expertise, and intervene in new domains, while still presenting themselves as neutral system-builders.",
    "advisors": ["Graham Jones"],
    "text": "Platformizing higher education : computer science and the making of MOOC infrastructures This dissertation investigates the role of software in institutional transformation using the example of Massive Open Online Courses or MOOCs. It ethnographically tracks the development of the software infrastructure being built for MOOCs, focusing on three communities-programmers, instructors, and researchers-who centrally participate in the MOOC start-ups' stated mission of reinventing higher education. It argues that MOOC infrastructures are best viewed as an example of a heterogeneous software assemblage that I call the \"software-as-platform,\" that is today being widely deployed and used in a number of industries and institutions. The software-as-platform consists primarily of software that holds together a variety of normative logics: open-endedness; fast, iterative, production processes; data-driven decision-making; governance for emergent effects; scalability; and personalization. Of these, the most important is that its creators give to it an open-endedness as to its ultimate purpose: thus, the assemblage is often framed using the language of \"tools\" or \"platform.\" I then argue that the software-as-platform is a vehicle through which the norms and practices of Silicon Valley are making their way into other institutions, a process I call \"platformization.\" Finally, I suggest that the software-as-platform enables the emergence of a new form of expertise: tool-making. Tool-makers see themselves as building software tools, whose ultimate purpose comes from their users. The tools themselves draw on many other kinds of expert knowledge chosen at the discretion of the tool-builders. The dissertation consists of four chapters bookended by an Introduction and a Conclusion. Chapter 2 is an analysis of the public discourse around MOOCs. Chapter 3 describes MOOC infrastructures, showing how a cluster of institutions, software, and people are organized to produce the plethora of courses as well knowledge about education. Chapter 4 tells the story about how edX, a MOOC start-up, turned itself from an educational organization into a software organization by deploying the software-as-platform, thereby transforming and displacing particular institutional roles. In Chapter 5, I analyze the practices of a rising class of tool-makers, computer scientists, and describe how they are able to draw on other kinds of expertise, and intervene in new domains, while still presenting themselves as neutral system-builders."
}, {
    "id": "oai:dspace.mit.edu:1721.1/29897",
    "title": "Forging ahead : the Ames family of Easton, Massachusetts and two centuries of industrial enterprise, 1635-1861",
    "abstract": "This dissertation uses the Ames Family of Easton, Massachusetts as a case study on development of business and industry in early nineteenth century America. From English iron-working roots transplanted to America in 1635 the artisan tradition of blacksmithing dominated the Ames family for generations. Oliver Ames was trained as a smith, but when he came to Easton in 1803 to focus on the manufacture of shovels, he made an important step in the evolution from artisan and craftsman to industrialist, a common transition well exemplified by Oliver Ames's life. The Ames story demonstrates that the \"Industrial Revolution\" was no revolution at all. It was a gradual and fluid evolution from one way of doing business to another, an evolution in which many older methods and beliefs (the importance of farming, the dependence on kin, devotion to the community, conservative capital investments...) served men like Oliver Ames well. Common mischaracterizations of industrial development as revolutionary slights the importance of early nineteenth century industry; encourages an inaccurate focus on the romantic nature of small, rural mills; and discourages any impulse to examine in detail the ways in which early industry operated and played a part in industrial development. In fact, the management and operation of many of these facilities was far more complex than is typically recognized. Many of the earliest industrialists struggled to understand and manage complicated issues such as labor, raw materials, shipping, sales, international trade, economics, technological and scientific understanding, and the impact of business on family and community.",
    "advisors": ["Merritt Roe Smith"],
    "text": "Forging ahead : the Ames family of Easton, Massachusetts and two centuries of industrial enterprise, 1635-1861 This dissertation uses the Ames Family of Easton, Massachusetts as a case study on development of business and industry in early nineteenth century America. From English iron-working roots transplanted to America in 1635 the artisan tradition of blacksmithing dominated the Ames family for generations. Oliver Ames was trained as a smith, but when he came to Easton in 1803 to focus on the manufacture of shovels, he made an important step in the evolution from artisan and craftsman to industrialist, a common transition well exemplified by Oliver Ames's life. The Ames story demonstrates that the \"Industrial Revolution\" was no revolution at all. It was a gradual and fluid evolution from one way of doing business to another, an evolution in which many older methods and beliefs (the importance of farming, the dependence on kin, devotion to the community, conservative capital investments...) served men like Oliver Ames well. Common mischaracterizations of industrial development as revolutionary slights the importance of early nineteenth century industry; encourages an inaccurate focus on the romantic nature of small, rural mills; and discourages any impulse to examine in detail the ways in which early industry operated and played a part in industrial development. In fact, the management and operation of many of these facilities was far more complex than is typically recognized. Many of the earliest industrialists struggled to understand and manage complicated issues such as labor, raw materials, shipping, sales, international trade, economics, technological and scientific understanding, and the impact of business on family and community."
}, {
    "id": "oai:dspace.mit.edu:1721.1/40976",
    "title": "Modeling proteins, making scientists : an ethnography of pedagogy and visual cultures in contemporary structural biology",
    "abstract": "This ethnography tracks visualization and pedagogy in the burgeoning field of structural biology. Structural biologists are a multidisciplinary group of researchers who produce models and animations of protein molecules using three-dimensional interactive computer graphics. As they ramp up the pace of structure determination, modeling a vast array of proteins, these researchers are shifting life science research agendas from decoding genetic sequence data to interpreting the multidimensional forms of molecular life. One major hurdle they face is training a new generation of scientists to work with multi-dimensional data forms. In this study I document the formation and propagation of tacit knowledge in structural biology laboratories, in classrooms, and at conferences. This research shows that structural biologists-in-training must cultivate a feel for proteins in order to visualize and interpret their activity in cells. I find that protein modeling relies heavily on a set of practices I call the body-work of modeling. These tacit skills include: a) forms of kinesthetic knowledge that structural biologists gain through building and manipulating molecular models, and by using their own bodies as mimetic models to help them figure out how proteins move and interact; and b) narrative strategies that assume a teleological relationship between form and function, and which figure proteins through analogies with familiar human-scale phenomena, such as the pervasive description of proteins as \"machines.\" What I find is that these researchers are not only transforming the objects of life science research: they are training a new generation of life scientists in forms of knowing attuned to the chemical affinities, physical forces and movements of protein molecules, and keyed to the tangible logic and rhetoric of \"molecular machines.\"",
    "advisors": ["Stefan Helmreich"],
    "text": "Modeling proteins, making scientists : an ethnography of pedagogy and visual cultures in contemporary structural biology This ethnography tracks visualization and pedagogy in the burgeoning field of structural biology. Structural biologists are a multidisciplinary group of researchers who produce models and animations of protein molecules using three-dimensional interactive computer graphics. As they ramp up the pace of structure determination, modeling a vast array of proteins, these researchers are shifting life science research agendas from decoding genetic sequence data to interpreting the multidimensional forms of molecular life. One major hurdle they face is training a new generation of scientists to work with multi-dimensional data forms. In this study I document the formation and propagation of tacit knowledge in structural biology laboratories, in classrooms, and at conferences. This research shows that structural biologists-in-training must cultivate a feel for proteins in order to visualize and interpret their activity in cells. I find that protein modeling relies heavily on a set of practices I call the body-work of modeling. These tacit skills include: a) forms of kinesthetic knowledge that structural biologists gain through building and manipulating molecular models, and by using their own bodies as mimetic models to help them figure out how proteins move and interact; and b) narrative strategies that assume a teleological relationship between form and function, and which figure proteins through analogies with familiar human-scale phenomena, such as the pervasive description of proteins as \"machines.\" What I find is that these researchers are not only transforming the objects of life science research: they are training a new generation of life scientists in forms of knowing attuned to the chemical affinities, physical forces and movements of protein molecules, and keyed to the tangible logic and rhetoric of \"molecular machines.\""
}, {
    "id": "oai:dspace.mit.edu:1721.1/69452",
    "title": "The rules of perception : American color science, 1831-1931",
    "abstract": "Although vision was seldom studied in Antebellum America, color and color perception became a critical field of scientific inquiry in the United States during the Gilded Age and progressive era. Through a historical investigation of color science in the United States in the late nineteenth and early twentieth centuries, I argue that attempts to scientifically measure, define, and regulate color were part of a wider program to construct a more rational, harmonious, and efficient American polity starting from one of the very baseline perceptual components of reality - the experience of color. As part of this program, I argue secondly that color science was as much a matter of prescription as description - that is, color scientists didn't simply endeavor to reveal the facts of perception and apply them to social problems, they wanted to train everyday citizens to see scientifically, and thereby create citizens whose eyes, bodies, and minds were both medically healthy and morally tuned to the needs of the modern American nation. Finally, I argue not simply that perception has a history - i.e. that perceptual practices change over time, and that, for Americans of a century ago, experiences of color sensations were not taken as given but had to be laboriously crafted - but also that this history weighs heavily upon our present day understanding of visual reality, as manifested not least of all in scientific studies of vision, language, and cognition. Employing a close reading of the archival and published sources of a range of actors including physicist Ogden Rood, semiotician Charles Peirce, logician Christine Ladd-Franklin, board game magnate Milton Bradley, and art professor Alfred Munsell, among others, this study reveals the origins of some of the most deeply-rooted conceptions of color in modern American culture.",
    "advisors": ["David S. Jones"],
    "text": "The rules of perception : American color science, 1831-1931 Although vision was seldom studied in Antebellum America, color and color perception became a critical field of scientific inquiry in the United States during the Gilded Age and progressive era. Through a historical investigation of color science in the United States in the late nineteenth and early twentieth centuries, I argue that attempts to scientifically measure, define, and regulate color were part of a wider program to construct a more rational, harmonious, and efficient American polity starting from one of the very baseline perceptual components of reality - the experience of color. As part of this program, I argue secondly that color science was as much a matter of prescription as description - that is, color scientists didn't simply endeavor to reveal the facts of perception and apply them to social problems, they wanted to train everyday citizens to see scientifically, and thereby create citizens whose eyes, bodies, and minds were both medically healthy and morally tuned to the needs of the modern American nation. Finally, I argue not simply that perception has a history - i.e. that perceptual practices change over time, and that, for Americans of a century ago, experiences of color sensations were not taken as given but had to be laboriously crafted - but also that this history weighs heavily upon our present day understanding of visual reality, as manifested not least of all in scientific studies of vision, language, and cognition. Employing a close reading of the archival and published sources of a range of actors including physicist Ogden Rood, semiotician Charles Peirce, logician Christine Ladd-Franklin, board game magnate Milton Bradley, and art professor Alfred Munsell, among others, this study reveals the origins of some of the most deeply-rooted conceptions of color in modern American culture."
}, {
    "id": "oai:dspace.mit.edu:1721.1/47825",
    "title": "The Yellow Revolution in Malwa : alternative arenas of struggle and the cultural politics of development",
    "abstract": "This dissertation engages with two analytical frameworks to explore questions of social transformation and structures of power in rural society in India. The first is a specific critique of various types of development discourse and development projects that have been elaborated by national and international elites during the last forty years, focusing on the dry land Malwa region in the central Indian state of Madhya Pradesh. This includes a project to introduce soyabean cultivation to the region in the 1970s, which has been post-facto labeled as a yellow revolution, and a discourse which argues that providing market information through new information and communication technologies is empowering farmers. I argue that these projects and discourse have mostly steered away from engaging with the structures of power framing rural society, and thus, have failed to bring about much change in the condition of rural people in central India. The second analytical framework is a recovery and foregrounding of alternate arenas of struggle that rural people in the Malwa region have been participating in. The platform of democratic politics is one such avenue that marginalized groups have used to make demands upon the state to provide them with support and allows them to hold the state accountable for the same. Participating in cultural projects that question and subvert the forms of caste and gender based exclusion that frame the lives of people is another such arena which provides women and adivasis (tribals) with a language of empowerment. This research argues that for the language and practice of development to have more relevance to the lives of the poor and for it to engage with the deeper aspirations in their lives, the role of these political and cultural projects as vital platforms for rural people to exercise agency and bring about change, must be recognized.",
    "advisors": ["Kenneth Keniston"],
    "text": "The Yellow Revolution in Malwa : alternative arenas of struggle and the cultural politics of development This dissertation engages with two analytical frameworks to explore questions of social transformation and structures of power in rural society in India. The first is a specific critique of various types of development discourse and development projects that have been elaborated by national and international elites during the last forty years, focusing on the dry land Malwa region in the central Indian state of Madhya Pradesh. This includes a project to introduce soyabean cultivation to the region in the 1970s, which has been post-facto labeled as a yellow revolution, and a discourse which argues that providing market information through new information and communication technologies is empowering farmers. I argue that these projects and discourse have mostly steered away from engaging with the structures of power framing rural society, and thus, have failed to bring about much change in the condition of rural people in central India. The second analytical framework is a recovery and foregrounding of alternate arenas of struggle that rural people in the Malwa region have been participating in. The platform of democratic politics is one such avenue that marginalized groups have used to make demands upon the state to provide them with support and allows them to hold the state accountable for the same. Participating in cultural projects that question and subvert the forms of caste and gender based exclusion that frame the lives of people is another such arena which provides women and adivasis (tribals) with a language of empowerment. This research argues that for the language and practice of development to have more relevance to the lives of the poor and for it to engage with the deeper aspirations in their lives, the role of these political and cultural projects as vital platforms for rural people to exercise agency and bring about change, must be recognized."
}, {
    "id": "oai:dspace.mit.edu:1721.1/40393",
    "title": "In sync over distance : flexible coordination through communication in geographically distributed software development work",
    "abstract": "In this dissertation, I examine how the members of a distributed software development team (LC) operating entirely virtually for four and half years developed useful social practices to collaborate across time and space. Based on various communication data from LC, I analyze the communicative structuring of distributed work in members' daily practices. I show that \"temporal flexibility,\" often mentioned as key advantage of virtual organizing, is socially accomplished through \"boundary management,\" as members negotiate different temporal boundaries and learn and adapt to others' temporal patterns. Second, I identify dynamic coordination practices in LC that interweave multiple modes of communication and coordination in evolving work contexts, and demonstrate how these coordination practices facilitate temporal flexibility in LC. Finally, I analyze how members used the asynchronous communication medium of email to coordinate their tasks, using the notion of genre and genre system.",
    "advisors": ["JoAnne Yates"],
    "text": "In sync over distance : flexible coordination through communication in geographically distributed software development work In this dissertation, I examine how the members of a distributed software development team (LC) operating entirely virtually for four and half years developed useful social practices to collaborate across time and space. Based on various communication data from LC, I analyze the communicative structuring of distributed work in members' daily practices. I show that \"temporal flexibility,\" often mentioned as key advantage of virtual organizing, is socially accomplished through \"boundary management,\" as members negotiate different temporal boundaries and learn and adapt to others' temporal patterns. Second, I identify dynamic coordination practices in LC that interweave multiple modes of communication and coordination in evolving work contexts, and demonstrate how these coordination practices facilitate temporal flexibility in LC. Finally, I analyze how members used the asynchronous communication medium of email to coordinate their tasks, using the notion of genre and genre system."
}, {
    "id": "oai:dspace.mit.edu:1721.1/120881",
    "title": "The plantation network : Brazilian bioenergy science and sustainability in the global South",
    "abstract": "This dissertation provides a multiscalar analysis of climate change solutions from the global South by investigating how bioscientists are leveraging postcolonial ecological legacies into the basis for what they envision as a sustainable future. In Brazil, scientists from different disciplines are reengineering sugarcane-a crop central to the colonial project-at molecular, organismic, and economic scales in order to expand biofuels as international energy commodities. I argue that biology has become central to what I call the plantation network: a postcolonial agricultural formation that includes laboratories as obligatory passage points in the growing of plants to meet human needs and desires, especially in the era of \"sustainability\" and \"green capitalism.\" My research uses the plantation network formation to show that even though Brazilian scientists work under ethical and ecological threats posed by climate change, they also rely on Brazilian history, ideology, and cultural practices as they reshape life forms, landscapes, and labor in Brazil and Mozambique. This multisited analysis draws on ethnographic research conducted with molecular biologists attempting to create the world's first commercially viable transgenic sugarcane plant, biochemists working to develop waste-reducing fermentation technologies by using bioprospected \"wild\" yeasts to digest sugarcane bagasse, and a think tank of agronomic economists seeking to transfer a \"Brazilian biofuel model\" to Lusophone Mozambique. For these scientists, Brazil's long history of sugarcane is coming to center on ethoses and practices of what they call \"sustentabilidade\" (sustainability): a form of technoscientifically-aided industrial development that contributes to environmental wellbeing while maintaining the possibility of continued capitalist production for future populations. The dissertation examines \"sustainability\" as it has emerged in these sites by considering the plantation as a pharmakon-like entity: at the same time (1) a destructive nexus of social-ecological relations that has propelled the harmful, unjust conditions that have led to calls for \"sustainable\" practices and principles and (2) a redemptive space for ethically-sound renewable fuel and food production that scientists believe is central to creating a more just, livable world. I investigate how scientific practices related to ethically-rendered biofuels are motivating changes to the biotechnologies, production techniques, and locations of sugarcane plantations.",
    "advisors": ["Stefan Helmreich"],
    "text": "The plantation network : Brazilian bioenergy science and sustainability in the global South This dissertation provides a multiscalar analysis of climate change solutions from the global South by investigating how bioscientists are leveraging postcolonial ecological legacies into the basis for what they envision as a sustainable future. In Brazil, scientists from different disciplines are reengineering sugarcane-a crop central to the colonial project-at molecular, organismic, and economic scales in order to expand biofuels as international energy commodities. I argue that biology has become central to what I call the plantation network: a postcolonial agricultural formation that includes laboratories as obligatory passage points in the growing of plants to meet human needs and desires, especially in the era of \"sustainability\" and \"green capitalism.\" My research uses the plantation network formation to show that even though Brazilian scientists work under ethical and ecological threats posed by climate change, they also rely on Brazilian history, ideology, and cultural practices as they reshape life forms, landscapes, and labor in Brazil and Mozambique. This multisited analysis draws on ethnographic research conducted with molecular biologists attempting to create the world's first commercially viable transgenic sugarcane plant, biochemists working to develop waste-reducing fermentation technologies by using bioprospected \"wild\" yeasts to digest sugarcane bagasse, and a think tank of agronomic economists seeking to transfer a \"Brazilian biofuel model\" to Lusophone Mozambique. For these scientists, Brazil's long history of sugarcane is coming to center on ethoses and practices of what they call \"sustentabilidade\" (sustainability): a form of technoscientifically-aided industrial development that contributes to environmental wellbeing while maintaining the possibility of continued capitalist production for future populations. The dissertation examines \"sustainability\" as it has emerged in these sites by considering the plantation as a pharmakon-like entity: at the same time (1) a destructive nexus of social-ecological relations that has propelled the harmful, unjust conditions that have led to calls for \"sustainable\" practices and principles and (2) a redemptive space for ethically-sound renewable fuel and food production that scientists believe is central to creating a more just, livable world. I investigate how scientific practices related to ethically-rendered biofuels are motivating changes to the biotechnologies, production techniques, and locations of sugarcane plantations."
}, {
    "id": "oai:dspace.mit.edu:1721.1/113948",
    "title": "Highway madness! : politics and citizen participation in postwar U.S. traffic safety technology and policy",
    "abstract": "Modern U.S. traffic safety policy is largely guided by three overarching principles that have influenced governments, industry, and community and citizen activists since the 1940s. The terms, education, engineering, and enforcement, detailed in the Action Program for Traffic Safety were developed by engineers and U.S. federal government traffic safety experts in response to growing concerns around rising traffic fatalities. In these guidelines, and the iterations that developed from them, responsibility for traffic safety shifted between drivers, policy makers, and the automotive industry. My dissertation examines the evolution of traffic safety policy, specifically looking at solutions to reach zero fatalities, over multiple decades. The traffic safety experts, including the auto industry, federal government, and community activists, striving for zero fatalities have reshaped traffic infrastructure, automotive regulation, and consumer perceptions of risky behaviors in an attempt to solve a major public health issue. Broadly following four themes, infrastructure, institutions, technology, and behavior, each chapter highlights how these actors mitigated risks and defined safety in order to find solutions to highway fatalities. To safety-concerned government officials and industry leaders, central actors in the development of federal traffic safety policy, traffic safety encompassed engineering, education, enforcement, citizenship, humanitarian, and moral issues. On the other hand, to women's community and activist groups, like MADD, traffic safety's focus was the education of drivers and pedestrians, and the prevention of crashes through educational and public health approaches. However, to working class white males, government mandated safety was viewed as an infringement upon their freedom as individuals to choose how to be safe and how to define their level of safety, regardless of its effects on others. Through analysis of these narratives emerges a more complete picture of the public health, education, and social policy implications of twentieth century traffic safety, the role of citizen activism in traffic safety policy development at the local, state, and federal levels, and the ways in which the traffic safety solutions have shifted over time.",
    "advisors": ["Merritt Roe Smith"],
    "text": "Highway madness! : politics and citizen participation in postwar U.S. traffic safety technology and policy Modern U.S. traffic safety policy is largely guided by three overarching principles that have influenced governments, industry, and community and citizen activists since the 1940s. The terms, education, engineering, and enforcement, detailed in the Action Program for Traffic Safety were developed by engineers and U.S. federal government traffic safety experts in response to growing concerns around rising traffic fatalities. In these guidelines, and the iterations that developed from them, responsibility for traffic safety shifted between drivers, policy makers, and the automotive industry. My dissertation examines the evolution of traffic safety policy, specifically looking at solutions to reach zero fatalities, over multiple decades. The traffic safety experts, including the auto industry, federal government, and community activists, striving for zero fatalities have reshaped traffic infrastructure, automotive regulation, and consumer perceptions of risky behaviors in an attempt to solve a major public health issue. Broadly following four themes, infrastructure, institutions, technology, and behavior, each chapter highlights how these actors mitigated risks and defined safety in order to find solutions to highway fatalities. To safety-concerned government officials and industry leaders, central actors in the development of federal traffic safety policy, traffic safety encompassed engineering, education, enforcement, citizenship, humanitarian, and moral issues. On the other hand, to women's community and activist groups, like MADD, traffic safety's focus was the education of drivers and pedestrians, and the prevention of crashes through educational and public health approaches. However, to working class white males, government mandated safety was viewed as an infringement upon their freedom as individuals to choose how to be safe and how to define their level of safety, regardless of its effects on others. Through analysis of these narratives emerges a more complete picture of the public health, education, and social policy implications of twentieth century traffic safety, the role of citizen activism in traffic safety policy development at the local, state, and federal levels, and the ways in which the traffic safety solutions have shifted over time."
}, {
    "id": "oai:dspace.mit.edu:1721.1/39579",
    "title": "Medicating race : heart disease and durable preoccupations with difference",
    "abstract": "This dissertation is an examination of intersections of race, pharmaceuticals, and heart disease over the course of the 20th century and today. Each of these parts has had a dynamic history, and when they are invoked together they provide a terrain for arguments about interventions in health and in justice in the present. An enduring aspect of discourses of heart disease over the past century has been articulating connections between characterizations of the modem American way of life and of heart disease. In that process, heart disease research and practice has participated in differentiating Americans, especially by race. This dissertation uses heart disease categories and the drugs prescribed for them as windows into racialized medicine. The chapters are organized in a way that is roughly chronological, beginning with the emergence of cardiology as a specialty just before World War II and the landmark longitudinal Framingham Heart Study that began shortly thereafter. A central chapter tracks the emergence and mobilization of African American hypertension as a disease category since the 1960s.",
    "advisors": ["David S. Jones"],
    "text": "Medicating race : heart disease and durable preoccupations with difference This dissertation is an examination of intersections of race, pharmaceuticals, and heart disease over the course of the 20th century and today. Each of these parts has had a dynamic history, and when they are invoked together they provide a terrain for arguments about interventions in health and in justice in the present. An enduring aspect of discourses of heart disease over the past century has been articulating connections between characterizations of the modem American way of life and of heart disease. In that process, heart disease research and practice has participated in differentiating Americans, especially by race. This dissertation uses heart disease categories and the drugs prescribed for them as windows into racialized medicine. The chapters are organized in a way that is roughly chronological, beginning with the emergence of cardiology as a specialty just before World War II and the landmark longitudinal Framingham Heart Study that began shortly thereafter. A central chapter tracks the emergence and mobilization of African American hypertension as a disease category since the 1960s."
}, {
    "id": "oai:dspace.mit.edu:1721.1/90084",
    "title": "Changed climate : networking, professionalization, and grassroots organizing in U.S. environmental organizations",
    "abstract": "My dissertation, \"Changed Climate: Networking, Professionalization, and Grassroots Organizing in U.S Environmental Organizations,\" explores the efforts of four established U.S. environmental NGOs to change their organizational cultures and routine practices to develop grassroots activism for climate change advocacy. I find that although actors within and outside the environmental movement recognize a collective failure to influence the U.S. policy process on climate change issues, their organizations have been unable to adapt to the current political environment. My data derives from extensive participant observation, semi-structured interviews with organizational staff and experts, and statistical analysis of organizational efforts to recruit volunteer participants and develop their leadership over a two-year period. I follow four environmental organizations as they sought to create of a national climate-focused social movement. Working in collaborative partnership with other state- and national-level NGOs under the moniker of the \"Climate Coalition,\" they initiated pilot organizing campaigns in June 2011 in three U.S. cities toward three intertwined goals of 1) building social movement power via local coalitions, 2) developing volunteer leadership capable of forging a social movement community, and 3) mobilizing the resources of that constituency in collective action to effect change. In Chapter 1, looking first at the network of organizations that comprised the Climate Coalition, I show that the network's novel configuration - a third party network administrator both coordinated the activities of the participating organizations and worked with them to set the network's strategy - produced rather than diminished the tensions inherent in inter-organizational collaboration. Turning next to the organizations themselves in Chapter 2, I explore the challenges of integrating new types of experts and expertise into existing organizational structures. In particular, I suggest that the focus on involving volunteer expertise through community organizing disrupted existing organizational notions of expertise and prevented large-scale organizational embrace of the movement building work. Finally, in Chapter 3 1 examine the experiences of the volunteers on one of the movement building campaigns, and argue that the role of the community organizer in cultivating and developing volunteer leadership is essential for understanding the long-term success of movement building work.",
    "advisors": ["Susan S. Silbey"],
    "text": "Changed climate : networking, professionalization, and grassroots organizing in U.S. environmental organizations My dissertation, \"Changed Climate: Networking, Professionalization, and Grassroots Organizing in U.S Environmental Organizations,\" explores the efforts of four established U.S. environmental NGOs to change their organizational cultures and routine practices to develop grassroots activism for climate change advocacy. I find that although actors within and outside the environmental movement recognize a collective failure to influence the U.S. policy process on climate change issues, their organizations have been unable to adapt to the current political environment. My data derives from extensive participant observation, semi-structured interviews with organizational staff and experts, and statistical analysis of organizational efforts to recruit volunteer participants and develop their leadership over a two-year period. I follow four environmental organizations as they sought to create of a national climate-focused social movement. Working in collaborative partnership with other state- and national-level NGOs under the moniker of the \"Climate Coalition,\" they initiated pilot organizing campaigns in June 2011 in three U.S. cities toward three intertwined goals of 1) building social movement power via local coalitions, 2) developing volunteer leadership capable of forging a social movement community, and 3) mobilizing the resources of that constituency in collective action to effect change. In Chapter 1, looking first at the network of organizations that comprised the Climate Coalition, I show that the network's novel configuration - a third party network administrator both coordinated the activities of the participating organizations and worked with them to set the network's strategy - produced rather than diminished the tensions inherent in inter-organizational collaboration. Turning next to the organizations themselves in Chapter 2, I explore the challenges of integrating new types of experts and expertise into existing organizational structures. In particular, I suggest that the focus on involving volunteer expertise through community organizing disrupted existing organizational notions of expertise and prevented large-scale organizational embrace of the movement building work. Finally, in Chapter 3 1 examine the experiences of the volunteers on one of the movement building campaigns, and argue that the role of the community organizer in cultivating and developing volunteer leadership is essential for understanding the long-term success of movement building work."
}, {
    "id": "oai:dspace.mit.edu:1721.1/29762",
    "title": "Making and remaking quantum field theory",
    "abstract": "In this thesis, I examine two episodes in the history of quantum field theory using different research techniques and historiographic approaches. The first episode occurred during the 1920's and 1930's when quantum mechanics and relativity were being reconciled. I present some of the central developments of that episode using an approach that relates questions asked by physicists to the structures of putative natural kinds upon which they predicated their research. The second episode occurred during the 1960's and 1970's when important features of quantum field theory were given new interpretations that arose from the exchange of methods and insights between particle physics, solid state physics, statistical mechanics and physical chemistry. Research for the second episode was conducted in collaboration with other historians and scientists using novel web-based and database-backed research tools.",
    "advisors": ["Evelyn Fox Keller, Jed Z. Buchwald", "Silvan S. Schweber"],
    "text": "Making and remaking quantum field theory In this thesis, I examine two episodes in the history of quantum field theory using different research techniques and historiographic approaches. The first episode occurred during the 1920's and 1930's when quantum mechanics and relativity were being reconciled. I present some of the central developments of that episode using an approach that relates questions asked by physicists to the structures of putative natural kinds upon which they predicated their research. The second episode occurred during the 1960's and 1970's when important features of quantum field theory were given new interpretations that arose from the exchange of methods and insights between particle physics, solid state physics, statistical mechanics and physical chemistry. Research for the second episode was conducted in collaboration with other historians and scientists using novel web-based and database-backed research tools."
}, {
    "id": "oai:dspace.mit.edu:1721.1/38205",
    "title": "Making lives under closure : birth and medicine in Palestine's waiting zones",
    "abstract": "Reproduction is a site for understanding the ways in which people reconceptualize and re-organize the world in which they live. This dissertation tries to understand the world of birth under the regime of closures and fragmentation that governs the lives of Palestinians. It describes how checkpoints, closures, curfews have come to characterize childbirth in Palestine. It illustrates how the changing infrastructure, economy and discourse around birth produce new experiences of life in the medical sphere and in a family. Oral histories, life histories, doctors', midwives' and mothers' accounts, news reports and literature speak of these new conditions and experiences of birth and life. The meanings and structures of medicine, family and motherhood are thus remade. Oral histories focus on a history of the health infrastructure and movements in medicine, in particular the sumud (steadfastness) movement and the popular health movement. They illustrate how the figure of the doctor overlaps with that of the political leader. They identify the new health infrastructures built to assist birth during the closure which have different politics than the earlier movements, marking the post-socialist age, but show remarkable continuities with them in their emergence, mobilization and hierarchies. These new infrastructures, economies and discourses produce changing stories about birth and changing subjects. I identify two genres of birth stories, the first, narrated by mothers and the second, collected from newspapers. The former is in the register of the ordinary. The mothers remember the space of the hospital, a socio-economic space signaling class, as well as the trip from home to hospital and back. The stories seem uncanny. Occupation, closures and warfare are simply part of the ordinary. By contrast, the newspaper birth stories are sensational. They tell of checkpoint and prison births, occupation, suffering and resistance. They speak of miraculous redemption but in opposition to mothers' narrations, they are familiar. Finally, listening to the inner worlds of birth-mothers under the impress of economic, political and domestic pressures this dissertation distinguishes \"enclosure\" as a worldview caused by occupation and family relations, thus re-evaluating meanings of family, motherhood and life.",
    "advisors": ["Michael Fischer"],
    "text": "Making lives under closure : birth and medicine in Palestine's waiting zones Reproduction is a site for understanding the ways in which people reconceptualize and re-organize the world in which they live. This dissertation tries to understand the world of birth under the regime of closures and fragmentation that governs the lives of Palestinians. It describes how checkpoints, closures, curfews have come to characterize childbirth in Palestine. It illustrates how the changing infrastructure, economy and discourse around birth produce new experiences of life in the medical sphere and in a family. Oral histories, life histories, doctors', midwives' and mothers' accounts, news reports and literature speak of these new conditions and experiences of birth and life. The meanings and structures of medicine, family and motherhood are thus remade. Oral histories focus on a history of the health infrastructure and movements in medicine, in particular the sumud (steadfastness) movement and the popular health movement. They illustrate how the figure of the doctor overlaps with that of the political leader. They identify the new health infrastructures built to assist birth during the closure which have different politics than the earlier movements, marking the post-socialist age, but show remarkable continuities with them in their emergence, mobilization and hierarchies. These new infrastructures, economies and discourses produce changing stories about birth and changing subjects. I identify two genres of birth stories, the first, narrated by mothers and the second, collected from newspapers. The former is in the register of the ordinary. The mothers remember the space of the hospital, a socio-economic space signaling class, as well as the trip from home to hospital and back. The stories seem uncanny. Occupation, closures and warfare are simply part of the ordinary. By contrast, the newspaper birth stories are sensational. They tell of checkpoint and prison births, occupation, suffering and resistance. They speak of miraculous redemption but in opposition to mothers' narrations, they are familiar. Finally, listening to the inner worlds of birth-mothers under the impress of economic, political and domestic pressures this dissertation distinguishes \"enclosure\" as a worldview caused by occupation and family relations, thus re-evaluating meanings of family, motherhood and life."
}, {
    "id": "oai:dspace.mit.edu:1721.1/93812",
    "title": "Inventing purity in the Atlantic sugar world, 1860-1930",
    "abstract": "This dissertation illuminates how expert labor makes a complex natural substance into a uniform global commodity. Drawing on both published sources and extensive archival research in the continental United States, in Scotland, and in Puerto Rico, it provides new insight into the workings of the empires of commodities that define modem capitalism. Chapter 1 shows that the notion that sugar has a single valuable molecular essence sucrose- has been used to explain its history as a commodity. Yet this essentialism is not a natural fact but a product of the political economy of the late nineteenth century itself. From the seventeenth century on, sugar production had relied on the experienced multisensory techniques of enslaved craftsmen. But after 1860, newly sophisticated factories began to appear throughout the Caribbean, producing sugar of unprecedented consistency and quality. Chapter 2 explores how the work of chemists was essential to managing labor within these new factories, whose owners attempted to eliminate the need for artisan work. Yet the more successfully chemists extracted sucrose from sugarcane, the more mechanical and obvious they made that extraction appear, and the more they effaced their own necessity. These efforts to use scientific expertise to de-skill sugar production were made possible, Chapter 3 shows, by the persistence of craft and cooperative production in Glasgow, where those factories' machines were built. Sugar engineering firms cultivated relationships with distant plantations, ensuring that draftsmen and engineers could design, maintain, and repair machines that would last many decades. It therefore shows how the devices that facilitated sugar's commodification have human histories themselves. Finally, Chapter 4 reveals how the valuation of sugar became a central political issue in the postbellum United States. The Federal government feared its means of enforcing sugar tariffs was being undermined by fraud on the part of Customs officers and by new forms of sugar itself. But supposedly objective chemical techniques were even harder for the state to supervise. In showing how powerful refiners shaped scientific practices to their own advantage, this chapter provides a new framework for historians' analyses of science, commodities, and corruption in the nineteenth century.",
    "advisors": ["David Kaiser"],
    "text": "Inventing purity in the Atlantic sugar world, 1860-1930 This dissertation illuminates how expert labor makes a complex natural substance into a uniform global commodity. Drawing on both published sources and extensive archival research in the continental United States, in Scotland, and in Puerto Rico, it provides new insight into the workings of the empires of commodities that define modem capitalism. Chapter 1 shows that the notion that sugar has a single valuable molecular essence sucrose- has been used to explain its history as a commodity. Yet this essentialism is not a natural fact but a product of the political economy of the late nineteenth century itself. From the seventeenth century on, sugar production had relied on the experienced multisensory techniques of enslaved craftsmen. But after 1860, newly sophisticated factories began to appear throughout the Caribbean, producing sugar of unprecedented consistency and quality. Chapter 2 explores how the work of chemists was essential to managing labor within these new factories, whose owners attempted to eliminate the need for artisan work. Yet the more successfully chemists extracted sucrose from sugarcane, the more mechanical and obvious they made that extraction appear, and the more they effaced their own necessity. These efforts to use scientific expertise to de-skill sugar production were made possible, Chapter 3 shows, by the persistence of craft and cooperative production in Glasgow, where those factories' machines were built. Sugar engineering firms cultivated relationships with distant plantations, ensuring that draftsmen and engineers could design, maintain, and repair machines that would last many decades. It therefore shows how the devices that facilitated sugar's commodification have human histories themselves. Finally, Chapter 4 reveals how the valuation of sugar became a central political issue in the postbellum United States. The Federal government feared its means of enforcing sugar tariffs was being undermined by fraud on the part of Customs officers and by new forms of sugar itself. But supposedly objective chemical techniques were even harder for the state to supervise. In showing how powerful refiners shaped scientific practices to their own advantage, this chapter provides a new framework for historians' analyses of science, commodities, and corruption in the nineteenth century."
}, {
    "id": "oai:dspace.mit.edu:1721.1/9168",
    "title": "Technologies of living substance : tissue culture and cellular life in twentieth century biomedicine",
    "abstract": "This thesis is a historical and cultural analysis of the development of tissue culture, that is, techniques for growing living cells and tissues outside of the bodies of complex organisms. The development of these techniques represents an important epistemological shift in biology from in vivo to in vitro experimentation. This was not just a shift from use of a whole organ/organism to use of a fragment of the body, but a shift in the visualization and conceptualization of the body and its processes as they occur at the level of cells and tissues. Here it is argued that tissue culture was an essential part of a new sense of cellular life, not as a static building block of larger bodies, but as a dynamic and interactive entity which undergoes constant change, and is the functional unit of processes of growth, reproduction, aging, cancer, infection, and death. The thesis begins with an analysis of the work of embryologist Ross Harrison in growing isolated fragments of embryonic nerve tissue outside of the body in 1907. It follows the elaboration of Harrison's work by the surgeon Alexis Carrel, and the more general development of the technique over the early decades of the twentieth century. There is a close examination of the techniques for visualizing cellular life grown outside of the body, as well as the appearance of reactions to this form of life in a wider public culture. The methodological approach is a close examination of the material practices of tissue culture laboratories, and the images, ideas, and information about cells and their relation to bodies produced thereby. The movement of these ideas and images from laboratory to public culture and back is at the center of the final chapters, in which the history of the first widely used human cell line, HeLa, is examined, and more recent legal and ethical debates about the status and ownership of the Mo cell line. Rather than being a comprehensive history of tissue culture, this thesis takes historical episodes from this twentieth century biomedical practice to analyze the constitution of cellular life through the objects of tissue culture, and the ways in which these objects have been scientifically, technically, and culturally productive.",
    "advisors": ["Michael M.J. Fischer"],
    "text": "Technologies of living substance : tissue culture and cellular life in twentieth century biomedicine This thesis is a historical and cultural analysis of the development of tissue culture, that is, techniques for growing living cells and tissues outside of the bodies of complex organisms. The development of these techniques represents an important epistemological shift in biology from in vivo to in vitro experimentation. This was not just a shift from use of a whole organ/organism to use of a fragment of the body, but a shift in the visualization and conceptualization of the body and its processes as they occur at the level of cells and tissues. Here it is argued that tissue culture was an essential part of a new sense of cellular life, not as a static building block of larger bodies, but as a dynamic and interactive entity which undergoes constant change, and is the functional unit of processes of growth, reproduction, aging, cancer, infection, and death. The thesis begins with an analysis of the work of embryologist Ross Harrison in growing isolated fragments of embryonic nerve tissue outside of the body in 1907. It follows the elaboration of Harrison's work by the surgeon Alexis Carrel, and the more general development of the technique over the early decades of the twentieth century. There is a close examination of the techniques for visualizing cellular life grown outside of the body, as well as the appearance of reactions to this form of life in a wider public culture. The methodological approach is a close examination of the material practices of tissue culture laboratories, and the images, ideas, and information about cells and their relation to bodies produced thereby. The movement of these ideas and images from laboratory to public culture and back is at the center of the final chapters, in which the history of the first widely used human cell line, HeLa, is examined, and more recent legal and ethical debates about the status and ownership of the Mo cell line. Rather than being a comprehensive history of tissue culture, this thesis takes historical episodes from this twentieth century biomedical practice to analyze the constitution of cellular life through the objects of tissue culture, and the ways in which these objects have been scientifically, technically, and culturally productive."
}, {
    "id": "oai:dspace.mit.edu:1721.1/45803",
    "title": "Planting improvement : the rhetoric and practice of scientific agriculture in northern British America, 1670-1820",
    "abstract": "\"Planting Improvement: The Rhetoric and Practice of Scientific Agriculture in Northern British America, 1670-1820,\" explores the history and cultural politics of environmental change in the British empire through a focus on rural land-use practices and the construction of scientific expertise in the cold temperate colonies of New England and Nova Scotia, from the late seventeenth through early nineteenth centuries. Improvement was an abiding mode of and justification for British imperialism through territorial expansion and early modern economic development. British American and anglophone colonists of a range of status positions embraced agricultural improvement, though to different degrees and in different ways. For all settler-farmers, improving extra-European land meant transforming native environments into neo-European agricultural landscapes that were aesthetically familiar. For elites in northern North America, agricultural improvement was additionally a science of the practical Enlightenment, which encompassed husbandry and horticulture, stadial theories of progress, and the objectives and methods of natural history, geography, and economic survey. By exchanging farming advice, botanical literature, and seeds, plants, and livestock with other naturalists and improvers in the republic of letters and scientific institutions in the region as well in England, Scotland, Sweden, Russia, and France, elites in New England and Nova Scotia took a uniquely scientific approach to colonial property development. By employing the rhetoric of science and flaunting their privileged access to transatlantic, European, and imperial networks, northern elites who formed agricultural societies, supported natural history professorships, and private, academic, or colonial botanical gardens, distinguished their land improvements from those of their neighbors. Moreover, they believed that scientific improvement could ameliorate the troublesome disadvantages of the region's nature-especially its climate, seasonal weather extremes, short growing seasons, uneven topography, and thin soils.",
    "advisors": ["Harriet Ritvo"],
    "text": "Planting improvement : the rhetoric and practice of scientific agriculture in northern British America, 1670-1820 \"Planting Improvement: The Rhetoric and Practice of Scientific Agriculture in Northern British America, 1670-1820,\" explores the history and cultural politics of environmental change in the British empire through a focus on rural land-use practices and the construction of scientific expertise in the cold temperate colonies of New England and Nova Scotia, from the late seventeenth through early nineteenth centuries. Improvement was an abiding mode of and justification for British imperialism through territorial expansion and early modern economic development. British American and anglophone colonists of a range of status positions embraced agricultural improvement, though to different degrees and in different ways. For all settler-farmers, improving extra-European land meant transforming native environments into neo-European agricultural landscapes that were aesthetically familiar. For elites in northern North America, agricultural improvement was additionally a science of the practical Enlightenment, which encompassed husbandry and horticulture, stadial theories of progress, and the objectives and methods of natural history, geography, and economic survey. By exchanging farming advice, botanical literature, and seeds, plants, and livestock with other naturalists and improvers in the republic of letters and scientific institutions in the region as well in England, Scotland, Sweden, Russia, and France, elites in New England and Nova Scotia took a uniquely scientific approach to colonial property development. By employing the rhetoric of science and flaunting their privileged access to transatlantic, European, and imperial networks, northern elites who formed agricultural societies, supported natural history professorships, and private, academic, or colonial botanical gardens, distinguished their land improvements from those of their neighbors. Moreover, they believed that scientific improvement could ameliorate the troublesome disadvantages of the region's nature-especially its climate, seasonal weather extremes, short growing seasons, uneven topography, and thin soils."
}, {
    "id": "oai:dspace.mit.edu:1721.1/39178",
    "title": "Trucking country : food politics and the transformation of rural life in Postwar America",
    "abstract": "Trucking replaced railroads as the primary link between rural producers and urban consumers in the mid-twentieth century. With this technological change came a fundamental transformation of the defining features of rural life after World War II. Trucking helped drive the shift from a New Deal-era political economy-based on centralized political authority, a highly regulated farm and food economy, and collective social values-to a postwar framework of anti-statism, minimal market regulation, and fierce individualism. Trucking and rural truck drivers were at the heart of what I call the \"marketing machine,\" a new kind of food economy that arose after World War II, characterized by decentralized food processors and supermarkets seeking high volume, low prices, and consistent quality to eliminate uncertainties from the food distribution chain. This marketing machine developed as a reaction against the statist food and farm policies of the New Deal. Government agricultural experts-economists, engineers, and policymakers-encouraged the growth of highway transportation in an effort to redefine the \"farm problem\" as an industrial problem, an issue to be solved by rural food processors and non-unionized \"independent\" truck drivers rather than price supports or acreage controls.",
    "advisors": ["Deborah K. Fitzgerald"],
    "text": "Trucking country : food politics and the transformation of rural life in Postwar America Trucking replaced railroads as the primary link between rural producers and urban consumers in the mid-twentieth century. With this technological change came a fundamental transformation of the defining features of rural life after World War II. Trucking helped drive the shift from a New Deal-era political economy-based on centralized political authority, a highly regulated farm and food economy, and collective social values-to a postwar framework of anti-statism, minimal market regulation, and fierce individualism. Trucking and rural truck drivers were at the heart of what I call the \"marketing machine,\" a new kind of food economy that arose after World War II, characterized by decentralized food processors and supermarkets seeking high volume, low prices, and consistent quality to eliminate uncertainties from the food distribution chain. This marketing machine developed as a reaction against the statist food and farm policies of the New Deal. Government agricultural experts-economists, engineers, and policymakers-encouraged the growth of highway transportation in an effort to redefine the \"farm problem\" as an industrial problem, an issue to be solved by rural food processors and non-unionized \"independent\" truck drivers rather than price supports or acreage controls."
}, {
    "id": "oai:dspace.mit.edu:1721.1/8801",
    "title": "Projects, management, and protean times : engineering enterprise in the United States, 1870-1960",
    "abstract": "In this dissertation, I trace methods for organizing skilled workers engaged in creative, limited-term projects in the United States between the nineteenth century and the 1950s. Examining eras of system building in technical fields-civil engineering in the nineteenth century, laboratory administration in the 1910s and 1920s, aircraft design in the 1930s, and electronics in the 1950s-I show that recent discourse on the management of innovation and change is a manifestation of a cyclically recurring conversation. This story complicates prevalent views of management theory and practice before World War II by recovering a thread obscured by emphasis on the organization of integrated, divisional companies and operative labor within them. Applying ideas from recent work in organization studies to distill common aspects of the management problems and labor processes individuals have confronted and theorized, I find common patterns: managers of construction firms, engineering departments, and research laboratories have again and again theorized the fast-moving, knowledge-intensive, relational organization, doing so long before these terms were available. Such thinking has been driven both by practical needs and because external pressures have forced explanation of seemingly uncontrolled, irrational work. Practically, the transferability of management techniques among settings such as construction and research has reflected kinships between labor and communication processes: each has involved skilled workers producing complex artifacts in uncertain physical, technical, and social environments.",
    "advisors": ["Merritt Roe Smith"],
    "text": "Projects, management, and protean times : engineering enterprise in the United States, 1870-1960 In this dissertation, I trace methods for organizing skilled workers engaged in creative, limited-term projects in the United States between the nineteenth century and the 1950s. Examining eras of system building in technical fields-civil engineering in the nineteenth century, laboratory administration in the 1910s and 1920s, aircraft design in the 1930s, and electronics in the 1950s-I show that recent discourse on the management of innovation and change is a manifestation of a cyclically recurring conversation. This story complicates prevalent views of management theory and practice before World War II by recovering a thread obscured by emphasis on the organization of integrated, divisional companies and operative labor within them. Applying ideas from recent work in organization studies to distill common aspects of the management problems and labor processes individuals have confronted and theorized, I find common patterns: managers of construction firms, engineering departments, and research laboratories have again and again theorized the fast-moving, knowledge-intensive, relational organization, doing so long before these terms were available. Such thinking has been driven both by practical needs and because external pressures have forced explanation of seemingly uncontrolled, irrational work. Practically, the transferability of management techniques among settings such as construction and research has reflected kinships between labor and communication processes: each has involved skilled workers producing complex artifacts in uncertain physical, technical, and social environments."
}, {
    "id": "oai:dspace.mit.edu:1721.1/17618",
    "title": "Managing a sea of information : shipboard command and control in the United States Navy, 1899-1945",
    "abstract": "This dissertation traces the history of shipboard command and control systems in the United States Navy from 1899, when the service first conducted experiments with wireless telegraphy, through World War II, the conflict which witnessed the birth of the modern shipboard information processing facility. It argues that early-to-mid twentieth century naval officers' development and employment of increasingly sophisticated shipboard command and control systems fundamentally altered the human experience of warfare at sea. Based predominately on archival research, Managing a Sea of Information follows a narrative format. It begins by examining the United States Navy's adoption of radio and challenges the notion that a conservative officer corps failed to appreciate the potential advantages of this new communications technology. The bulk of the study explores the Navy' s development of shipboard command and control systems from World War I through the beginning of World War II, focusing particularly on the efforts of operational commanders to maximize their capabilities through the adoption of devices, methods, and procedures for the collection, processing, and dissemination of information. These efforts gradually changed the nature of command at sea, from an environment in which commanders could make informed tactical decisions with relatively limited input from subordinates, to one characterized by epistemic actions and socially-distributed cognition. The dissertation concludes with a brief analysis of shipboard command and control systems during the Second World War, concentrating especially on the United States Navy's creation of the Combat Information Center (CIC).",
    "advisors": ["David A. Mindell, Merritt Roe Smith", "David Alan Rosenberg"],
    "text": "Managing a sea of information : shipboard command and control in the United States Navy, 1899-1945 This dissertation traces the history of shipboard command and control systems in the United States Navy from 1899, when the service first conducted experiments with wireless telegraphy, through World War II, the conflict which witnessed the birth of the modern shipboard information processing facility. It argues that early-to-mid twentieth century naval officers' development and employment of increasingly sophisticated shipboard command and control systems fundamentally altered the human experience of warfare at sea. Based predominately on archival research, Managing a Sea of Information follows a narrative format. It begins by examining the United States Navy's adoption of radio and challenges the notion that a conservative officer corps failed to appreciate the potential advantages of this new communications technology. The bulk of the study explores the Navy' s development of shipboard command and control systems from World War I through the beginning of World War II, focusing particularly on the efforts of operational commanders to maximize their capabilities through the adoption of devices, methods, and procedures for the collection, processing, and dissemination of information. These efforts gradually changed the nature of command at sea, from an environment in which commanders could make informed tactical decisions with relatively limited input from subordinates, to one characterized by epistemic actions and socially-distributed cognition. The dissertation concludes with a brief analysis of shipboard command and control systems during the Second World War, concentrating especially on the United States Navy's creation of the Combat Information Center (CIC)."
}, {
    "id": "oai:dspace.mit.edu:1721.1/69811",
    "title": "Influenza : a study of contemporary medical politics",
    "abstract": "Over the past decade, the prevention and control of seasonal and pandemic influenza has grown to be one of the largest and most visible public health policies. This dissertation considers contemporary influenza policy as a case study in what I call medical politics, in which a disease that for most people is rather unremarkable has become the focus of intense (and costly) public health campaigns based on a shaky scientific basis. The dissertation seeks to explain how this could happen. The first two chapters show how influenza and its pandemics are marketed through an appeal to numerous scientific claims. Drawing on governmental marketing materials, statements by officials, and policy documents, I try to let officials speak for themselves and, as much as possible, refrain from analysis. Chapter 3 tells the story of the 2009 novel influenza H1N1 outbreak, showing how official understandings about influenza were called into question by an outbreak far milder than experts had predicted, and discusses investigations which highlighted the role of industry in shaping influenza policy. Chapter 4 analyzes official scientific claims regarding influenza, and argues that degree to which influenza is a serious public health problem is actually unclear. Furthermore, influenza vaccine effectiveness has been vastly overstated, predictive models of pandemic influenza are demonstrably flawed, and officials conflate true influenza with influenza-like illness (ILl), an often overlooked but critical distinction which allows officials to mislead the public into holding false assumptions about the potential benefits of influenza vaccine. Chapter 5 highlights the centrality of \"virus-centric thinking\" and the ethic of \"saving lives\" in public health practice as important factors that help explain how such a situation can exist and persist in light of the evidence. Chapter 6 addresses the policy implications of the dissertation's findings.",
    "advisors": ["Theodore A. Postol"],
    "text": "Influenza : a study of contemporary medical politics Over the past decade, the prevention and control of seasonal and pandemic influenza has grown to be one of the largest and most visible public health policies. This dissertation considers contemporary influenza policy as a case study in what I call medical politics, in which a disease that for most people is rather unremarkable has become the focus of intense (and costly) public health campaigns based on a shaky scientific basis. The dissertation seeks to explain how this could happen. The first two chapters show how influenza and its pandemics are marketed through an appeal to numerous scientific claims. Drawing on governmental marketing materials, statements by officials, and policy documents, I try to let officials speak for themselves and, as much as possible, refrain from analysis. Chapter 3 tells the story of the 2009 novel influenza H1N1 outbreak, showing how official understandings about influenza were called into question by an outbreak far milder than experts had predicted, and discusses investigations which highlighted the role of industry in shaping influenza policy. Chapter 4 analyzes official scientific claims regarding influenza, and argues that degree to which influenza is a serious public health problem is actually unclear. Furthermore, influenza vaccine effectiveness has been vastly overstated, predictive models of pandemic influenza are demonstrably flawed, and officials conflate true influenza with influenza-like illness (ILl), an often overlooked but critical distinction which allows officials to mislead the public into holding false assumptions about the potential benefits of influenza vaccine. Chapter 5 highlights the centrality of \"virus-centric thinking\" and the ethic of \"saving lives\" in public health practice as important factors that help explain how such a situation can exist and persist in light of the evidence. Chapter 6 addresses the policy implications of the dissertation's findings."
}, {
    "id": "oai:dspace.mit.edu:1721.1/107527",
    "title": "Doing Dutch Wax cloth : practice, politics, and 'the new Africa'",
    "abstract": "This dissertation examines how Africa's place in the world is negotiated in different forms of material engagement with Dutch Wax cloth--designing, advertising, selling, buying, and tailoring-along the cloth's trajectory between the Netherlands and Togo. Derived from a manual Javanese textile printing technique, Dutch Wax cloth has been machine-printed in the Netherlands since the late 19th century, and was introduced to West Africa in the early 20th century. Lom, Togo was a hub for its distribution throughout West and Central Africa for much of the 20th century. The cloth's visual and material attributes were historically developed through exchanges between West African consumers and European manufacturers and Dutch Wax has since been integrated into both dress practices and processes of social reproduction in Togo, as in much of West Africa. Further, in recent years, the cloth's producer has been rebranding itself from a textile manufacturer for Africa into a global luxury design and fashion brand. As such, Dutch Wax cloth has and continues to not only mediate but also embody West African participation in the global. By examining how Dutch Wax is \"done\" in various sites of practice along its path-how it is given form, and what is produced alongside these forms-this multi-sited ethnography brings to light how Africa's relationship to and place in the global is negotiated in the practices of designers, advertisers, sellers, buyers, and tailors across the Netherlands, Togo, and \"the global.\" I argue that the view of Africa-in-the-world (Ferguson 2006) that emerges in each of these sites of practice and across all five is one that is characterized by a tenuous play between absence and presence, visibility and invisibility, inclusion and exclusion. Even as it offers a seductive alternative to past discourses about a \"hopeless,\" \"crisis-ridden,\" \"old Africa,\" the \"New Africa\" remains decidedly layered and multiple.",
    "advisors": ["Christine Walley"],
    "text": "Doing Dutch Wax cloth : practice, politics, and 'the new Africa' This dissertation examines how Africa's place in the world is negotiated in different forms of material engagement with Dutch Wax cloth--designing, advertising, selling, buying, and tailoring-along the cloth's trajectory between the Netherlands and Togo. Derived from a manual Javanese textile printing technique, Dutch Wax cloth has been machine-printed in the Netherlands since the late 19th century, and was introduced to West Africa in the early 20th century. Lom, Togo was a hub for its distribution throughout West and Central Africa for much of the 20th century. The cloth's visual and material attributes were historically developed through exchanges between West African consumers and European manufacturers and Dutch Wax has since been integrated into both dress practices and processes of social reproduction in Togo, as in much of West Africa. Further, in recent years, the cloth's producer has been rebranding itself from a textile manufacturer for Africa into a global luxury design and fashion brand. As such, Dutch Wax cloth has and continues to not only mediate but also embody West African participation in the global. By examining how Dutch Wax is \"done\" in various sites of practice along its path-how it is given form, and what is produced alongside these forms-this multi-sited ethnography brings to light how Africa's relationship to and place in the global is negotiated in the practices of designers, advertisers, sellers, buyers, and tailors across the Netherlands, Togo, and \"the global.\" I argue that the view of Africa-in-the-world (Ferguson 2006) that emerges in each of these sites of practice and across all five is one that is characterized by a tenuous play between absence and presence, visibility and invisibility, inclusion and exclusion. Even as it offers a seductive alternative to past discourses about a \"hopeless,\" \"crisis-ridden,\" \"old Africa,\" the \"New Africa\" remains decidedly layered and multiple."
}, {
    "id": "oai:dspace.mit.edu:1721.1/8670",
    "title": "Sustainable development and comprehensive capital : The post-Soviet decline of Central Asia",
    "abstract": "The general post-Soviet decline of the states of Central Asia (Kazakhstan, Kyrgyzstan, Tajikistan, Turkmenistan, and Uzbekistan) mirrors specific declines in the robustness of these states' stocks of financial, physical, natural, human, organizational, and social capital assets. This loss of various kinds of capital assets over the past decade reduces the current potential and capacity of the region to implement reforms for sustainable development. While Central Asia entered the 20th century as a comparatively marginal and underdeveloped area of the world, during the Soviet period it amassed appreciable stocks of capital, especially human, physical, and social capital. The emergence of a vibrant scientific community in Central Asia during the middle of the century marked one of the most rapid expansions of scientific prestige, talent, and institutions in the developing world. With the disassembly of the Soviet Union, development and reform projects within Central Asia and funded by foreign donors have failed to achieve their development and reform goals. Within the environmental sphere, the post-Soviet period, despite a massive investment in environmental aid to the region from the West and Japan, has yielded few environmental benefits and seen the worsening of several environmental conditions, captured in the desiccation of the Aral Sea and the collapse of Caspian Sea fisheries.",
    "advisors": ["Peter C. Perdue"],
    "text": "Sustainable development and comprehensive capital : The post-Soviet decline of Central Asia The general post-Soviet decline of the states of Central Asia (Kazakhstan, Kyrgyzstan, Tajikistan, Turkmenistan, and Uzbekistan) mirrors specific declines in the robustness of these states' stocks of financial, physical, natural, human, organizational, and social capital assets. This loss of various kinds of capital assets over the past decade reduces the current potential and capacity of the region to implement reforms for sustainable development. While Central Asia entered the 20th century as a comparatively marginal and underdeveloped area of the world, during the Soviet period it amassed appreciable stocks of capital, especially human, physical, and social capital. The emergence of a vibrant scientific community in Central Asia during the middle of the century marked one of the most rapid expansions of scientific prestige, talent, and institutions in the developing world. With the disassembly of the Soviet Union, development and reform projects within Central Asia and funded by foreign donors have failed to achieve their development and reform goals. Within the environmental sphere, the post-Soviet period, despite a massive investment in environmental aid to the region from the West and Japan, has yielded few environmental benefits and seen the worsening of several environmental conditions, captured in the desiccation of the Aral Sea and the collapse of Caspian Sea fisheries."
}, {
    "id": "oai:dspace.mit.edu:1721.1/17575",
    "title": "Fighting engineers : the U.S. Navy and mechanical engineering, 1840-1905",
    "abstract": "Fighting Engineers examines social conflict as the cause of the formation of professional mechanical engineering in the nineteenth century U.S. Navy. In the middle of that century, the Navy began to utilize steam engines for motive power. Navy administrators recognized the need for engineering officers to design and operate ships' steam power plants, but the social and political status of staff engineering officers was unclear. Their rank was relative to line officers, the men who navigated the ship and commanded the crew. Engineers possessed no legal command authority. This created problems as engineers' responsibilities increased during the Civil War. In response to shortcomings evident in the training of the engineer corps during the Civil War, the U.S. Naval Academy in the postwar period designed an unprecedented technical curriculum. Through this program, the Navy trained the nation's first group of modern mechanical engineers. As Navy engineers built their profession after the war, they attempted to redefine what it meant to be a naval officer. The officer ideal moved from the aristocratic warrior of the antebellum period to a college educated, scientifically minded professional late in the century. To maximize the political utility of their technical expertise, Navy engineers had to spread their idea of mechanical engineering and engineering education to a broader audience. In the 1880s, they chose to do so in an unprecedented way. They promoted legislation that allowed them to serve as engineering professors at American universities. This foray into academia was a continuation of the long-standing government policy of internal improvements and federal technology sponsorship.",
    "advisors": ["David A. Mindell"],
    "text": "Fighting engineers : the U.S. Navy and mechanical engineering, 1840-1905 Fighting Engineers examines social conflict as the cause of the formation of professional mechanical engineering in the nineteenth century U.S. Navy. In the middle of that century, the Navy began to utilize steam engines for motive power. Navy administrators recognized the need for engineering officers to design and operate ships' steam power plants, but the social and political status of staff engineering officers was unclear. Their rank was relative to line officers, the men who navigated the ship and commanded the crew. Engineers possessed no legal command authority. This created problems as engineers' responsibilities increased during the Civil War. In response to shortcomings evident in the training of the engineer corps during the Civil War, the U.S. Naval Academy in the postwar period designed an unprecedented technical curriculum. Through this program, the Navy trained the nation's first group of modern mechanical engineers. As Navy engineers built their profession after the war, they attempted to redefine what it meant to be a naval officer. The officer ideal moved from the aristocratic warrior of the antebellum period to a college educated, scientifically minded professional late in the century. To maximize the political utility of their technical expertise, Navy engineers had to spread their idea of mechanical engineering and engineering education to a broader audience. In the 1880s, they chose to do so in an unprecedented way. They promoted legislation that allowed them to serve as engineering professors at American universities. This foray into academia was a continuation of the long-standing government policy of internal improvements and federal technology sponsorship."
}, {
    "id": "oai:dspace.mit.edu:1721.1/9389",
    "title": "Foreign Knowledge or art nation, earthquake nation : architecture, seismology, carpentry, the West, and Japan, 1876-1923",
    "abstract": "This dissertation follows British professors at Tokyo's late nineteenth century College of Technology (Kobudaigaku) and continues into the twentieth century with the Japanese students they trained. My first chapters map out an argument between British disciplines over Japanese 'adaptation' and/or 'resistance' to nature, a conflict driven by the development of the modem science of seismology in Tokyo. Seismology was a unique cross-cultural project - a 'Western' instrumental science invented and first institutionalized in a non-Western place. I discuss bow artifacts as diverse as seismographs, five-story wooden pagoda, and Mt. Fuji became 'boundary objects' in a fierce dispute between spokesmen for science and an over the character of the Japanese landscape and people. The latter chapters explain bow young Japanese architects and seismologists re-mapped the discursive and instrumental terrains of their British teachers, challenging foreign knowledge-production from inside colonizing disciplines. The text is framed around the story of the Great Nobi Earthquake of 1891. According to contemporary Japanese narratives, the great earthquake (the most powerful in modem Japanese history) was particularity damaging to the new 'foreign' infrastructure, and caused Japanese to seriously question, for the first time, the efficacy of foreign knowledge. 'Japan's earthquake problem' went from being one of bow to import European resistance into a fragile nation, to one of how to make a uniquely fragile imported infrastructure resist the power of Japanese nature. I critically re-tell this Japanese story as a corrective to European and American images of Meiji .Japan as a 'pupil country' and the West as a 'teacher culture'. \"Foreign Knowledge\" demonstrates in very concrete ways bow science and technology, art and architecture, gender, race, and class co-constructed Meiji Japan. Distinctions between 'artistic' and 'scientific' representations of culture/nature were particularly fluid in late nineteenth century Tokyo. Architects in my text often speak in the name of science and seismologists become an critics and even ethnographers. The narrative is also trans-national; centered in Tokyo, it follows Japanese architects, scientists, and carpenters to Britain, Italy, the United States, and Formosa.",
    "advisors": ["Merrit Roe Smith"],
    "text": "Foreign Knowledge or art nation, earthquake nation : architecture, seismology, carpentry, the West, and Japan, 1876-1923 This dissertation follows British professors at Tokyo's late nineteenth century College of Technology (Kobudaigaku) and continues into the twentieth century with the Japanese students they trained. My first chapters map out an argument between British disciplines over Japanese 'adaptation' and/or 'resistance' to nature, a conflict driven by the development of the modem science of seismology in Tokyo. Seismology was a unique cross-cultural project - a 'Western' instrumental science invented and first institutionalized in a non-Western place. I discuss bow artifacts as diverse as seismographs, five-story wooden pagoda, and Mt. Fuji became 'boundary objects' in a fierce dispute between spokesmen for science and an over the character of the Japanese landscape and people. The latter chapters explain bow young Japanese architects and seismologists re-mapped the discursive and instrumental terrains of their British teachers, challenging foreign knowledge-production from inside colonizing disciplines. The text is framed around the story of the Great Nobi Earthquake of 1891. According to contemporary Japanese narratives, the great earthquake (the most powerful in modem Japanese history) was particularity damaging to the new 'foreign' infrastructure, and caused Japanese to seriously question, for the first time, the efficacy of foreign knowledge. 'Japan's earthquake problem' went from being one of bow to import European resistance into a fragile nation, to one of how to make a uniquely fragile imported infrastructure resist the power of Japanese nature. I critically re-tell this Japanese story as a corrective to European and American images of Meiji .Japan as a 'pupil country' and the West as a 'teacher culture'. \"Foreign Knowledge\" demonstrates in very concrete ways bow science and technology, art and architecture, gender, race, and class co-constructed Meiji Japan. Distinctions between 'artistic' and 'scientific' representations of culture/nature were particularly fluid in late nineteenth century Tokyo. Architects in my text often speak in the name of science and seismologists become an critics and even ethnographers. The narrative is also trans-national; centered in Tokyo, it follows Japanese architects, scientists, and carpenters to Britain, Italy, the United States, and Formosa."
}, {
    "id": "oai:dspace.mit.edu:1721.1/9388",
    "title": "Expositions, museums, and technological display : building cultural institutions for the \"inventor citizen\" in the late nineteenth century United States",
    "abstract": "The dissertation is an historical study of the interactions between technologists and museums in the late nineteenth United States, the role of international expositions-such as Philadelphia in 1876 and Chicago in 1893-in these interactions, and the rise of technology collections in those museums. Through archival sources, as well as published primary and secondary source material, the dissertation examines the role of engineers and the public in creating technological collections in museums dominated by natural history specimens. It focuses on intersections between industry, engineers, international expositions, and museums in the nineteenth century by considering the cases of the Smithsonian's National Museum and the Field Columbian Museum. This research explores technology and its cultural roles, how technology related to or differed from other aspects of American culture, and how this may have precluded the establishment of a national museum dedicated to mechanical arts, technology and America's inventor citizens, even while some engineers brokered a place for technological collections to develop. Despite objections and a lack of support from the higher administration within the museums, mechanical and technological collections developed. In an era of enthusiasm for technology, invention, and mechanics, forces outside the museums pushed the development of the collections. In particular, a group of engineers, as curators and exhibit designers; played roles in the celebration of technological achievement and at the expositions, in the attempts to establish mechanical arts and technology collections at the two prominent museums, and in the connections between technologists and museums that proved essential to the development of the collections. In addition, pressure from a public audience enthused about technology and machines aided such collections by influencing museum administration. This dissertation argues that engineers became mediators between the museum world and the world of engineering by brokering the culture of technology and securing a subordinate, yet permanent place for technology within the museum world. Key issues in the negotiation and brokering include the nature of the culture of technology, the professionalization process of engineers and their need for social status and cultural recognition, and the place of technology in nineteenth century lives and hierarchies.",
    "advisors": ["Merrit Roe Smith"],
    "text": "Expositions, museums, and technological display : building cultural institutions for the \"inventor citizen\" in the late nineteenth century United States The dissertation is an historical study of the interactions between technologists and museums in the late nineteenth United States, the role of international expositions-such as Philadelphia in 1876 and Chicago in 1893-in these interactions, and the rise of technology collections in those museums. Through archival sources, as well as published primary and secondary source material, the dissertation examines the role of engineers and the public in creating technological collections in museums dominated by natural history specimens. It focuses on intersections between industry, engineers, international expositions, and museums in the nineteenth century by considering the cases of the Smithsonian's National Museum and the Field Columbian Museum. This research explores technology and its cultural roles, how technology related to or differed from other aspects of American culture, and how this may have precluded the establishment of a national museum dedicated to mechanical arts, technology and America's inventor citizens, even while some engineers brokered a place for technological collections to develop. Despite objections and a lack of support from the higher administration within the museums, mechanical and technological collections developed. In an era of enthusiasm for technology, invention, and mechanics, forces outside the museums pushed the development of the collections. In particular, a group of engineers, as curators and exhibit designers; played roles in the celebration of technological achievement and at the expositions, in the attempts to establish mechanical arts and technology collections at the two prominent museums, and in the connections between technologists and museums that proved essential to the development of the collections. In addition, pressure from a public audience enthused about technology and machines aided such collections by influencing museum administration. This dissertation argues that engineers became mediators between the museum world and the world of engineering by brokering the culture of technology and securing a subordinate, yet permanent place for technology within the museum world. Key issues in the negotiation and brokering include the nature of the culture of technology, the professionalization process of engineers and their need for social status and cultural recognition, and the place of technology in nineteenth century lives and hierarchies."
}, {
    "id": "oai:dspace.mit.edu:1721.1/113741",
    "title": "Twitter and the body parodic : global acts of re-creation and recreation",
    "abstract": "This dissertation investigates Twitter parody accounts as a form of social critique and linguistic play across English, Japanese, and Arabic-one that is collaboratively created by the users, policymakers, and architects of Twitter. Together, apart, and in different constellations with governments and news media, these actors use parody accounts to recreate and experiment with everything from law to what constitutes a person. I argue that the Twitter parody account, both as negative critique and ambiguous personification play, is an off-platform use-an unintended use of platform, site, or app that is allowed to endure, with varying degrees of official encouragement, silence, and ignorance. Drawing on ethnographic, linguistic, and legal analysis, the dissertation details the contours of this use, its adversaries and proponents among traditional structures of authority, and how the platform has ratified and deployed it globally. Chapter 1, Aspect Shift, examines how a parody account works at a linguistic level through the name and profile photo play of a classic political parody account. Chapter 2, The Account-Person, proposes that personhood on Twitter is a cyborg entity and investigates five elements the shape this account-person: number, body, position, world, and time. Turning to parody accounts' relationship with authority, chapter 3, Warranting Parody, investigates why some in positions of authority mobilize apparatuses of power against parody accounts. Not all governmental employees, however, see parody accounts as threats. Chapter 4, Tweeting Like a State, explores the development of norms around parody among a key, but often overlooked group of contemporary interpreters of representative government: governmental social media managers. Chapter 5, The Social Media Contract, argues that the history of Twitter's parody policy is the history of its still-emerging social contract, a contract shaped by user demands, the abdication of traditional authorities, and Twitter's own interests. This social contract has uneven globality-as chapter 6, Of Policyness and Global Polysemy, shows through examining Twitter's parody policy across languages. Finally, in the conclusion I bring these various strands together through the concept of usership, a member relationship entangled with citizenship yet largely asserted and negotiated with corporations rather than governments.",
    "advisors": ["Graham Jones"],
    "text": "Twitter and the body parodic : global acts of re-creation and recreation This dissertation investigates Twitter parody accounts as a form of social critique and linguistic play across English, Japanese, and Arabic-one that is collaboratively created by the users, policymakers, and architects of Twitter. Together, apart, and in different constellations with governments and news media, these actors use parody accounts to recreate and experiment with everything from law to what constitutes a person. I argue that the Twitter parody account, both as negative critique and ambiguous personification play, is an off-platform use-an unintended use of platform, site, or app that is allowed to endure, with varying degrees of official encouragement, silence, and ignorance. Drawing on ethnographic, linguistic, and legal analysis, the dissertation details the contours of this use, its adversaries and proponents among traditional structures of authority, and how the platform has ratified and deployed it globally. Chapter 1, Aspect Shift, examines how a parody account works at a linguistic level through the name and profile photo play of a classic political parody account. Chapter 2, The Account-Person, proposes that personhood on Twitter is a cyborg entity and investigates five elements the shape this account-person: number, body, position, world, and time. Turning to parody accounts' relationship with authority, chapter 3, Warranting Parody, investigates why some in positions of authority mobilize apparatuses of power against parody accounts. Not all governmental employees, however, see parody accounts as threats. Chapter 4, Tweeting Like a State, explores the development of norms around parody among a key, but often overlooked group of contemporary interpreters of representative government: governmental social media managers. Chapter 5, The Social Media Contract, argues that the history of Twitter's parody policy is the history of its still-emerging social contract, a contract shaped by user demands, the abdication of traditional authorities, and Twitter's own interests. This social contract has uneven globality-as chapter 6, Of Policyness and Global Polysemy, shows through examining Twitter's parody policy across languages. Finally, in the conclusion I bring these various strands together through the concept of usership, a member relationship entangled with citizenship yet largely asserted and negotiated with corporations rather than governments."
}, {
    "id": "oai:dspace.mit.edu:1721.1/86283",
    "title": "Empire's metropolis : money time & space in Colonial Bombay, 1870-1930",
    "abstract": "The thesis utilises newly available legal and municipal archives to study the historical geography of colonial Bombay through five interlocking themes and periods from 1870-1930. This spans the period between the boom and bust in the cotton trade during and after the American Civil War - when Bombay was a colonial mercantile port - to its emergence as of one of India and Asia's largest industrial cities after the First World War. Separate chapters explore the history of railway and telegraph networks, standardisation and time-keeping, land acquisition and valuation, cadastral surveying and property registration, and the urban built environment. From the perspective of the colonial city, the history of these formations looks less like the smooth unfolding of singular standards of money, time or space, than a protracted war of position fought out across a century by experts, elites and the masses. This thesis seeks to deepen the social and political history of urbanization in South Asia beyond concepts of colonial technology transfer or nationalist resistance by examining the everyday politics of stock and real estate speculation, public clocks, land and private property, maps and topographical surveys, and buildings and streets in colonial Bombay. These \"modern\" technologies of calculation, coordination and control in the urban environment both created and depended on new scales of power and capital accumulation, or particular configurations of industrial technologies, civic institutions and urban space.",
    "advisors": ["Michael M.J. Fischer"],
    "text": "Empire's metropolis : money time & space in Colonial Bombay, 1870-1930 The thesis utilises newly available legal and municipal archives to study the historical geography of colonial Bombay through five interlocking themes and periods from 1870-1930. This spans the period between the boom and bust in the cotton trade during and after the American Civil War - when Bombay was a colonial mercantile port - to its emergence as of one of India and Asia's largest industrial cities after the First World War. Separate chapters explore the history of railway and telegraph networks, standardisation and time-keeping, land acquisition and valuation, cadastral surveying and property registration, and the urban built environment. From the perspective of the colonial city, the history of these formations looks less like the smooth unfolding of singular standards of money, time or space, than a protracted war of position fought out across a century by experts, elites and the masses. This thesis seeks to deepen the social and political history of urbanization in South Asia beyond concepts of colonial technology transfer or nationalist resistance by examining the everyday politics of stock and real estate speculation, public clocks, land and private property, maps and topographical surveys, and buildings and streets in colonial Bombay. These \"modern\" technologies of calculation, coordination and control in the urban environment both created and depended on new scales of power and capital accumulation, or particular configurations of industrial technologies, civic institutions and urban space."
}, {
    "id": "oai:dspace.mit.edu:1721.1/39177",
    "title": "Manufacturing muscle : the hot rod industry and the American fascination with speed, 1915-1984",
    "abstract": "This dissertation focuses on the pursuits of a particular subset of automobile users: hot rodders, those who modify their standard production automobiles for improved performance. More specifically, this project examines the history of the speed equipment industry - the aftermarket subsector which manufactures high-performance products for hot rodders - from its infancy in the 1910s through the mid 1980s. The thesis begins by examining the role of technological enthusiasm in the early growth of hot rodding, focusing in particular on the ways in which this enthusiasm led a handful of individuals to begin to manufacture high-performance parts in the 1910s, 1920s, and 1930s. After tracing the wartime experiences of these industry pioneers, the project then explores the ways in which, in the midst of America's postwar affluence, the spectacular growth both of the high-performance industry and of hot rodding itself helped spawn the youth-oriented musclecar movement upon which the Big Three would later feed. In its examination of the 1940s and 1950s, the dissertation closely examines the evolution of this industry's production methods in an attempt to understand the manufacturing dynamics of a market-sensitive, flexibly-oriented, late-twentieth-century industrial sector.",
    "advisors": ["Merritt Roe Smith"],
    "text": "Manufacturing muscle : the hot rod industry and the American fascination with speed, 1915-1984 This dissertation focuses on the pursuits of a particular subset of automobile users: hot rodders, those who modify their standard production automobiles for improved performance. More specifically, this project examines the history of the speed equipment industry - the aftermarket subsector which manufactures high-performance products for hot rodders - from its infancy in the 1910s through the mid 1980s. The thesis begins by examining the role of technological enthusiasm in the early growth of hot rodding, focusing in particular on the ways in which this enthusiasm led a handful of individuals to begin to manufacture high-performance parts in the 1910s, 1920s, and 1930s. After tracing the wartime experiences of these industry pioneers, the project then explores the ways in which, in the midst of America's postwar affluence, the spectacular growth both of the high-performance industry and of hot rodding itself helped spawn the youth-oriented musclecar movement upon which the Big Three would later feed. In its examination of the 1940s and 1950s, the dissertation closely examines the evolution of this industry's production methods in an attempt to understand the manufacturing dynamics of a market-sensitive, flexibly-oriented, late-twentieth-century industrial sector."
}, {
    "id": "oai:dspace.mit.edu:1721.1/120880",
    "title": "Bodies at war : National Security in American controversies over animal & human experimentation from WWI to the War on Terror",
    "abstract": "The rhetoric and apparatus of national security have played critical roles in American controversies over animal and human experimentation from the dawn of the Twentieth Century to today's \"War on Terror.\" Drawing on archival and Freedom of Information Act (FOIA) research, this dissertation traces how American partisans in the enduring vivisection controversy have sought to mobilize national security concerns to tar their domestic political adversaries as enemy agents of foreign enemies from the Kaiser and Hitler to Stalin and Al-Qaeda. Further, this study explores how these efforts have intersected with issues of gender, slavery, and the pathologizing of political dissent, as well as campaigns for the absolute freedom of research, the functioning of Nazism and the Holocaust in the American political imagination, civil liberties in the Post-9/11 world, and ongoing debates over animal rights, the Federal Bureau of Investigation (FBI), and domestic terrorism.",
    "advisors": ["Harriet Ritvo"],
    "text": "Bodies at war : National Security in American controversies over animal & human experimentation from WWI to the War on Terror The rhetoric and apparatus of national security have played critical roles in American controversies over animal and human experimentation from the dawn of the Twentieth Century to today's \"War on Terror.\" Drawing on archival and Freedom of Information Act (FOIA) research, this dissertation traces how American partisans in the enduring vivisection controversy have sought to mobilize national security concerns to tar their domestic political adversaries as enemy agents of foreign enemies from the Kaiser and Hitler to Stalin and Al-Qaeda. Further, this study explores how these efforts have intersected with issues of gender, slavery, and the pathologizing of political dissent, as well as campaigns for the absolute freedom of research, the functioning of Nazism and the Holocaust in the American political imagination, civil liberties in the Post-9/11 world, and ongoing debates over animal rights, the Federal Bureau of Investigation (FBI), and domestic terrorism."
}, {
    "id": "oai:dspace.mit.edu:1721.1/93813",
    "title": "Rigging the world : 3D modeling and the seduction of the real",
    "abstract": "Evidence from history, archaeology, and the social sciences suggests that making models of the world has anchored our understanding of it since the earliest days. From models of deities, dwellings and weapons to molecules and planetary systems, models have been tools for thinking and imagining, as well as planning and building. Though there are many possible definitions, a digital model is understood here essentially as a miniature virtual world-a distillation that captures the essence of some aspect of the larger physical or imagined world-which provides a vehicle for virtual exploration or manipulation of that world, and a focal point for debates about its nature. This dissertation explores the communities engaged in making 3D digital models used in computer animation-globally dispersed communities linked by their professional tools and practices and their shared use of algorithms to sculpt geometry in the spaces of the machine. My research builds on the work of historians of computing and computer-aided design as well as insights on objects and their meanings by scholars such as Sherry Turkle, Peter Galison and Loraine Daston. To this conversation I contribute a view of digital models as meaningful objects, both tactile and evocative, around which conversations on expertise, craft, nature and representation coalesce. I draw on the work of Merritt Roe Smith on the contributions of government funding to innovation, and David Kaiser's work on the role of representations in circulating professional identities and shaping professional communities. Finally I draw on the work of those who have thought deeply about creativity and digital design tools and practices, including Rudolph Arnheim and Malcolm McCullough. I have also benefitted from Lev Manovich's work on software in the evolving field of digital humanities and software studies. I base my understanding of 3D modeling practices on a series of interviews conducted with a widely-dispersed community of artists, programmers and technical specialists who collaborate, sometimes over great distances, in creating 3D models for the entertainment industry. I argue that the idea of 3D modeling was shaped by the intersections of contrasting styles of abstraction practiced by artists and engineers. The interactions of present-day modelers with their models are part of an emerging discourse with the world, opening new possibilities for human interaction with the world's objects. As an example of the complex, global flow of people, objects and ideas I contrast two animation studios on opposite sides of the world, located in Connecticut and New Zealand. Though each of these is far from Hollywood-traditionally regarded as the heart of the film and animation industries-3D modeling practices, shared software and migrating workers link them to each other and to a global community of 3D thinkers and makers. Digital 3D modeling-sculpting characters, objects and environments, and even 3D printing the objects-emerges as a powerful way of connecting the self to the world. Finally, I examine the use of models as archives of real world objects and their attributes, with a case study a natural history museum's 3D modeling of a dinosaur.",
    "advisors": ["Merritt Roe Smith"],
    "text": "Rigging the world : 3D modeling and the seduction of the real Evidence from history, archaeology, and the social sciences suggests that making models of the world has anchored our understanding of it since the earliest days. From models of deities, dwellings and weapons to molecules and planetary systems, models have been tools for thinking and imagining, as well as planning and building. Though there are many possible definitions, a digital model is understood here essentially as a miniature virtual world-a distillation that captures the essence of some aspect of the larger physical or imagined world-which provides a vehicle for virtual exploration or manipulation of that world, and a focal point for debates about its nature. This dissertation explores the communities engaged in making 3D digital models used in computer animation-globally dispersed communities linked by their professional tools and practices and their shared use of algorithms to sculpt geometry in the spaces of the machine. My research builds on the work of historians of computing and computer-aided design as well as insights on objects and their meanings by scholars such as Sherry Turkle, Peter Galison and Loraine Daston. To this conversation I contribute a view of digital models as meaningful objects, both tactile and evocative, around which conversations on expertise, craft, nature and representation coalesce. I draw on the work of Merritt Roe Smith on the contributions of government funding to innovation, and David Kaiser's work on the role of representations in circulating professional identities and shaping professional communities. Finally I draw on the work of those who have thought deeply about creativity and digital design tools and practices, including Rudolph Arnheim and Malcolm McCullough. I have also benefitted from Lev Manovich's work on software in the evolving field of digital humanities and software studies. I base my understanding of 3D modeling practices on a series of interviews conducted with a widely-dispersed community of artists, programmers and technical specialists who collaborate, sometimes over great distances, in creating 3D models for the entertainment industry. I argue that the idea of 3D modeling was shaped by the intersections of contrasting styles of abstraction practiced by artists and engineers. The interactions of present-day modelers with their models are part of an emerging discourse with the world, opening new possibilities for human interaction with the world's objects. As an example of the complex, global flow of people, objects and ideas I contrast two animation studios on opposite sides of the world, located in Connecticut and New Zealand. Though each of these is far from Hollywood-traditionally regarded as the heart of the film and animation industries-3D modeling practices, shared software and migrating workers link them to each other and to a global community of 3D thinkers and makers. Digital 3D modeling-sculpting characters, objects and environments, and even 3D printing the objects-emerges as a powerful way of connecting the self to the world. Finally, I examine the use of models as archives of real world objects and their attributes, with a case study a natural history museum's 3D modeling of a dinosaur."
}, {
    "id": "oai:dspace.mit.edu:1721.1/8019",
    "title": "Techno-Territories : the spatial, technological and social reorganization of office work",
    "abstract": "In this thesis, I examine the current reorganization of office or information work in its technological, spatial, social and cultural aspects. Based on ethnographic and historical methods, I explore how information and communication technologies, and spatial designs combined with specific organizational visions shape the social organization and culture of work. Analyzing ideas as well as material configurations, design as well as use, and developments in offices as well as those beyond the office, I am particularly concerned with the new forms of life and cultural formations these developments produce. The 1990s saw a development towards more flexible, mobile and virtual ways of officing, such as non-territorial offices and remote work. Analyzing the alternative officing movement in the context of wider economic and cultural changes, I demonstrate the strategic role technology plays for the movement's vision of a less place-based definition of work. Suggesting an alternative intellectual origin for today's office concepts different from Taylorism, I opt for the concept of office landscaping, an influential German office concept developed after WW2 and inspired by cybernetics. Fieldwork in a present day innovative office design firm reveals a deep tension in its office designs between a more flexible and mobile organizational goal, and a more communicative and collaborative one - a tension that is exacerbated by technology. Further exploring the designers' own mobile and non-territorial office, I introduce the notion of placemaking to explain the observed friction in the mobile ways of the office. I also find a reconfigured power dynamic that is no longer based on space ownership, but rather on mobility and ownership of technology, or \"techno-territory.\"",
    "advisors": ["Michael M.J. Fischer"],
    "text": "Techno-Territories : the spatial, technological and social reorganization of office work In this thesis, I examine the current reorganization of office or information work in its technological, spatial, social and cultural aspects. Based on ethnographic and historical methods, I explore how information and communication technologies, and spatial designs combined with specific organizational visions shape the social organization and culture of work. Analyzing ideas as well as material configurations, design as well as use, and developments in offices as well as those beyond the office, I am particularly concerned with the new forms of life and cultural formations these developments produce. The 1990s saw a development towards more flexible, mobile and virtual ways of officing, such as non-territorial offices and remote work. Analyzing the alternative officing movement in the context of wider economic and cultural changes, I demonstrate the strategic role technology plays for the movement's vision of a less place-based definition of work. Suggesting an alternative intellectual origin for today's office concepts different from Taylorism, I opt for the concept of office landscaping, an influential German office concept developed after WW2 and inspired by cybernetics. Fieldwork in a present day innovative office design firm reveals a deep tension in its office designs between a more flexible and mobile organizational goal, and a more communicative and collaborative one - a tension that is exacerbated by technology. Further exploring the designers' own mobile and non-territorial office, I introduce the notion of placemaking to explain the observed friction in the mobile ways of the office. I also find a reconfigured power dynamic that is no longer based on space ownership, but rather on mobility and ownership of technology, or \"techno-territory.\""
}, {
    "id": "oai:dspace.mit.edu:1721.1/42423",
    "title": "Executive coaching : crafting a versatile self in corporate America",
    "abstract": "In recent years, coaching has become a major form of personal and professional development service offered to executives to help develop leadership skills, enhance performance, and remediate patterns of problematic workplace behavior. This dissertation examines the emergence and development of executive coaching in the United States as a new form of professional expertise. Drawing on eighteen months of ethnographic research, the majority of which took place in New York City, this study analyzes the ways in which executive coaching brings together theories of individual psychology and of organizational efficiency in order to increase functionality and productivity at work. Executive coaching is: a) a new form of professional expertise, b) a management tool to increase productivity and efficiency at work, c) a window to changing notions of the self and personhood in America and, finally d) an access point to the corporate world. This study explores these four dimensions of executive coaching. I argue that the emergence of coaching is a product of and a response to a fast changing business environment where continuous improvement is required to adapt to the volatility of changes. Change in the larger context (corporate settings and business environments) is not to be resisted or criticized but to be enabled through the change of the self. This dissertation illustrates and explains the grounds of a shift away from systemic approaches and systemic criticism towards individualistic approaches. Coaching emerges in and becomes an illustration of a neo-liberal economy that emphasizes constant retraining of a self that is versatile, pragmatic and fragmented.",
    "advisors": ["Michael M.J. Fischer"],
    "text": "Executive coaching : crafting a versatile self in corporate America In recent years, coaching has become a major form of personal and professional development service offered to executives to help develop leadership skills, enhance performance, and remediate patterns of problematic workplace behavior. This dissertation examines the emergence and development of executive coaching in the United States as a new form of professional expertise. Drawing on eighteen months of ethnographic research, the majority of which took place in New York City, this study analyzes the ways in which executive coaching brings together theories of individual psychology and of organizational efficiency in order to increase functionality and productivity at work. Executive coaching is: a) a new form of professional expertise, b) a management tool to increase productivity and efficiency at work, c) a window to changing notions of the self and personhood in America and, finally d) an access point to the corporate world. This study explores these four dimensions of executive coaching. I argue that the emergence of coaching is a product of and a response to a fast changing business environment where continuous improvement is required to adapt to the volatility of changes. Change in the larger context (corporate settings and business environments) is not to be resisted or criticized but to be enabled through the change of the self. This dissertation illustrates and explains the grounds of a shift away from systemic approaches and systemic criticism towards individualistic approaches. Coaching emerges in and becomes an illustration of a neo-liberal economy that emphasizes constant retraining of a self that is versatile, pragmatic and fragmented."
}, {
    "id": "oai:dspace.mit.edu:1721.1/66037",
    "title": "Accounting for taste : regulating food labeling in the \"affluent society,\" 1945-1995",
    "abstract": "This dissertation traces a transformation in the U.S. Food and Drug Administration's governance of food markets during the second half of the 20th century. In response to new correlations between diet and risk of disease, anxieties about (over)abundant food supplies, and changing notions of personal versus collective responsibility in an affluent society, the FDA changed how it regulated food labeling. Following WWII, the agency developed a set of standard recipes with fixed common name labels (such as \"peanut butter\" or \"tomato soup\"), or \"standards of identity,\" for all mass-produced foods. However, the appearance of new diet foods and public health concerns undermined this system. Beginning in the 1970s, the FDA shifted its policies. Rather than rely on standardized identities, the agency required companies to provide informative labels such as the ingredients panel, nutrition labels, and various science-based health claims. Agency officials believed that such information would enable consumers to make responsible health decisions through market purchases. Food labeling is explored as a regulatory assemblage that draws together a variety of political, legal, corporate, and technoscientific interests and practices. The five chapters are organized chronologically. The first two describe how a shift in focus among nutrition scientists from concern for the undernourished to a concern with overeating led to the introduction onto the market of engineered foods capitalizing off popular interest in diet and health. A middle chapter describes a series of institutional scandals that generated the political animus to change the FDA's system, and registered a broader \"shock of recognition\" that Americans' views about food and food politics had changed. The final two chapters describe the introduction of \"Nutrition Information\" labeling in the 1970s and the mandatory \"Nutrition Facts\" panel in the 1990s. By looking at the regulation of labels as a kind of public-private infrastructure for information, the turn to compositional labeling can be understood not merely as a shift in representation-from whole foods to foods as nutrients-but more broadly as a retooling of food markets to embed notions about personal responsibility for health into the ways that food was designed, marketed, and consumed.",
    "advisors": ["Deborah K. Fitzgerald"],
    "text": "Accounting for taste : regulating food labeling in the \"affluent society,\" 1945-1995 This dissertation traces a transformation in the U.S. Food and Drug Administration's governance of food markets during the second half of the 20th century. In response to new correlations between diet and risk of disease, anxieties about (over)abundant food supplies, and changing notions of personal versus collective responsibility in an affluent society, the FDA changed how it regulated food labeling. Following WWII, the agency developed a set of standard recipes with fixed common name labels (such as \"peanut butter\" or \"tomato soup\"), or \"standards of identity,\" for all mass-produced foods. However, the appearance of new diet foods and public health concerns undermined this system. Beginning in the 1970s, the FDA shifted its policies. Rather than rely on standardized identities, the agency required companies to provide informative labels such as the ingredients panel, nutrition labels, and various science-based health claims. Agency officials believed that such information would enable consumers to make responsible health decisions through market purchases. Food labeling is explored as a regulatory assemblage that draws together a variety of political, legal, corporate, and technoscientific interests and practices. The five chapters are organized chronologically. The first two describe how a shift in focus among nutrition scientists from concern for the undernourished to a concern with overeating led to the introduction onto the market of engineered foods capitalizing off popular interest in diet and health. A middle chapter describes a series of institutional scandals that generated the political animus to change the FDA's system, and registered a broader \"shock of recognition\" that Americans' views about food and food politics had changed. The final two chapters describe the introduction of \"Nutrition Information\" labeling in the 1970s and the mandatory \"Nutrition Facts\" panel in the 1990s. By looking at the regulation of labels as a kind of public-private infrastructure for information, the turn to compositional labeling can be understood not merely as a shift in representation-from whole foods to foods as nutrients-but more broadly as a retooling of food markets to embed notions about personal responsibility for health into the ways that food was designed, marketed, and consumed."
}, {
    "id": "oai:dspace.mit.edu:1721.1/39172",
    "title": "Characterizing radio channels : the science and technology of propagation and interference, 1900-1935",
    "abstract": "Guglielmo Marconi's trans-Atlantic wireless experiment in 1900 marked the beginning of a communication revolution that transformed the open space above the earth into channels of information flow. This dissertation grapples with the historical conditions that gave rise to such a transformation: the studies of radio-wave propagation and the treatments of radio interferences in early twentieth-century America and Western Europe. The part on propagation examines the debate between the surface diffraction theory and the atmospheric reflection theory for long waves, the development of the ionic refraction theory for short waves, the evidential quests for the existence of the ionosphere, and the studies of the geomagnetic effects on propagation. The part on interferences focuses on the engineering efforts toward the characterization of atmospheric noise and signal-intensity fluctuations, the policies of radio-channel allocation for fighting man-made interference, and the scientific research into electronic tube noise. By the mid-30s, the results from these endeavors had considerably improved the quality of radio communication. Characterizing Radio Channels builds a bridge between the history of science and the history of technology by inspecting an immaterial engineering entity--radio channels--whose control required significant scientific research. In the history of science, it contributes to an integrated study of electrical physics and geophysics. In the history of technology, it enriches radio history, epistemology of engineering knowledge, consumer studies, and the studies of technological policies. Combining both fields with the concept of radio channels enables a new understanding of the historical conditions that made the information society",
    "advisors": ["David A. Mindell"],
    "text": "Characterizing radio channels : the science and technology of propagation and interference, 1900-1935 Guglielmo Marconi's trans-Atlantic wireless experiment in 1900 marked the beginning of a communication revolution that transformed the open space above the earth into channels of information flow. This dissertation grapples with the historical conditions that gave rise to such a transformation: the studies of radio-wave propagation and the treatments of radio interferences in early twentieth-century America and Western Europe. The part on propagation examines the debate between the surface diffraction theory and the atmospheric reflection theory for long waves, the development of the ionic refraction theory for short waves, the evidential quests for the existence of the ionosphere, and the studies of the geomagnetic effects on propagation. The part on interferences focuses on the engineering efforts toward the characterization of atmospheric noise and signal-intensity fluctuations, the policies of radio-channel allocation for fighting man-made interference, and the scientific research into electronic tube noise. By the mid-30s, the results from these endeavors had considerably improved the quality of radio communication. Characterizing Radio Channels builds a bridge between the history of science and the history of technology by inspecting an immaterial engineering entity--radio channels--whose control required significant scientific research. In the history of science, it contributes to an integrated study of electrical physics and geophysics. In the history of technology, it enriches radio history, epistemology of engineering knowledge, consumer studies, and the studies of technological policies. Combining both fields with the concept of radio channels enables a new understanding of the historical conditions that made the information society"
}, {
    "id": "oai:dspace.mit.edu:1721.1/43219",
    "title": "The wired wilderness : electronic surveillance and environmental values in wildlife biology",
    "abstract": "In the second half of the twentieth century, American wildlife biologists incorporated Cold War-era surveillance technologies into their practices in order to render wild animals and their habitats legible and manageable. One of the most important of these was wildlife radio-tracking, in which collars and tags containing miniature transmitters were used to locate individual animals in the field. In addition to producing new ecological insights, radio-tracking served as a site where relationships among scientists, animals, hunters, animal rights activists, environmentalists, and others involved in wildlife conservation could be embodied and contested. While scholars have tended to interpret surveillance technologies in terms of the extension of human control over nature and society, I show how technological, biological, and ecological factors made such control fragmentary and open to reappropriation. Wildlife radio-tracking created vulnerabilities as well as capabilities; it provided opportunities for connection as well as for control. I begin by showing how biologists in Minnesota and Illinois in the early 1960s used radio-tracking to establish intimate, technologically-mediated, situated relationships with game animals such as ruffed grouse, which they hoped would bolster their authority vis-a-vis recreational hunters. I then show how the technique was contested by environmentalists when biologists applied it to iconic \"wilderness wildlife\" such as grizzly bears in Yellowstone National Park in the 1960s and 1970s. One way for biologists to render radio-tracking acceptable in the face of such opposition was to emphasize its continuity with traditional practices, as they did in a radio-tagging study of tigers in Nepal in the 1970s.",
    "advisors": ["Harriet Ritvo"],
    "text": "The wired wilderness : electronic surveillance and environmental values in wildlife biology In the second half of the twentieth century, American wildlife biologists incorporated Cold War-era surveillance technologies into their practices in order to render wild animals and their habitats legible and manageable. One of the most important of these was wildlife radio-tracking, in which collars and tags containing miniature transmitters were used to locate individual animals in the field. In addition to producing new ecological insights, radio-tracking served as a site where relationships among scientists, animals, hunters, animal rights activists, environmentalists, and others involved in wildlife conservation could be embodied and contested. While scholars have tended to interpret surveillance technologies in terms of the extension of human control over nature and society, I show how technological, biological, and ecological factors made such control fragmentary and open to reappropriation. Wildlife radio-tracking created vulnerabilities as well as capabilities; it provided opportunities for connection as well as for control. I begin by showing how biologists in Minnesota and Illinois in the early 1960s used radio-tracking to establish intimate, technologically-mediated, situated relationships with game animals such as ruffed grouse, which they hoped would bolster their authority vis-a-vis recreational hunters. I then show how the technique was contested by environmentalists when biologists applied it to iconic \"wilderness wildlife\" such as grizzly bears in Yellowstone National Park in the 1960s and 1970s. One way for biologists to render radio-tracking acceptable in the face of such opposition was to emphasize its continuity with traditional practices, as they did in a radio-tagging study of tigers in Nepal in the 1970s."
}, {
    "id": "oai:dspace.mit.edu:1721.1/120645",
    "title": "Meditations in an emergency : social scientists and the problem of conflict in Cold War America",
    "abstract": "Through the mode of conceptual history, this dissertation examines some of the forms dissent could take within academic social science in the United States from roughly 1945-1970. The concept in question is \"conflict.\" There are many stories one could tell about this concept and its transformations in postwar American social science, but in this dissertation I focus on one in particular: how certain social scientists sought to frame conflict as a problem of knowledge, by stretching the concept to fit the global proportions of the bipolar world that seemed to have emerged from World War II, and then using that conceptualization to oppose the Cold War. The dissertation first considers a specific moment of conceptual change, when some social scientists sought to redefine \"conflict\" in the immediate aftermath of World War II, so that it would be capacious enough to describe conflict at all levels of analysis, from the intrapersonal to the international. From there, it follows a cadre of social scientists who used that novel conceptualization to build an intellectual movement around a new journal and research center starting in the mid- 1950s. The scholars who participated in that movement, known as \"peace research\" or ''conflict resolution,\" endeavored to construct a \"general theory of conflict,\" which they would then employ to challenge the notion that the Cold War was inevitable. The language of mid-century social science was the idiom in which they expressed their dissent. Although this was to become an international movement, this dissertation focuses on its American incarnation, which came to fruition at the University of Michigan in Ann Arbor beginning around 1957. The dissertation then looks closely at how two of the leading theorists of that movement modeled conflict in the early 1960s, and considers the ethical and political impulses that animated their work, demonstrating that it was possible for some intellectuals to inhabit the dual role of academic social scientist and social critic in the early 1960s. It concludes with a brief set of reflections on the United States Institute of Peace, an independent federal institute established in 1984 to embody the dream of \"conflict resolution.\"",
    "advisors": ["Christopher Capozzola"],
    "text": "Meditations in an emergency : social scientists and the problem of conflict in Cold War America Through the mode of conceptual history, this dissertation examines some of the forms dissent could take within academic social science in the United States from roughly 1945-1970. The concept in question is \"conflict.\" There are many stories one could tell about this concept and its transformations in postwar American social science, but in this dissertation I focus on one in particular: how certain social scientists sought to frame conflict as a problem of knowledge, by stretching the concept to fit the global proportions of the bipolar world that seemed to have emerged from World War II, and then using that conceptualization to oppose the Cold War. The dissertation first considers a specific moment of conceptual change, when some social scientists sought to redefine \"conflict\" in the immediate aftermath of World War II, so that it would be capacious enough to describe conflict at all levels of analysis, from the intrapersonal to the international. From there, it follows a cadre of social scientists who used that novel conceptualization to build an intellectual movement around a new journal and research center starting in the mid- 1950s. The scholars who participated in that movement, known as \"peace research\" or ''conflict resolution,\" endeavored to construct a \"general theory of conflict,\" which they would then employ to challenge the notion that the Cold War was inevitable. The language of mid-century social science was the idiom in which they expressed their dissent. Although this was to become an international movement, this dissertation focuses on its American incarnation, which came to fruition at the University of Michigan in Ann Arbor beginning around 1957. The dissertation then looks closely at how two of the leading theorists of that movement modeled conflict in the early 1960s, and considers the ethical and political impulses that animated their work, demonstrating that it was possible for some intellectuals to inhabit the dual role of academic social scientist and social critic in the early 1960s. It concludes with a brief set of reflections on the United States Institute of Peace, an independent federal institute established in 1984 to embody the dream of \"conflict resolution.\""
}, {
    "id": "oai:dspace.mit.edu:1721.1/17820",
    "title": "Bodies of information : reinventing bodies and practice in medical education",
    "abstract": "This dissertation recounts the development of graphic models of human bodies and virtual reality simulators for teaching anatomy and surgery to medical students, residents, and physicians. It considers how researchers from disciplinary cultures in medicine, engineering, and computer programming come together to build these technologies, bringing with them values and assumptions about bodies from each of their disciplines, values and assumptions that must be negotiated and that often are made material and embedded in these new technologies. It discusses how the technological objects being created privilege the body as a dynamic and interactive system, in contrast to the description and taxonomic body of traditional anatomy and medicine. It describes the ways that these technologies create new sensory means of knowing bodies. And it discusses the larger cultural values that these technologies reify or challenge. The methodology of this dissertation is ethnography. I consider in-depth one laboratory at a major medical school, as well as other laboratories and researchers in the field of virtual medicine. I study actors in the emerging field of virtual medicine as they work in laboratories, at conferences, and in collaborations with one another. I consider the social formations that are developing with this new discipline. Methods include participant observation of laboratory activities, teaching, surgery, and conferences and extensive, in-depth interviewing of actors in the field. I draw on the literatures in the anthropology of science, technology, and medicine, the sociology of science, technology, and medicine, and the history of science and technology to argue that \"bodies of information\" are part of a bio-engineering revolution.",
    "advisors": ["Sherry Turkle"],
    "text": "Bodies of information : reinventing bodies and practice in medical education This dissertation recounts the development of graphic models of human bodies and virtual reality simulators for teaching anatomy and surgery to medical students, residents, and physicians. It considers how researchers from disciplinary cultures in medicine, engineering, and computer programming come together to build these technologies, bringing with them values and assumptions about bodies from each of their disciplines, values and assumptions that must be negotiated and that often are made material and embedded in these new technologies. It discusses how the technological objects being created privilege the body as a dynamic and interactive system, in contrast to the description and taxonomic body of traditional anatomy and medicine. It describes the ways that these technologies create new sensory means of knowing bodies. And it discusses the larger cultural values that these technologies reify or challenge. The methodology of this dissertation is ethnography. I consider in-depth one laboratory at a major medical school, as well as other laboratories and researchers in the field of virtual medicine. I study actors in the emerging field of virtual medicine as they work in laboratories, at conferences, and in collaborations with one another. I consider the social formations that are developing with this new discipline. Methods include participant observation of laboratory activities, teaching, surgery, and conferences and extensive, in-depth interviewing of actors in the field. I draw on the literatures in the anthropology of science, technology, and medicine, the sociology of science, technology, and medicine, and the history of science and technology to argue that \"bodies of information\" are part of a bio-engineering revolution."
}, {
    "id": "oai:dspace.mit.edu:1721.1/45804",
    "title": "The promiscuity of freedom : development and governance in the age of neoliberal networks",
    "abstract": "This study brings together science and technology studies, political anthropology, and Latin American studies, by studying the practices and political reasoning of neoliberal networks in Peru. It analyses the extension of such networks by studying the relationships and subjectivities cultivated under two contemporary state-led projects: an initiative promoting intellectual property rights among traditional artisans as tools for rural development, and a national effort to encourage the uptake of free/libre and open source software based resources. Promising to modernize government and prepare citizens for the global, information-based economy, these projects frame their reforms as new, contemporary models for economic development. This work demonstrate how key to the success of such projects is the remaking of rural and urban citizens into \"free\" and modern individuals who are able to independently self- realize using the tools and logics of information networks. It argues that such plans rely on the ability to bring diverse actors - including state planners, transnational corporations, traditional artisans, rural communities, urban technology experts, and transnational activists -- into strategic alliance, or what can become coded as relations of promiscuity. What brings these partnerships together and seduces such disparate actors into alliance isn't so much the promise of increased technology access. It is instead the promise of \"freedom\" and the opportunity for diversely situated subjects to realize themselves as \"modern individuals.\"",
    "advisors": ["Sherry Turkle"],
    "text": "The promiscuity of freedom : development and governance in the age of neoliberal networks This study brings together science and technology studies, political anthropology, and Latin American studies, by studying the practices and political reasoning of neoliberal networks in Peru. It analyses the extension of such networks by studying the relationships and subjectivities cultivated under two contemporary state-led projects: an initiative promoting intellectual property rights among traditional artisans as tools for rural development, and a national effort to encourage the uptake of free/libre and open source software based resources. Promising to modernize government and prepare citizens for the global, information-based economy, these projects frame their reforms as new, contemporary models for economic development. This work demonstrate how key to the success of such projects is the remaking of rural and urban citizens into \"free\" and modern individuals who are able to independently self- realize using the tools and logics of information networks. It argues that such plans rely on the ability to bring diverse actors - including state planners, transnational corporations, traditional artisans, rural communities, urban technology experts, and transnational activists -- into strategic alliance, or what can become coded as relations of promiscuity. What brings these partnerships together and seduces such disparate actors into alliance isn't so much the promise of increased technology access. It is instead the promise of \"freedom\" and the opportunity for diversely situated subjects to realize themselves as \"modern individuals.\""
}, {
    "id": "oai:dspace.mit.edu:1721.1/62963",
    "title": "Pharmaceutical relationships : intersections of illness, fantasy, and capital in the age of direct-to-consumer marketing",
    "abstract": "This dissertation is a multi-sited ethnography among marketers, consumer-patients and psychiatrists in the U.S. It explores the recent history of styles of pharmaceutical advertising that have come about in response to FDA regulations and ethical issues raised by patients and the press about how the pharmaceutical industry shapes drug research. Specifically this dissertation explores the role of direct-to-consumer drug marketing (DTC) in the consumption and experience of antidepressants, including a cultural shift in the U.S. towards how the consumer negotiates new ethical injunctions to manage his or her own identity through pharmaceuticals. A key focus is how marketers carve out their own ethical niche from which they innovate on ways to persuade consumer audiences with scientific facts that double as public relations. This dissertation gives special attention to how individuals encounter and incorporate the putative neuroscience of DTC advertising of antidepressants to negotiate their personal knowledge of illness, and to manage their identity, everyday practices, and professional pursuits. From these ethnographic encounters I have identified \"illness,\" \"fantasy,\" and \"capital\" as three key themes for my analysis of DTC marketing. In turn I have combined the very different literatures on illness (which address patient advocacy movements and health care seeking and questions of how medical diagnoses can be deployed as social norms), fantasy (which address psychoanalytic conceptions of desire and self, as well as semiotic understandings of consumption), and capital (which address health care market competition, and negotiations with the FDA over truth in advertising). In sum, this dissertation offers a thick description of \"ethical identity management\" in the contemporary landscape of U.S. pharmaceutical consumption.",
    "advisors": ["Joseph Dumit"],
    "text": "Pharmaceutical relationships : intersections of illness, fantasy, and capital in the age of direct-to-consumer marketing This dissertation is a multi-sited ethnography among marketers, consumer-patients and psychiatrists in the U.S. It explores the recent history of styles of pharmaceutical advertising that have come about in response to FDA regulations and ethical issues raised by patients and the press about how the pharmaceutical industry shapes drug research. Specifically this dissertation explores the role of direct-to-consumer drug marketing (DTC) in the consumption and experience of antidepressants, including a cultural shift in the U.S. towards how the consumer negotiates new ethical injunctions to manage his or her own identity through pharmaceuticals. A key focus is how marketers carve out their own ethical niche from which they innovate on ways to persuade consumer audiences with scientific facts that double as public relations. This dissertation gives special attention to how individuals encounter and incorporate the putative neuroscience of DTC advertising of antidepressants to negotiate their personal knowledge of illness, and to manage their identity, everyday practices, and professional pursuits. From these ethnographic encounters I have identified \"illness,\" \"fantasy,\" and \"capital\" as three key themes for my analysis of DTC marketing. In turn I have combined the very different literatures on illness (which address patient advocacy movements and health care seeking and questions of how medical diagnoses can be deployed as social norms), fantasy (which address psychoanalytic conceptions of desire and self, as well as semiotic understandings of consumption), and capital (which address health care market competition, and negotiations with the FDA over truth in advertising). In sum, this dissertation offers a thick description of \"ethical identity management\" in the contemporary landscape of U.S. pharmaceutical consumption."
}, {
    "id": "oai:dspace.mit.edu:1721.1/93811",
    "title": "Making biosecurity, making Mexico : an ethnography of biological invasion",
    "abstract": "This dissertation tracks what happens when biology, that is, both life forms and knowledge about them, becomes the object of security. While increasing global traffic has led to a greater degree of movement of people, animals, plants, and microbes, biosecurity measures are concerned with regulating circulation and seek to work against such possibly homogenizing forces by both documenting and maintaining the distinctiveness of life forms in different places. Through ethnographic research in Mexico, I track the social logics, scientific practices, and institutional forms that underwrite biosecurity in three areas: invasive species control, emerging infectious disease research, and the use of transgenic organisms. I examine how conservationists working in Mexican settings - particularly on islands - alternately protect or exterminate the various life forms they encounter; how microbiologists and immunologists studying infectious diseases in Mexico make claims about the relationships between environments, bodies, and viral ecologies; and how ecologists regulate the use of genetically modified organisms (GMOs) and turn them into bureaucratic objects. All these projects entail defining \"native\" life forms and establishing what is unique and valuable about Mexican biology. By bringing together this assortment of interlocutors and research sites I map how biosecurity projects establish the ways that a shared biological substantiality connects the nation and how human and non-human life forms are incorporated into political identities. Through these projects scientists produce knowledge about Mexican biology (including who or what is included or excluded in these populations). As this knowledge in turn informs political efforts to improve human and ecological health, biosecurity projects become ways in which science and the nation in Mexico are coconstituted. I address the production of biosecurity in two canonical places of science, the lab and the field, and I argue for the importance of a third scientific space, the office, a space where scientists engaged with bureaucratic processes and shaped the administration of Mexican ecosystems. Further, I argue that in Mexico biopolitics and biosecurity are no longer only about the regulation of human life, but have been extended beyond the human to encompass animal, plant, and microbial worlds. Mexican biopolitics have become multispecies projects.",
    "advisors": ["Stefan Helmreich"],
    "text": "Making biosecurity, making Mexico : an ethnography of biological invasion This dissertation tracks what happens when biology, that is, both life forms and knowledge about them, becomes the object of security. While increasing global traffic has led to a greater degree of movement of people, animals, plants, and microbes, biosecurity measures are concerned with regulating circulation and seek to work against such possibly homogenizing forces by both documenting and maintaining the distinctiveness of life forms in different places. Through ethnographic research in Mexico, I track the social logics, scientific practices, and institutional forms that underwrite biosecurity in three areas: invasive species control, emerging infectious disease research, and the use of transgenic organisms. I examine how conservationists working in Mexican settings - particularly on islands - alternately protect or exterminate the various life forms they encounter; how microbiologists and immunologists studying infectious diseases in Mexico make claims about the relationships between environments, bodies, and viral ecologies; and how ecologists regulate the use of genetically modified organisms (GMOs) and turn them into bureaucratic objects. All these projects entail defining \"native\" life forms and establishing what is unique and valuable about Mexican biology. By bringing together this assortment of interlocutors and research sites I map how biosecurity projects establish the ways that a shared biological substantiality connects the nation and how human and non-human life forms are incorporated into political identities. Through these projects scientists produce knowledge about Mexican biology (including who or what is included or excluded in these populations). As this knowledge in turn informs political efforts to improve human and ecological health, biosecurity projects become ways in which science and the nation in Mexico are coconstituted. I address the production of biosecurity in two canonical places of science, the lab and the field, and I argue for the importance of a third scientific space, the office, a space where scientists engaged with bureaucratic processes and shaped the administration of Mexican ecosystems. Further, I argue that in Mexico biopolitics and biosecurity are no longer only about the regulation of human life, but have been extended beyond the human to encompass animal, plant, and microbial worlds. Mexican biopolitics have become multispecies projects."
}, {
    "id": "oai:dspace.mit.edu:1721.1/39195",
    "title": "Japan and Taiwan in the wake of bio-globalization : drugs, race and standards",
    "abstract": "This is a study of Japan and Taiwan's different responses to the expansion of the global drug industry. The thesis focuses on the problematic of \"voicing,\" of how a state can make its interests heard in the International Conference on Harmonization of Technical Requirements for Registration of Pharmaceuticals for Human Use (ICH). The ICH is a unique project that facilitates the formation of a single global market by creating universal standards for clinical trials and drug approvals. Tracing, through \"slow motion\" ethnography, step by step, why Japan claims a racial difference requires additional local clinical trials with \"Asian bodies,\" this thesis rejects conventional interpretations of protectionism for Japan's resistance to globalization. It argues that more than protectionism is involved, and that a rich ethnographic understanding of Japan's medical infrastructure is required to understand the claim of biological, cultural, and national differences, as well as biostatistical arguments about the ambiguities of \"extrapolation\" of clinical data from one place to another.",
    "advisors": ["Michael M.J. Fischer"],
    "text": "Japan and Taiwan in the wake of bio-globalization : drugs, race and standards This is a study of Japan and Taiwan's different responses to the expansion of the global drug industry. The thesis focuses on the problematic of \"voicing,\" of how a state can make its interests heard in the International Conference on Harmonization of Technical Requirements for Registration of Pharmaceuticals for Human Use (ICH). The ICH is a unique project that facilitates the formation of a single global market by creating universal standards for clinical trials and drug approvals. Tracing, through \"slow motion\" ethnography, step by step, why Japan claims a racial difference requires additional local clinical trials with \"Asian bodies,\" this thesis rejects conventional interpretations of protectionism for Japan's resistance to globalization. It argues that more than protectionism is involved, and that a rich ethnographic understanding of Japan's medical infrastructure is required to understand the claim of biological, cultural, and national differences, as well as biostatistical arguments about the ambiguities of \"extrapolation\" of clinical data from one place to another."
}, {
    "id": "oai:dspace.mit.edu:1721.1/50110",
    "title": "From enthusiasm to practice : users, systems, and technology in high-end audio",
    "abstract": "This is a story about technology, users, and music. It is about an approach to the design, manipulation, and arrangement of technologies in small-scale systems to achieve particular aesthetic goals - goals that are at once subjective and contingent. These goals emerge from enthusiasm for technology, for system-building, and for music among members of a community of users, and the promise of the emotional rewards derived from these elements in combination. It is a story about how enthusiasm and passion become practice, and how particular technologies, system-building activities, listening, debating, innovating, and interacting form that practice. Using both historical and ethnographic research methods, including fieldwork and oral history interviews, this dissertation is focused on how and why user communities mobilize around particular technologies and socio-technical systems. In particular, it concerns how users' aesthetic sensibilities and enthusiasm for technology can shape both technologies themselves and the processes of technological innovation. These issues are explored through a study of the small but enthusiastic high-end audio community in the United States. These users express needs, desires, and aesthetic motivations towards technology that set them apart from mainstream consumers, but also reveal important and under-recognized aspects of human relationships with technology more broadly. Covering the emergence and growth of high-end audio from the early 1970s to 2000, I trace some of the major technology transitions during this period and their associated social elements, including the shift from vacuum tube to solid-state electronics in the 1970s, and from analog vinyl records to digital compact discs in the 1980s. I show how this community came to understand technology, science, and their own social behavior through powerful emotional and aesthetic responses to music and the technologies used to reproduce music in the home. I further show how focusing on technology's users can recast assumptions about the ingredients and conditions necessary to foster technological innovation.",
    "advisors": ["David Kaiser"],
    "text": "From enthusiasm to practice : users, systems, and technology in high-end audio This is a story about technology, users, and music. It is about an approach to the design, manipulation, and arrangement of technologies in small-scale systems to achieve particular aesthetic goals - goals that are at once subjective and contingent. These goals emerge from enthusiasm for technology, for system-building, and for music among members of a community of users, and the promise of the emotional rewards derived from these elements in combination. It is a story about how enthusiasm and passion become practice, and how particular technologies, system-building activities, listening, debating, innovating, and interacting form that practice. Using both historical and ethnographic research methods, including fieldwork and oral history interviews, this dissertation is focused on how and why user communities mobilize around particular technologies and socio-technical systems. In particular, it concerns how users' aesthetic sensibilities and enthusiasm for technology can shape both technologies themselves and the processes of technological innovation. These issues are explored through a study of the small but enthusiastic high-end audio community in the United States. These users express needs, desires, and aesthetic motivations towards technology that set them apart from mainstream consumers, but also reveal important and under-recognized aspects of human relationships with technology more broadly. Covering the emergence and growth of high-end audio from the early 1970s to 2000, I trace some of the major technology transitions during this period and their associated social elements, including the shift from vacuum tube to solid-state electronics in the 1970s, and from analog vinyl records to digital compact discs in the 1980s. I show how this community came to understand technology, science, and their own social behavior through powerful emotional and aesthetic responses to music and the technologies used to reproduce music in the home. I further show how focusing on technology's users can recast assumptions about the ingredients and conditions necessary to foster technological innovation."
}, {
    "id": "oai:dspace.mit.edu:1721.1/65321",
    "title": "More information is not the problem : spinning climate change, vernaculars, and emergent forms of life",
    "abstract": "This dissertation argues that alongside the dominant discourse occurring in and through media in the midst of immense transformation, social networks and affiliations provide a vital translation of science in varied vernaculars such that climate change is becoming invested with diverse meanings, ethics, and/or morality. Based on ethnographic research, this dissertation analyzes such processes of translation and articulation occurring among five different discursive communities actively enunciating the fact and meaning of climate change through their own vernaculars. The five groups are: 1) Arctic indigenous representatives that are part of the Inuit Circumpolar Council, 2) corporate social responsibility activists working with Ceres 3) American evangelical Christians active in the nascent movement known as Creation Care, 4) leading science journalists, and 5) scientists who often act as science-policy experts. This dissertation tracks the formation by which evidence comes to matter and have meaning for groups, and the ways in which this process transforms the definition of and questions posed by climate change. It posits that climate change constitutes an emergent form of life replete with multiple, competing instantiations that feed into, configure, and continually revise definitions of and models of/for climate change. Such articulations and attempts at defining climate change are full of friction as epistemologies, forms of life, advocacy, and expertise evolve and bump up against one another in a process of socialization, negotiation, and meaning-making. In this framework, climate change is a simultaneous intellectual, scientific, and moral challenge - it is both a problem of assessing what is happening, what might happen, and how to act in the world. The presentation and circulation of information provide only partial answers. Partnering facts with multiple codes for meaning, ethics, and morality delineate what the stakes and risks entail, articulating rationales to act. These diverse partnerships produce attendant translations, assemblages, modes of speech, and material forms of training and disciplining that enroll scientific findings and policy aspirations.",
    "advisors": ["Michael M. J. Fischer"],
    "text": "More information is not the problem : spinning climate change, vernaculars, and emergent forms of life This dissertation argues that alongside the dominant discourse occurring in and through media in the midst of immense transformation, social networks and affiliations provide a vital translation of science in varied vernaculars such that climate change is becoming invested with diverse meanings, ethics, and/or morality. Based on ethnographic research, this dissertation analyzes such processes of translation and articulation occurring among five different discursive communities actively enunciating the fact and meaning of climate change through their own vernaculars. The five groups are: 1) Arctic indigenous representatives that are part of the Inuit Circumpolar Council, 2) corporate social responsibility activists working with Ceres 3) American evangelical Christians active in the nascent movement known as Creation Care, 4) leading science journalists, and 5) scientists who often act as science-policy experts. This dissertation tracks the formation by which evidence comes to matter and have meaning for groups, and the ways in which this process transforms the definition of and questions posed by climate change. It posits that climate change constitutes an emergent form of life replete with multiple, competing instantiations that feed into, configure, and continually revise definitions of and models of/for climate change. Such articulations and attempts at defining climate change are full of friction as epistemologies, forms of life, advocacy, and expertise evolve and bump up against one another in a process of socialization, negotiation, and meaning-making. In this framework, climate change is a simultaneous intellectual, scientific, and moral challenge - it is both a problem of assessing what is happening, what might happen, and how to act in the world. The presentation and circulation of information provide only partial answers. Partnering facts with multiple codes for meaning, ethics, and morality delineate what the stakes and risks entail, articulating rationales to act. These diverse partnerships produce attendant translations, assemblages, modes of speech, and material forms of training and disciplining that enroll scientific findings and policy aspirations."
}, {
    "id": "oai:dspace.mit.edu:1721.1/86284",
    "title": "Animal madness : a natural history of disorder",
    "abstract": "Beginning in the late 19 th century, changing conceptions of relatedness between people and other animals -- and animals' assumed capacities for, or susceptibilities to, mental or emotional distress-- were influenced by debates over what it meant to be both human and sane in Britain and the United States. Through a historical, partly-ethnographic, investigation of animal insanity in various times and places in the Anglo-American world from the late I 9 th century through the early 21st, I argue that identifying animal madness, insanity, nervous disorders, anxiety disorders, phobias, depression, obsessive compulsivities, suicidal behaviors and more, has not only served as a way of affixing meaning to puzzling animal acts, but has been used to denote borders (or lack thereof) between certain groups of humans and certain groups of animals. As with other divisions, such as those hinging on race, gender, nationality or class, ideas surrounding which humans and which other animals could experience particular forms of insanity have been used to justify certain forms of treatment (or mistreatment), to rationalize needs for confinement or freedom, or to determine what sorts of people and other creatures were deserving of rights and to what degree. I suggest that the history of attempts to identify certain emotional phenomena such as melancholy and suicidal behavior in horses and monkeys, to, more recently, obsessive-compulsivity in parrots and PTSD in military dogs, demonstrates that other animals have acted as mirrors and proxies for disordered Anglo-American minds for more than a century. Drawing upon archival sources, published literature in the fields of ethology, psychology, psychiatry, psychopharmacology, and the veterinary sciences, as well as environmental history, history of medicine and animal studies, combined with interviews and participant observation, I argue that attempts to locate insanity, mental illness, dysfunction and \"normalcy\" among nonhumans has had wide-ranging effects on diagnostic and therapeutic practices in humans and other animals alike in the United States and Britain.",
    "advisors": ["Harriet Ritvo"],
    "text": "Animal madness : a natural history of disorder Beginning in the late 19 th century, changing conceptions of relatedness between people and other animals -- and animals' assumed capacities for, or susceptibilities to, mental or emotional distress-- were influenced by debates over what it meant to be both human and sane in Britain and the United States. Through a historical, partly-ethnographic, investigation of animal insanity in various times and places in the Anglo-American world from the late I 9 th century through the early 21st, I argue that identifying animal madness, insanity, nervous disorders, anxiety disorders, phobias, depression, obsessive compulsivities, suicidal behaviors and more, has not only served as a way of affixing meaning to puzzling animal acts, but has been used to denote borders (or lack thereof) between certain groups of humans and certain groups of animals. As with other divisions, such as those hinging on race, gender, nationality or class, ideas surrounding which humans and which other animals could experience particular forms of insanity have been used to justify certain forms of treatment (or mistreatment), to rationalize needs for confinement or freedom, or to determine what sorts of people and other creatures were deserving of rights and to what degree. I suggest that the history of attempts to identify certain emotional phenomena such as melancholy and suicidal behavior in horses and monkeys, to, more recently, obsessive-compulsivity in parrots and PTSD in military dogs, demonstrates that other animals have acted as mirrors and proxies for disordered Anglo-American minds for more than a century. Drawing upon archival sources, published literature in the fields of ethology, psychology, psychiatry, psychopharmacology, and the veterinary sciences, as well as environmental history, history of medicine and animal studies, combined with interviews and participant observation, I argue that attempts to locate insanity, mental illness, dysfunction and \"normalcy\" among nonhumans has had wide-ranging effects on diagnostic and therapeutic practices in humans and other animals alike in the United States and Britain."
}, {
    "id": "oai:dspace.mit.edu:1721.1/90081",
    "title": "Institutes for innovation : the emergence of academic-industrial cooperation and narratives of progress in the early 20th century",
    "abstract": "Early 20th century America is a critical context for understanding industrial innovation. Departing from a focus on innovation itself as manifested through the creation of new products and consumer opportunities, this project focuses instead on an important infrastructure for innovation - academic-industrial cooperation. Its particular emphasis is on the Mellon Institute for Industrial Research and the Massachusetts Institute of Technology. The Mellon Institute, an independent nonprofit entity devoted to the promotion of industrial research, contributed not only through its novel scientific work, but also through its efforts aimed at engaging broad audiences through popular writing. As a competing model, this dissertation also examines interdisciplinary laboratories and administrative structures at MIT to argue that these schemes for academic-industrial cooperation that began as an informal series of ad hoc arrangements between researchers and corporate partners were increasingly formalized and centralized into a unique educational model that combined fundamental science and industrially relevant research. Rarely used archival materials are drawn on to argue that \"narratives of progress,\" shared stories and rhetoric that were conceived for, and deployed in the service of, a particular idea of creating a better world through the enterprise of science were essential components of institutional and industrial change. Mechanisms for academic-industrial cooperation, no matter how well organized or funded, could not stand alone without a foundational narrative to give them broader purpose and context. Building on an institutional approach and employing a novel analysis of narrative as text, the built environment, and exhibit, this study offers new perspective on sites of academic-industrial cooperation as institutes for innovation.",
    "advisors": ["Anne McCants"],
    "text": "Institutes for innovation : the emergence of academic-industrial cooperation and narratives of progress in the early 20th century Early 20th century America is a critical context for understanding industrial innovation. Departing from a focus on innovation itself as manifested through the creation of new products and consumer opportunities, this project focuses instead on an important infrastructure for innovation - academic-industrial cooperation. Its particular emphasis is on the Mellon Institute for Industrial Research and the Massachusetts Institute of Technology. The Mellon Institute, an independent nonprofit entity devoted to the promotion of industrial research, contributed not only through its novel scientific work, but also through its efforts aimed at engaging broad audiences through popular writing. As a competing model, this dissertation also examines interdisciplinary laboratories and administrative structures at MIT to argue that these schemes for academic-industrial cooperation that began as an informal series of ad hoc arrangements between researchers and corporate partners were increasingly formalized and centralized into a unique educational model that combined fundamental science and industrially relevant research. Rarely used archival materials are drawn on to argue that \"narratives of progress,\" shared stories and rhetoric that were conceived for, and deployed in the service of, a particular idea of creating a better world through the enterprise of science were essential components of institutional and industrial change. Mechanisms for academic-industrial cooperation, no matter how well organized or funded, could not stand alone without a foundational narrative to give them broader purpose and context. Building on an institutional approach and employing a novel analysis of narrative as text, the built environment, and exhibit, this study offers new perspective on sites of academic-industrial cooperation as institutes for innovation."
}, {
    "id": "oai:dspace.mit.edu:1721.1/17844",
    "title": "The archive of place : environment and the contested past of a North American plateau",
    "abstract": "This is a study of the role that the interpretation of material evidence plays in historical consciousness and social memory. It consists of three case studies from the Chilcotin Plateau in the west-central part of present-day British Columbia. In each, a conflict in the mid-1990s over the nature of the past and its relevance for the present allowed underlying stories to emerge. As different groups struggled to control the fate of the region and its resources, they invoked very different understandings of its past, understandings based in part on the material traces that they found there. Taken together, the case studies illustrate the fact that there is an extensive division of interpretive labor when it comes to the material evidence of the past. Like other kinds of labor, this interpretation takes part in a political economy. Studies of material evidence are done to further the interests of individuals or groups, are valued and exchanged with one another, and are important in the delineation of property rights, the enforcement of laws and the justification of ideologies. What emerges is not an authoritative or univocal environmental history of a place, but rather a contest to find a past which will be usable in the present and future. The constant interpretation of material evidence allows people to situate themselves with respect to place, time and other people.",
    "advisors": ["Harriet Ritvo"],
    "text": "The archive of place : environment and the contested past of a North American plateau This is a study of the role that the interpretation of material evidence plays in historical consciousness and social memory. It consists of three case studies from the Chilcotin Plateau in the west-central part of present-day British Columbia. In each, a conflict in the mid-1990s over the nature of the past and its relevance for the present allowed underlying stories to emerge. As different groups struggled to control the fate of the region and its resources, they invoked very different understandings of its past, understandings based in part on the material traces that they found there. Taken together, the case studies illustrate the fact that there is an extensive division of interpretive labor when it comes to the material evidence of the past. Like other kinds of labor, this interpretation takes part in a political economy. Studies of material evidence are done to further the interests of individuals or groups, are valued and exchanged with one another, and are important in the delineation of property rights, the enforcement of laws and the justification of ideologies. What emerges is not an authoritative or univocal environmental history of a place, but rather a contest to find a past which will be usable in the present and future. The constant interpretation of material evidence allows people to situate themselves with respect to place, time and other people."
}, {
    "id": "oai:dspace.mit.edu:1721.1/104558",
    "title": "Salvage cartographies : mapping, futures, and landscapes in northwest British Columbia",
    "abstract": "This dissertation examines how the proliferation of digital mapping technologies and the contraction of government research institutions have reformatted contests over resources, sovereignty, and local belonging in the neoliberal era. The two groups at the heart of this multilocale ethnography, government forest ecologists and Indigenous Geographic Information Systems (GIS) specialists, share entangled histories throughout rural North America. This is particularly true on the Gitxsan and Gitanyow traditional territories in northwest British Columbia. As climate change and emergent forest diseases destabilize both Indigenous and settler communities' abilities to predict and plan for environmental shifts, disparate experts are learning to leverage marginalized maps and ecological succession models to reconstitute modes of professional succession rendered precarious by government reforms and internal tribal conflicts. The opening chapters of the dissertation examine two experimental institutions - an independent forest ecology research center in Smithers, B.C., and a defunct GIS analysis team based on a nearby Gitxsan reserve - to examine how rural scenes of collaboration complicate the modalities of influence and organizational coherency often attributed to professional scientific networks. Later chapters explore experimental forest and traditional territories where ecologists and Indigenous GIS specialists have sought to articulate risks and project landscape futures by producing technical knowledge. For both communities, transects, grids, and other techniques of marking space have forced them to negotiate tensions between the temporal decay of these spaces and the lifespans of individual researchers. The concluding chapter examine the agencies of archives and simulations produced by two separate long-term forestry modeling groups. By treating their discarded models as anchors of a kind of professional legitimacy no longer stably recognized by a changing provincial government, I argue that senior forestry modelers are struggling to frame their work within longer historical narratives which supersede the temporalities of the state. Twentieth century conservationism drew heavily on essentialized discourses of \"nature\" and \"culture\" to construct old-growth rainforests and other contested spaces as objects worthy of protection. This dissertation examines the destabilization of these classification systems, and the palimpsest of legal definitions and lived concepts of territory left behind as regulatory responsibilities devolve and dissolve.",
    "advisors": ["Michael M. J. Fischer"],
    "text": "Salvage cartographies : mapping, futures, and landscapes in northwest British Columbia This dissertation examines how the proliferation of digital mapping technologies and the contraction of government research institutions have reformatted contests over resources, sovereignty, and local belonging in the neoliberal era. The two groups at the heart of this multilocale ethnography, government forest ecologists and Indigenous Geographic Information Systems (GIS) specialists, share entangled histories throughout rural North America. This is particularly true on the Gitxsan and Gitanyow traditional territories in northwest British Columbia. As climate change and emergent forest diseases destabilize both Indigenous and settler communities' abilities to predict and plan for environmental shifts, disparate experts are learning to leverage marginalized maps and ecological succession models to reconstitute modes of professional succession rendered precarious by government reforms and internal tribal conflicts. The opening chapters of the dissertation examine two experimental institutions - an independent forest ecology research center in Smithers, B.C., and a defunct GIS analysis team based on a nearby Gitxsan reserve - to examine how rural scenes of collaboration complicate the modalities of influence and organizational coherency often attributed to professional scientific networks. Later chapters explore experimental forest and traditional territories where ecologists and Indigenous GIS specialists have sought to articulate risks and project landscape futures by producing technical knowledge. For both communities, transects, grids, and other techniques of marking space have forced them to negotiate tensions between the temporal decay of these spaces and the lifespans of individual researchers. The concluding chapter examine the agencies of archives and simulations produced by two separate long-term forestry modeling groups. By treating their discarded models as anchors of a kind of professional legitimacy no longer stably recognized by a changing provincial government, I argue that senior forestry modelers are struggling to frame their work within longer historical narratives which supersede the temporalities of the state. Twentieth century conservationism drew heavily on essentialized discourses of \"nature\" and \"culture\" to construct old-growth rainforests and other contested spaces as objects worthy of protection. This dissertation examines the destabilization of these classification systems, and the palimpsest of legal definitions and lived concepts of territory left behind as regulatory responsibilities devolve and dissolve."
}, {
    "id": "oai:dspace.mit.edu:1721.1/104562",
    "title": "Intimate cartographies : body maps and the epistemic encounter in China and Britain, 1893-1985",
    "abstract": "This dissertation explores how body maps served as a site for theoretical, experimental, and cultural entanglements between \"Chinese medicine\" and \"biomedicine.\" It explores how body atlases produced under varying social and cultural conditions involved similar ontological questions with diverging social and cultural implications. These ontological questions set into motion disparate theories about the body that continue to destabilize contemporary medical practice. I explore the fate of maps that medical practitioners in China and Britain traced on paper and on people. Rather than representing what could be directly observed, these maps made visible what could be felt. Body maps offer a unique approach to transnational histories of science and medicine because they existed as meticulously crafted artifacts of visual perception and material evidence that carried social and political currency. In particular, I follow how Chinese physiologists re-presented meridian paths for acupuncture moxibustion practice and the conceptual friction that these maps introduced once they were compared with sensation maps that British neurologists produced to identify peripheral nerve clusters and distinct areas of pain. Amidst state-building efforts in the early twentieth century, medical practitioners in China reproduced meridian maps to emphasize the technical and systematic virtues of acupuncture moxibustion. Yet, meridian maps presented an ontological problem, as standardizing its paths required fixing locations along courses that shifted in living bodies. This dissertation picks up where political historians leave off, examining transformations in medical theory and tracking how individuals concerned with constructing the legacy of medicine in China eventually came to resurrect abandoned neurophysiological maps produced in late nineteenth century Britain. Through a careful excavation of image and text, I demonstrate how efforts to locate shifting areas on the surface of the body conflicted and cohered with discourses of science. I argue that \"intimate cartographies,\" or maps based on individual encounters of the body, challenged standards of visualizing and describing unseen physiological systems. These maps sat at the intersection of epistemic practices, where the circulation of images, ideas, and individuals contributed to the complex convergence of body maps across regimes of knowledge.",
    "advisors": ["Emma Teng"],
    "text": "Intimate cartographies : body maps and the epistemic encounter in China and Britain, 1893-1985 This dissertation explores how body maps served as a site for theoretical, experimental, and cultural entanglements between \"Chinese medicine\" and \"biomedicine.\" It explores how body atlases produced under varying social and cultural conditions involved similar ontological questions with diverging social and cultural implications. These ontological questions set into motion disparate theories about the body that continue to destabilize contemporary medical practice. I explore the fate of maps that medical practitioners in China and Britain traced on paper and on people. Rather than representing what could be directly observed, these maps made visible what could be felt. Body maps offer a unique approach to transnational histories of science and medicine because they existed as meticulously crafted artifacts of visual perception and material evidence that carried social and political currency. In particular, I follow how Chinese physiologists re-presented meridian paths for acupuncture moxibustion practice and the conceptual friction that these maps introduced once they were compared with sensation maps that British neurologists produced to identify peripheral nerve clusters and distinct areas of pain. Amidst state-building efforts in the early twentieth century, medical practitioners in China reproduced meridian maps to emphasize the technical and systematic virtues of acupuncture moxibustion. Yet, meridian maps presented an ontological problem, as standardizing its paths required fixing locations along courses that shifted in living bodies. This dissertation picks up where political historians leave off, examining transformations in medical theory and tracking how individuals concerned with constructing the legacy of medicine in China eventually came to resurrect abandoned neurophysiological maps produced in late nineteenth century Britain. Through a careful excavation of image and text, I demonstrate how efforts to locate shifting areas on the surface of the body conflicted and cohered with discourses of science. I argue that \"intimate cartographies,\" or maps based on individual encounters of the body, challenged standards of visualizing and describing unseen physiological systems. These maps sat at the intersection of epistemic practices, where the circulation of images, ideas, and individuals contributed to the complex convergence of body maps across regimes of knowledge."
}, {
    "id": "oai:dspace.mit.edu:1721.1/9169",
    "title": "The making of a multiple purpose dam : engineering culture, the U.S. Bureau of Reclamation, and Grand Coulee Dam, 1917-1942",
    "abstract": "This dissertation examines how Americans have transformed the environment through the construction of new technologies and the roles of technical professionals in bringing about these changes. In the twentieth century, federal engineers, working with local Western boosters and their federal superiors, transformed the West's waterscape. Between 1900 and 1970, engineers of the U.S. Bureau of Reclamation--only one of three federal agencies that built dams-constructed over 400. On the Columbia River alone, federal engineers constructed thirteen large dams that turned the nation's fourth largest river into a chain of lakes. Engineers wrought this transformation with multiple purpose dams-a new style of dam building in the twentieth century. This style combined a new way of understanding how dams should be used with new approaches to financing the construction of dams and to designing and siting dam structures. Engineers built dams that developed water resources for multiple uses: navigation, flood control, irrigation, and hydroelectricity production. They financed these dams by allocating the costs among uses and then obtaining funds either through federal grants (for navigation and flood control) or loans (for irrigation and hydroelectricity). Engineers frequently designed these dams as traditional concrete gravity structures and sited them at prime locations on major rivers. This work examines the rise of the multiple purpose style of dam building through the history of Grand Coulee Dam. Located on the Columbia River in Washington State, Grand Coulee Dam is one in a group of large dams planned in the 1920s and built in the 1930s during whose design and construction federal engineers worked out this new style of building. It shows that engineers came to prefer this style because it fulfilled conservationists' hopes for \"comprehensive planning\" of water resources and it eliminated financial problems with federal irrigation activities. Engineers alone did not launch this building program: local constituencies favored development and Franklin Roosevelt's new administration supported relief projects, conservation programs, and government involvement in the electrical industry. Working together, these three groups built a political coalition for multiple purpose dams that successfully underpinned expanded building.",
    "advisors": ["Deborah Fitzgerald"],
    "text": "The making of a multiple purpose dam : engineering culture, the U.S. Bureau of Reclamation, and Grand Coulee Dam, 1917-1942 This dissertation examines how Americans have transformed the environment through the construction of new technologies and the roles of technical professionals in bringing about these changes. In the twentieth century, federal engineers, working with local Western boosters and their federal superiors, transformed the West's waterscape. Between 1900 and 1970, engineers of the U.S. Bureau of Reclamation--only one of three federal agencies that built dams-constructed over 400. On the Columbia River alone, federal engineers constructed thirteen large dams that turned the nation's fourth largest river into a chain of lakes. Engineers wrought this transformation with multiple purpose dams-a new style of dam building in the twentieth century. This style combined a new way of understanding how dams should be used with new approaches to financing the construction of dams and to designing and siting dam structures. Engineers built dams that developed water resources for multiple uses: navigation, flood control, irrigation, and hydroelectricity production. They financed these dams by allocating the costs among uses and then obtaining funds either through federal grants (for navigation and flood control) or loans (for irrigation and hydroelectricity). Engineers frequently designed these dams as traditional concrete gravity structures and sited them at prime locations on major rivers. This work examines the rise of the multiple purpose style of dam building through the history of Grand Coulee Dam. Located on the Columbia River in Washington State, Grand Coulee Dam is one in a group of large dams planned in the 1920s and built in the 1930s during whose design and construction federal engineers worked out this new style of building. It shows that engineers came to prefer this style because it fulfilled conservationists' hopes for \"comprehensive planning\" of water resources and it eliminated financial problems with federal irrigation activities. Engineers alone did not launch this building program: local constituencies favored development and Franklin Roosevelt's new administration supported relief projects, conservation programs, and government involvement in the electrical industry. Working together, these three groups built a political coalition for multiple purpose dams that successfully underpinned expanded building."
}, {
    "id": "oai:dspace.mit.edu:1721.1/40394",
    "title": "The Soviet Farm Complex : industrial agriculture in a Socialist context, 1945-1965",
    "abstract": "\"The Soviet Farm Complex\" is a history of food, farming and the environment in the postwar Soviet Union. It tells the story of how different technical and institutional authorities created an industrial Soviet countryside in the generation after World War II. Beyond the leadership of the Soviet state, international trade relationships, new technologies, unusual scientific cultures, stubborn environmental realities and human shortcomings played important roles in shaping the progress of agricultural change. Four historical fields inform this project: the history of technology, agricultural history, Soviet history and environmental history. Each of the five chapters addresses a different time, place and theme in the history of the Soviet countryside, providing a close-up view of the most important aspects of postwar rural change. Soviet agricultural reform has often been interpreted as a failure: a textbook case of poor central planning and destructive, high-modernist logic on the part of the Soviet state. In fact, this study shows that the collective farming system as a whole was not particularly dysfunctional, nor was it doomed to failure simply by virtue of being centrally planned.",
    "advisors": ["Deborah K. Fitzgerald"],
    "text": "The Soviet Farm Complex : industrial agriculture in a Socialist context, 1945-1965 \"The Soviet Farm Complex\" is a history of food, farming and the environment in the postwar Soviet Union. It tells the story of how different technical and institutional authorities created an industrial Soviet countryside in the generation after World War II. Beyond the leadership of the Soviet state, international trade relationships, new technologies, unusual scientific cultures, stubborn environmental realities and human shortcomings played important roles in shaping the progress of agricultural change. Four historical fields inform this project: the history of technology, agricultural history, Soviet history and environmental history. Each of the five chapters addresses a different time, place and theme in the history of the Soviet countryside, providing a close-up view of the most important aspects of postwar rural change. Soviet agricultural reform has often been interpreted as a failure: a textbook case of poor central planning and destructive, high-modernist logic on the part of the Soviet state. In fact, this study shows that the collective farming system as a whole was not particularly dysfunctional, nor was it doomed to failure simply by virtue of being centrally planned."
}, {
    "id": "oai:dspace.mit.edu:1721.1/55162",
    "title": "Accidents, engineering and history at NASA: 1967-2003",
    "abstract": "The manned spaceflight program of the National Aeronautics and Space Administration (NASA) has suffered three fatal accidents: one in the Apollo program and two in the Space Transportation System (the Shuttle). These were the fatal fire in Apollo 204 (Apollo 1) in 1967, the explosion of the Solid Rocket Booster in STS-51L (Challenger) in 1986, and the destruction of the orbiter in STS-107 (Columbia). Three astronauts lost their lives in 1967, and in each Shuttle accident seven astronauts were killed. Following each of these fatal accidents, a significant investigation was conducted and a comprehensive investigation report produced. These investigation reports each served to create public narratives of the reasons for the accidents. The reports shaped the accidents' legacies for the space program and for large-scale complex engineering projects more generally. This thesis re-examines the evidence produced to investigate and explain each accident. By analyzing the investigation reports critically, as well as reviewing the accidents themselves, this work considers how engineering cultures and practices at NASA shifted to meet the changing demands of the space program. It argues that the public narratives of the accidents are not completely congruent with the engineering evidence, and that these very selective narratives are influential in shaping future strengths (and weaknesses) at NASA. By re-examining the accident evidence, the reports, and the role of each accident in shaping NASA engineering cultures, the thesis provides a view of engineering very different from what is apparent in previous historical work on the space program.",
    "advisors": ["David A. Mindell"],
    "text": "Accidents, engineering and history at NASA: 1967-2003 The manned spaceflight program of the National Aeronautics and Space Administration (NASA) has suffered three fatal accidents: one in the Apollo program and two in the Space Transportation System (the Shuttle). These were the fatal fire in Apollo 204 (Apollo 1) in 1967, the explosion of the Solid Rocket Booster in STS-51L (Challenger) in 1986, and the destruction of the orbiter in STS-107 (Columbia). Three astronauts lost their lives in 1967, and in each Shuttle accident seven astronauts were killed. Following each of these fatal accidents, a significant investigation was conducted and a comprehensive investigation report produced. These investigation reports each served to create public narratives of the reasons for the accidents. The reports shaped the accidents' legacies for the space program and for large-scale complex engineering projects more generally. This thesis re-examines the evidence produced to investigate and explain each accident. By analyzing the investigation reports critically, as well as reviewing the accidents themselves, this work considers how engineering cultures and practices at NASA shifted to meet the changing demands of the space program. It argues that the public narratives of the accidents are not completely congruent with the engineering evidence, and that these very selective narratives are influential in shaping future strengths (and weaknesses) at NASA. By re-examining the accident evidence, the reports, and the role of each accident in shaping NASA engineering cultures, the thesis provides a view of engineering very different from what is apparent in previous historical work on the space program."
}, {
    "id": "oai:dspace.mit.edu:1721.1/63235",
    "title": "Technologies of the operator : engineering the pilot in the U.S. and Japan, 1930-1960",
    "abstract": "This study examines the assemblage of scientific knowledge, engineering practices, measuring instruments, and civilian and military institutions in the U.S. and Japan that went into the construction of the machine operator as a historically situated category of person in the midtwentieth century. Over the three decades from 1930 to 1960, American psychologists, physiologists, anthropologists, and engineers produced a large body of knowledge, instruments, and techniques with which to understand, select, and train aircraft pilots. The figure of the pilot thus constructed was less of a \"flier\" engaged in speedy movements and adventures than of an \"operator\" with disciplined attention and posture. The conditions that constituted the aircraft operator were multifarious: spatial, virtual, psychological, anthropometrical, political, and cultural. I first examine the Link Trainer, a ground-based flight trainer, and explore how the meaning of \"flying\" shifted with the use of instrument flying technique and the experience of simulated training on the ground. Then I show how psychologists redefined flying from a problem of movement to a problem of attention in their research on pilot selection tests, especially by contrasting the validity of physiological tests and psychomotor tests. Concurrently, physical anthropologists were articulating two different ways of relating the pilot's body to flying; one was the correlation between physique and one's success as a pilot and the other was the dimensional configuration of the body in the space of the cockpit. In postwar Japan, this American notion of the pilot served as the model for Japanese pilots, who embraced American norms and conventions for flying after a long ban on aviation. Even the bodies of Japanese pilots were measured and compared with those of Americans. As the scientists and engineers in postwar America extended wartime knowledge and techniques to study various situations of machine operation, aircraft pilots also came to stand for human individuals more generally, forming the conceptual basis of human factors engineering or ergonomics. Through this expansion and generalization of the pilot, a particular type of human-the one who operates machines through displays and controls-came into being as an object of study and control.",
    "advisors": ["David A. Mindell"],
    "text": "Technologies of the operator : engineering the pilot in the U.S. and Japan, 1930-1960 This study examines the assemblage of scientific knowledge, engineering practices, measuring instruments, and civilian and military institutions in the U.S. and Japan that went into the construction of the machine operator as a historically situated category of person in the midtwentieth century. Over the three decades from 1930 to 1960, American psychologists, physiologists, anthropologists, and engineers produced a large body of knowledge, instruments, and techniques with which to understand, select, and train aircraft pilots. The figure of the pilot thus constructed was less of a \"flier\" engaged in speedy movements and adventures than of an \"operator\" with disciplined attention and posture. The conditions that constituted the aircraft operator were multifarious: spatial, virtual, psychological, anthropometrical, political, and cultural. I first examine the Link Trainer, a ground-based flight trainer, and explore how the meaning of \"flying\" shifted with the use of instrument flying technique and the experience of simulated training on the ground. Then I show how psychologists redefined flying from a problem of movement to a problem of attention in their research on pilot selection tests, especially by contrasting the validity of physiological tests and psychomotor tests. Concurrently, physical anthropologists were articulating two different ways of relating the pilot's body to flying; one was the correlation between physique and one's success as a pilot and the other was the dimensional configuration of the body in the space of the cockpit. In postwar Japan, this American notion of the pilot served as the model for Japanese pilots, who embraced American norms and conventions for flying after a long ban on aviation. Even the bodies of Japanese pilots were measured and compared with those of Americans. As the scientists and engineers in postwar America extended wartime knowledge and techniques to study various situations of machine operation, aircraft pilots also came to stand for human individuals more generally, forming the conceptual basis of human factors engineering or ergonomics. Through this expansion and generalization of the pilot, a particular type of human-the one who operates machines through displays and controls-came into being as an object of study and control."
}, {
    "id": "oai:dspace.mit.edu:1721.1/84366",
    "title": "The herds shot round the world : native breeds and the British Empire, 1800-1900",
    "abstract": "This dissertation explores the relationship between types of livestock and place in the context of Great Britain's expanding agro-pastoral empire. Specifically, it examines how the distribution and circulation of breeds of livestock native to the British Isles influenced understandings of kind and location-of the dynamic interaction between heredity, human influence and environmental conditions, and their various fluid effects on ovine and bovine diversity. Drawing on extensive archival work in the United Kingdom, New Zealand, and Australia, I trace both the national origins and imperial expansion of British breeds. As Britain industrialized in the early nineteenth century, breeders faced the need to convert the specificity of their animals into fungibility while maintaining the distinctive character of their breeds, seemingly incompatible aims that nonetheless guaranteed the economic viability of their stock. Thus they reoriented local variability towards market standardization, transforming regional types of cattle and sheep into geographically transposable, bulky, and quick-fattening beasts suited for increasingly sophisticated economies and industrialized production. Tension between standardization and specialization shaped the dispersal of breeds throughout the empire as well. Here, stockbreeders served two masters: the unfamiliar climates and topographies of Australia, New Zealand, and North America, which demanded local adaptations, and the British consumer, whose dinner table was the end of the line for the bulk of colonial beef and mutton. As they tried to balance local adaptation and metropolitan taste, breeders experimented with heredity, testing the limits of contemporary understandings of heritability and breed plasticity, and developed of new strains of livestock genetically derived from British breeds, but culturally, economically and environmentally hybrid. In the process, imperialism itself was instantiated in these animals. Bodies of sheep and cattle were remade to suit new lands and later to fill the refrigerated holds of ocean liners. The empire itself was recast as a vast apparatus for feeding Britons. This system, divested of its imperial trappings and disseminated still further, brings meat to tables around the world today.",
    "advisors": ["Harriet Ritvo"],
    "text": "The herds shot round the world : native breeds and the British Empire, 1800-1900 This dissertation explores the relationship between types of livestock and place in the context of Great Britain's expanding agro-pastoral empire. Specifically, it examines how the distribution and circulation of breeds of livestock native to the British Isles influenced understandings of kind and location-of the dynamic interaction between heredity, human influence and environmental conditions, and their various fluid effects on ovine and bovine diversity. Drawing on extensive archival work in the United Kingdom, New Zealand, and Australia, I trace both the national origins and imperial expansion of British breeds. As Britain industrialized in the early nineteenth century, breeders faced the need to convert the specificity of their animals into fungibility while maintaining the distinctive character of their breeds, seemingly incompatible aims that nonetheless guaranteed the economic viability of their stock. Thus they reoriented local variability towards market standardization, transforming regional types of cattle and sheep into geographically transposable, bulky, and quick-fattening beasts suited for increasingly sophisticated economies and industrialized production. Tension between standardization and specialization shaped the dispersal of breeds throughout the empire as well. Here, stockbreeders served two masters: the unfamiliar climates and topographies of Australia, New Zealand, and North America, which demanded local adaptations, and the British consumer, whose dinner table was the end of the line for the bulk of colonial beef and mutton. As they tried to balance local adaptation and metropolitan taste, breeders experimented with heredity, testing the limits of contemporary understandings of heritability and breed plasticity, and developed of new strains of livestock genetically derived from British breeds, but culturally, economically and environmentally hybrid. In the process, imperialism itself was instantiated in these animals. Bodies of sheep and cattle were remade to suit new lands and later to fill the refrigerated holds of ocean liners. The empire itself was recast as a vast apparatus for feeding Britons. This system, divested of its imperial trappings and disseminated still further, brings meat to tables around the world today."
}, {
    "id": "oai:dspace.mit.edu:1721.1/39176",
    "title": "The State Machine : politics, ideology, and computation in Chile, 1964-1973",
    "abstract": "This dissertation argues that Chile's history of computing is tightly interwoven with the history of the Chilean state. It begins by documenting the government use of mechanical tabulating machines during the 1920s and 1930s and concludes with the disbanding of the state computer enterprise known as ECOM in 1991. The dissertation pays particular attention to the period between 1964 and 1973, which was marked by the presidencies of Christian Democrat Eduardo Frei Montalva and Socialist Salvador Allende Gossens. The dissertation addresses three central questions. First, it asks how Chilean economic policies and political events shaped the country's technological history. Second, it asks what we can learn from computers if we treat them as texts for understanding historical processes in Chile, the Latin American region, and the developing world. Finally, it addresses how Chile's political leaders used computing machines in their attempts to control Chile's social, economic, and political development and to forward their plans to modernize the Chilean nation. It argues that computers proved valuable not only in producing the Chile of today but in articulating national goals that changed over time.",
    "advisors": ["David A. Mindell"],
    "text": "The State Machine : politics, ideology, and computation in Chile, 1964-1973 This dissertation argues that Chile's history of computing is tightly interwoven with the history of the Chilean state. It begins by documenting the government use of mechanical tabulating machines during the 1920s and 1930s and concludes with the disbanding of the state computer enterprise known as ECOM in 1991. The dissertation pays particular attention to the period between 1964 and 1973, which was marked by the presidencies of Christian Democrat Eduardo Frei Montalva and Socialist Salvador Allende Gossens. The dissertation addresses three central questions. First, it asks how Chilean economic policies and political events shaped the country's technological history. Second, it asks what we can learn from computers if we treat them as texts for understanding historical processes in Chile, the Latin American region, and the developing world. Finally, it addresses how Chile's political leaders used computing machines in their attempts to control Chile's social, economic, and political development and to forward their plans to modernize the Chilean nation. It argues that computers proved valuable not only in producing the Chile of today but in articulating national goals that changed over time."
}, {
    "id": "oai:dspace.mit.edu:1721.1/47827",
    "title": "Propheteering : a cultural history of prediction in the Gilded Age",
    "abstract": "This study of the changing practices and perceptions of prediction in the late nineteenth century reveals the process by which Americans came to rationalize economic and cultural uncertainty into modern life. Forecasts of all kinds were ubiquitous in the late nineteenth century; as the United States fashioned itself into an urban-industrial power with a national economy and an increasingly corporate and bureaucratic society, prediction became an increasingly significant scientific, economic, and cultural practice. As a postbellum crisis of certainty destabilized ways of thinking about the future-in science, social science, and religion-predictions, whether accurate or not, offered illusions of control over one's future to citizens of a rapidly modernizing America. I argue that the late-century search for predictability found as much uncertainty as it did certainty, that consumers of predictions were at once desirous and dismissive of forecasts that often took on greater cultural than economic value, and that producers and consumers of prediction together rationalized uncertainty and shaped a new cultural acceptance of the predictable unpredictability of modern life. In the first half of the dissertation I analyze the work of U.S. Department of Agriculture statisticians, private cotton estimators, Weather Bureau forecasters, and local \"weather prophets,\" all of whom sought to systematically convert their observations into economically valuable predictions. In the second half of the dissertation I focus on the work of utopian novelist Edward Bellamy, fortune-tellers, and spirit mediums, whose prophecies circulated by the thousands through rural and urban America.",
    "advisors": ["Merritt Roe Smith"],
    "text": "Propheteering : a cultural history of prediction in the Gilded Age This study of the changing practices and perceptions of prediction in the late nineteenth century reveals the process by which Americans came to rationalize economic and cultural uncertainty into modern life. Forecasts of all kinds were ubiquitous in the late nineteenth century; as the United States fashioned itself into an urban-industrial power with a national economy and an increasingly corporate and bureaucratic society, prediction became an increasingly significant scientific, economic, and cultural practice. As a postbellum crisis of certainty destabilized ways of thinking about the future-in science, social science, and religion-predictions, whether accurate or not, offered illusions of control over one's future to citizens of a rapidly modernizing America. I argue that the late-century search for predictability found as much uncertainty as it did certainty, that consumers of predictions were at once desirous and dismissive of forecasts that often took on greater cultural than economic value, and that producers and consumers of prediction together rationalized uncertainty and shaped a new cultural acceptance of the predictable unpredictability of modern life. In the first half of the dissertation I analyze the work of U.S. Department of Agriculture statisticians, private cotton estimators, Weather Bureau forecasters, and local \"weather prophets,\" all of whom sought to systematically convert their observations into economically valuable predictions. In the second half of the dissertation I focus on the work of utopian novelist Edward Bellamy, fortune-tellers, and spirit mediums, whose prophecies circulated by the thousands through rural and urban America."
}, {
    "id": "oai:dspace.mit.edu:1721.1/112018",
    "title": "Sparking controversy : the contested use of noninvasive brain stimulation",
    "abstract": "My dissertation examines the controversy over transcranial direct current stimulation (tDCS), a noninvasive form of brain stimulation that is thought to provide a constant low level of electrical current to the brain. Although scientists have been experimenting with tDCS in both healthy and clinical populations for the last fifteen years, in late 2011 a movement arose wherein \"lay\" individuals began constructing their own tDCS devices, or purchasing consumer devices, to stimulate their brains outside of academic or medical settings for self-improvement purposes. Not surprisingly, the lay use of tDCS has not been well received by researchers, who have termed it \"fringe\" or \"unorthodox.\" This work studies the conflict over tDCS: what is tDCS, who gets to use it, and who studies it? What are the multiple social worlds that tDCS inhabits, how is the technology interpreted and utilized in each, and how does each group authorize or discredit the other's use? My dissertation incorporates interviews, observations, an online survey, archival research, and legal analyses to probe aspects of the controversy from different angles. The first chapter introduces tDCS technology and chronicles the rise of the do-it-yourself movement and the subsequent emergence of direct-to-consumer devices. In the second chapter, I present an in-depth qualitative study of the practices of home users of tDCS; the third chapter offers a quantitative look at those who have purchased a consumer tDCS device, based on the results of an online survey. The fourth chapter addresses regulatory issues surrounding consumer tDCS devices, providing a comprehensive analysis of relevant legal doctrines and laws. The fifth chapter covers historical precedents for the home use of electrical stimulation, with a focus on uses of the medical battery between 1870 and 1920 in the United States. In the sixth chapter, I compare the medical battery to tDCS, arguing that the controversy over the home use of tDCS is not novel or even surprising, but rather the latest wave in a series of ongoing attempts by lay individuals to utilize electricity for therapeutic purposes.",
    "advisors": ["Susan S. Silbey"],
    "text": "Sparking controversy : the contested use of noninvasive brain stimulation My dissertation examines the controversy over transcranial direct current stimulation (tDCS), a noninvasive form of brain stimulation that is thought to provide a constant low level of electrical current to the brain. Although scientists have been experimenting with tDCS in both healthy and clinical populations for the last fifteen years, in late 2011 a movement arose wherein \"lay\" individuals began constructing their own tDCS devices, or purchasing consumer devices, to stimulate their brains outside of academic or medical settings for self-improvement purposes. Not surprisingly, the lay use of tDCS has not been well received by researchers, who have termed it \"fringe\" or \"unorthodox.\" This work studies the conflict over tDCS: what is tDCS, who gets to use it, and who studies it? What are the multiple social worlds that tDCS inhabits, how is the technology interpreted and utilized in each, and how does each group authorize or discredit the other's use? My dissertation incorporates interviews, observations, an online survey, archival research, and legal analyses to probe aspects of the controversy from different angles. The first chapter introduces tDCS technology and chronicles the rise of the do-it-yourself movement and the subsequent emergence of direct-to-consumer devices. In the second chapter, I present an in-depth qualitative study of the practices of home users of tDCS; the third chapter offers a quantitative look at those who have purchased a consumer tDCS device, based on the results of an online survey. The fourth chapter addresses regulatory issues surrounding consumer tDCS devices, providing a comprehensive analysis of relevant legal doctrines and laws. The fifth chapter covers historical precedents for the home use of electrical stimulation, with a focus on uses of the medical battery between 1870 and 1920 in the United States. In the sixth chapter, I compare the medical battery to tDCS, arguing that the controversy over the home use of tDCS is not novel or even surprising, but rather the latest wave in a series of ongoing attempts by lay individuals to utilize electricity for therapeutic purposes."
}, {
    "id": "oai:dspace.mit.edu:1721.1/93814",
    "title": "Project Apollo, Cold War diplomacy and the American framing of global interdependence",
    "abstract": "This dissertation examines the distinctive and critical role that space exploration played in American foreign relations and national image making in the 1960s. Proposed by President John F. Kennedy in 1961, Project Apollo was established, in large part, as a means of demonstrating American power and promoting technocratic values in an international landscape defined by the Cold War, the collapse of colonialism, and the emergence of newly independent nations. While existing scholarship has gestured to this geopolitical context, it has tended to examine activity that takes place on American or lunar soil. This dissertation argues that the geopolitical context was not simply a backdrop but instead the main theater of Project Apollo. By embedding this familiar story back in its global context, this dissertation reinterprets the established narrative of Project Apollo in three significant ways. First, it places greater emphasis on the international stage and the relationship between the US and the world. Second, while the role of the Executive Branch remains essential to this story, this dissertation shifts the focus from engineers and managers, to key actors within the State Department and United States Information Agency, as well as foreign leaders and the world public. Finally, the role of Project Apollo in foreign relations, and public diplomacy in particular, becomes the defining feature of this investigation. By examining how US government elites promoted and disseminated information about space exploration to support American foreign relations interests, this dissertation offers a lens onto attempts to establish national power by fusing perceived values and strengths of science and technology- like rationality and progress- with the image of the nation's political system. These efforts, this dissertation demonstrates, were not only aimed at boosting American prestige, but were also strategic attempts to promote an idea of global unity and progress ushered in by American scientific and technological leadership.",
    "advisors": ["David Mindell"],
    "text": "Project Apollo, Cold War diplomacy and the American framing of global interdependence This dissertation examines the distinctive and critical role that space exploration played in American foreign relations and national image making in the 1960s. Proposed by President John F. Kennedy in 1961, Project Apollo was established, in large part, as a means of demonstrating American power and promoting technocratic values in an international landscape defined by the Cold War, the collapse of colonialism, and the emergence of newly independent nations. While existing scholarship has gestured to this geopolitical context, it has tended to examine activity that takes place on American or lunar soil. This dissertation argues that the geopolitical context was not simply a backdrop but instead the main theater of Project Apollo. By embedding this familiar story back in its global context, this dissertation reinterprets the established narrative of Project Apollo in three significant ways. First, it places greater emphasis on the international stage and the relationship between the US and the world. Second, while the role of the Executive Branch remains essential to this story, this dissertation shifts the focus from engineers and managers, to key actors within the State Department and United States Information Agency, as well as foreign leaders and the world public. Finally, the role of Project Apollo in foreign relations, and public diplomacy in particular, becomes the defining feature of this investigation. By examining how US government elites promoted and disseminated information about space exploration to support American foreign relations interests, this dissertation offers a lens onto attempts to establish national power by fusing perceived values and strengths of science and technology- like rationality and progress- with the image of the nation's political system. These efforts, this dissertation demonstrates, were not only aimed at boosting American prestige, but were also strategic attempts to promote an idea of global unity and progress ushered in by American scientific and technological leadership."
}, {
    "id": "oai:dspace.mit.edu:1721.1/63236",
    "title": "Crafting life : a sensory ethnography of fabricated biologies",
    "abstract": "This ethnography tracks a diverse set of practices I term \"constructive biologies,\" by which I mean efforts in the post-genomic life sciences to understand how biology works by making new biological things. I examine five fields of constructive biology - synthetic biology, DIY (do-it-yourself) biology, hyperbolic crochet, sonocytology, and molecular gastronomy - investigating how they are enmeshed in sensory engagements that employ craftwork as a means of grasping biology. Synthetic biology is a community of bioengineers who aim to fabricate standardized biological systems using genetic components and manufacturing principles borrowed from engineering. DIY biology is a community of \"biohackers\" who appropriate synthetic biologists' terminologies, standards, and commitment to freely exchanging biomaterials in order to do hobbyist biological engineering in their homes. The Hyperbolic Crochet Coral Reef is a distributed venture of thousands of women who are cooperatively fabricating a series of yarn and plastic coral reefs in order to build a material simulation of oceanic morphologies and evolutionary theories. Sonocytology, a technique in nanotechnology research, uses scanning probe microscopes to \"listen to\" cellular vibrations and \"feel\" the topologies of cells and cellular components. Molecular gastronomy is a movement in which practitioners - physical chemists and biochemists who study food, and chefs who apply their results - use biochemical principles and laboratory apparatuses to further cooking and the culinary arts. In analyzing these fields, I draw on histories of experimental biology, anthropological accounts of artisanship, science studies work on embodiment and tacit knowledge in scientific practice, and sensory ethnography. Based on data gathered from participant-observation and interviewing, I argue for thinking about making new biological things as a form of \"crafting,\" an analytic that illuminates five aspects of contemporary biological manufacture: 1) sensory cultivation, 2) ongoing participation with biological media and forms, 3) the integration of making biological things and practitioners' selfmaking, 4) the embedding of social relations, interests, norms, and modes of exchange in built artifacts, and 5) the combination of making and knowing. In this study, I argue that both biology the substance and biology the discipline are currently being remade, and that increasingly, life scientists apprehend \"life\" through its manufacture.",
    "advisors": ["Stefan Helmreich"],
    "text": "Crafting life : a sensory ethnography of fabricated biologies This ethnography tracks a diverse set of practices I term \"constructive biologies,\" by which I mean efforts in the post-genomic life sciences to understand how biology works by making new biological things. I examine five fields of constructive biology - synthetic biology, DIY (do-it-yourself) biology, hyperbolic crochet, sonocytology, and molecular gastronomy - investigating how they are enmeshed in sensory engagements that employ craftwork as a means of grasping biology. Synthetic biology is a community of bioengineers who aim to fabricate standardized biological systems using genetic components and manufacturing principles borrowed from engineering. DIY biology is a community of \"biohackers\" who appropriate synthetic biologists' terminologies, standards, and commitment to freely exchanging biomaterials in order to do hobbyist biological engineering in their homes. The Hyperbolic Crochet Coral Reef is a distributed venture of thousands of women who are cooperatively fabricating a series of yarn and plastic coral reefs in order to build a material simulation of oceanic morphologies and evolutionary theories. Sonocytology, a technique in nanotechnology research, uses scanning probe microscopes to \"listen to\" cellular vibrations and \"feel\" the topologies of cells and cellular components. Molecular gastronomy is a movement in which practitioners - physical chemists and biochemists who study food, and chefs who apply their results - use biochemical principles and laboratory apparatuses to further cooking and the culinary arts. In analyzing these fields, I draw on histories of experimental biology, anthropological accounts of artisanship, science studies work on embodiment and tacit knowledge in scientific practice, and sensory ethnography. Based on data gathered from participant-observation and interviewing, I argue for thinking about making new biological things as a form of \"crafting,\" an analytic that illuminates five aspects of contemporary biological manufacture: 1) sensory cultivation, 2) ongoing participation with biological media and forms, 3) the integration of making biological things and practitioners' selfmaking, 4) the embedding of social relations, interests, norms, and modes of exchange in built artifacts, and 5) the combination of making and knowing. In this study, I argue that both biology the substance and biology the discipline are currently being remade, and that increasingly, life scientists apprehend \"life\" through its manufacture."
}, {
    "id": "oai:dspace.mit.edu:1721.1/39578",
    "title": "Those who don't know : modernity, risk, and transition in Hanoi's local markets",
    "abstract": "My research is about the particular effects of Vietnam's economic liberalization program (known as \"doi moi\") on the local food and market system in Hanoi. Doi moi policies, which began in the late 1980s, have instituted major changes in both the national system of agricultural production and in Hanoi's local system of marketplaces. The doi moi reforms have created many new opportunities in Hanoi, but they have also re-configured social relationships and market spaces along the food chain to present new kinds of risk for consumers. These include harmful chemicals, goods of uncertain quality, and sellers who operate outside of the moral obligations of the dominant system of personal relationships. These things have not yet been resolved through regulation and have therefore been left to consumers and sellers to work out among themselves. The competition between various actors to manage foodborne risk in the absence of state regulation has taken place amidst the state's campaign to re-order Hanoi's market system according to neoiberal ideals.",
    "advisors": ["Deborah Fitzgerald"],
    "text": "Those who don't know : modernity, risk, and transition in Hanoi's local markets My research is about the particular effects of Vietnam's economic liberalization program (known as \"doi moi\") on the local food and market system in Hanoi. Doi moi policies, which began in the late 1980s, have instituted major changes in both the national system of agricultural production and in Hanoi's local system of marketplaces. The doi moi reforms have created many new opportunities in Hanoi, but they have also re-configured social relationships and market spaces along the food chain to present new kinds of risk for consumers. These include harmful chemicals, goods of uncertain quality, and sellers who operate outside of the moral obligations of the dominant system of personal relationships. These things have not yet been resolved through regulation and have therefore been left to consumers and sellers to work out among themselves. The competition between various actors to manage foodborne risk in the absence of state regulation has taken place amidst the state's campaign to re-order Hanoi's market system according to neoiberal ideals."
}, {
    "id": "oai:dspace.mit.edu:1721.1/107039",
    "title": "Algorithmic detectives against child trafficking : data, entrapment, and the new global policing network",
    "abstract": "My dissertation explores how \"anti-trafficking\" has emerged as a global network of humanitarian professionals, law enforcement, and software companies collaborating to address the issue of child exploitation and trafficking online. I argue that the anti-trafficking network consolidates expertise through a shared moralizing politics of bureaucracy and carceral sensibility of securitization. This network mobilizes the issue of child protection to expand the reach of technologies of search and prediction, and to afford legitimation to a newly normalized level of digital surveillance. My findings are based on three years of ethnographic fieldwork and interviews with the United Nations and anti-trafficking organizations in Thailand, with a child protection NGO and police in the Netherlands, and with software companies and law enforcement in the United States. I use two case studies to support my argument that the child protection movement has motivated the expansion of digital policing and surveillance: 1) image detection software developed in collaboration between social media and software companies and international law enforcement organizations; and 2) the design and deployment of a 3D moving avatar of a photorealistic girl used in a child sex exploitation sting operation by an NGO working with an advertising firm. I draw from queer feminist phenomenology to introduce 'proximity' as a governing concept for understanding expert sociality and digital surveillance. Child protection operates in a global affective economy of fear, in which the risk of violence is always anticipated and close. The new global policing network keeps exploitation proximate through the humanitarian ideology of emancipation that motivates child protection, and through publicity of technological campaigns, in order to produce public acquiescence to the spectacles of digital surveillance, shaming, and punishment.",
    "advisors": ["Heather Paxson"],
    "text": "Algorithmic detectives against child trafficking : data, entrapment, and the new global policing network My dissertation explores how \"anti-trafficking\" has emerged as a global network of humanitarian professionals, law enforcement, and software companies collaborating to address the issue of child exploitation and trafficking online. I argue that the anti-trafficking network consolidates expertise through a shared moralizing politics of bureaucracy and carceral sensibility of securitization. This network mobilizes the issue of child protection to expand the reach of technologies of search and prediction, and to afford legitimation to a newly normalized level of digital surveillance. My findings are based on three years of ethnographic fieldwork and interviews with the United Nations and anti-trafficking organizations in Thailand, with a child protection NGO and police in the Netherlands, and with software companies and law enforcement in the United States. I use two case studies to support my argument that the child protection movement has motivated the expansion of digital policing and surveillance: 1) image detection software developed in collaboration between social media and software companies and international law enforcement organizations; and 2) the design and deployment of a 3D moving avatar of a photorealistic girl used in a child sex exploitation sting operation by an NGO working with an advertising firm. I draw from queer feminist phenomenology to introduce 'proximity' as a governing concept for understanding expert sociality and digital surveillance. Child protection operates in a global affective economy of fear, in which the risk of violence is always anticipated and close. The new global policing network keeps exploitation proximate through the humanitarian ideology of emancipation that motivates child protection, and through publicity of technological campaigns, in order to produce public acquiescence to the spectacles of digital surveillance, shaming, and punishment."
}, {
    "id": "oai:dspace.mit.edu:1721.1/84367",
    "title": "Conditional inequalities : American pure and applied mathematics, 1940-1975",
    "abstract": "This study investigates the status of mathematical knowledge in mid-century America. It is motivated by questions such as: when did mathematical theories become applicable to a wide range of fields from medicine to the social science? How did this change occur? I ask after the implications of this transformation for the development of mathematics as an academic discipline and how it affected what it meant to be a mathematician. How did mathematicians understand the relation between abstractions and generalizations on the one hand and their manifestation in concrete problems on the other? Mathematics in Cold War America was caught between the sciences and the humanities. This dissertation tracks the ways this tension between the two shaped the development of professional identities, pedagogical regimes, and the epistemological commitments of the American mathematical community in the postwar period. Focusing on the constructed division between pure and applied mathematics, it therefore investigates the relationship of scientific ideas to academic and governmental institutions, showing how the two are mutually inclusive. Examining the disciplinary formation of postwar mathematics, I show how ideas about what mathematics is and what it should be crystallized in institutional contexts, and how in turn these institutions reshaped those ideas. Tuning in to the ways different groups of mathematicians strove to make sense of the transformations in their fields and the way they struggled to implement their ideological convictions into specific research agendas and training programs sheds light on the co-construction of mathematics, the discipline, and mathematics as a body of knowledge. The relation between pure and applied mathematics and between mathematics and the rest of the sciences were disciplinary concerns as much as they were philosophical musings. As the reconfiguration of the mathematical field during the second half of the twentieth century shows, the dynamic relation between the natural and the human sciences reveals as much about institutions, practices, and nations as it does about epistemological commitments.",
    "advisors": ["David Kaiser"],
    "text": "Conditional inequalities : American pure and applied mathematics, 1940-1975 This study investigates the status of mathematical knowledge in mid-century America. It is motivated by questions such as: when did mathematical theories become applicable to a wide range of fields from medicine to the social science? How did this change occur? I ask after the implications of this transformation for the development of mathematics as an academic discipline and how it affected what it meant to be a mathematician. How did mathematicians understand the relation between abstractions and generalizations on the one hand and their manifestation in concrete problems on the other? Mathematics in Cold War America was caught between the sciences and the humanities. This dissertation tracks the ways this tension between the two shaped the development of professional identities, pedagogical regimes, and the epistemological commitments of the American mathematical community in the postwar period. Focusing on the constructed division between pure and applied mathematics, it therefore investigates the relationship of scientific ideas to academic and governmental institutions, showing how the two are mutually inclusive. Examining the disciplinary formation of postwar mathematics, I show how ideas about what mathematics is and what it should be crystallized in institutional contexts, and how in turn these institutions reshaped those ideas. Tuning in to the ways different groups of mathematicians strove to make sense of the transformations in their fields and the way they struggled to implement their ideological convictions into specific research agendas and training programs sheds light on the co-construction of mathematics, the discipline, and mathematics as a body of knowledge. The relation between pure and applied mathematics and between mathematics and the rest of the sciences were disciplinary concerns as much as they were philosophical musings. As the reconfiguration of the mathematical field during the second half of the twentieth century shows, the dynamic relation between the natural and the human sciences reveals as much about institutions, practices, and nations as it does about epistemological commitments."
}, {
    "id": "oai:dspace.mit.edu:1721.1/39577",
    "title": "Empire of energy : environment, geopolitics, and American technology before the age of oil",
    "abstract": "This dissertation asks how the United States physically built its global empire. Between 1840 and 1930, empire building involved the establishment of a network of naval bases and coaling stations. By focusing on energy, I reconceptualize the American overseas empire as neither inevitable nor geographically predetermined. I trace how coal shaped U.S. expansion, how this expansion influenced ideas about national security, and how these security concerns affected the global environment. Coal reveals continuities in American foreign relations that link overseas expansion to responses to the introduction of steam power into ocean travel. As the Navy sought coal, it progressively assembled the familiar contours of America's global reach. The dissertation addresses both global and local history. It shows how policy makers before the Civil War demonstrated tremendous creativity in initiating geological investigations, diplomatic arrangements, and commercial agreements in foreign territories. Between the Civil War and 1898, these approaches gradually gave way to a more singular effort by the Navy to control strategic ports around the world. Soon, coal was so central to the Navy that coaling strategy and technology formed a foundation for the education of elite officers at the Naval War College, where its study shaped the planning for future wars. Attention to Americans in Borneo, Japan, the Isthmus of ...",
    "advisors": ["David I. Kaiser"],
    "text": "Empire of energy : environment, geopolitics, and American technology before the age of oil This dissertation asks how the United States physically built its global empire. Between 1840 and 1930, empire building involved the establishment of a network of naval bases and coaling stations. By focusing on energy, I reconceptualize the American overseas empire as neither inevitable nor geographically predetermined. I trace how coal shaped U.S. expansion, how this expansion influenced ideas about national security, and how these security concerns affected the global environment. Coal reveals continuities in American foreign relations that link overseas expansion to responses to the introduction of steam power into ocean travel. As the Navy sought coal, it progressively assembled the familiar contours of America's global reach. The dissertation addresses both global and local history. It shows how policy makers before the Civil War demonstrated tremendous creativity in initiating geological investigations, diplomatic arrangements, and commercial agreements in foreign territories. Between the Civil War and 1898, these approaches gradually gave way to a more singular effort by the Navy to control strategic ports around the world. Soon, coal was so central to the Navy that coaling strategy and technology formed a foundation for the education of elite officers at the Naval War College, where its study shaped the planning for future wars. Attention to Americans in Borneo, Japan, the Isthmus of ..."
}, {
    "id": "oai:dspace.mit.edu:1721.1/69453",
    "title": "Corporate bodies and chemical bonds : an STS analysis of natural gas development in the United States",
    "abstract": "Natural gas extraction in the United States in the early 21st century has transformed social, physical, legal and biological landscapes. The technique of hydraulic fracturing, which entails the high-pressure injection into subsurface shale formations of synthetic chemical mixtures, has been viewed by the natural gas industry as a practice of great promise. But there is another side to the story. The first half of this dissertation explores an innovative scientific approach to studying the possible deleterious impacts on human health and the environment of the release of chemicals used in gas extraction. Via participant-observation within a small scientific advocacy organization, The Endocrine Disruption Exchange (TEDX), I follow the development of a database of chemicals used in natural gas extraction, a database that seeks to document not only what these chemicals are (many are proprietary), but also what sorts of bodily and ecological effects these substances may have. I analyze ethnographically how TEDX transformed an information vacuum around fracturing and generated fierce regional and national debates about the public health effects of this activity. The second portion of the dissertation expands TEDX's databasing methodology by reporting on a set of online user-generated databasing and mapping tools developed to interconnect communities encountering the corporate forces and chemical processes animating gas development. Shale gas extraction is an intensive technological practice and requires the delicate calibration of corporate, governmental, and legal apparatuses in order to proceed. The industry operates at county, state, and federal levels, and has in many instances been able to organize regulatory environments suited to rapid and lucrative gas extraction. In the midst of such multi-scalar deterritorializing forces, communities may have little legal or technical recourse if they think that they have been subject to chemical and corporate forces that undermine their financial, bodily, and social security. ExtrAct, a research group I co-founded and directed with artist and technologist Chris Csikszentmihalyi, sought to intervene in these processes by developing a suite of online mapping and databasing tools through which \"gas patch\" communities could share information, network, study and respond to industry activity across states. Using ExtrAct as an example this dissertation explores how social sciences and the academy at large can invest in developing research tools, methods, and programs designed for non-corporate ends, perhaps redressing in the process the informational and technical imbalances faced by communities dealing with large-scale multinational industries whose infrastructure and impacts are largely invisible to public scrutiny. The dissertation describes one potential method for such engaged scientific and social scientific research: an iterative, ethnographically informed process that I term \"STS in Practice.\"",
    "advisors": ["Stefan Helmreich"],
    "text": "Corporate bodies and chemical bonds : an STS analysis of natural gas development in the United States Natural gas extraction in the United States in the early 21st century has transformed social, physical, legal and biological landscapes. The technique of hydraulic fracturing, which entails the high-pressure injection into subsurface shale formations of synthetic chemical mixtures, has been viewed by the natural gas industry as a practice of great promise. But there is another side to the story. The first half of this dissertation explores an innovative scientific approach to studying the possible deleterious impacts on human health and the environment of the release of chemicals used in gas extraction. Via participant-observation within a small scientific advocacy organization, The Endocrine Disruption Exchange (TEDX), I follow the development of a database of chemicals used in natural gas extraction, a database that seeks to document not only what these chemicals are (many are proprietary), but also what sorts of bodily and ecological effects these substances may have. I analyze ethnographically how TEDX transformed an information vacuum around fracturing and generated fierce regional and national debates about the public health effects of this activity. The second portion of the dissertation expands TEDX's databasing methodology by reporting on a set of online user-generated databasing and mapping tools developed to interconnect communities encountering the corporate forces and chemical processes animating gas development. Shale gas extraction is an intensive technological practice and requires the delicate calibration of corporate, governmental, and legal apparatuses in order to proceed. The industry operates at county, state, and federal levels, and has in many instances been able to organize regulatory environments suited to rapid and lucrative gas extraction. In the midst of such multi-scalar deterritorializing forces, communities may have little legal or technical recourse if they think that they have been subject to chemical and corporate forces that undermine their financial, bodily, and social security. ExtrAct, a research group I co-founded and directed with artist and technologist Chris Csikszentmihalyi, sought to intervene in these processes by developing a suite of online mapping and databasing tools through which \"gas patch\" communities could share information, network, study and respond to industry activity across states. Using ExtrAct as an example this dissertation explores how social sciences and the academy at large can invest in developing research tools, methods, and programs designed for non-corporate ends, perhaps redressing in the process the informational and technical imbalances faced by communities dealing with large-scale multinational industries whose infrastructure and impacts are largely invisible to public scrutiny. The dissertation describes one potential method for such engaged scientific and social scientific research: an iterative, ethnographically informed process that I term \"STS in Practice.\""
}, {
    "id": "oai:dspace.mit.edu:1721.1/28258",
    "title": "Biocapital : the constitution of post-genomic life",
    "abstract": "(cont.) In the process, this thesis intervenes in social theoretical debates not simply around the nature and production of knowledge and value, but also around the place of larger belief-systems - relating to religion, nation and ethics - in such productive enterprises. It simultaneously intervenes in conceptual debates within cultural anthropology regarding methodological questions that surround the undertaking of comparative ethnographic projects of powerful sites of knowledge production and value generation in a globalized world.",
    "advisors": ["Michael M.J. Fischer"],
    "text": "Biocapital : the constitution of post-genomic life (cont.) In the process, this thesis intervenes in social theoretical debates not simply around the nature and production of knowledge and value, but also around the place of larger belief-systems - relating to religion, nation and ethics - in such productive enterprises. It simultaneously intervenes in conceptual debates within cultural anthropology regarding methodological questions that surround the undertaking of comparative ethnographic projects of powerful sites of knowledge production and value generation in a globalized world."
}, {
    "id": "oai:dspace.mit.edu:1721.1/93810",
    "title": "Insiders and outsiders : nuclear arms control experts in Cold War America",
    "abstract": "This dissertation presents a history of the community of nuclear arms control experts in the United States during the middle and later years of the Cold War, the age of thermonuclear ballistic missiles. Arms control experts were, in many interesting ways, both insiders and outsiders to the American \"nuclear state.\" The dissertation begins by exploring the formation of strategic arms control in the years leading up to 1960, showing how arms control emerged from the mixing of local communities of disarmament advocates and theorists of nuclear deterrence. Rather than inevitable doctrinal unity, early arms control was highly local and contingent. In particular, the crucial concept of \"stability\" was open to multiple interpretations. In the 1960s, arms control problems motivated groundbreaking scientific research. Elite contract consultants to the government contemplated the use of lasers as weapons against ballistic missiles. As consultants performed calculations and experiments in the context of classified discussions and studies, they founded a new field of physics called nonlinear optics. In the late 1960s, strategic arms control became a public issue during a complex political dispute over missile defense. Arms control experts mediated and fueled this controversy by participating in a surprising range of activity, rallying alongside local residents whose neighborhoods would be impacted by missile defense installations, and criticizing defense policy in Congressional testimony-even as they worked their connections to the White House. In the 1960s and 1970s arms controllers shaped a changing institutional landscape for the support of arms control expertise. They built arms control into a new government agency, and later drew on the resources of philanthropic foundations to create major university arms control centers. By the 1980s, arms control reached peak public visibility amid controversy over the Reagan administration's Strategic Defense Initiative. This dissertation uses the private papers and correspondence of numerous experts, a wide range of arms control publications, and government records to explore the diverse practices of arms control. It engages a wider discussion among historians about the status of Cold War elites, the relationship between experts and the American state, and the character of scientific knowledge during the Cold War.",
    "advisors": ["David Kaiser"],
    "text": "Insiders and outsiders : nuclear arms control experts in Cold War America This dissertation presents a history of the community of nuclear arms control experts in the United States during the middle and later years of the Cold War, the age of thermonuclear ballistic missiles. Arms control experts were, in many interesting ways, both insiders and outsiders to the American \"nuclear state.\" The dissertation begins by exploring the formation of strategic arms control in the years leading up to 1960, showing how arms control emerged from the mixing of local communities of disarmament advocates and theorists of nuclear deterrence. Rather than inevitable doctrinal unity, early arms control was highly local and contingent. In particular, the crucial concept of \"stability\" was open to multiple interpretations. In the 1960s, arms control problems motivated groundbreaking scientific research. Elite contract consultants to the government contemplated the use of lasers as weapons against ballistic missiles. As consultants performed calculations and experiments in the context of classified discussions and studies, they founded a new field of physics called nonlinear optics. In the late 1960s, strategic arms control became a public issue during a complex political dispute over missile defense. Arms control experts mediated and fueled this controversy by participating in a surprising range of activity, rallying alongside local residents whose neighborhoods would be impacted by missile defense installations, and criticizing defense policy in Congressional testimony-even as they worked their connections to the White House. In the 1960s and 1970s arms controllers shaped a changing institutional landscape for the support of arms control expertise. They built arms control into a new government agency, and later drew on the resources of philanthropic foundations to create major university arms control centers. By the 1980s, arms control reached peak public visibility amid controversy over the Reagan administration's Strategic Defense Initiative. This dissertation uses the private papers and correspondence of numerous experts, a wide range of arms control publications, and government records to explore the diverse practices of arms control. It engages a wider discussion among historians about the status of Cold War elites, the relationship between experts and the American state, and the character of scientific knowledge during the Cold War."
}, {
    "id": "oai:dspace.mit.edu:1721.1/69451",
    "title": "Placing outer space : an earthly ethnography of other worlds",
    "abstract": "This dissertation concerns the role of place in scientific practice. Ideas of place, I argue, shape and are shaped by science. I specifically look at the community of planetary scientists who, though they cannot step foot on the objects they study, transform planets into places. This is an ethnographic work that draws on 18 months of fieldwork during which time I encountered several different communities of planetary scientists. At MIT, I worked alongside astronomers looking for planets around other stars. These \"exoplanet\" astronomers transformed numerical counts of photons into complex worlds with atmospheres and weather. Data visualizations characterized the work of a community learning to see unseen planets in specific, place-based ways. I also traveled with an astronomer to a Chilean observatory where she studied the night sky hoping to find a \"habitable planet.\" Many other astronomers share this goal and have designed various ways to detect a planet like Earth. The importance of these projects signifies that exoplanet astronomers are more interested in finding planetary kin - planets that are familiar places - than exotic aliens. To determine how the planetary places created by exoplanet astronomers differ from those in our own Solar System, I spent time at the NASA Ames Research Center with a group of computer scientists who create high resolution and three-dimensional maps of Mars. These maps reflect the kind of place Mars is today: it is available to everyone to explore, it is displayed such that you can imagine standing on the surface, and it is presented as geologically dynamic in ways similar to Earth. Even though these maps help give Mars a sense of place, Martian science is still stymied by the inability to send humans to its surface. Instead, planetary scientists travel to terrestrial sites deemed to be \"Mars-like\" to approximate performing geologic fieldwork on Mars. I went to one of these locations to see how, during these outings, Mars and Earth become entwined as scientists forge connections between two planetary places. These diverse scientific activities, I conclude, are transforming our view of the cosmos. Outer space is becoming outer place.",
    "advisors": ["Stefan Helmreich"],
    "text": "Placing outer space : an earthly ethnography of other worlds This dissertation concerns the role of place in scientific practice. Ideas of place, I argue, shape and are shaped by science. I specifically look at the community of planetary scientists who, though they cannot step foot on the objects they study, transform planets into places. This is an ethnographic work that draws on 18 months of fieldwork during which time I encountered several different communities of planetary scientists. At MIT, I worked alongside astronomers looking for planets around other stars. These \"exoplanet\" astronomers transformed numerical counts of photons into complex worlds with atmospheres and weather. Data visualizations characterized the work of a community learning to see unseen planets in specific, place-based ways. I also traveled with an astronomer to a Chilean observatory where she studied the night sky hoping to find a \"habitable planet.\" Many other astronomers share this goal and have designed various ways to detect a planet like Earth. The importance of these projects signifies that exoplanet astronomers are more interested in finding planetary kin - planets that are familiar places - than exotic aliens. To determine how the planetary places created by exoplanet astronomers differ from those in our own Solar System, I spent time at the NASA Ames Research Center with a group of computer scientists who create high resolution and three-dimensional maps of Mars. These maps reflect the kind of place Mars is today: it is available to everyone to explore, it is displayed such that you can imagine standing on the surface, and it is presented as geologically dynamic in ways similar to Earth. Even though these maps help give Mars a sense of place, Martian science is still stymied by the inability to send humans to its surface. Instead, planetary scientists travel to terrestrial sites deemed to be \"Mars-like\" to approximate performing geologic fieldwork on Mars. I went to one of these locations to see how, during these outings, Mars and Earth become entwined as scientists forge connections between two planetary places. These diverse scientific activities, I conclude, are transforming our view of the cosmos. Outer space is becoming outer place."
}, {
    "id": "oai:dspace.mit.edu:1721.1/63234",
    "title": "Negotiating nature : expertise and environment in the Klamath River Basin",
    "abstract": "\"Negotiating Nature\" explores resource management in action and the intertwined roles of law and science in environmental conflicts in the Upper Klamath River Basin in southern Oregon. I follow disputes over the management of water and endangered species. I develop several themes: first, how these disputes demonstrate the growing connections between scientific and legal authority in environmental matters. This occurs because environmental laws often limit participation in disputes to those who can offer \"scientific data\" in support of their claims. I call this situation \"scientific legality\" and suggest that increasingly, one's ability to make legal claims is closely tied to one's ability to muster scientific authority behind those claims. Second, how the growing importance of scientific expertise in environmental decision-making has affected the ways that groups frame environmental claims. Third, how negotiations over environmental rights, regulations, and policies shape not only management efforts, but also narratives of environmental relationships. In Part One, I discuss the Endangered Species Act of 1973, which legally mandates that only scientific considerations can be taken into account in certain aspects of endangered species management. As a result, the Act has impacted the ways people frame claims about endangered species. I then discuss how the Klamath Tribes of American Indians have responded this situation, and the implications of this for presumed divisions between the environmental knowledge of scientists and native peoples. In Part Two, I examine a 1975 water rights case, United States v. Adair et al. I explore how the court drew on and reproduced prominent narratives of American Indian history, and the ways these narratives bounded the agency of the Klamath in relation to the environment and the colonial process. In Part Three, I examine a dispute in 2001 over endangered species. In this conflict, a dispute over policy quickly became a dispute over the scientific claims that legitimated the policy. Expert disagreement ensued. Although political explanations for expert disagreement were common, I suggest that a more underlying cause was the unavoidable uncertainties of ecological claims. These uncertainties were politically useful to those who wanted to stall management action and maintain the status quo.",
    "advisors": ["Deborah K. Fitzgerald, Harriet Ritvo", "Susan S. Silbey"],
    "text": "Negotiating nature : expertise and environment in the Klamath River Basin \"Negotiating Nature\" explores resource management in action and the intertwined roles of law and science in environmental conflicts in the Upper Klamath River Basin in southern Oregon. I follow disputes over the management of water and endangered species. I develop several themes: first, how these disputes demonstrate the growing connections between scientific and legal authority in environmental matters. This occurs because environmental laws often limit participation in disputes to those who can offer \"scientific data\" in support of their claims. I call this situation \"scientific legality\" and suggest that increasingly, one's ability to make legal claims is closely tied to one's ability to muster scientific authority behind those claims. Second, how the growing importance of scientific expertise in environmental decision-making has affected the ways that groups frame environmental claims. Third, how negotiations over environmental rights, regulations, and policies shape not only management efforts, but also narratives of environmental relationships. In Part One, I discuss the Endangered Species Act of 1973, which legally mandates that only scientific considerations can be taken into account in certain aspects of endangered species management. As a result, the Act has impacted the ways people frame claims about endangered species. I then discuss how the Klamath Tribes of American Indians have responded this situation, and the implications of this for presumed divisions between the environmental knowledge of scientists and native peoples. In Part Two, I examine a 1975 water rights case, United States v. Adair et al. I explore how the court drew on and reproduced prominent narratives of American Indian history, and the ways these narratives bounded the agency of the Klamath in relation to the environment and the colonial process. In Part Three, I examine a dispute in 2001 over endangered species. In this conflict, a dispute over policy quickly became a dispute over the scientific claims that legitimated the policy. Expert disagreement ensued. Although political explanations for expert disagreement were common, I suggest that a more underlying cause was the unavoidable uncertainties of ecological claims. These uncertainties were politically useful to those who wanted to stall management action and maintain the status quo."
}, {
    "id": "oai:dspace.mit.edu:1721.1/113945",
    "title": "The bureaucracy of empathy : vivisection and the question of animal pain in Britain, 1876-1912",
    "abstract": "This dissertation examines the mutually reinforcing connections between science and law and their construction of pain in British regulation of animal experimentation. It investigates the Home Office's implementation of the Cruelty to Animals Act (1876), the first effort anywhere in the world to impose legal restrictions on vivisection, during the three decades following its enactment. The study ends in 1912 with the findings of a second Royal Commission that evaluated the workings of the Act. The Commission reaffirmed many of the Home Office polices regarding vivisection and their underlying premises. The Act mandated official supervision of scientific experiments that \"calculated to give pain\" to animal subjects. Implementing the Act therefore necessitated the identification and quantification of pain. This requirement created what I term the \"bureaucracy of empathy,\" an attempt to systemize the understanding of animal suffering through administrative mechanisms. Practicing empathy was integral to some bureaucratic tasks, for example, attaching the right certificate to an inoculation experiment. Additionally, various factors including legal settings and scientific knowledge informed and situated this empathy with animals, when, for instance, an inspector drafted a report about mutilated monkeys while visiting a physiology laboratory. My analysis unravels that defining animal pain was often intertwined with the definition of an experiment. Law and science co-constitution of pain and experiments conditioned both the daily work of administering the law and the practices of experimenters. This dynamic led to the adoption of technologies such as anesthesia and pain scoring models, which provided legal-medical means to control pain in research and to ostensibly create a cruelty free experimental fact. A new pain-based ethical order was established, designed by law officers, civil servants, and court judges as much as by physiologists, remaking the relationships between experimenters, state representatives, and laboratory animals.",
    "advisors": ["Harriet Ritvo"],
    "text": "The bureaucracy of empathy : vivisection and the question of animal pain in Britain, 1876-1912 This dissertation examines the mutually reinforcing connections between science and law and their construction of pain in British regulation of animal experimentation. It investigates the Home Office's implementation of the Cruelty to Animals Act (1876), the first effort anywhere in the world to impose legal restrictions on vivisection, during the three decades following its enactment. The study ends in 1912 with the findings of a second Royal Commission that evaluated the workings of the Act. The Commission reaffirmed many of the Home Office polices regarding vivisection and their underlying premises. The Act mandated official supervision of scientific experiments that \"calculated to give pain\" to animal subjects. Implementing the Act therefore necessitated the identification and quantification of pain. This requirement created what I term the \"bureaucracy of empathy,\" an attempt to systemize the understanding of animal suffering through administrative mechanisms. Practicing empathy was integral to some bureaucratic tasks, for example, attaching the right certificate to an inoculation experiment. Additionally, various factors including legal settings and scientific knowledge informed and situated this empathy with animals, when, for instance, an inspector drafted a report about mutilated monkeys while visiting a physiology laboratory. My analysis unravels that defining animal pain was often intertwined with the definition of an experiment. Law and science co-constitution of pain and experiments conditioned both the daily work of administering the law and the practices of experimenters. This dynamic led to the adoption of technologies such as anesthesia and pain scoring models, which provided legal-medical means to control pain in research and to ostensibly create a cruelty free experimental fact. A new pain-based ethical order was established, designed by law officers, civil servants, and court judges as much as by physiologists, remaking the relationships between experimenters, state representatives, and laboratory animals."
}, {
    "id": "oai:dspace.mit.edu:1721.1/113946",
    "title": "Making a digital working class : Uber drivers in Boston, 2016-2017",
    "abstract": "Pocket computers, called \"smartphones,\" have become a part of everyday life over the past decade. Most people now routinely carry around with them millions of times more computing power than generated the Apollo mission to the Moon. They use it to access, process, and share information quickly and cheaply, in furtherance of the things people have long done: buying and selling, socializing, and so on, yet faster and across greater distances-characteristic of what we call \"modernity.\" This has affected the ways in which people are working, and who is working, doing what, today. This thesis reports the results of a field study of one new kind of laborer who has been brought into work consequent to the smartphone: Uber drivers. The author conducted ethnographic fieldwork over one year in Boston, Massachusetts, and the surrounding area using ride-along sampling, participant observation, lengthy interviewing, and systematic coding in order to better understand a software-organized, person-to-person labor market in which the person who does the labor also brings the capital in the form of a vehicle used to provide transportation to other people. The first chapter of the thesis provides a typology of Uber drivers based on semi-random sampling through ride-alongs. The second chapter describes collective action that was undertaken by Uber drivers at Boston's Logan Airport in the form of a strike against the algorithm, which was an effort to induce the software to perceive an (artificial) driver shortage, leading to an increase in the price of fares. The third chapter offers a theory of the structure of Uber as an organization that mobilizes labor by using software to facilitate economic transactions that are triangulated between two users and the firm. The chapter also explains how this structure was particularly apt at mobilizing large numbers of people to carry out \"regulatory breach,\" as they worked as Uber drivers doing the equivalent of taxi or livery work without complying with any of the applicable legal regulations. The final chapter explains how analysis of the field data, in combination with the new theoretical insights of the thesis, drives a conclusion suggested by the thesis title: that Uber has made a digital working class.",
    "advisors": ["Susan S. Silbey"],
    "text": "Making a digital working class : Uber drivers in Boston, 2016-2017 Pocket computers, called \"smartphones,\" have become a part of everyday life over the past decade. Most people now routinely carry around with them millions of times more computing power than generated the Apollo mission to the Moon. They use it to access, process, and share information quickly and cheaply, in furtherance of the things people have long done: buying and selling, socializing, and so on, yet faster and across greater distances-characteristic of what we call \"modernity.\" This has affected the ways in which people are working, and who is working, doing what, today. This thesis reports the results of a field study of one new kind of laborer who has been brought into work consequent to the smartphone: Uber drivers. The author conducted ethnographic fieldwork over one year in Boston, Massachusetts, and the surrounding area using ride-along sampling, participant observation, lengthy interviewing, and systematic coding in order to better understand a software-organized, person-to-person labor market in which the person who does the labor also brings the capital in the form of a vehicle used to provide transportation to other people. The first chapter of the thesis provides a typology of Uber drivers based on semi-random sampling through ride-alongs. The second chapter describes collective action that was undertaken by Uber drivers at Boston's Logan Airport in the form of a strike against the algorithm, which was an effort to induce the software to perceive an (artificial) driver shortage, leading to an increase in the price of fares. The third chapter offers a theory of the structure of Uber as an organization that mobilizes labor by using software to facilitate economic transactions that are triangulated between two users and the firm. The chapter also explains how this structure was particularly apt at mobilizing large numbers of people to carry out \"regulatory breach,\" as they worked as Uber drivers doing the equivalent of taxi or livery work without complying with any of the applicable legal regulations. The final chapter explains how analysis of the field data, in combination with the new theoretical insights of the thesis, drives a conclusion suggested by the thesis title: that Uber has made a digital working class."
}, {
    "id": "oai:dspace.mit.edu:1721.1/8669",
    "title": "The expertise of germs : practice, language and authority in American bacteriology, 1899-1924",
    "abstract": "This thesis traces the development of American bacteriology during the first quarter of the twentieth century. While bacteriology experienced a period of rapid growth, an enduring disciplinary anxiety equally characterized the field. In particular, bacteriologists feared increasing specialization and conceptual fragmentation. Leading practitioners repeatedly worried that their science constituted a collection of unrelated techniques, carried out in the service to other practical endeavors without the benefit of an underlying theory or unifying language. I suggest that the sources of bacteriology's rapid professional growth equally accounted for this sense of conceptual impoverishment and disciplinary privation. Typically, bacteriologists focused on what bacteria did rather than what they were in any biological sense. The first three chapters provide a comprehensive survey of the institutional contexts bacteriology (e.g., medical schools, public health laboratories, water sanitation works, dairies, land-grant colleges, and agricultural experiment stations). For the most part, bacteriologists studied bacteria only so far as to isolate, identify and eliminate pathogens. Dairy and soil bacteriologists, however, sought to distinguish productive types of bacteria, and render those forms more active, a direction that led them to consider a range of phenomena and organisms normally occluded by the practices of medical, public health, and sanitary bacteriology.",
    "advisors": ["Deborah Fitzgerald"],
    "text": "The expertise of germs : practice, language and authority in American bacteriology, 1899-1924 This thesis traces the development of American bacteriology during the first quarter of the twentieth century. While bacteriology experienced a period of rapid growth, an enduring disciplinary anxiety equally characterized the field. In particular, bacteriologists feared increasing specialization and conceptual fragmentation. Leading practitioners repeatedly worried that their science constituted a collection of unrelated techniques, carried out in the service to other practical endeavors without the benefit of an underlying theory or unifying language. I suggest that the sources of bacteriology's rapid professional growth equally accounted for this sense of conceptual impoverishment and disciplinary privation. Typically, bacteriologists focused on what bacteria did rather than what they were in any biological sense. The first three chapters provide a comprehensive survey of the institutional contexts bacteriology (e.g., medical schools, public health laboratories, water sanitation works, dairies, land-grant colleges, and agricultural experiment stations). For the most part, bacteriologists studied bacteria only so far as to isolate, identify and eliminate pathogens. Dairy and soil bacteriologists, however, sought to distinguish productive types of bacteria, and render those forms more active, a direction that led them to consider a range of phenomena and organisms normally occluded by the practices of medical, public health, and sanitary bacteriology."
}, {
    "id": "oai:dspace.mit.edu:1721.1/106749",
    "title": "Governing the shark : predators and people in the twentieth century and beyond",
    "abstract": "This dissertation examines the history of shark-human interactions in the twentieth and twenty-first centuries. It argues that the mid-twentieth century onward saw a series of conjunctures -technological, cultural, and scientific -that thrust sharks and humans into unprecedented levels of contact. This led to both a rise in preoccupation with sharks, and an emergence of new stakeholder groups that sought produce knowledge about them. The conflicting definitions, attitudes, and responses to sharks presented by these various groups are linked to greater trends in science, culture, and society. In particular, the way humans write and think about sharks and other man-eating predators has deep links to the position we see ourselves occupying in the environment. Further, anxieties about sharks are strongly tied to the complicated cultural relationships that people have with the marine environment, both as a place of wonder and terror. Lastly, sharks also allow us to examine the technologies we use to tame and navigate the ocean, as the shifts that brought humans and sharks into closer proximity were intertwined with new technologies that changed the ways humans interacted with marine spaces. Each chapter presents case studies from the United States and South Africa, juxtaposing the responses by each region. The opening chapter charts the rise of shark attack numbers in the mid-century. It traces the impact of highly publicized shark attacks in the U.S. and South Africa in the 1950s, which resulted in differing approaches to combat the threat of shark attack. Chapter Two explores the intersections between popular depictions of sharks and changing perceptions of shark behavior, centering on the ur-text of shark literature: Jaws. Chapter Three traces the advent of shark tourism, and examines the controversy surrounding white shark cage diving in South Africa. Chapter Four explores the response of Cape Cod communities to an influx of white sharks into the region, drawing parallels with earlier historical examples of predator eradication and conservation. The dissertation thus argues that studying shark-human interactions allows for the interrogation of divisions between myth and science, experts and laypersons, popular culture and scientific knowledge, humans and the environment.",
    "advisors": ["Harriet Ritvo"],
    "text": "Governing the shark : predators and people in the twentieth century and beyond This dissertation examines the history of shark-human interactions in the twentieth and twenty-first centuries. It argues that the mid-twentieth century onward saw a series of conjunctures -technological, cultural, and scientific -that thrust sharks and humans into unprecedented levels of contact. This led to both a rise in preoccupation with sharks, and an emergence of new stakeholder groups that sought produce knowledge about them. The conflicting definitions, attitudes, and responses to sharks presented by these various groups are linked to greater trends in science, culture, and society. In particular, the way humans write and think about sharks and other man-eating predators has deep links to the position we see ourselves occupying in the environment. Further, anxieties about sharks are strongly tied to the complicated cultural relationships that people have with the marine environment, both as a place of wonder and terror. Lastly, sharks also allow us to examine the technologies we use to tame and navigate the ocean, as the shifts that brought humans and sharks into closer proximity were intertwined with new technologies that changed the ways humans interacted with marine spaces. Each chapter presents case studies from the United States and South Africa, juxtaposing the responses by each region. The opening chapter charts the rise of shark attack numbers in the mid-century. It traces the impact of highly publicized shark attacks in the U.S. and South Africa in the 1950s, which resulted in differing approaches to combat the threat of shark attack. Chapter Two explores the intersections between popular depictions of sharks and changing perceptions of shark behavior, centering on the ur-text of shark literature: Jaws. Chapter Three traces the advent of shark tourism, and examines the controversy surrounding white shark cage diving in South Africa. Chapter Four explores the response of Cape Cod communities to an influx of white sharks into the region, drawing parallels with earlier historical examples of predator eradication and conservation. The dissertation thus argues that studying shark-human interactions allows for the interrogation of divisions between myth and science, experts and laypersons, popular culture and scientific knowledge, humans and the environment."
}, {
    "id": "oai:dspace.mit.edu:1721.1/39179",
    "title": "Flesh yours, bones mine : the making of the biomedical subject in Turkey",
    "abstract": "With the emergence of biomedical technologies, human body parts from living or dead donors have become commodities in the international networks of trade. This dissertation tries to understand religious, political and ethical discourses on this emerging economy and how it creates its subjects in Turkey. By drawing analogies from the early days of anatomy and mental health practices and by using a three-fold-corpus (state, body and law) as a framework, it illustrates how the state, the religious law on the body, and social inequalities turn some subjects to objects of this biomedical practice while extending life for others. Life histories, oral histories, doctors' and patients' accounts, media reporting, urban legends, cinematography, theater, poetry and literature speak of this new biomedical life. The meaning of the cadaver, the brain-dead body, and the living donor are reevaluated. The personhood of suicides, the homeless, the poor, the mentally ill, the immigrant, and women are all questioned with the redefinition of boundaries of life and death. Biomedicine effects this kind of social change.",
    "advisors": ["Michael M.J. Fischer"],
    "text": "Flesh yours, bones mine : the making of the biomedical subject in Turkey With the emergence of biomedical technologies, human body parts from living or dead donors have become commodities in the international networks of trade. This dissertation tries to understand religious, political and ethical discourses on this emerging economy and how it creates its subjects in Turkey. By drawing analogies from the early days of anatomy and mental health practices and by using a three-fold-corpus (state, body and law) as a framework, it illustrates how the state, the religious law on the body, and social inequalities turn some subjects to objects of this biomedical practice while extending life for others. Life histories, oral histories, doctors' and patients' accounts, media reporting, urban legends, cinematography, theater, poetry and literature speak of this new biomedical life. The meaning of the cadaver, the brain-dead body, and the living donor are reevaluated. The personhood of suicides, the homeless, the poor, the mentally ill, the immigrant, and women are all questioned with the redefinition of boundaries of life and death. Biomedicine effects this kind of social change."
}, {
    "id": "oai:dspace.mit.edu:1721.1/104559",
    "title": "Economy electric : techno-economics, neoliberalism, and electricity in the United States",
    "abstract": "This dissertation is a study of emergent economic forms of life. It investigates recent remakings of economic existence and modes of disseminating these forms of life, and does so with particular reference to the crafting of electricity markets in the United States. It draws on more than a year of fieldwork among experts and users involved in electricity exchange. The experts and users among whom I conducted participant observation include computer programmers who assist companies that trade in electricity markets by collecting information and making trading suggestions, electrical engineers who design new infrastructures such as electricity markets for buying and selling electricity in bulk, psychologists and social scientists who study people's electricity consumption behavior to generate economic technologies to save money to users and providers of electricity, and citizen groups based in West Virginia and rural Illinois that organize against electricity markets' exclusion of consumers from decision-making mechanisms. Bringing questions of economic anthropology to bear upon the emergent literature of the anthropology of infrastructures, I propose that new economic forms of existence often come to being though infrastructure building and maintenance. For the last 20 years, experts of diverse technical backgrounds have been reprogramming the electric grid to allow for enhanced calculative choice and competition - principles at the core of the neoliberal agenda. I demonstrate that people who do not necessarily concern themselves with the formal study of economics often take the lead in creating and propagating wide-ranging economic emergent forms of life, such as neoliberalism, across the social field. To zero in on their work, I develop the concept of \"techno-economics\": an approach that understands commodities, whether they are living nonhumans such as livestock or inorganic processes like electricity, as more than passive receptacles of human design, and locates humans within their efforts to commoditize and marketize unruly objects, like electricity - a commodity that cannot be stored in warehouses or shipped on highways. Anthropological studies of the techno-economic, I suggest, are best equipped to make connections in ethnographic representation between otherwise disparate nodes of social life, like expertise and wires, law and steel, and finally, economics and electricity.",
    "advisors": ["Michael M. J. Fischer"],
    "text": "Economy electric : techno-economics, neoliberalism, and electricity in the United States This dissertation is a study of emergent economic forms of life. It investigates recent remakings of economic existence and modes of disseminating these forms of life, and does so with particular reference to the crafting of electricity markets in the United States. It draws on more than a year of fieldwork among experts and users involved in electricity exchange. The experts and users among whom I conducted participant observation include computer programmers who assist companies that trade in electricity markets by collecting information and making trading suggestions, electrical engineers who design new infrastructures such as electricity markets for buying and selling electricity in bulk, psychologists and social scientists who study people's electricity consumption behavior to generate economic technologies to save money to users and providers of electricity, and citizen groups based in West Virginia and rural Illinois that organize against electricity markets' exclusion of consumers from decision-making mechanisms. Bringing questions of economic anthropology to bear upon the emergent literature of the anthropology of infrastructures, I propose that new economic forms of existence often come to being though infrastructure building and maintenance. For the last 20 years, experts of diverse technical backgrounds have been reprogramming the electric grid to allow for enhanced calculative choice and competition - principles at the core of the neoliberal agenda. I demonstrate that people who do not necessarily concern themselves with the formal study of economics often take the lead in creating and propagating wide-ranging economic emergent forms of life, such as neoliberalism, across the social field. To zero in on their work, I develop the concept of \"techno-economics\": an approach that understands commodities, whether they are living nonhumans such as livestock or inorganic processes like electricity, as more than passive receptacles of human design, and locates humans within their efforts to commoditize and marketize unruly objects, like electricity - a commodity that cannot be stored in warehouses or shipped on highways. Anthropological studies of the techno-economic, I suggest, are best equipped to make connections in ethnographic representation between otherwise disparate nodes of social life, like expertise and wires, law and steel, and finally, economics and electricity."
}, {
    "id": "oai:dspace.mit.edu:1721.1/112883",
    "title": "Trial and Error : medical marijuana, the absence of evidence, and the allure of anecdote",
    "abstract": "For the past four years, Christy Shake has given her son marijuana extract six times a day to ease his childhood epilepsy. Hers is a compelling story that highlights the potential benefits of medical cannabis. But in the wake of antiquated and inflexible federal legislation, anecdotal reports like these are essentially all we have. More than half the states in the U.S. have voted to legalize medical marijuana, as thousands contend it's a viable treatment for a growing list of conditions. Nevertheless, as more and more patients gain access to cannabis, neither they nor their physicians understand exactly what they're receiving from local dispensaries. Patients, caregivers, scientists, physicians, pharmaceutical companies, and dispensary growers alike are calling for changes to government policies that restrict research. It's high time to separate politics from science.",
    "advisors": ["David Corcoran"],
    "text": "Trial and Error : medical marijuana, the absence of evidence, and the allure of anecdote For the past four years, Christy Shake has given her son marijuana extract six times a day to ease his childhood epilepsy. Hers is a compelling story that highlights the potential benefits of medical cannabis. But in the wake of antiquated and inflexible federal legislation, anecdotal reports like these are essentially all we have. More than half the states in the U.S. have voted to legalize medical marijuana, as thousands contend it's a viable treatment for a growing list of conditions. Nevertheless, as more and more patients gain access to cannabis, neither they nor their physicians understand exactly what they're receiving from local dispensaries. Patients, caregivers, scientists, physicians, pharmaceutical companies, and dispensary growers alike are calling for changes to government policies that restrict research. It's high time to separate politics from science."
}, {
    "id": "oai:dspace.mit.edu:1721.1/112882",
    "title": "SuperAgers : do octogenarians with exceptional memory hold the key to healthy aging?",
    "abstract": "That older relative who stays preternaturally sharp long into their 80's or 90's may hold within their skull the secret to understanding how we lose, and keep, our memories. There are many different ways of aging successfully, but a growing group of scientists at Northwestern university and elsewhere are zeroing in on why some people keep the recall you'd expect of a middle-ager well into their 9th and 10th decades. The scientists do everything they can to get to know these the owners of these brains -- their abilities, their genes, and the stories of their lives -- then, when they die, dissect the brains themselves. Will the craniums of these successful \"SuperAgers\" give science some leverage in the battle against dementia, or even against aging itself?",
    "advisors": ["Seth Mnookin"],
    "text": "SuperAgers : do octogenarians with exceptional memory hold the key to healthy aging? That older relative who stays preternaturally sharp long into their 80's or 90's may hold within their skull the secret to understanding how we lose, and keep, our memories. There are many different ways of aging successfully, but a growing group of scientists at Northwestern university and elsewhere are zeroing in on why some people keep the recall you'd expect of a middle-ager well into their 9th and 10th decades. The scientists do everything they can to get to know these the owners of these brains -- their abilities, their genes, and the stories of their lives -- then, when they die, dissect the brains themselves. Will the craniums of these successful \"SuperAgers\" give science some leverage in the battle against dementia, or even against aging itself?"
}, {
    "id": "oai:dspace.mit.edu:1721.1/119910",
    "title": "From the sea to the stars : the forgotten journeys of the Philippines' ancient explorers",
    "abstract": "Linguistic, genetic, and archaeological evidence indicate that the Philippines has been inhabited by humans for many thousands of years. By what means the earliest settlers arrived in the archipelago is still a mystery, but a growing body of evidence points to the likelihood that they possessed seafaring technology. If so, then modern Filipinos -- who are even now making their first tentative steps into space -- are heirs to a rich heritage of exploration, the story of which has yet to be fully told.",
    "advisors": ["Marcia Bartusiak"],
    "text": "From the sea to the stars : the forgotten journeys of the Philippines' ancient explorers Linguistic, genetic, and archaeological evidence indicate that the Philippines has been inhabited by humans for many thousands of years. By what means the earliest settlers arrived in the archipelago is still a mystery, but a growing body of evidence points to the likelihood that they possessed seafaring technology. If so, then modern Filipinos -- who are even now making their first tentative steps into space -- are heirs to a rich heritage of exploration, the story of which has yet to be fully told."
}, {
    "id": "oai:dspace.mit.edu:1721.1/120668",
    "title": "Fission and fury in Perry, Ohio : one town's fight to save their nuclear power plant",
    "abstract": "Before much of America learned to fear atomic energy, towns like Perry, Ohio, learned to love it. For over thirty years the Perry Nuclear Power Plant has been the linchpin of the small Rust Belt community, bringing flush budgets and well-paying jobs to an area with little other industry. But like many nuclear power plants in the U.S., the Perry plant is aging, costly to maintain, and unable to compete with the nearly two-decade run of record-low natural gas prices. On the isolated shores of Lake Erie, Perry is now caught in a global energy shift. In the coming years, more than two-thirds of the nuclear power plants in America are similarly at risk of shut down, the consequences of which will leave deep voids in the diversity of America's energy grid and depleted tax bases in the rural towns that house nuclear power plants. Residents and town officials in Perry, however, are not going quietly into the retrenchment of America's nuclear energy industry.",
    "advisors": ["Seth Mnookin"],
    "text": "Fission and fury in Perry, Ohio : one town's fight to save their nuclear power plant Before much of America learned to fear atomic energy, towns like Perry, Ohio, learned to love it. For over thirty years the Perry Nuclear Power Plant has been the linchpin of the small Rust Belt community, bringing flush budgets and well-paying jobs to an area with little other industry. But like many nuclear power plants in the U.S., the Perry plant is aging, costly to maintain, and unable to compete with the nearly two-decade run of record-low natural gas prices. On the isolated shores of Lake Erie, Perry is now caught in a global energy shift. In the coming years, more than two-thirds of the nuclear power plants in America are similarly at risk of shut down, the consequences of which will leave deep voids in the diversity of America's energy grid and depleted tax bases in the rural towns that house nuclear power plants. Residents and town officials in Perry, however, are not going quietly into the retrenchment of America's nuclear energy industry."
}, {
    "id": "oai:dspace.mit.edu:1721.1/119967",
    "title": "The promise and perils of personalized learning : keeping students at the center of the ed tech revolution",
    "abstract": "As access to education technology - high-speed Internet connectivity, lower-cost computers, and online learning programs - has increased over the past five years in K-12 schools in the United States, the debate over technology's place in the classroom, specifically its ability to usher in a new era of education personalized to meet the needs of every individual student, has raged on. Much of the narrative perpetuated by technology companies around educational reform has centered on an idea that outside, tech-driven \"disruption\" is needed in order for real transformation. However, many school districts have found more success moving towards personalized learning when the disruption is homegrown, scaled carefully, involves all community stakeholders, and is driven by pedagogy, not technology. This thesis examines in depth one school district, Kettle Moraine School District in Wisconsin, and their success in creating personalized learning experiences for their students, as a case study for how other districts might approach homegrown disruptions of their own.",
    "advisors": ["Thomas Levenson"],
    "text": "The promise and perils of personalized learning : keeping students at the center of the ed tech revolution As access to education technology - high-speed Internet connectivity, lower-cost computers, and online learning programs - has increased over the past five years in K-12 schools in the United States, the debate over technology's place in the classroom, specifically its ability to usher in a new era of education personalized to meet the needs of every individual student, has raged on. Much of the narrative perpetuated by technology companies around educational reform has centered on an idea that outside, tech-driven \"disruption\" is needed in order for real transformation. However, many school districts have found more success moving towards personalized learning when the disruption is homegrown, scaled carefully, involves all community stakeholders, and is driven by pedagogy, not technology. This thesis examines in depth one school district, Kettle Moraine School District in Wisconsin, and their success in creating personalized learning experiences for their students, as a case study for how other districts might approach homegrown disruptions of their own."
}, {
    "id": "oai:dspace.mit.edu:1721.1/113459",
    "title": "Operational images and the interpretive turn",
    "abstract": "Over the past several decades, computers have allowed for the increasingly voluminous and rapid ingest of images. These images, made for machine legibility, are called \"operational images,\" a term coined by Harun Farocki. They are made for machines, by machines; they are not made to represent an object, but are part of an operation. Yet these operational images are only the most recent chapter in a longer history of logistical and instrumental use of images. Through the history of cartography, surveillance, and reconnaissance runs a long tale of instrumentalization, a history of calculable images primed for machine-readability. Before computers allowed for a truly \"operational\" image that could be harvested and interpreted independently, there were many other logistical images -- only these predecessors kept humans in the operational loop. These days, so-called deep learning allows for a new development in the operational image -- not only are humans excluded, but machines are performing inscrutable assessments; they interpret images and provide conclusions while their rationales remain opaque. These images are part of an interpretive turn. This sort of image use is difficult to demystify, confront, and confound. To contemplate effective strategies, it helps to look at the broader context of subversion of the logistical image, reaching back to early instances of artistic intervention to help inform the present and future.",
    "advisors": ["William Uricchio"],
    "text": "Operational images and the interpretive turn Over the past several decades, computers have allowed for the increasingly voluminous and rapid ingest of images. These images, made for machine legibility, are called \"operational images,\" a term coined by Harun Farocki. They are made for machines, by machines; they are not made to represent an object, but are part of an operation. Yet these operational images are only the most recent chapter in a longer history of logistical and instrumental use of images. Through the history of cartography, surveillance, and reconnaissance runs a long tale of instrumentalization, a history of calculable images primed for machine-readability. Before computers allowed for a truly \"operational\" image that could be harvested and interpreted independently, there were many other logistical images -- only these predecessors kept humans in the operational loop. These days, so-called deep learning allows for a new development in the operational image -- not only are humans excluded, but machines are performing inscrutable assessments; they interpret images and provide conclusions while their rationales remain opaque. These images are part of an interpretive turn. This sort of image use is difficult to demystify, confront, and confound. To contemplate effective strategies, it helps to look at the broader context of subversion of the logistical image, reaching back to early instances of artistic intervention to help inform the present and future."
}, {
    "id": "oai:dspace.mit.edu:1721.1/112886",
    "title": "The Angelman Approach : hacking DNA to treat a rare disease",
    "abstract": "One of every hundred children is born with a disease caused by a single abnormal gene. In the case of Angelman Syndrome, the genetic defect leaves patients mentally disabled, largely or completely unable to speak, and prone to seizures and sleep difficulties. Many Angelman researchers are trying to figure out precisely how those symptoms develop, but why study all the individual effects when you could go right to the root of the problem? Recent advances in medicine and technology are increasingly allowing clinicians to treat genetic illnesses by directly manipulating patients' DNA, and a number of scientists are now investigating ways to leverage those discoveries for individuals with Angelman Syndrome. Their work could lead to potent therapies for the disease, and - maybe - even a cure.",
    "advisors": ["Marcia Bartusiak"],
    "text": "The Angelman Approach : hacking DNA to treat a rare disease One of every hundred children is born with a disease caused by a single abnormal gene. In the case of Angelman Syndrome, the genetic defect leaves patients mentally disabled, largely or completely unable to speak, and prone to seizures and sleep difficulties. Many Angelman researchers are trying to figure out precisely how those symptoms develop, but why study all the individual effects when you could go right to the root of the problem? Recent advances in medicine and technology are increasingly allowing clinicians to treat genetic illnesses by directly manipulating patients' DNA, and a number of scientists are now investigating ways to leverage those discoveries for individuals with Angelman Syndrome. Their work could lead to potent therapies for the disease, and - maybe - even a cure."
}, {
    "id": "oai:dspace.mit.edu:1721.1/119972",
    "title": "Can this burger save the planet? : synthetic beef and the dream of an American animal-free diet",
    "abstract": "Sustainable food movements are focused on reducing meat consumption for one simple reason: meat is extremely environmentally costly. This enormous resource use by one industry makes it an appealing target for those looking to reduce American resource use. As a result, many are looking at ways to make livestock more sustainable. And there are two main ways to do so. Clover Food Lab represents one idea: a return to local farming with an emphasis on a plant-based diet. Impossible Foods is the second: using biotechnology to provide a sacrifice free alternative, synthetic beef. These companies aren't solutions in of themselves, but proposals on the way solutions should be implemented. The partnership between these two opposing strategies reveals the promises and pitfalls of trying to reform the American diet and, most importantly, that above all else our food system is unsustainable as it is now.",
    "advisors": ["Thomas Levenson"],
    "text": "Can this burger save the planet? : synthetic beef and the dream of an American animal-free diet Sustainable food movements are focused on reducing meat consumption for one simple reason: meat is extremely environmentally costly. This enormous resource use by one industry makes it an appealing target for those looking to reduce American resource use. As a result, many are looking at ways to make livestock more sustainable. And there are two main ways to do so. Clover Food Lab represents one idea: a return to local farming with an emphasis on a plant-based diet. Impossible Foods is the second: using biotechnology to provide a sacrifice free alternative, synthetic beef. These companies aren't solutions in of themselves, but proposals on the way solutions should be implemented. The partnership between these two opposing strategies reveals the promises and pitfalls of trying to reform the American diet and, most importantly, that above all else our food system is unsustainable as it is now."
}, {
    "id": "oai:dspace.mit.edu:1721.1/104026",
    "title": "Taking nature's pulse",
    "abstract": "People have taken delight in nature throughout human history, but more recently the work of the natural historian has become more like that of the scientist. Using methods and tools of science, today's naturalists can record nature with precision-and through this, learn more about it. Ecologists now pay heed to the often-forgotten sense of hearing. The Tropical Ecology Lab at University of Puerto Rico, San Piedras, blurs the lines between natural history and science. An array of remote microphones collects sounds from the forests and wetlands, and researchers use computers to analyze the soundscapes themselves.",
    "advisors": ["Thomas Levenson"],
    "text": "Taking nature's pulse People have taken delight in nature throughout human history, but more recently the work of the natural historian has become more like that of the scientist. Using methods and tools of science, today's naturalists can record nature with precision-and through this, learn more about it. Ecologists now pay heed to the often-forgotten sense of hearing. The Tropical Ecology Lab at University of Puerto Rico, San Piedras, blurs the lines between natural history and science. An array of remote microphones collects sounds from the forests and wetlands, and researchers use computers to analyze the soundscapes themselves."
}, {
    "id": "oai:dspace.mit.edu:1721.1/106762",
    "title": "Your brain on 9 volts : the specter and hype of electrical brain stimulation",
    "abstract": "The potential of electricity to improve the brain has captivated many. Electrical gadgets attract the rich and the poor, the educated and uneducated, the scientist and the charlatan. Over hundreds of years, people have tried everything from shocking away headaches with live torpedo fish, to bombarding patients' brains with so much current that their bodies convulse. A more innocuous technology has since emerged: transcranial direct current stimulation, or tDCS. All it takes to build is a small battery, two wires, two electrodes, and salt water. The idea is that by priming the brain with a mild electrical current, an incoming stimulus would be easier to process. In other words: less mental effort to learn something new, like recovering from a stroke or improving ski jump performance. Three primary communities are interested in tDCS today: do-it-yourselfers, clinical researchers, and neurotechnology companies. They want it for different reasons, and yet they are still wary of one another. But tDCS, in all of its simplicity, is actually not so simple-and neither is the human brain. What makes it so appealing to so many people of so many different backgrounds? How does it work? And does it deliver?",
    "advisors": ["Marcia Bartusiak"],
    "text": "Your brain on 9 volts : the specter and hype of electrical brain stimulation The potential of electricity to improve the brain has captivated many. Electrical gadgets attract the rich and the poor, the educated and uneducated, the scientist and the charlatan. Over hundreds of years, people have tried everything from shocking away headaches with live torpedo fish, to bombarding patients' brains with so much current that their bodies convulse. A more innocuous technology has since emerged: transcranial direct current stimulation, or tDCS. All it takes to build is a small battery, two wires, two electrodes, and salt water. The idea is that by priming the brain with a mild electrical current, an incoming stimulus would be easier to process. In other words: less mental effort to learn something new, like recovering from a stroke or improving ski jump performance. Three primary communities are interested in tDCS today: do-it-yourselfers, clinical researchers, and neurotechnology companies. They want it for different reasons, and yet they are still wary of one another. But tDCS, in all of its simplicity, is actually not so simple-and neither is the human brain. What makes it so appealing to so many people of so many different backgrounds? How does it work? And does it deliver?"
}, {
    "id": "oai:dspace.mit.edu:1721.1/119968",
    "title": "Invisible scars : how domestic violence victims have been left out of the discussion on traumatic brain injuries",
    "abstract": "Traumatic brain injuries are one of the most common injuries in domestic violence, with studies finding that approximately 75 percent of women tested report at least one TBI. These injuries leave invisible scars in the form of memory problems. But despite the large prevalence of TBIs in the population, there is a lack of research, stunted by both funding and a lack of subjects. The trouble with research extends to chronic traumatic encephalopathy, a degenerative disease caused by repetitive hits to the head. Although domestic violence researchers suggest that the population will develop CTE, which is only diagnosed post-mortem, a lack of donated brains means the disease has yet to be found among domestic violence victims.",
    "advisors": ["David Corcoran"],
    "text": "Invisible scars : how domestic violence victims have been left out of the discussion on traumatic brain injuries Traumatic brain injuries are one of the most common injuries in domestic violence, with studies finding that approximately 75 percent of women tested report at least one TBI. These injuries leave invisible scars in the form of memory problems. But despite the large prevalence of TBIs in the population, there is a lack of research, stunted by both funding and a lack of subjects. The trouble with research extends to chronic traumatic encephalopathy, a degenerative disease caused by repetitive hits to the head. Although domestic violence researchers suggest that the population will develop CTE, which is only diagnosed post-mortem, a lack of donated brains means the disease has yet to be found among domestic violence victims."
}, {
    "id": "oai:dspace.mit.edu:1721.1/92660",
    "title": "Conflicting Frames : the dispute over the meaning of rolezinhos in Brazilian media",
    "abstract": "This research analyzes the battle of frames in the controversy surrounding rolezinhos- flashmobs organized by low-income youth in Brazilian shopping malls. To analyze the framing of these events, a corpus of 4,523 online articles was compiled. These articles, published between December 7th, 2013, and February 23 rd, 2014, were investigated using Media Cloud-the system for large scale content analysis developed by the Berkman Center at Harvard and the MIT Center for Civic Media. Data from Facebook indicated which articles received more attention on the social network. A framing analysis was performed to describe the conflicting frames in the debate. The 60 most popular texts--those that attracted 55% of the social media attention in the corpus-were content analyzed. They served as an input for a hierarchical cluster analysis algorithm that grouped articles with similar frame elements. The result of the cluster analysis led to the identification of three frames: one that criminalized rolezinhos or at least tried to discourage them (arrastdo frame), another that acquitted the youth and blamed police, government, State, or society for discriminating poor citizens (apartheid frame), and a third frame that criticized both conservatives and progressives for using the controversy to push their particular agendas (middle ground frame). After finding the keywords that singled out each frame, natural language processing methods helped to describe the genesis and evolution of those frames in the overall corpus as well as the framing strategies of the main actors.",
    "advisors": ["Ethan Zuckerman"],
    "text": "Conflicting Frames : the dispute over the meaning of rolezinhos in Brazilian media This research analyzes the battle of frames in the controversy surrounding rolezinhos- flashmobs organized by low-income youth in Brazilian shopping malls. To analyze the framing of these events, a corpus of 4,523 online articles was compiled. These articles, published between December 7th, 2013, and February 23 rd, 2014, were investigated using Media Cloud-the system for large scale content analysis developed by the Berkman Center at Harvard and the MIT Center for Civic Media. Data from Facebook indicated which articles received more attention on the social network. A framing analysis was performed to describe the conflicting frames in the debate. The 60 most popular texts--those that attracted 55% of the social media attention in the corpus-were content analyzed. They served as an input for a hierarchical cluster analysis algorithm that grouped articles with similar frame elements. The result of the cluster analysis led to the identification of three frames: one that criminalized rolezinhos or at least tried to discourage them (arrastdo frame), another that acquitted the youth and blamed police, government, State, or society for discriminating poor citizens (apartheid frame), and a third frame that criticized both conservatives and progressives for using the controversy to push their particular agendas (middle ground frame). After finding the keywords that singled out each frame, natural language processing methods helped to describe the genesis and evolution of those frames in the overall corpus as well as the framing strategies of the main actors."
}, {
    "id": "oai:dspace.mit.edu:1721.1/119973",
    "title": "The deepest paradox : seafloor mining and its future",
    "abstract": "Metals mined from the seafloor could support tomorrow's technological and clean energy innovations. Though the mineralogical and geochemical significance of seafloor deposits, which lie thousands of meters below the water's surface in geological formations such as polymetallic nodules, ferromanganese crusts, and seafloor massive sulfides is well-established, the biological and ecological profiles of these sites are still actively developing. As a result, the two scientific disciplines - geochemistry and biology - have advanced at different rates. Regions of the seafloor including the Clarion-Clipperton Fracture Zone, the Prime Crust Zone, and inactive or waning hydrothermal vent systems have attracted attention for their unique concentration of metals used in electronics and strong magnets. With commercial mining activities set to commence in 2019 by Canadian company Nautilus Minerals, it is time to assess the paradoxical nature of seafloor mining: to mine the seafloor to support sustainable and efficient technological development on the land above.",
    "advisors": ["Marcia Bartusiak"],
    "text": "The deepest paradox : seafloor mining and its future Metals mined from the seafloor could support tomorrow's technological and clean energy innovations. Though the mineralogical and geochemical significance of seafloor deposits, which lie thousands of meters below the water's surface in geological formations such as polymetallic nodules, ferromanganese crusts, and seafloor massive sulfides is well-established, the biological and ecological profiles of these sites are still actively developing. As a result, the two scientific disciplines - geochemistry and biology - have advanced at different rates. Regions of the seafloor including the Clarion-Clipperton Fracture Zone, the Prime Crust Zone, and inactive or waning hydrothermal vent systems have attracted attention for their unique concentration of metals used in electronics and strong magnets. With commercial mining activities set to commence in 2019 by Canadian company Nautilus Minerals, it is time to assess the paradoxical nature of seafloor mining: to mine the seafloor to support sustainable and efficient technological development on the land above."
}, {
    "id": "oai:dspace.mit.edu:1721.1/112888",
    "title": "Ghost Forests of the Mid-Atlantic : how sea-level rise is killing our coastlines",
    "abstract": "Up and down the eastern seaboard of the United States, ocean levels are rising at rates faster than just about anywhere in the world. Coastal forests are dying off as a result-an early warning, if people will pay attention, of the disruptive changes in store for both natural ecosystems and human habitation. Dying coastal forests herald other coastal landscape changes: after the forests start to die, so do the marshes that live in zones between ocean and forest. As sea-level rise and human development combine to narrow the range of coastal ecosystems, problems arise for local flora and fauna, natural nutrient cycles, and coastal communities.",
    "advisors": ["Toby Lester"],
    "text": "Ghost Forests of the Mid-Atlantic : how sea-level rise is killing our coastlines Up and down the eastern seaboard of the United States, ocean levels are rising at rates faster than just about anywhere in the world. Coastal forests are dying off as a result-an early warning, if people will pay attention, of the disruptive changes in store for both natural ecosystems and human habitation. Dying coastal forests herald other coastal landscape changes: after the forests start to die, so do the marshes that live in zones between ocean and forest. As sea-level rise and human development combine to narrow the range of coastal ecosystems, problems arise for local flora and fauna, natural nutrient cycles, and coastal communities."
}, {
    "id": "oai:dspace.mit.edu:1721.1/106745",
    "title": "Fake the dawn : digital game mechanics and the construction of gender in fictional worlds",
    "abstract": "This thesis considers the ways in which digital game mechanics (interactive inputs) contribute to games' worldbuilding. In particular, this work is concerned with the replication and reinforcement of problematic gender roles through game mechanics that express positive (\"warm\") interactions between characters, namely healing, protection, and building relationships. The method used has been adapted from structural analysis via literary theory, as informed by game studies, media studies methodologies, and feminist epistemologies. Game mechanics are analyzed both across and within primary texts (consisting of Japanese-developed games from the action and role-playing genres) in relation to characters' representation. Through this analysis, I found that characters who are women and girls are often associated with physical weakness, nature-based magic, and nurturing (or absent) personalities, whereas characters who are men and boys often protect women through physical combat, heal through medical means, and keep an emotional distance from others. Relationships built through game mechanics rely on one-sided agency and potential that renders lovers and friends as characters who exist to support the player character in achieving the primary goals of the game. Through these findings, I conclude that even warm interactions in games carry negative, even potentially violent and oppressive, representations and that there is thusly a need for design interventions on the mechanical level to mitigate violence in game worlds and the reinforcement of negative real world stereotypes.",
    "advisors": ["Edward Schiappa"],
    "text": "Fake the dawn : digital game mechanics and the construction of gender in fictional worlds This thesis considers the ways in which digital game mechanics (interactive inputs) contribute to games' worldbuilding. In particular, this work is concerned with the replication and reinforcement of problematic gender roles through game mechanics that express positive (\"warm\") interactions between characters, namely healing, protection, and building relationships. The method used has been adapted from structural analysis via literary theory, as informed by game studies, media studies methodologies, and feminist epistemologies. Game mechanics are analyzed both across and within primary texts (consisting of Japanese-developed games from the action and role-playing genres) in relation to characters' representation. Through this analysis, I found that characters who are women and girls are often associated with physical weakness, nature-based magic, and nurturing (or absent) personalities, whereas characters who are men and boys often protect women through physical combat, heal through medical means, and keep an emotional distance from others. Relationships built through game mechanics rely on one-sided agency and potential that renders lovers and friends as characters who exist to support the player character in achieving the primary goals of the game. Through these findings, I conclude that even warm interactions in games carry negative, even potentially violent and oppressive, representations and that there is thusly a need for design interventions on the mechanical level to mitigate violence in game worlds and the reinforcement of negative real world stereotypes."
}, {
    "id": "oai:dspace.mit.edu:1721.1/92659",
    "title": "To create live treatments of actuality : an investigation of the emerging field of live documentary practice",
    "abstract": "Keywords: documentary, interactive, live, liveness, ephemerality, interactivity, theater, performance, television, televisuality, database, data, live data, real time Abstract: The field of documentary is undergoing a transformation as it collides with digital technologies. A new arena of Interactive Documentary production is thriving, and critics and scholars are taking note. Within this field, there is less attention to new opportunities and new theoretical challenges for live practices within the documentary sphere. This thesis argues for a fuller conceptualization of Live Documentary practice. First, it questions the current state of assumptions about documentary, as a form related to the 'document,' as a particularly film-leaning form, and as a lasting and historicizing form of discourse. Next, it examines the historical underpinnings of two forms of live documentary practice and exemplar projects of each: Live Performance Documentary and Live Subject Documentary. The former is situated in the media category of live theater and performance, and the second, the author will argue, is an instantiation of television in its earliest configuration as a device for two-way audio-visual communications and not just unidirectional broadcasting. The study concludes by positing a third medium-specific form of live documentary native to the computer, the Live Data Documentary. This final, more speculative form is defined by drawing on the meanings of 'liveness' examined in the previous chapters and the history of real time computing to generate a suggested framing for computer-native live documentary practice.",
    "advisors": ["William Uricchio"],
    "text": "To create live treatments of actuality : an investigation of the emerging field of live documentary practice Keywords: documentary, interactive, live, liveness, ephemerality, interactivity, theater, performance, television, televisuality, database, data, live data, real time Abstract: The field of documentary is undergoing a transformation as it collides with digital technologies. A new arena of Interactive Documentary production is thriving, and critics and scholars are taking note. Within this field, there is less attention to new opportunities and new theoretical challenges for live practices within the documentary sphere. This thesis argues for a fuller conceptualization of Live Documentary practice. First, it questions the current state of assumptions about documentary, as a form related to the 'document,' as a particularly film-leaning form, and as a lasting and historicizing form of discourse. Next, it examines the historical underpinnings of two forms of live documentary practice and exemplar projects of each: Live Performance Documentary and Live Subject Documentary. The former is situated in the media category of live theater and performance, and the second, the author will argue, is an instantiation of television in its earliest configuration as a device for two-way audio-visual communications and not just unidirectional broadcasting. The study concludes by positing a third medium-specific form of live documentary native to the computer, the Live Data Documentary. This final, more speculative form is defined by drawing on the meanings of 'liveness' examined in the previous chapters and the history of real time computing to generate a suggested framing for computer-native live documentary practice."
}, {
    "id": "oai:dspace.mit.edu:1721.1/92636",
    "title": "Seizing a species : the story of the Great Salt Lake brine shrimp harvest",
    "abstract": "In the early 1950s, C.C. \"Sparkplug\" Sanders began harvesting brine shrimp from Utah's Great Salt Lake. Sanders built up a small business selling their eggs, called \"cysts, to aquarium stores across the country. During the 80s, cysts were found to be an effective food source for aquaculture and a multimillion-dollar commercial harvesting industry quickly emerged. As the cysts rose in value, competition between harvesters grew fierce and annual catches soon began to drop. Environmentalists also became concerned, as the shrimp are an important food source for millions of migratory birds. The harvest was almost entirely unregulated during this period. Unlike other fisheries, where industry members have fought government intervention, many of the harvesters called on the state to increase oversight. Scientists hired by Utah's natural resource agency found that no comprehensive studies had ever been conducted on the lake's ecosystem, complicating initial efforts to manage the harvest. A twenty-year effort by the state, harvesters and other stakeholders to develop a science-based management strategy has recently begun to pay off as cyst populations appear to be stabilizing and the harvesting industry has once again become profitable.",
    "advisors": ["Philip Hilts"],
    "text": "Seizing a species : the story of the Great Salt Lake brine shrimp harvest In the early 1950s, C.C. \"Sparkplug\" Sanders began harvesting brine shrimp from Utah's Great Salt Lake. Sanders built up a small business selling their eggs, called \"cysts, to aquarium stores across the country. During the 80s, cysts were found to be an effective food source for aquaculture and a multimillion-dollar commercial harvesting industry quickly emerged. As the cysts rose in value, competition between harvesters grew fierce and annual catches soon began to drop. Environmentalists also became concerned, as the shrimp are an important food source for millions of migratory birds. The harvest was almost entirely unregulated during this period. Unlike other fisheries, where industry members have fought government intervention, many of the harvesters called on the state to increase oversight. Scientists hired by Utah's natural resource agency found that no comprehensive studies had ever been conducted on the lake's ecosystem, complicating initial efforts to manage the harvest. A twenty-year effort by the state, harvesters and other stakeholders to develop a science-based management strategy has recently begun to pay off as cyst populations appear to be stabilizing and the harvesting industry has once again become profitable."
}, {
    "id": "oai:dspace.mit.edu:1721.1/106742",
    "title": "Champagne for the Blind : Paul Bach-y-Rita, neuroscience's forgotten genius",
    "abstract": "Dr. Paul Bach-y-Rita was a visionary neuroscientist and an early pioneer of the theory of neuroplasticity. He is the father of sensory substitution, a field which explores how one sensory modality can be transferred to another. This work culminated in the invention of the Brainport, a device that transmits information through electrodes on the tongue. Bach-y- Rita's company, Wicab, developed two versions of the Brainport. One uses visual information to reveal the sighted world to the blind; another uses body alignment information to help \"wobblers\" (individuals with vestibular conditions) navigate. The author received exclusive access to Bach-y-Rita's unpublished memoirs. These papers-supplemented by visits to Bach-y-Rita's home in Wisconsin and personal interviews with his family and colleagues-help tell the story of a revolutionary technology that failed to reach the public who needed it.",
    "advisors": ["Seth Mnookin"],
    "text": "Champagne for the Blind : Paul Bach-y-Rita, neuroscience's forgotten genius Dr. Paul Bach-y-Rita was a visionary neuroscientist and an early pioneer of the theory of neuroplasticity. He is the father of sensory substitution, a field which explores how one sensory modality can be transferred to another. This work culminated in the invention of the Brainport, a device that transmits information through electrodes on the tongue. Bach-y- Rita's company, Wicab, developed two versions of the Brainport. One uses visual information to reveal the sighted world to the blind; another uses body alignment information to help \"wobblers\" (individuals with vestibular conditions) navigate. The author received exclusive access to Bach-y-Rita's unpublished memoirs. These papers-supplemented by visits to Bach-y-Rita's home in Wisconsin and personal interviews with his family and colleagues-help tell the story of a revolutionary technology that failed to reach the public who needed it."
}, {
    "id": "oai:dspace.mit.edu:1721.1/92635",
    "title": "Preying on the predator : the shark fin controversy",
    "abstract": "The consumption of shark fin soup dates back to the Ming Dynasty in China, when it was served to emperors. Today, the cultural delicacy represents wealth, status, and power. Over the past 30 years, with the rising middle class in China, the demand for shark fins has surged. To address the increasing demand, a group of fishermen came to realize there was little value in carting massive shark bodies to shore when all they needed were the highly valued fins. So they sliced off the fins, and threw the still living, rudderless sharks to die in the open ocean. So began the gruesome practice known as \"shark finning.\" Shark populations have been unable to withstand the demand for their fins, and dozens of species are now threatened or endangered. From enhancing legislation to control the shark fin market to building sustainable fisheries to promoting synthetic shark fin soup - efforts to address the issue of shark depletion are seemingly endless. And yet despite these efforts, both the market for shark fins and global catch rates have continued unabated. If the demand for fins and the practice of shark finning continue at the current rate, human interference may forever change the nature of our oceans.",
    "advisors": ["Marcia Bartusiak"],
    "text": "Preying on the predator : the shark fin controversy The consumption of shark fin soup dates back to the Ming Dynasty in China, when it was served to emperors. Today, the cultural delicacy represents wealth, status, and power. Over the past 30 years, with the rising middle class in China, the demand for shark fins has surged. To address the increasing demand, a group of fishermen came to realize there was little value in carting massive shark bodies to shore when all they needed were the highly valued fins. So they sliced off the fins, and threw the still living, rudderless sharks to die in the open ocean. So began the gruesome practice known as \"shark finning.\" Shark populations have been unable to withstand the demand for their fins, and dozens of species are now threatened or endangered. From enhancing legislation to control the shark fin market to building sustainable fisheries to promoting synthetic shark fin soup - efforts to address the issue of shark depletion are seemingly endless. And yet despite these efforts, both the market for shark fins and global catch rates have continued unabated. If the demand for fins and the practice of shark finning continue at the current rate, human interference may forever change the nature of our oceans."
}, {
    "id": "oai:dspace.mit.edu:1721.1/92629",
    "title": "One fish, two fish, lungfish, youfish : embracing traditional taxonomy in a molecular world",
    "abstract": "In today's increasingly digitized, data-driven world, the \"old ways\" of doing things, especially science, are quickly abandoned in favor of newer, ostensibly better methods. One such discipline is the ancient study of taxonomy, the discovery and organization of life on Earth. New techniques like DNA sequencing are allowing taxonomists to gain insight into the tangled web of relationships between species (among the Acanthomorph fish, for example). But is the newest, shiniest toy always the best? Are we in danger of losing vital information about the world if we abandon the thousands of years of cumulative human knowledge to gather dust in basements? This thesis explores the current crossroads at which taxonomy finds itself, and offers a solution to preserve the past while diving headlong into the future.",
    "advisors": ["Alan Lightman"],
    "text": "One fish, two fish, lungfish, youfish : embracing traditional taxonomy in a molecular world In today's increasingly digitized, data-driven world, the \"old ways\" of doing things, especially science, are quickly abandoned in favor of newer, ostensibly better methods. One such discipline is the ancient study of taxonomy, the discovery and organization of life on Earth. New techniques like DNA sequencing are allowing taxonomists to gain insight into the tangled web of relationships between species (among the Acanthomorph fish, for example). But is the newest, shiniest toy always the best? Are we in danger of losing vital information about the world if we abandon the thousands of years of cumulative human knowledge to gather dust in basements? This thesis explores the current crossroads at which taxonomy finds itself, and offers a solution to preserve the past while diving headlong into the future."
}, {
    "id": "oai:dspace.mit.edu:1721.1/92633",
    "title": "The ruins of science : whatever happened to the Tevatron?",
    "abstract": "The Tevatron was the world's highest energy particle accelerator for more than two decades. Built at Fermi National Accelerator Laboratory in Batavia, Illinois in the early 1980s, the machine accelerated protons and antiprotons through its 4.26-mile ring of magnets and smashed them together in one of two 5,000 ton detectors that traced and measured the collision debris. Scientists then analyzed the results in search of new fundamental particles or a deeper understanding of existing ones, and in 1995, they discovered the top quark, one of only 17 known fundamental particles in the universe. The discovery made headlines around the world and became the Tevatron's crowning achievement. When the U.S. Department of Energy decided to shut the Tevatron down in 2011 after a more powerful collider began running in Europe, the old machine entered a kind of limbo. Its life in the world of experimental particle physics was over, but there were no plans for its remains. Using the Tevatron as a case study, this thesis asks the fundamental question: what can and should be done with the ruins that lie in the wake of progress? In doing so, it examines a difficult challenge facing today's science and technology museum curators, namely how to preserve the historical and scientific value of important artifacts amid the acceleration of scientific progress and the growing prevalence of big science.",
    "advisors": ["Corby Kummer"],
    "text": "The ruins of science : whatever happened to the Tevatron? The Tevatron was the world's highest energy particle accelerator for more than two decades. Built at Fermi National Accelerator Laboratory in Batavia, Illinois in the early 1980s, the machine accelerated protons and antiprotons through its 4.26-mile ring of magnets and smashed them together in one of two 5,000 ton detectors that traced and measured the collision debris. Scientists then analyzed the results in search of new fundamental particles or a deeper understanding of existing ones, and in 1995, they discovered the top quark, one of only 17 known fundamental particles in the universe. The discovery made headlines around the world and became the Tevatron's crowning achievement. When the U.S. Department of Energy decided to shut the Tevatron down in 2011 after a more powerful collider began running in Europe, the old machine entered a kind of limbo. Its life in the world of experimental particle physics was over, but there were no plans for its remains. Using the Tevatron as a case study, this thesis asks the fundamental question: what can and should be done with the ruins that lie in the wake of progress? In doing so, it examines a difficult challenge facing today's science and technology museum curators, namely how to preserve the historical and scientific value of important artifacts amid the acceleration of scientific progress and the growing prevalence of big science."
}, {
    "id": "oai:dspace.mit.edu:1721.1/92634",
    "title": "Succulent and spiny : the Bahamas' quest for a sustainable lobster fishery",
    "abstract": "The Caribbean spiny lobster fishery is one of the most important industries in the economy of the Bahamas, and in turn it is one of the largest lobster industries in the world. The natural geography of the Bahamas makes its waters into a lobster haven that Bahamian fishermen have successfully exploited over the past few decades. In 2009, in order to safeguard the industry's future and earn a higher margin, the government and the lobster processors together sought sustainability certification for their product. However, they came up short. The international assessors deemed the data on the health of the lobster stocks to be too minimal, and the legal structures to protect the lobster from over-harvesting to be too weak. In response, the government, together with the World Wildlife Fund, set up a program called the Fishery Improvement Project to get the country's lobster industry on the right track. Under the auspices of the Fishery Improvement Project, the government, local and international NGOs, the processors, and the fishermen themselves are contributing to improving the availability of information on the lobster and to crafting new laws to control the industry. Despite successes in improved communication and stock assessments, there are many obstacles to be overcome: differences of opinion, the spread-out nature of the country, and the limited resources available to enforce the laws. Through interviews with fishermen, government officials, processors, and scientists, this thesis tells the story of how the Fishery Improvement Project began, what it has accomplished, and where the lobster and the humans who harvest them might go from here, when the program wraps up and the fishery reenters the sustainability certification process.",
    "advisors": ["Thomas Levenson"],
    "text": "Succulent and spiny : the Bahamas' quest for a sustainable lobster fishery The Caribbean spiny lobster fishery is one of the most important industries in the economy of the Bahamas, and in turn it is one of the largest lobster industries in the world. The natural geography of the Bahamas makes its waters into a lobster haven that Bahamian fishermen have successfully exploited over the past few decades. In 2009, in order to safeguard the industry's future and earn a higher margin, the government and the lobster processors together sought sustainability certification for their product. However, they came up short. The international assessors deemed the data on the health of the lobster stocks to be too minimal, and the legal structures to protect the lobster from over-harvesting to be too weak. In response, the government, together with the World Wildlife Fund, set up a program called the Fishery Improvement Project to get the country's lobster industry on the right track. Under the auspices of the Fishery Improvement Project, the government, local and international NGOs, the processors, and the fishermen themselves are contributing to improving the availability of information on the lobster and to crafting new laws to control the industry. Despite successes in improved communication and stock assessments, there are many obstacles to be overcome: differences of opinion, the spread-out nature of the country, and the limited resources available to enforce the laws. Through interviews with fishermen, government officials, processors, and scientists, this thesis tells the story of how the Fishery Improvement Project began, what it has accomplished, and where the lobster and the humans who harvest them might go from here, when the program wraps up and the fishery reenters the sustainability certification process."
}, {
    "id": "oai:dspace.mit.edu:1721.1/106746",
    "title": "Subconcussive blows in high school football : putting young brains at risk",
    "abstract": "In 2009, Larry Leverenz, Eric Nauman, and Thomas Talavage at Purdue University formed the Purdue Neurotrauma Group (PNG), and set out to study concussions in high school football. They set up a study that combined helmet sensors with fMRI brain scans and cognitive testing, hoping to figure out what happens when a player gets a concussion on the field. Instead, they uncovered something shocking and wholly unexpected. Players' brains were significantly changing even in the absence of concussions, due to an accumulation of smaller impacts called subconcussive blows. Years of subsequent research have only confirmed their initial results-season after season, they found that about half of the players in their study that didn't sustain concussions exhibited significant brain changes over the course of a season. They don't yet know exactly how these brain changes relate to short or long-term cognitive damage, but when their findings are scaled across the landscape of high school football, the implications are enormous-brain changes may be occurring in some half a million teenaged athletes. However, even as public awareness of concussions and chronic traumatic encephalopathy (CTE) reaches new heights, subconcussive blows continue to fly under the radar. For the past seven years, the PNG has run their research on a shoestring budget, and now, at the end of their funding, they are running out of time and options. Meanwhile, in a few short months, 1.1 million high school football players will suit up for the start of football season.",
    "advisors": ["Seth Mnookin"],
    "text": "Subconcussive blows in high school football : putting young brains at risk In 2009, Larry Leverenz, Eric Nauman, and Thomas Talavage at Purdue University formed the Purdue Neurotrauma Group (PNG), and set out to study concussions in high school football. They set up a study that combined helmet sensors with fMRI brain scans and cognitive testing, hoping to figure out what happens when a player gets a concussion on the field. Instead, they uncovered something shocking and wholly unexpected. Players' brains were significantly changing even in the absence of concussions, due to an accumulation of smaller impacts called subconcussive blows. Years of subsequent research have only confirmed their initial results-season after season, they found that about half of the players in their study that didn't sustain concussions exhibited significant brain changes over the course of a season. They don't yet know exactly how these brain changes relate to short or long-term cognitive damage, but when their findings are scaled across the landscape of high school football, the implications are enormous-brain changes may be occurring in some half a million teenaged athletes. However, even as public awareness of concussions and chronic traumatic encephalopathy (CTE) reaches new heights, subconcussive blows continue to fly under the radar. For the past seven years, the PNG has run their research on a shoestring budget, and now, at the end of their funding, they are running out of time and options. Meanwhile, in a few short months, 1.1 million high school football players will suit up for the start of football season."
}, {
    "id": "oai:dspace.mit.edu:1721.1/119975",
    "title": "The people and the park : how a small Mexican community created one of the world's most successful marine preserves",
    "abstract": "Cabo Pulmo National Park is a 27-square mile protected area in the Gulf of California, near the southern end of Mexico's Baja Peninsula. The park surrounds one of the oldest coral reefs on the western coast of North America. Once damaged and depleted by overfishing, the reef has seen an incredible recovery since its protection in 1995. This recovery is due in large part to the efforts of the very people who once fished the reef. The adjacent community of Cabo Pulmo, in collaboration with a group of scientists from the Universidad Autnoma de Baja California Sur in La Paz, Mexico, requested the marine protected area, acted as vigilante enforcers for the park's rules, and worked to prevent proposed developments that might damage the ecosystem. As the ecosystem has recovered, they have been able to reap the economic benefits of the park, opening dive shops and restaurants. The story of their struggles and triumphs can provide valuable lessons for community-based conservation efforts around the world.",
    "advisors": ["Toby Lester"],
    "text": "The people and the park : how a small Mexican community created one of the world's most successful marine preserves Cabo Pulmo National Park is a 27-square mile protected area in the Gulf of California, near the southern end of Mexico's Baja Peninsula. The park surrounds one of the oldest coral reefs on the western coast of North America. Once damaged and depleted by overfishing, the reef has seen an incredible recovery since its protection in 1995. This recovery is due in large part to the efforts of the very people who once fished the reef. The adjacent community of Cabo Pulmo, in collaboration with a group of scientists from the Universidad Autnoma de Baja California Sur in La Paz, Mexico, requested the marine protected area, acted as vigilante enforcers for the park's rules, and worked to prevent proposed developments that might damage the ecosystem. As the ecosystem has recovered, they have been able to reap the economic benefits of the park, opening dive shops and restaurants. The story of their struggles and triumphs can provide valuable lessons for community-based conservation efforts around the world."
}, {
    "id": "oai:dspace.mit.edu:1721.1/106747",
    "title": "Evolution in the Cornbelt : how a few special species are adapting to industrial agriculture",
    "abstract": "Over the last 150 years, humans have wrought sweeping changes to the Great Plains. What was once the prairie is now the Corn Belt-row crops planted from fencerow to fencerow. What does this mean for the native wildlife, which evolved for millions of years to live only on the prairie? Here are the stories of three species-cliff swallows, western corn rootworms, and prairie deer mice-that natural selection has reshaped to thrive in the new agricultural landscape. With his finches, Charles Darwin read the record of evolution in the past. In the Corn Belt, today's scientists can see evolution in real time.",
    "advisors": ["Marcia Bartusiak"],
    "text": "Evolution in the Cornbelt : how a few special species are adapting to industrial agriculture Over the last 150 years, humans have wrought sweeping changes to the Great Plains. What was once the prairie is now the Corn Belt-row crops planted from fencerow to fencerow. What does this mean for the native wildlife, which evolved for millions of years to live only on the prairie? Here are the stories of three species-cliff swallows, western corn rootworms, and prairie deer mice-that natural selection has reshaped to thrive in the new agricultural landscape. With his finches, Charles Darwin read the record of evolution in the past. In the Corn Belt, today's scientists can see evolution in real time."
}, {
    "id": "oai:dspace.mit.edu:1721.1/106763",
    "title": "Geographies of nowhere : Smeltertown and the rising wave of environmental refugees",
    "abstract": "We don't often think of modern American communities as places that disappear. But lead pollution erased the tiny Texas community of Smeltertown from the map. And Smeltertown isn't alone. Across America we've scraped communities from the landscape, smudged them from our memories. Pollution made these places unfit for human habitation. It turned the residents of these communities into environmental refugees. Another kind of pollution climate change - threatens to push even more people from their homes. That these communities are gone is tragic. That there are billions of climate change refugees poised to join these environmental refugees is terrifying. What can we do to stop this tide? What can lessons can we learn from the towns that have already disappeared? What lessons can we learn from Smeltertown?",
    "advisors": ["Thomas Levenson"],
    "text": "Geographies of nowhere : Smeltertown and the rising wave of environmental refugees We don't often think of modern American communities as places that disappear. But lead pollution erased the tiny Texas community of Smeltertown from the map. And Smeltertown isn't alone. Across America we've scraped communities from the landscape, smudged them from our memories. Pollution made these places unfit for human habitation. It turned the residents of these communities into environmental refugees. Another kind of pollution climate change - threatens to push even more people from their homes. That these communities are gone is tragic. That there are billions of climate change refugees poised to join these environmental refugees is terrifying. What can we do to stop this tide? What can lessons can we learn from the towns that have already disappeared? What lessons can we learn from Smeltertown?"
}, {
    "id": "oai:dspace.mit.edu:1721.1/106748",
    "title": "Swimming sentinels : climate clues from stranded marine mammals",
    "abstract": "From skinny sea lions on beaches in California, to hundreds of enormous dead whales in the fjords of Chile, scientists have been recently puzzled by a spate of dead and dying marine mammals. These events are so complicated- influenced by disease, biotoxins, ecosystem changes, and human interaction-that their cause can appear impossible to untangle. Yet a growing body of evidence strongly suggests that climate change has a hand in them all. This thesis examines marine mammal stranding events of the past and present to show how climate change will, and already has, impacted marine mammals, and how these events could serve as proxies for broader ecosystem changes in the years to come. By paying attention to whales and dolphins, seals and sea otters, we may be able to learn something about our planet, and how its changes will impact its most abundant mammal: us.",
    "advisors": ["Thomas Levenson"],
    "text": "Swimming sentinels : climate clues from stranded marine mammals From skinny sea lions on beaches in California, to hundreds of enormous dead whales in the fjords of Chile, scientists have been recently puzzled by a spate of dead and dying marine mammals. These events are so complicated- influenced by disease, biotoxins, ecosystem changes, and human interaction-that their cause can appear impossible to untangle. Yet a growing body of evidence strongly suggests that climate change has a hand in them all. This thesis examines marine mammal stranding events of the past and present to show how climate change will, and already has, impacted marine mammals, and how these events could serve as proxies for broader ecosystem changes in the years to come. By paying attention to whales and dolphins, seals and sea otters, we may be able to learn something about our planet, and how its changes will impact its most abundant mammal: us."
}, {
    "id": "oai:dspace.mit.edu:1721.1/112884",
    "title": "Senses lost : the impossible dilemma of Usher Syndrome, and its possible solutions",
    "abstract": "Usher Syndrome is an inherited disease that leads to the progressive loss of hearing and vision (retinitis pigmentosa). Increasingly, genetic testing, either through panels or whole exome sequencing, lets people know which of the twelve genes identified to date is responsible for the loss of their senses. Researchers are using these genetic ascertainment data to identify patients for clinical trials: There is no approved treatment for retinitis pigmentosa. A philanthropically-funded translational research program led by Dr. Edwin Stone at the University of Iowa seeks to provide an at-cost personalized gene therapy for everybody with Ushers, regardless how rare. His efforts focus transfecting patient-derived induced pluripotent stem cells with a viral gene vector to replace the broken Ushers gene. Meanwhile, a phase 1/11 clinical trial led by Dr. Eric Pierce and ReNeuron takes a different approach-injecting participants' subretinal space with healthy donor stem cells. Critically, both of these methods risk remaining vision. This is the story of two people with Ushers -- an infant with MYO7A -- associated Ushers who was genetically diagnosed in her first year of life, and a retired man who likely suffers from USH2A-associated Ushers, whose life experience exemplifies the condition, but whose specific genetic mutation has never been identified. Both have opted for cochlear implants to improve their hearing, and both work to adapt each day to their changing senses.",
    "advisors": ["Russ Rymer"],
    "text": "Senses lost : the impossible dilemma of Usher Syndrome, and its possible solutions Usher Syndrome is an inherited disease that leads to the progressive loss of hearing and vision (retinitis pigmentosa). Increasingly, genetic testing, either through panels or whole exome sequencing, lets people know which of the twelve genes identified to date is responsible for the loss of their senses. Researchers are using these genetic ascertainment data to identify patients for clinical trials: There is no approved treatment for retinitis pigmentosa. A philanthropically-funded translational research program led by Dr. Edwin Stone at the University of Iowa seeks to provide an at-cost personalized gene therapy for everybody with Ushers, regardless how rare. His efforts focus transfecting patient-derived induced pluripotent stem cells with a viral gene vector to replace the broken Ushers gene. Meanwhile, a phase 1/11 clinical trial led by Dr. Eric Pierce and ReNeuron takes a different approach-injecting participants' subretinal space with healthy donor stem cells. Critically, both of these methods risk remaining vision. This is the story of two people with Ushers -- an infant with MYO7A -- associated Ushers who was genetically diagnosed in her first year of life, and a retired man who likely suffers from USH2A-associated Ushers, whose life experience exemplifies the condition, but whose specific genetic mutation has never been identified. Both have opted for cochlear implants to improve their hearing, and both work to adapt each day to their changing senses."
}, {
    "id": "oai:dspace.mit.edu:1721.1/112887",
    "title": "Media of Mass Destruction : how fake news is killing Italy's olive trees",
    "abstract": "In 2013, the plant pathogen Xylellafastidiosa was found in Salento, Italy's most southeastern region, famous for its centuries-old olive trees. Spread by insects, the bacterium is decimating those trees and compromising the production of olive oil, which accounts for a considerable part of the national output. Since there are no means to cure sick plants, the authorities ordered emergency measures to contain the disease, which included removing infected trees and using pesticides against insect vectors. In Salento, these measures aroused intense public opposition. Following a vilifying media campaign and under public pressure, an Italian court halted the containment measures and accused the scientists who detected Xylella as having caused the problem in the first place. The absence of a plan to contain the epidemic, the criminal charges against the scientists, and the public resistance due to inaccurate information may fuel the spread of the disease to the rest of Italy and eventually to the entire Mediterranean basin, with catastrophic economic consequences.",
    "advisors": ["Alan Paige Lightman"],
    "text": "Media of Mass Destruction : how fake news is killing Italy's olive trees In 2013, the plant pathogen Xylellafastidiosa was found in Salento, Italy's most southeastern region, famous for its centuries-old olive trees. Spread by insects, the bacterium is decimating those trees and compromising the production of olive oil, which accounts for a considerable part of the national output. Since there are no means to cure sick plants, the authorities ordered emergency measures to contain the disease, which included removing infected trees and using pesticides against insect vectors. In Salento, these measures aroused intense public opposition. Following a vilifying media campaign and under public pressure, an Italian court halted the containment measures and accused the scientists who detected Xylella as having caused the problem in the first place. The absence of a plan to contain the epidemic, the criminal charges against the scientists, and the public resistance due to inaccurate information may fuel the spread of the disease to the rest of Italy and eventually to the entire Mediterranean basin, with catastrophic economic consequences."
}, {
    "id": "oai:dspace.mit.edu:1721.1/112626",
    "title": "Melvin Calvin : Nobel-Winning chemist and SETI scientist wannabe",
    "abstract": "Melvin Calvin spent more than a decade answering one longstanding question in biochemistry: how did plants use carbon dioxide to manufacture carbohydrates in photosynthesis? This research earned Calvin a Nobel Prize-an honor that catapulted him to international fame, secured him spots on presidential advisory committees, and got him plenty of textbook mentions. But even though Calvin's claim to fame was his work on photosynthesis, his longestrunning passion project was investigating the origins of life in the universe. Astrobiology efforts peppered his career, from theorizing about chemical evolution to inspecting meteorites and moon rocks to joining the Order of the Dolphin at the first Search for Extraterrestrial Intelligence (SETI) conference in 1961.",
    "advisors": ["Marcia Bartusiak"],
    "text": "Melvin Calvin : Nobel-Winning chemist and SETI scientist wannabe Melvin Calvin spent more than a decade answering one longstanding question in biochemistry: how did plants use carbon dioxide to manufacture carbohydrates in photosynthesis? This research earned Calvin a Nobel Prize-an honor that catapulted him to international fame, secured him spots on presidential advisory committees, and got him plenty of textbook mentions. But even though Calvin's claim to fame was his work on photosynthesis, his longestrunning passion project was investigating the origins of life in the universe. Astrobiology efforts peppered his career, from theorizing about chemical evolution to inspecting meteorites and moon rocks to joining the Order of the Dolphin at the first Search for Extraterrestrial Intelligence (SETI) conference in 1961."
}, {
    "id": "oai:dspace.mit.edu:1721.1/106743",
    "title": "Climate nudges : psychological tools to fix a warming planet",
    "abstract": "What if, during your next luxuriously long shower, a small device on the showerhead catches your eye? It counts gallons like a stopwatch. As the numbers grow, a cartoon polar bear despairingly watches the iceberg beneath him slowly melt. Small psychological nudges like this have profoundly shaped people's choices in countless fields, from medicine to economics to policy. But can these tiny nudges help us take on the largest problems? Before the Goliath climate change, here stand the pioneers in psychology, economics and energy management, with a behavioral slingshot in hand.",
    "advisors": ["Russ Rymer"],
    "text": "Climate nudges : psychological tools to fix a warming planet What if, during your next luxuriously long shower, a small device on the showerhead catches your eye? It counts gallons like a stopwatch. As the numbers grow, a cartoon polar bear despairingly watches the iceberg beneath him slowly melt. Small psychological nudges like this have profoundly shaped people's choices in countless fields, from medicine to economics to policy. But can these tiny nudges help us take on the largest problems? Before the Goliath climate change, here stand the pioneers in psychology, economics and energy management, with a behavioral slingshot in hand."
}, {
    "id": "oai:dspace.mit.edu:1721.1/92631",
    "title": "The beast within : measuring the minds of zoo animals",
    "abstract": "Though zoos have come far from their early days of concrete boxes in caring for their residents' physical health, zoo animals' mental health-the feelings and thoughts beneath the furry and scaly exteriors-has only recently become a serious field of research. The fear of anthropomorphism, or the furnishing of non-human entities with human characteristics such as \"happy\" or \"depressed,\" has discouraged scientists for decades from approaching this seemingly unscientific and unknowable topic. But as the concept of welfare becomes increasingly lauded as the main focus of zoos, crucial to zoos' attendance, their respect by society, and their future existence, zoo keepers, curators, and researchers are beginning to seek out new ways to discover and understand their animals' true feelings-broadening 'animal welfare' to include minds as well as bodies. This thesis explores new studies, technologies, and ways of thinking about animal mental welfare among zoo researchers. Specifically, the thesis focuses on researchers at Brookfield Zoo in Chicago, who have developed a unique tool for studying welfare based on the idea that animals have emotions that can and should be ascertained-and that keepers, those who spend long periods of time with the animals, have the ability to tell how their animals are feeling.",
    "advisors": ["Marcia Bartusiak"],
    "text": "The beast within : measuring the minds of zoo animals Though zoos have come far from their early days of concrete boxes in caring for their residents' physical health, zoo animals' mental health-the feelings and thoughts beneath the furry and scaly exteriors-has only recently become a serious field of research. The fear of anthropomorphism, or the furnishing of non-human entities with human characteristics such as \"happy\" or \"depressed,\" has discouraged scientists for decades from approaching this seemingly unscientific and unknowable topic. But as the concept of welfare becomes increasingly lauded as the main focus of zoos, crucial to zoos' attendance, their respect by society, and their future existence, zoo keepers, curators, and researchers are beginning to seek out new ways to discover and understand their animals' true feelings-broadening 'animal welfare' to include minds as well as bodies. This thesis explores new studies, technologies, and ways of thinking about animal mental welfare among zoo researchers. Specifically, the thesis focuses on researchers at Brookfield Zoo in Chicago, who have developed a unique tool for studying welfare based on the idea that animals have emotions that can and should be ascertained-and that keepers, those who spend long periods of time with the animals, have the ability to tell how their animals are feeling."
}, {
    "id": "oai:dspace.mit.edu:1721.1/113548",
    "title": "Narrative as an aid for the doctor-patient relationship in China",
    "abstract": "In recent years, the incidence of violence against Chinese doctors has increased dramatically, with the scale, frequency and viciousness of attacks shocking the world. The challenging doctor-patient relationship remains a complicated issue with no single cause. When the tension intensifies, some news media tend to blame the doctors, using misleading narratives to create sensationalism, thereby aggravating the antagonism between the society and medical professionals. Much scholarship has focused on exploring the social, economic, political, legal, and medical aspects of the doctor- patient relationship. In contrast, little research has been done to interrogate the media's role in contributing to the tension. Additionally, although most studies are concerned with proposing suggestions, no study has posed an intervention to combat the twisted depictions of doctors and to abate the worsening doctor-patient tension. To this end, this thesis examines the role of the media to provide an explanatory analysis of its influence on the doctor-patient relationship, and then leverages on the power of narrative to offer an intervention as an aid to the current doctor-patient tension. User feedback has been collected and analyzed to measure the effectiveness of this project. The aim of this intervention is to help promote perspective taking, increase awareness, and foster understanding toward medical professionals in China.",
    "advisors": ["Scot Osterweil"],
    "text": "Narrative as an aid for the doctor-patient relationship in China In recent years, the incidence of violence against Chinese doctors has increased dramatically, with the scale, frequency and viciousness of attacks shocking the world. The challenging doctor-patient relationship remains a complicated issue with no single cause. When the tension intensifies, some news media tend to blame the doctors, using misleading narratives to create sensationalism, thereby aggravating the antagonism between the society and medical professionals. Much scholarship has focused on exploring the social, economic, political, legal, and medical aspects of the doctor- patient relationship. In contrast, little research has been done to interrogate the media's role in contributing to the tension. Additionally, although most studies are concerned with proposing suggestions, no study has posed an intervention to combat the twisted depictions of doctors and to abate the worsening doctor-patient tension. To this end, this thesis examines the role of the media to provide an explanatory analysis of its influence on the doctor-patient relationship, and then leverages on the power of narrative to offer an intervention as an aid to the current doctor-patient tension. User feedback has been collected and analyzed to measure the effectiveness of this project. The aim of this intervention is to help promote perspective taking, increase awareness, and foster understanding toward medical professionals in China."
}, {
    "id": "oai:dspace.mit.edu:1721.1/112885",
    "title": "The Parataxonomist Revolution : how a group of rural Costa Ricans discovered 10,000 new species",
    "abstract": "In northwestern Costa Rica, a team of rural workers called parataxonomists has been inventorying butterfly and moth species for 30 years. Just as a paramedic provides a first round of medical care, a parataxonomist does the on-the-ground work of taxonomy-collection, preparation and data gathering-before sending a specimen on to be analyzed. The parataxonomy program, led by biologists Daniel Janzen and Winnie Hallwachs, is part of the unique conservation model of Costa Rica's Area de Conservacidn Guanacaste (ACG). Hiring local people, rather than students or academics, as permanent field researchers upset traditional research structures, but has paid off for science and for local communities. Some 10,000 new species have been identified through these efforts. The parataxonomists benefit from steady employment in areas of little economic opportunity, and in turn serve as a voice for conservation in their communities. But even as the parataxonomy model is praised abroad-and is being adopted in other countries-its future in Costa Rica is tenuous. This thesis looks at the lives of the parataxonomists of the ACG and the impact of their work. It explores the rise and fall of Costa Rica's National Biodiversity Institute (INBio) and the state of parataxonomy as a model for research and conservation.",
    "advisors": ["Seth Mnookin"],
    "text": "The Parataxonomist Revolution : how a group of rural Costa Ricans discovered 10,000 new species In northwestern Costa Rica, a team of rural workers called parataxonomists has been inventorying butterfly and moth species for 30 years. Just as a paramedic provides a first round of medical care, a parataxonomist does the on-the-ground work of taxonomy-collection, preparation and data gathering-before sending a specimen on to be analyzed. The parataxonomy program, led by biologists Daniel Janzen and Winnie Hallwachs, is part of the unique conservation model of Costa Rica's Area de Conservacidn Guanacaste (ACG). Hiring local people, rather than students or academics, as permanent field researchers upset traditional research structures, but has paid off for science and for local communities. Some 10,000 new species have been identified through these efforts. The parataxonomists benefit from steady employment in areas of little economic opportunity, and in turn serve as a voice for conservation in their communities. But even as the parataxonomy model is praised abroad-and is being adopted in other countries-its future in Costa Rica is tenuous. This thesis looks at the lives of the parataxonomists of the ACG and the impact of their work. It explores the rise and fall of Costa Rica's National Biodiversity Institute (INBio) and the state of parataxonomy as a model for research and conservation."
}, {
    "id": "oai:dspace.mit.edu:1721.1/92632",
    "title": "I carry you in my heart : facing an incurable prenatal diagnosis",
    "abstract": "Prenatal diagnosis has given doctors the ability to predict problems before a child is even born. But what happens when the information gleaned from these tests is that the child is fatally sick? Doctors call these \"futile\" pregnancies. The increasing sophistication and prevalence of prenatal diagnostic tests means that prospective parents and their doctors are grappling with ethical questions unheard of just half a century ago. Legislators try to demarcate what choices are \"good\" and \"bad\". However, there is no good choice when it comes to a fatally ill infant. While archival research is used to frame modem perspectives, this thesis aims to explore the different choices women make and the difficulties they must grapple with in this day and age.",
    "advisors": ["Corby Kummer"],
    "text": "I carry you in my heart : facing an incurable prenatal diagnosis Prenatal diagnosis has given doctors the ability to predict problems before a child is even born. But what happens when the information gleaned from these tests is that the child is fatally sick? Doctors call these \"futile\" pregnancies. The increasing sophistication and prevalence of prenatal diagnostic tests means that prospective parents and their doctors are grappling with ethical questions unheard of just half a century ago. Legislators try to demarcate what choices are \"good\" and \"bad\". However, there is no good choice when it comes to a fatally ill infant. While archival research is used to frame modem perspectives, this thesis aims to explore the different choices women make and the difficulties they must grapple with in this day and age."
}]